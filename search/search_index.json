{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Disruptive Architectures: IA e IoT","text":"<p>Ol\u00e1 pessoal, bem-vindos! Nesta p\u00e1gina voc\u00ea encontrar\u00e1 os conte\u00fados da disciplina (atividades, laborat\u00f3rios, materiais de apoio, dicas e refer\u00eancias) organizados para estudo e acompanhamento das aulas.</p> <ul> <li>Curso: Tecnologia em Desenvolvimento de Sistemas (TDS)  </li> <li>Disciplina: Disruptive Architectures: IA e IoT  </li> <li>Turmas (2026): 2TDSB e 2TDSPG  </li> <li>Reposit\u00f3rio com todos os arquivos est\u00e1 disponivel em: https://github.com/arnaldojr/DisruptiveArchitectures/</li> </ul> <p>Prof. Arnaldo Viana</p>"},{"location":"#objetivos-de-aprendizagem","title":"Objetivos de aprendizagem","text":"<p>Ao final da disciplina, o estudante ser\u00e1 capaz de:</p>"},{"location":"#inteligencia-artificial-deep-learning-e-ia-generativa","title":"Intelig\u00eancia Artificial (Deep Learning e IA Generativa)","text":"<ul> <li>Explicar os fundamentos de Deep Learning e IA Generativa, reconhecendo quando cada abordagem \u00e9 adequada.</li> <li>Preparar dados e construir pipelines simples de treino/valida\u00e7\u00e3o.</li> <li>Treinar e avaliar modelos (m\u00e9tricas, overfitting, valida\u00e7\u00e3o, interpreta\u00e7\u00e3o de resultados).</li> <li>Desenvolver projetos envolvendo IA.</li> </ul>"},{"location":"#internet-das-coisas-iot","title":"Internet das Coisas (IoT)","text":"<ul> <li>Compreender os conceitos essenciais de IoT (sensores/atuadores, conectividade, protocolos e arquitetura).</li> <li>Identificar tecnologias habilitadoras e trade-offs (lat\u00eancia, energia, custo, conectividade).</li> <li>Programar e integrar componentes para construir prot\u00f3tipos IoT (aquisi\u00e7\u00e3o de dados, comunica\u00e7\u00e3o e automa\u00e7\u00e3o).</li> </ul>"},{"location":"#o-que-preciso-tersaber-para-acompanhar-esse-curso","title":"O que preciso ter/saber para acompanhar esse curso?","text":"<ul> <li>L\u00f3gica de programa\u00e7\u00e3o</li> <li>Python b\u00e1sico</li> <li>Algebra linear</li> <li>Vontade de praticar: a disciplina \u00e9 orientada a desafios e laborat\u00f3rios</li> </ul>"},{"location":"#dinamica-das-aulas","title":"Din\u00e2mica das aulas:","text":"<p>O curso \u00e9 baseado em desafios (exerc\u00edcios, atividades pr\u00e1ticas e pequenas pesquisas) que conectam teoria e pr\u00e1tica.</p> <p>As aulas s\u00e3o divididas em laborat\u00f3rios curtos, cada um com objetivos espec\u00edficos e uma entrega clara (c\u00f3digo ou notebook).</p> <p></p>"},{"location":"#quais-software-preciso-instalar-para-acompanhar-esse-curso","title":"Quais software preciso instalar para acompanhar esse curso?","text":"<p>Basicamente, vamos trabalhar com scripts em python e algumas bibliotecas que podem ser executados localmente ou em nuvem. </p> <p>Como sugest\u00e3o de instala\u00e7\u00e3o local:</p> <ul> <li>Python 3.x.</li> <li>Jupyter Notebook.</li> <li>Anaconda.</li> <li>Arduino IDE</li> </ul> <p>Em nuvem:</p> <ul> <li>Google Colab.</li> <li>Kaggle.</li> </ul>"},{"location":"#bibliografia","title":"Bibliografia","text":"<ul> <li>Stewart Russel e Peter Norvig . Intelig\u00eancia artificial. 3\u00aa. Ed., Rio de Janeiro: Campus, 2012.</li> <li>George F. Luger . Intelig\u00eancia Artificial, 6\u00aa ed. S\u00e3o Paulo: Pearson Education do Brasil, 2013 (biblioteca virtual)</li> <li>Aur\u00e9lien Geron. 2019. Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems</li> </ul>"},{"location":"agenda/agenda/","title":"Agenda","text":""},{"location":"agenda/agenda/#cronograma-1o-semestre-2026","title":"Cronograma 1\u00ba Semestre - 2026","text":"1\u00ba Semestre \u2014 Conte\u00fado (IoT) 2TDSBQuarta-Feira 2TDSPGSexta-Feira Aula MagnaApresenta\u00e7\u00e3o do curso, din\u00e2mica das aulas, datas importantes (CP) 11/02/2026 13/02/2026 Hardware e Programa\u00e7\u00e3o B\u00e1sica do ESP32IDE, GPIO, PWM, debounce, Sleep Mode. 25/02/2026 20/02/2026 Sensores e AtuadoresBibliotecas, Sensores (Digitais/Anal\u00f3gicos), ADC, Atuadores, smoothing 04/03/2026 27/02/2026 Comunica\u00e7\u00e3o Serial + Protocolo localSerial para diagn\u00f3stico, recebimento de comando 11/03/2026 06/03/2026 CP1 \u2014 Avalia\u00e7\u00e3o em aulaSistemas Embarcados B\u00e1sico (sensor + atuador + robustez + debug serial) 18/03/2026 13/03/2026 Wi-Fi no ESP32 + fundamentos de redeconex\u00e3o/reconex\u00e3o, IP, falhas comuns, timeout 25/03/2026 20/03/2026 WebServer no ESP32 (HTTP)HTML simples, endpoints b\u00e1sicos (/status, /actuate) 01/04/2026 27/03/2026 REST + JSONcontrato de dados, valida\u00e7\u00e3o b\u00e1sica, testes (curl/Postman) 08/04/2026 10/04/2026 MQTTbroker, pub/sub, t\u00f3picos, QoS, retained, LWT 15/04/2026 17/04/2026 Gateway IoT com Node-REDflows, regras, bridge HTTP\u2194MQTT, normaliza\u00e7\u00e3o de payload 22/04/2026 24/04/2026 Dashboards + Persist\u00eancia m\u00ednimadashboard (Node-RED) + armazenamento simples (CSV/SQLite) 29/04/2026 24/04/2026 CP2 \u2014 Avalia\u00e7\u00e3o em aulaSistema IoT (device + rede + gateway + dashboard funcional) 06/05/2026 08/05/2026 Kickoff do CP3 (Projeto IoT)proposta, requisitos, arquitetura, DoD, divis\u00e3o de tarefas 13/05/2026 15/05/2026 CP3 \u2014 Apresenta\u00e7\u00e3o do Projeto IoTdemo + perguntas t\u00e9cnicas + documenta\u00e7\u00e3o 20/05/2026 22/05/2026"},{"location":"aulas/IA/batalharedes/batalha_das_redes/","title":"Batalha das redes","text":"In\u00a0[161]: Copied! <pre>## importa dataset\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\ndef iris():\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    return X_train, X_test, y_train, y_test\n</pre> ## importa dataset from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split  def iris():     iris = load_iris()     X, y = iris.data, iris.target     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)      return X_train, X_test, y_train, y_test  In\u00a0[141]: Copied! <pre>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndef heart():\n    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n    columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n\n    heart_disease_data = pd.read_csv(url, header=None, names=columns, na_values=\"?\")\n    #valores ausentes (NaN), substitue pela m\u00e9dia da coluna\n    heart_disease_data.fillna(heart_disease_data.mean(), inplace=True)  \n\n    X = heart_disease_data.drop('target', axis=1)\n    y = heart_disease_data[\"target\"].values\n    # Converta os valores de r\u00f3tulo 1-4 em 1\n    y = np.where(y &gt; 0, 1, 0)\n\n\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    return X_train, X_test, y_train, y_test\n</pre> import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler  def heart():     url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"     columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']      heart_disease_data = pd.read_csv(url, header=None, names=columns, na_values=\"?\")     #valores ausentes (NaN), substitue pela m\u00e9dia da coluna     heart_disease_data.fillna(heart_disease_data.mean(), inplace=True)        X = heart_disease_data.drop('target', axis=1)     y = heart_disease_data[\"target\"].values     # Converta os valores de r\u00f3tulo 1-4 em 1     y = np.where(y &gt; 0, 1, 0)       scaler = StandardScaler()     X_scaled = scaler.fit_transform(X)       X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)          return X_train, X_test, y_train, y_test In\u00a0[162]: Copied! <pre>import tensorflow as tf\n\ndef cifar():\n    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\n    X_train = X_train.astype('float32')\n    X_test = X_test.astype('float32')\n    #normaliza os dados para o pixel ficar com valores entre 0 e 1\n    X_train = X_train / 255.0\n    X_test = X_test / 255.0 \n\n    print('shape original')\n    print('X_train: {}, X_test: {}, y_train:{}, y_test:{}'.format(X_train.shape, X_test.shape, y_train.shape, y_test.shape))\n    print('shape redimensionado, flatten')\n    X_train = X_train.reshape(X_train.shape[0], -1)\n    X_test = X_test.reshape(X_test.shape[0], -1)\n\n    return X_train, X_test, y_train, y_test \n</pre> import tensorflow as tf  def cifar():     (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()      X_train = X_train.astype('float32')     X_test = X_test.astype('float32')     #normaliza os dados para o pixel ficar com valores entre 0 e 1     X_train = X_train / 255.0     X_test = X_test / 255.0       print('shape original')     print('X_train: {}, X_test: {}, y_train:{}, y_test:{}'.format(X_train.shape, X_test.shape, y_train.shape, y_test.shape))     print('shape redimensionado, flatten')     X_train = X_train.reshape(X_train.shape[0], -1)     X_test = X_test.reshape(X_test.shape[0], -1)      return X_train, X_test, y_train, y_test  In\u00a0[163]: Copied! <pre># Importa\u00e7\u00e3o das bibliotecas necess\u00e1rias\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import BatchNormalization\nfrom keras.utils.np_utils import to_categorical  \n\n# importa as m\u00e9tricas de avalia\u00e7\u00e3o\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n</pre> # Importa\u00e7\u00e3o das bibliotecas necess\u00e1rias import numpy as np import pandas as pd import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.layers import Dropout from tensorflow.keras.layers import BatchNormalization from keras.utils.np_utils import to_categorical    # importa as m\u00e9tricas de avalia\u00e7\u00e3o from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score  In\u00a0[\u00a0]: Copied! <pre>### dataset da rodada, basta descomentar uma das linhas abaixo\n\n#dataset = 'rodada1'\n#dataset = 'rodada2'\ndataset = 'rodada3'\n\n\n\n# Fun\u00e7\u00e3o para carregar e preprocessar o dataset\ndef load_and_preprocess_data(dataset):\n    if dataset == \"rodada1\":\n        X_train, X_test, y_train, y_test = iris()\n        \n    elif dataset == \"rodada2\":\n        X_train, X_test, y_train, y_test = heart()\n        \n    else:\n        X_train, X_test, y_train, y_test = cifar()\n    \n    return X_train, X_test, y_train, y_test\n\n# Carregue e preprocess os dados\nX_train, X_test, y_train, y_test = load_and_preprocess_data(dataset)\n\nprint(\"\\n---------LEIA COM ATEN\u00c7\u00c3O--------------------------\")\nprint('\\nX_train: {}, X_test: {}\\ny_train: {}, y_test: {}'.format(X_train.shape, X_test.shape, y_train.shape, y_test.shape))\n</pre> ### dataset da rodada, basta descomentar uma das linhas abaixo  #dataset = 'rodada1' #dataset = 'rodada2' dataset = 'rodada3'    # Fun\u00e7\u00e3o para carregar e preprocessar o dataset def load_and_preprocess_data(dataset):     if dataset == \"rodada1\":         X_train, X_test, y_train, y_test = iris()              elif dataset == \"rodada2\":         X_train, X_test, y_train, y_test = heart()              else:         X_train, X_test, y_train, y_test = cifar()          return X_train, X_test, y_train, y_test  # Carregue e preprocess os dados X_train, X_test, y_train, y_test = load_and_preprocess_data(dataset)  print(\"\\n---------LEIA COM ATEN\u00c7\u00c3O--------------------------\") print('\\nX_train: {}, X_test: {}\\ny_train: {}, y_test: {}'.format(X_train.shape, X_test.shape, y_train.shape, y_test.shape)) In\u00a0[\u00a0]: Copied! <pre># Fun\u00e7\u00e3o para criar o modelo MLP\ndef create_model(dataset):\n    model = Sequential()\n    # Adicione as camadas aqui\n    \n\n  \n\n \n    # Compile o modelo N\u00e3o altera aqui, por enquanto.\n    if dataset == 'rodada2': model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    else: model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n\n# Crie o modelo\nprint(dataset)\nmodel = create_model(dataset)\n\nmodel.summary()\nprint(\"Loss function:\", model.loss)\nprint(\"Optimizer name:\", model.optimizer.get_config()[\"name\"])\n</pre> # Fun\u00e7\u00e3o para criar o modelo MLP def create_model(dataset):     model = Sequential()     # Adicione as camadas aqui                 # Compile o modelo N\u00e3o altera aqui, por enquanto.     if dataset == 'rodada2': model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])     else: model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])          return model  # Crie o modelo print(dataset) model = create_model(dataset)  model.summary() print(\"Loss function:\", model.loss) print(\"Optimizer name:\", model.optimizer.get_config()[\"name\"])  In\u00a0[\u00a0]: Copied! <pre># Treine o modelo\nhistory = model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n\n#Validad\u00e7\u00e3o\ntrain_loss, train_acc = model.evaluate(X_train,  y_train, verbose=2)\ntest_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n</pre> # Treine o modelo history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)  #Validad\u00e7\u00e3o train_loss, train_acc = model.evaluate(X_train,  y_train, verbose=2) test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2) In\u00a0[\u00a0]: Copied! <pre>## exibe os graficos da fun\u00e7\u00e3o loss e acuracia\n\nhistory_df = pd.DataFrame(history.history)\n\nhistory_df[['loss','val_loss']].plot();\nhistory_df[['accuracy','val_accuracy']].plot();\n</pre> ## exibe os graficos da fun\u00e7\u00e3o loss e acuracia  history_df = pd.DataFrame(history.history)  history_df[['loss','val_loss']].plot(); history_df[['accuracy','val_accuracy']].plot();  In\u00a0[\u00a0]: Copied! <pre># Fun\u00e7\u00e3o para avaliar o modelo\ndef train_and_evaluate_model(model, dataset, X_train, X_test, y_train, y_test):\n\n    # Fa\u00e7a previs\u00f5es no conjunto de teste\n    if dataset =='rodada2':\n      y_pred = np.round(model.predict(X_test))\n    else:\n      y_pred = np.argmax(model.predict(X_test), axis=-1)\n    \n    # Converta os r\u00f3tulos para inteiros\n    y_test = y_test.astype(int)\n    y_pred = y_pred.astype(int)\n\n    \n    # Calcule as m\u00e9tricas de avalia\u00e7\u00e3o\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average='weighted')\n    recall = recall_score(y_test, y_pred, average='weighted')\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    \n    return accuracy, precision, recall, f1\n\n# Treine e avalie o modelo\naccuracy, precision, recall, f1 = train_and_evaluate_model(model,dataset, X_train, X_test, y_train, y_test)\n\n# Exiba os resultados\nprint(\"Acur\u00e1cia:\", accuracy)\nprint(\"Precis\u00e3o:\", precision)\nprint(\"Revoca\u00e7\u00e3o:\", recall)\nprint(\"F1-Score:\", f1)\n</pre> # Fun\u00e7\u00e3o para avaliar o modelo def train_and_evaluate_model(model, dataset, X_train, X_test, y_train, y_test):      # Fa\u00e7a previs\u00f5es no conjunto de teste     if dataset =='rodada2':       y_pred = np.round(model.predict(X_test))     else:       y_pred = np.argmax(model.predict(X_test), axis=-1)          # Converta os r\u00f3tulos para inteiros     y_test = y_test.astype(int)     y_pred = y_pred.astype(int)           # Calcule as m\u00e9tricas de avalia\u00e7\u00e3o     accuracy = accuracy_score(y_test, y_pred)     precision = precision_score(y_test, y_pred, average='weighted')     recall = recall_score(y_test, y_pred, average='weighted')     f1 = f1_score(y_test, y_pred, average='weighted')          return accuracy, precision, recall, f1  # Treine e avalie o modelo accuracy, precision, recall, f1 = train_and_evaluate_model(model,dataset, X_train, X_test, y_train, y_test)  # Exiba os resultados print(\"Acur\u00e1cia:\", accuracy) print(\"Precis\u00e3o:\", precision) print(\"Revoca\u00e7\u00e3o:\", recall) print(\"F1-Score:\", f1)  In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"aulas/IA/batalharedes/batalha_das_redes/#2-redes-neurais","title":"2. Redes Neurais\u00b6","text":""},{"location":"aulas/IA/batalharedes/batalha_das_redes/#objetivos","title":"Objetivos\u00b6","text":"<ul> <li>Conhecer e praticar Redes Neurais MLP</li> </ul>"},{"location":"aulas/IA/batalharedes/batalha_das_redes/#batalha-das-redes","title":"Batalha das redes\u00b6","text":"<p>Bem-vindos \u00e0 nossa emocionante competi\u00e7\u00e3o de Redes Neurais Multilayer Perceptron (MLP)! Hoje, voc\u00eas participar\u00e3o de uma competi\u00e7\u00e3o estilo Kaggle simplificada, projetada para colocar suas habilidades \u00e0 prova e acelerar sua aprendizagem em um ambiente divertido e colaborativo.</p> <p>Ao longo desta aula, voc\u00eas enfrentar\u00e3o tr\u00eas rodadas de desafios, cada uma com um dataset de dificuldade crescente. O objetivo \u00e9 criar e otimizar modelos de redes neurais MLP para resolver problemas de classifica\u00e7\u00e3o. Voc\u00eas trabalhar\u00e3o em duplas para desenvolver as melhores solu\u00e7\u00f5es poss\u00edveis, competindo uns contra os outros para ver quem alcan\u00e7a o melhor desempenho.</p> <p>A competi\u00e7\u00e3o \u00e9 estruturada da seguinte maneira:</p> <ul> <li>Primeira rodada: Dataset f\u00e1cil para que todos possam se familiarizar com o processo e come\u00e7ar a se aquecer.</li> <li>Segunda rodada: Dataset de dificuldade m\u00e9dia para desafiar suas habilidades e encoraj\u00e1-los a explorar t\u00e9cnicas avan\u00e7adas de otimiza\u00e7\u00e3o.</li> <li>Terceira rodada: Dataset dif\u00edcil, onde voc\u00eas colocar\u00e3o \u00e0 prova tudo o que aprenderam, desenvolvendo solu\u00e7\u00f5es para problemas complexos e realistas.</li> </ul> <p>Voc\u00eas ser\u00e3o avaliados com base no desempenho de suas redes neurais e em crit\u00e9rios relacionados \u00e0 arquitetura e complexidade da rede. Isso inclui m\u00e9tricas como acur\u00e1cia, F1-Score e o n\u00famero de neur\u00f4nios usados no modelo. O objetivo \u00e9 incentivar a cria\u00e7\u00e3o de solu\u00e7\u00f5es eficientes e de alto desempenho.</p> <p>Preparem-se para mergulhar no mundo das redes neurais e aprender atrav\u00e9s da experi\u00eancia pr\u00e1tica. A competi\u00e7\u00e3o ser\u00e1 acirrada, mas, no final, todos sair\u00e3o ganhando com o conhecimento e as habilidades adquiridas.</p> <p>Boa sorte a todos e que ven\u00e7a a melhor solu\u00e7\u00e3o!</p>"},{"location":"aulas/IA/batalharedes/batalha_das_redes/#descricao-dos-datasets-para-as-rodadas-de-competicao","title":"Descri\u00e7\u00e3o dos Datasets para as Rodadas de Competi\u00e7\u00e3o\u00b6","text":"<p>Ao longo desta competi\u00e7\u00e3o, voc\u00eas enfrentar\u00e3o tr\u00eas rodadas de desafios, cada uma com um dataset de dificuldade crescente. Abaixo est\u00e3o as descri\u00e7\u00f5es dos datasets selecionados para cada rodada:</p>"},{"location":"aulas/IA/batalharedes/batalha_das_redes/#primeira-rodada-dataset-facil-iris-dataset","title":"Primeira rodada - Dataset f\u00e1cil: Iris Dataset\u00b6","text":"<p>O <code>Iris dataset</code> voc\u00ea j\u00e1 conhece, \u00e9 um conjunto cl\u00e1ssico de dados usado para problemas de classifica\u00e7\u00e3o. Ele cont\u00e9m 150 amostras de flores de \u00edris, divididas em 3 classes, cada uma representando um tipo de \u00edris (Setosa, Versicolour e Virginica). Para cada amostra, h\u00e1 quatro caracter\u00edsticas: comprimento e largura das s\u00e9palas e p\u00e9talas.</p> <p>O objetivo \u00e9 criar um modelo MLP para classificar corretamente o tipo de \u00edris com base nessas caracter\u00edsticas.</p> <p>Para carregar o Iris dataset:</p>"},{"location":"aulas/IA/batalharedes/batalha_das_redes/#segunda-rodada-dataset-medio-heart-disease-uci-dataset","title":"Segunda rodada - Dataset m\u00e9dio: Heart Disease UCI Dataset\u00b6","text":"<p>O <code>Heart Disease UCI dataset</code> \u00e9 um conjunto de dados m\u00e9dicos que cont\u00e9m informa\u00e7\u00f5es sobre pacientes e a presen\u00e7a de doen\u00e7as card\u00edacas. S\u00e3o 303 amostras com 13 caracter\u00edsticas, incluindo idade, sexo, press\u00e3o arterial em repouso e n\u00edveis de colesterol.</p> <p>O objetivo \u00e9 classificar as amostras em duas classes: presen\u00e7a ou aus\u00eancia de doen\u00e7a card\u00edaca.</p> <p>Para carregar o Heart Disease UCI dataset:</p>"},{"location":"aulas/IA/batalharedes/batalha_das_redes/#terceira-rodada-dataset-dificil-cifar10-dataset","title":"Terceira rodada - Dataset dif\u00edcil: Cifar10 Dataset\u00b6","text":"<p>O <code>cifar10</code> \u00e9 um conjunto de dados mais desafiador, contendo mais de 60.000 imagens em cores de 32x32 pixels, representando 10 classes de objetos capturados em imagens reais.</p> <p>link: https://www.cs.toronto.edu/~kriz/cifar.html</p> <p>O objetivo \u00e9 criar um modelo MLP capaz de classificar corretamente cada imagem em seu respectivo d\u00edgito.</p> <p>Para carregar o SVHN dataset:</p>"},{"location":"aulas/IA/batalharedes/batalha_das_redes/#codigo-base","title":"C\u00f3digo base\u00b6","text":"<p>O seu objetivo \u00e9 a cria\u00e7\u00e3o da rede MLP mais eficiente, este co\u00f3digo base te auxilia no restante.</p> <p>Para rodar, execute as celulas de c\u00f3digo abaixo e fa\u00e7a as altera\u00e7\u00f5es onde for solicitado:</p>"},{"location":"aulas/IA/batalharedes/batalha_das_redes/#primeiro-passo","title":"Primeiro passo\u00b6","text":"<p>Aqui voc\u00ea deve escolhar do dataset.</p> <p>Defina o dataset de acordo com rodada da competi\u00e7\u00e3o.</p> <ul> <li>dataset = 'rodada1'   --&gt; primeira rodada</li> <li>dataset = 'rodada2'   --&gt; primeira rodada</li> <li>dataset = 'rodada3'   --&gt; primeira rodada</li> </ul>"},{"location":"aulas/IA/batalharedes/batalha_das_redes/#segundo-passo","title":"Segundo passo\u00b6","text":"<p>Crie sua rede neural MLP dentro da fun\u00e7\u00e3o create_model().</p> <p>\u00c9 aqui que voc\u00ea vai trabalhar! Use a fun\u00e7\u00e3o create_model() para definir a arquitetura da sua rede neural MLP.</p>"},{"location":"aulas/IA/batalharedes/batalha_das_redes/#dicas","title":"Dicas\u00b6","text":"<ul> <li>Adicione as camadas Densas, Dropout, etc.</li> <li>Adicione/altere quantidade de neuronios.</li> <li>Adicione/altere fun\u00e7\u00e3o de ativa\u00e7\u00e3o ('relu','softmax','sigmoid')</li> </ul> <p>Exemplos:</p> <ul> <li>model.add(Dense(18, activation='relu', input_shape=(4,)))</li> <li>model.add(Dropout(rate=0.5))</li> <li>model.add(BatchNormalization())</li> </ul>"},{"location":"aulas/IA/batalharedes/batalha_das_redes/#terceiro-passo","title":"Terceiro passo\u00b6","text":"<p>Treine o seu modelo, aqui voce deve trabalhar para definir os parametros de treinamento da rede neural.</p>"},{"location":"aulas/IA/batalharedes/batalha_das_redes/#dicas","title":"Dicas\u00b6","text":"<ul> <li>Altere quantidade de epocas, batch_size e validation_split</li> </ul>"},{"location":"aulas/IA/batalharedes/batalha_das_redes/#quarto-passo","title":"Quarto passo\u00b6","text":"<p>Avalia\u00e7\u00e3o do modelo treinado</p>"},{"location":"aulas/IA/batalharedes/batalha_das_redes/#pontuacao","title":"Pontua\u00e7\u00e3o\u00b6","text":"<p>Para computar seus pontos anote os resultados obtidos</p> <ul> <li>Acur\u00e1cia</li> <li>Quantidade de camadas</li> </ul>"},{"location":"aulas/IA/batalharedes/batalha_das_redes/#se-der-empate","title":"se der empate\u00b6","text":"<ul> <li>F1-Scores</li> <li>Quantidade de parametros trein\u00e1veis</li> </ul>"},{"location":"aulas/IA/intro/","title":"Introdu\u00e7\u00e3o","text":"<p>Fa\u00e7a o download do pdf de Introdu\u00e7\u00e3o.</p> <ul> <li>arquivo pdf: Introdu\u00e7\u00e3o</li> </ul>"},{"location":"aulas/IA/intro/#organizacao-do-semestre","title":"Organiza\u00e7\u00e3o do semestre","text":"<p>Este semestre est\u00e1 dividido em 4 blocos principais:</p> <ol> <li>Explora\u00e7\u00e3o de dados e aprendizado supervisionado</li> <li>DataFrames, classifica\u00e7\u00e3o e regress\u00e3o</li> <li>Redes neurais (Deep Learning)</li> <li>Perceptron, MLP, CNNs e compara\u00e7\u00e3o de arquiteturas</li> <li>Redes avan\u00e7adas e vis\u00e3o computacional</li> <li>Transfer Learning, YOLO e aplica\u00e7\u00f5es reais</li> <li>Deploy e integra\u00e7\u00e3o</li> <li>Publica\u00e7\u00e3o de modelos e projeto final</li> </ol>"},{"location":"aulas/IA/lab01/dataframe%20copy/","title":"Dataframe copy","text":"In\u00a0[2]: Copied! <pre># instala\u00e7\u00e3o\n!pip install pandas matplotlib seaborn numpy\n</pre> # instala\u00e7\u00e3o !pip install pandas matplotlib seaborn numpy <pre>Collecting pandas\n  Downloading pandas-2.3.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (91 kB)\n  Downloading pandas-2.3.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (91 kB)\nCollecting matplotlib\n  Using cached matplotlib-3.10.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\nCollecting seaborn\nCollecting matplotlib\n  Using cached matplotlib-3.10.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\nCollecting seaborn\n  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\nCollecting numpy\n  Using cached numpy-2.3.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\nCollecting numpy\n  Using cached numpy-2.3.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\nCollecting pytz&gt;=2020.1 (from pandas)\n  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\nCollecting pytz&gt;=2020.1 (from pandas)\n  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting tzdata&gt;=2022.7 (from pandas)\n  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting tzdata&gt;=2022.7 (from pandas)\n  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting contourpy&gt;=1.0.1 (from matplotlib)\n  Using cached contourpy-1.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.5 kB)\nCollecting cycler&gt;=0.10 (from matplotlib)\n  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\nCollecting contourpy&gt;=1.0.1 (from matplotlib)\n  Using cached contourpy-1.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.5 kB)\nCollecting cycler&gt;=0.10 (from matplotlib)\n  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\nCollecting fonttools&gt;=4.22.0 (from matplotlib)\n  Using cached fonttools-4.59.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (107 kB)\nCollecting fonttools&gt;=4.22.0 (from matplotlib)\n  Using cached fonttools-4.59.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (107 kB)\nCollecting kiwisolver&gt;=1.3.1 (from matplotlib)\n  Using cached kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.2 kB)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\nCollecting kiwisolver&gt;=1.3.1 (from matplotlib)\n  Using cached kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.2 kB)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\nCollecting pillow&gt;=8 (from matplotlib)\n  Using cached pillow-11.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.0 kB)\nCollecting pillow&gt;=8 (from matplotlib)\n  Using cached pillow-11.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.0 kB)\nCollecting pyparsing&gt;=2.3.1 (from matplotlib)\n  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\nCollecting pyparsing&gt;=2.3.1 (from matplotlib)\n  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: six&gt;=1.5 in /Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.17.0)\nDownloading pandas-2.3.1-cp312-cp312-macosx_11_0_arm64.whl (10.7 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/10.7 MB ? eta -:--:--Requirement already satisfied: six&gt;=1.5 in /Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.17.0)\nDownloading pandas-2.3.1-cp312-cp312-macosx_11_0_arm64.whl (10.7 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10.7/10.7 MB 6.7 MB/s  0:00:01eta 0:00:01\nUsing cached matplotlib-3.10.5-cp312-cp312-macosx_11_0_arm64.whl (8.1 MB)\nUsing cached seaborn-0.13.2-py3-none-any.whl (294 kB)\nUsing cached numpy-2.3.2-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\nUsing cached contourpy-1.3.3-cp312-cp312-macosx_11_0_arm64.whl (273 kB)\nUsing cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10.7/10.7 MB 6.7 MB/s  0:00:01\nUsing cached matplotlib-3.10.5-cp312-cp312-macosx_11_0_arm64.whl (8.1 MB)\nUsing cached seaborn-0.13.2-py3-none-any.whl (294 kB)\nUsing cached numpy-2.3.2-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\nUsing cached contourpy-1.3.3-cp312-cp312-macosx_11_0_arm64.whl (273 kB)\nUsing cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\nUsing cached fonttools-4.59.0-cp312-cp312-macosx_10_13_universal2.whl (2.8 MB)\nUsing cached kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl (65 kB)\nUsing cached pillow-11.3.0-cp312-cp312-macosx_11_0_arm64.whl (4.7 MB)\nUsing cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\nUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\nUsing cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\nUsing cached fonttools-4.59.0-cp312-cp312-macosx_10_13_universal2.whl (2.8 MB)\nUsing cached kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl (65 kB)\nUsing cached pillow-11.3.0-cp312-cp312-macosx_11_0_arm64.whl (4.7 MB)\nUsing cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\nUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\nUsing cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\nInstalling collected packages: pytz, tzdata, pyparsing, pillow, numpy, kiwisolver, fonttools, cycler, pandas, contourpy, matplotlib, seaborn\nInstalling collected packages: pytz, tzdata, pyparsing, pillow, numpy, kiwisolver, fonttools, cycler, pandas, contourpy, matplotlib, seaborn\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12/12 [seaborn]1/12 [seaborn]ib]\nSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.59.0 kiwisolver-1.4.8 matplotlib-3.10.5 numpy-2.3.2 pandas-2.3.1 pillow-11.3.0 pyparsing-3.2.3 pytz-2025.2 seaborn-0.13.2 tzdata-2025.2\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12/12 [seaborn]\nSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.59.0 kiwisolver-1.4.8 matplotlib-3.10.5 numpy-2.3.2 pandas-2.3.1 pillow-11.3.0 pyparsing-3.2.3 pytz-2025.2 seaborn-0.13.2 tzdata-2025.2\n</pre> In\u00a0[3]: Copied! <pre># Importando as bibliotecas necess\u00e1rias\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configura\u00e7\u00f5es para melhor visualiza\u00e7\u00e3o\nplt.style.use('default')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"\u2705 Bibliotecas importadas com sucesso!\")\nprint(f\"Vers\u00e3o do Pandas: {pd.__version__}\")\n</pre> # Importando as bibliotecas necess\u00e1rias import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns  # Configura\u00e7\u00f5es para melhor visualiza\u00e7\u00e3o plt.style.use('default') sns.set_palette(\"husl\") plt.rcParams['figure.figsize'] = (10, 6)  print(\"\u2705 Bibliotecas importadas com sucesso!\") print(f\"Vers\u00e3o do Pandas: {pd.__version__}\") <pre>\u2705 Bibliotecas importadas com sucesso!\nVers\u00e3o do Pandas: 2.3.1\n</pre> In\u00a0[4]: Copied! <pre># Carregando o dataset Iris\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n\n# Definindo os nomes das colunas conforme documenta\u00e7\u00e3o\ncolumn_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n\n# Carregando os dados com os nomes corretos das colunas\ndf = pd.read_csv(url, header=None, names=column_names)\n\nprint(f\"\ud83d\udcca Dimens\u00f5es: {df.shape[0]} linhas \u00d7 {df.shape[1]} colunas\")\n</pre> # Carregando o dataset Iris url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"  # Definindo os nomes das colunas conforme documenta\u00e7\u00e3o column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']  # Carregando os dados com os nomes corretos das colunas df = pd.read_csv(url, header=None, names=column_names)  print(f\"\ud83d\udcca Dimens\u00f5es: {df.shape[0]} linhas \u00d7 {df.shape[1]} colunas\") <pre>\ud83d\udcca Dimens\u00f5es: 150 linhas \u00d7 5 colunas\n</pre> In\u00a0[6]: Copied! <pre># Visualizando as primeiras linhas\n\ndf.head()\n</pre> # Visualizando as primeiras linhas  df.head() Out[6]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa In\u00a0[8]: Copied! <pre># Informa\u00e7\u00f5es gerais sobre o dataset\n\ndf.info()\n</pre> # Informa\u00e7\u00f5es gerais sobre o dataset  df.info()  <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  150 non-null    float64\n 1   sepal_width   150 non-null    float64\n 2   petal_length  150 non-null    float64\n 3   petal_width   150 non-null    float64\n 4   species       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB\n</pre> In\u00a0[\u00a0]: Copied! <pre># Explorando as esp\u00e9cies (classes)\n\nspecies_counts = df['species'].value_counts()\nprint(species_counts)\n</pre> # Explorando as esp\u00e9cies (classes)  species_counts = df['species'].value_counts() print(species_counts)   <pre>species\nIris-setosa        50\nIris-versicolor    50\nIris-virginica     50\nName: count, dtype: int64\n</pre> In\u00a0[\u00a0]: Copied! <pre>## sua resposta aqui\n</pre> ## sua resposta aqui In\u00a0[16]: Copied! <pre># Estat\u00edsticas descritivas completas\n\ndf.describe()\n</pre> # Estat\u00edsticas descritivas completas  df.describe()   Out[16]: sepal_length sepal_width petal_length petal_width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.054000 3.758667 1.198667 std 0.828066 0.433594 1.764420 0.763161 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 In\u00a0[18]: Copied! <pre># usa groupby para filtrar\ndf.groupby('species').mean()\n</pre> # usa groupby para filtrar df.groupby('species').mean() Out[18]: sepal_length sepal_width petal_length petal_width species Iris-setosa 5.006 3.418 1.464 0.244 Iris-versicolor 5.936 2.770 4.260 1.326 Iris-virginica 6.588 2.974 5.552 2.026 In\u00a0[\u00a0]: Copied! <pre># Sua resposta aqui\n</pre> # Sua resposta aqui   In\u00a0[\u00a0]: Copied! <pre># Simulando problemas nos dados para fins educativos\nprint(\"SIMULANDO PROBLEMAS NOS DADOS:\")\nprint(\"=\" * 40)\n\n# Criando uma c\u00f3pia para experimentar\ndf_dirty = df.copy()\n\n# Introduzindo valores faltantes aleatoriamente\nnp.random.seed(42)  # Para reprodutibilidade\nfor col in df_dirty.columns[:-1]:  # N\u00e3o mexe na coluna species\n    # Seleciona 5 posi\u00e7\u00f5es aleat\u00f3rias para cada coluna\n    missing_indices = np.random.choice(df_dirty.index, 5, replace=False)\n    df_dirty.loc[missing_indices, col] = np.nan\n\nprint(\"Valores faltantes inseridos!\")\nprint(f\"Total de valores faltantes: {df_dirty.isnull().sum().sum()}\")\n</pre> # Simulando problemas nos dados para fins educativos print(\"SIMULANDO PROBLEMAS NOS DADOS:\") print(\"=\" * 40)  # Criando uma c\u00f3pia para experimentar df_dirty = df.copy()  # Introduzindo valores faltantes aleatoriamente np.random.seed(42)  # Para reprodutibilidade for col in df_dirty.columns[:-1]:  # N\u00e3o mexe na coluna species     # Seleciona 5 posi\u00e7\u00f5es aleat\u00f3rias para cada coluna     missing_indices = np.random.choice(df_dirty.index, 5, replace=False)     df_dirty.loc[missing_indices, col] = np.nan  print(\"Valores faltantes inseridos!\") print(f\"Total de valores faltantes: {df_dirty.isnull().sum().sum()}\")  <pre>SIMULANDO PROBLEMAS NOS DADOS:\n========================================\nValores faltantes inseridos!\nTotal de valores faltantes: 20\n</pre> Out[\u00a0]: <pre>(150, 5)</pre> In\u00a0[36]: Copied! <pre># Estrat\u00e9gia 1: Remo\u00e7\u00e3o de linhas com valores faltantes\n\ndf_dropped = df_dirty.dropna()\n\ndisplay(df_dirty.info(),df_dropped.info())\n</pre> # Estrat\u00e9gia 1: Remo\u00e7\u00e3o de linhas com valores faltantes  df_dropped = df_dirty.dropna()  display(df_dirty.info(),df_dropped.info())  <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  145 non-null    float64\n 1   sepal_width   145 non-null    float64\n 2   petal_length  145 non-null    float64\n 3   petal_width   145 non-null    float64\n 4   species       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 130 entries, 0 to 148\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  130 non-null    float64\n 1   sepal_width   130 non-null    float64\n 2   petal_length  130 non-null    float64\n 3   petal_width   130 non-null    float64\n 4   species       130 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.1+ KB\n</pre> <pre>None</pre> <pre>None</pre> In\u00a0[42]: Copied! <pre># Estrat\u00e9gia 2: Imputa\u00e7\u00e3o com m\u00e9dia\n\n\ndf_imputed = df_dirty.copy()\nnumeric_columns = df_imputed.select_dtypes(include=[np.number]).columns\n\n# Calcula m\u00e9dias\nmean_values = df_imputed[numeric_columns].mean()\n\n# Conta valores ausentes\nmissing_counts = df_imputed[numeric_columns].isnull().sum()\n\n# Preenche os valores\ndf_imputed.fillna(mean_values, inplace=True)\n\n# Imprime resumo\nfor col in numeric_columns:\n    print(f\"{col}: {missing_counts[col]} valores preenchidos com {mean_values[col]:.2f}\")\n\n# Verifica resultado\nprint(f\"\\nValores faltantes ap\u00f3s imputa\u00e7\u00e3o: {df_imputed.isnull().sum().sum()}\")\n\ndisplay(df_dirty.info(), df_imputed.info())\n</pre> # Estrat\u00e9gia 2: Imputa\u00e7\u00e3o com m\u00e9dia   df_imputed = df_dirty.copy() numeric_columns = df_imputed.select_dtypes(include=[np.number]).columns  # Calcula m\u00e9dias mean_values = df_imputed[numeric_columns].mean()  # Conta valores ausentes missing_counts = df_imputed[numeric_columns].isnull().sum()  # Preenche os valores df_imputed.fillna(mean_values, inplace=True)  # Imprime resumo for col in numeric_columns:     print(f\"{col}: {missing_counts[col]} valores preenchidos com {mean_values[col]:.2f}\")  # Verifica resultado print(f\"\\nValores faltantes ap\u00f3s imputa\u00e7\u00e3o: {df_imputed.isnull().sum().sum()}\")  display(df_dirty.info(), df_imputed.info())  <pre>sepal_length: 5 valores preenchidos com 5.82\nsepal_width: 5 valores preenchidos com 3.05\npetal_length: 5 valores preenchidos com 3.74\npetal_width: 5 valores preenchidos com 1.18\n\nValores faltantes ap\u00f3s imputa\u00e7\u00e3o: 0\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  145 non-null    float64\n 1   sepal_width   145 non-null    float64\n 2   petal_length  145 non-null    float64\n 3   petal_width   145 non-null    float64\n 4   species       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  150 non-null    float64\n 1   sepal_width   150 non-null    float64\n 2   petal_length  150 non-null    float64\n 3   petal_width   150 non-null    float64\n 4   species       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB\n</pre> <pre>None</pre> <pre>None</pre> In\u00a0[45]: Copied! <pre># Simulando problemas nos dados\n# Criando uma c\u00f3pia para experimentar\ndf_dirty = df.copy()\n\n# Introduzindo valores faltantes aleatoriamente\nnp.random.seed(42)  # Para reprodutibilidade\nfor col in df_dirty.columns[:-1]:  # N\u00e3o mexe na coluna species\n    # Seleciona 5 posi\u00e7\u00f5es aleat\u00f3rias para cada coluna\n    missing_indices = np.random.choice(df_dirty.index, 15, replace=False)\n    df_dirty.loc[missing_indices, col] = np.nan\n\nprint(\"Valores faltantes inseridos!\")\n\n\n# Sua resposta aqui em baixo \n\n# agora \u00e9 com voc\u00ea!...\n</pre> # Simulando problemas nos dados # Criando uma c\u00f3pia para experimentar df_dirty = df.copy()  # Introduzindo valores faltantes aleatoriamente np.random.seed(42)  # Para reprodutibilidade for col in df_dirty.columns[:-1]:  # N\u00e3o mexe na coluna species     # Seleciona 5 posi\u00e7\u00f5es aleat\u00f3rias para cada coluna     missing_indices = np.random.choice(df_dirty.index, 15, replace=False)     df_dirty.loc[missing_indices, col] = np.nan  print(\"Valores faltantes inseridos!\")   # Sua resposta aqui em baixo   # agora \u00e9 com voc\u00ea!...      <pre>Valores faltantes inseridos!\n</pre> In\u00a0[51]: Copied! <pre># Lembra de histograma, que exibe uma gr\u00e1fico de frequ\u00eancia.\ndf.hist(bins=20,figsize=(15, 15))\nplt.show()\n</pre>  # Lembra de histograma, que exibe uma gr\u00e1fico de frequ\u00eancia. df.hist(bins=20,figsize=(15, 15)) plt.show()  In\u00a0[52]: Copied! <pre># Histograma apenas de um atributo\n\nplt.hist(df['sepal_length'], bins=100)\n\n\nplt.title('Histograma de Sepal Length')\nplt.xlabel('Sepal Length')\nplt.ylabel('Contagem')\nplt.show()\n</pre> # Histograma apenas de um atributo  plt.hist(df['sepal_length'], bins=100)   plt.title('Histograma de Sepal Length') plt.xlabel('Sepal Length') plt.ylabel('Contagem') plt.show() In\u00a0[54]: Copied! <pre># box plot de todos \ndf.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False, figsize=(15, 15))\nplt.show()\n</pre> # box plot de todos  df.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False, figsize=(15, 15)) plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre># sua resposta aqui em baixo\n</pre> # sua resposta aqui em baixo      In\u00a0[57]: Copied! <pre># Scatter plot b\u00e1sico\n# Grafico de dispers\u00e3o\n\nplt.scatter(df['sepal_length'], df['petal_length'])\n\nplt.title('Sepal Length vs Petal Length')\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Petal Length (cm)')\nplt.show()\n</pre> # Scatter plot b\u00e1sico # Grafico de dispers\u00e3o  plt.scatter(df['sepal_length'], df['petal_length'])  plt.title('Sepal Length vs Petal Length') plt.xlabel('Sepal Length (cm)') plt.ylabel('Petal Length (cm)') plt.show() In\u00a0[59]: Copied! <pre># Os mesmos dados mas agora cada classe de uma cor diferente\n\ncolors = {'Iris-setosa':'red', 'Iris-versicolor':'blue', 'Iris-virginica':'green'}\n\nplt.scatter(df['sepal_length'], df['petal_length'], c=df['species'].map(colors), label=colors)\n\nplt.title('Sepal Length vs Petal Length')\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Petal Length (cm)')\nplt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in colors.values()], labels=colors.keys())\nplt.show()\n</pre> # Os mesmos dados mas agora cada classe de uma cor diferente  colors = {'Iris-setosa':'red', 'Iris-versicolor':'blue', 'Iris-virginica':'green'}  plt.scatter(df['sepal_length'], df['petal_length'], c=df['species'].map(colors), label=colors)  plt.title('Sepal Length vs Petal Length') plt.xlabel('Sepal Length (cm)') plt.ylabel('Petal Length (cm)') plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in colors.values()], labels=colors.keys()) plt.show() In\u00a0[\u00a0]: Copied! <pre># Usando seaborn para um pair plot profissional\nimport seaborn as sns\n\nplt.figure(figsize=(12, 10))\nsns.pairplot(df, hue='species', height=2.5, aspect=1.2)\n</pre> # Usando seaborn para um pair plot profissional import seaborn as sns  plt.figure(figsize=(12, 10)) sns.pairplot(df, hue='species', height=2.5, aspect=1.2)   Out[\u00a0]: <pre>&lt;seaborn.axisgrid.PairGrid at 0x11fa6c7a0&gt;</pre> <pre>&lt;Figure size 1200x1000 with 0 Axes&gt;</pre> In\u00a0[68]: Copied! <pre>cols = ['sepal_length', 'sepal_width', 'petal_length','petal_width']\ncorr_matx = df[cols].corr()\n\nheatmap = sns.heatmap(corr_matx,cbar=True,annot=True,square=True,fmt='.2f',annot_kws={'size': 15},yticklabels=cols,xticklabels=cols,cmap='Dark2')\n</pre> cols = ['sepal_length', 'sepal_width', 'petal_length','petal_width'] corr_matx = df[cols].corr()  heatmap = sns.heatmap(corr_matx,cbar=True,annot=True,square=True,fmt='.2f',annot_kws={'size': 15},yticklabels=cols,xticklabels=cols,cmap='Dark2')"},{"location":"aulas/IA/lab01/dataframe%20copy/#analise-exploratoria-de-dados-com-pandas","title":"An\u00e1lise Explorat\u00f3ria de Dados com Pandas\u00b6","text":""},{"location":"aulas/IA/lab01/dataframe%20copy/#objetivos","title":"Objetivos\u00b6","text":"<p>Ao final desta aula, voc\u00ea ser\u00e1 capaz de:</p> <ol> <li>Compreender os conceitos fundamentais de an\u00e1lise explorat\u00f3ria de dados (EDA)</li> <li>Utilizar a biblioteca pandas para manipular e analisar datasets</li> <li>Aplicar t\u00e9cnicas de limpeza e prepara\u00e7\u00e3o de dados</li> <li>Criar visualiza\u00e7\u00f5es informativas para explorar padr\u00f5es nos dados</li> <li>Interpretar estat\u00edsticas descritivas e correla\u00e7\u00f5es</li> <li>Desenvolver insights a partir da an\u00e1lise dos dados</li> </ol>"},{"location":"aulas/IA/lab01/dataframe%20copy/#o-que-e-analise-exploratoria-de-dados-eda","title":"O que \u00e9 An\u00e1lise Explorat\u00f3ria de Dados (EDA)?\u00b6","text":"<p>A An\u00e1lise Explorat\u00f3ria de Dados \u00e9 como fazer uma \"investiga\u00e7\u00e3o\" dos seus dados antes de aplicar qualquer modelo ou algoritmo. \u00c9 o momento em que voc\u00ea:</p> <ul> <li>Explora os dados para entender sua estrutura</li> <li>Limpa e prepara os dados para an\u00e1lise</li> <li>Visualiza padr\u00f5es e tend\u00eancias</li> <li>Gera insights e hip\u00f3teses</li> </ul>"},{"location":"aulas/IA/lab01/dataframe%20copy/#analogia-o-detetive-de-dados","title":"Analogia: O Detetive de Dados\u00b6","text":"<p>Imagine que voc\u00ea \u00e9 um detetive investigando um mist\u00e9rio. Os dados s\u00e3o suas pistas, e a EDA \u00e9 o processo de:</p> <ol> <li>Catalogar todas as evid\u00eancias (conhecer os dados)</li> <li>Procurar pistas falsas ou danificadas (limpeza)</li> <li>Encontrar padr\u00f5es suspeitos (visualiza\u00e7\u00f5es)</li> <li>Formular teorias sobre o caso (insights)</li> </ol>"},{"location":"aulas/IA/lab01/dataframe%20copy/#introducao-ao-pandas","title":"Introdu\u00e7\u00e3o ao Pandas \ud83d\udc3c\u00b6","text":"<p>O Pandas \u00e9 a ferramenta mais popular para an\u00e1lise de dados em Python.</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#conceitos-fundamentais","title":"Conceitos Fundamentais:\u00b6","text":"<ul> <li>DataFrame: Uma tabela bidimensional (como uma planilha Excel)</li> <li>Series: Uma coluna \u00fanica de dados (como uma lista com \u00edndices)</li> <li>Index: Os r\u00f3tulos das linhas</li> </ul>"},{"location":"aulas/IA/lab01/dataframe%20copy/#documentacao-oficial","title":"Documenta\u00e7\u00e3o Oficial\u00b6","text":"<ul> <li>Pandas Documentation</li> <li>Getting Started Guide</li> </ul>"},{"location":"aulas/IA/lab01/dataframe%20copy/#instalacao","title":"Instala\u00e7\u00e3o\u00b6","text":"<pre>pip install pandas matplotlib seaborn numpy\n</pre>"},{"location":"aulas/IA/lab01/dataframe%20copy/#conhecendo-o-dataset-iris","title":"Conhecendo o Dataset Iris\u00b6","text":""},{"location":"aulas/IA/lab01/dataframe%20copy/#sobre-o-dataset","title":"Sobre o Dataset\u00b6","text":"<p>O Iris Dataset \u00e9 um dos conjuntos de dados mais famosos na ci\u00eancia de dados, criado pelo bi\u00f3logo Ronald Fisher em 1936. Ele cont\u00e9m medi\u00e7\u00f5es de diferentes esp\u00e9cies de flores \u00edris.</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#caracteristicas-features","title":"Caracter\u00edsticas (Features):\u00b6","text":"<ol> <li>sepal_length: Comprimento da s\u00e9pala (cm)</li> <li>sepal_width: Largura da s\u00e9pala (cm)</li> <li>petal_length: Comprimento da p\u00e9tala (cm)</li> <li>petal_width: Largura da p\u00e9tala (cm)</li> <li>species: Esp\u00e9cie da flor (classe alvo)</li> </ol>"},{"location":"aulas/IA/lab01/dataframe%20copy/#especies","title":"Esp\u00e9cies:\u00b6","text":"<ul> <li>Iris Setosa: Geralmente menor, p\u00e9talas mais estreitas</li> <li>Iris Versicolor: Tamanho m\u00e9dio</li> <li>Iris Virginica: Geralmente maior, p\u00e9talas mais largas</li> </ul>"},{"location":"aulas/IA/lab01/dataframe%20copy/#primeira-exploracao-dos-dados","title":"Primeira Explora\u00e7\u00e3o dos Dados\u00b6","text":"<p>Agora vamos fazer nossa primeira \"investiga\u00e7\u00e3o\" dos dados. Esta \u00e9 sempre a primeira etapa de qualquer projeto de ci\u00eancia de dados.</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#perguntas-que-devemos-responder","title":"Perguntas que devemos responder:\u00b6","text":"<ol> <li>Como os dados est\u00e3o estruturados?</li> <li>Quantos registros temos?</li> <li>Existem valores faltantes?</li> <li>Que tipos de dados est\u00e3o presentes?</li> <li>Como est\u00e3o distribu\u00eddas as classes?</li> </ol>"},{"location":"aulas/IA/lab01/dataframe%20copy/#checkpoint-1-verificacao-de-aprendizado","title":"\u2705 Checkpoint 1: Verifica\u00e7\u00e3o de Aprendizado\u00b6","text":"<p>Antes de prosseguir, vamos verificar se voc\u00ea entendeu os conceitos b\u00e1sicos:</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#questoes-para-reflexao","title":"Quest\u00f5es para Reflex\u00e3o:\u00b6","text":"<ol> <li>Estrutura dos Dados: Quantos exemplos de cada esp\u00e9cie temos no dataset?</li> <li>Qualidade dos Dados: Existem valores faltantes que precisam ser tratados?</li> <li>Tipos de Dados: Quais colunas s\u00e3o num\u00e9ricas e qual \u00e9 categ\u00f3rica?</li> <li>Balanceamento: O dataset est\u00e1 balanceado entre as classes?</li> </ol>"},{"location":"aulas/IA/lab01/dataframe%20copy/#estatisticas-descritivas","title":"Estat\u00edsticas Descritivas\u00b6","text":"<p>As estat\u00edsticas descritivas nos d\u00e3o uma vis\u00e3o geral num\u00e9rica dos nossos dados. S\u00e3o como um \"raio-X\" que revela caracter\u00edsticas importantes.</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#algumas-metricas","title":"Algumas M\u00e9tricas:\u00b6","text":"<ul> <li>Count: N\u00famero de observa\u00e7\u00f5es</li> <li>Mean: M\u00e9dia aritm\u00e9tica</li> <li>Std: Desvio padr\u00e3o (dispers\u00e3o)</li> <li>Min/Max: Valores m\u00ednimo e m\u00e1ximo</li> <li>25%, 50%, 75%: Quartis (Q1, mediana, Q3)</li> </ul>"},{"location":"aulas/IA/lab01/dataframe%20copy/#como-interpretar","title":"Como Interpretar:\u00b6","text":"<ul> <li>M\u00e9dia vs Mediana: Se muito diferentes, pode indicar outliers</li> <li>Desvio Padr\u00e3o Alto: Dados muito dispersos</li> <li>Quartis: Ajudam a entender a distribui\u00e7\u00e3o</li> </ul>"},{"location":"aulas/IA/lab01/dataframe%20copy/#agrupando-dados","title":"Agrupando dados\u00b6","text":"<p>O m\u00e9todo <code>groupby</code> \u00e9 usada para agrupar dados com base em valores espec\u00edficos de uma ou mais colunas.</p> <p>Permite realizar opera\u00e7\u00f5es em grupos de dados e \u00e9 muito \u00fatil e versatil para an\u00e1lise e agrega\u00e7\u00e3o de dado.</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#exercicio-pratico-1-analise-por-especie","title":"Exerc\u00edcio Pr\u00e1tico 1: An\u00e1lise por Esp\u00e9cie\u00b6","text":"<p>Agora vamos aprofundar nossa an\u00e1lise comparando as caracter\u00edsticas entre diferentes esp\u00e9cies.</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#sua-missao","title":"Sua Miss\u00e3o:\u00b6","text":"<p>Analise as estat\u00edsticas descritivas de cada esp\u00e9cie e responda:</p> <ol> <li>Qual esp\u00e9cie tem as maiores s\u00e9palas em m\u00e9dia?</li> <li>Qual caracter\u00edstica varia mais entre as esp\u00e9cies?</li> <li>Existe sobreposi\u00e7\u00e3o entre as medi\u00e7\u00f5es das esp\u00e9cies?</li> </ol>"},{"location":"aulas/IA/lab01/dataframe%20copy/#limpeza-e-preparacao-de-dados","title":"Limpeza e Prepara\u00e7\u00e3o de Dados\u00b6","text":"<p>Na vida real, os dados raramente v\u00eam limpos e prontos para an\u00e1lise. Vamos simular alguns problemas comuns e aprender a resolv\u00ea-los.</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#problemas-comuns","title":"Problemas Comuns:\u00b6","text":"<ul> <li>Valores faltantes (NaN, null)</li> <li>Dados duplicados</li> <li>Outliers (valores extremos)</li> <li>Inconsist\u00eancias nos dados</li> </ul>"},{"location":"aulas/IA/lab01/dataframe%20copy/#estrategias-de-tratamento","title":"Estrat\u00e9gias de Tratamento:\u00b6","text":"<ol> <li>Remo\u00e7\u00e3o: Deletar linhas/colunas problem\u00e1ticas</li> <li>Imputa\u00e7\u00e3o: Preencher com valores calculados</li> </ol>"},{"location":"aulas/IA/lab01/dataframe%20copy/#estrategias-de-limpeza","title":"Estrat\u00e9gias de Limpeza\u00b6","text":"<p>Vamos explorar diferentes abordagens para lidar com valores faltantes:</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#estrategia-1-remocao-com-dropna","title":"Estrat\u00e9gia 1: Remo\u00e7\u00e3o com dropna()\u00b6","text":"<ul> <li>Vantagem: Simples e r\u00e1pida</li> <li>Desvantagem: Perde informa\u00e7\u00e3o</li> <li>Quando usar: Poucos valores faltantes (&lt; 5%)</li> </ul>"},{"location":"aulas/IA/lab01/dataframe%20copy/#estrategia-2-imputacao-com-fillna","title":"Estrat\u00e9gia 2: Imputa\u00e7\u00e3o com fillna()\u00b6","text":"<ul> <li>M\u00e9dia: Para dados num\u00e9ricos sem outliers</li> <li>Mediana: Para dados com outliers</li> <li>Moda: Para dados categ\u00f3ricos</li> <li>Interpola\u00e7\u00e3o: Para s\u00e9ries temporais</li> </ul>"},{"location":"aulas/IA/lab01/dataframe%20copy/#exercicio-pratico-2-lidando-com-dados-faltantes","title":"Exerc\u00edcio Pr\u00e1tico 2: Lidando com Dados Faltantes\u00b6","text":"<p>Durante o processo de coleta de dados, algumas medi\u00e7\u00f5es das flores foram perdidas. Antes de fazer qualquer an\u00e1lise por esp\u00e9cie, \u00e9 essencial decidir como lidar com os valores ausentes, garantindo que as estat\u00edsticas n\u00e3o sejam distorcidas.</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#sua-missao","title":"Sua Miss\u00e3o:\u00b6","text":"<ol> <li><p>Identifique as colunas que possuem valores faltantes e quantifique quantos valores ausentes existem em cada uma.</p> </li> <li><p>Escolha uma estrat\u00e9gia adequada para lidar com os valores ausentes:</p> <ul> <li>Imputa\u00e7\u00e3o (por m\u00e9dia, mediana ou outro crit\u00e9rio)</li> <li>Remo\u00e7\u00e3o das linhas incompletas</li> </ul> </li> <li><p>Aplique a estrat\u00e9gia escolhida e, somente ap\u00f3s o tratamento, calcule as estat\u00edsticas descritivas por esp\u00e9cie.</p> </li> <li><p>Com base nos dados tratados, responda:</p> <ol> <li>Qual esp\u00e9cie tem as maiores s\u00e9palas em m\u00e9dia?</li> <li>Qual caracter\u00edstica apresenta maior varia\u00e7\u00e3o entre as esp\u00e9cies?</li> <li>H\u00e1 ind\u00edcios de sobreposi\u00e7\u00e3o entre as distribui\u00e7\u00f5es das esp\u00e9cies?</li> </ol> </li> <li><p>Compare os resultados aqui com os os obtidos anteriormente e avalie o impacto no tratamento de dados.</p> </li> </ol>"},{"location":"aulas/IA/lab01/dataframe%20copy/#analise-visual-dos-dados","title":"An\u00e1lise Visual dos Dados\u00b6","text":"<p>A visualiza\u00e7\u00e3o \u00e9 uma das ferramentas mais poderosas para entender dados.</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#tipos-de-graficos-e-quando-usar","title":"Tipos de Gr\u00e1ficos e Quando Usar:\u00b6","text":"Gr\u00e1fico Quando Usar Exemplo Histograma Distribui\u00e7\u00e3o de uma vari\u00e1vel Idades dos clientes Box Plot Comparar distribui\u00e7\u00f5es Sal\u00e1rios por departamento Scatter Plot Rela\u00e7\u00e3o entre vari\u00e1veis Altura vs Peso Bar Chart Comparar categorias Vendas por regi\u00e3o"},{"location":"aulas/IA/lab01/dataframe%20copy/#histogramas-entendendo-distribuicoes","title":"Histogramas: Entendendo Distribui\u00e7\u00f5es\u00b6","text":"<p>O histograma mostra como os valores est\u00e3o distribu\u00eddos. \u00c9 como um \"mapa de densidade\" dos seus dados.</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#o-que-observar","title":"O que observar:\u00b6","text":"<ul> <li>Forma: Normal, assim\u00e9trica, bimodal?</li> <li>Centro: Onde est\u00e3o concentrados os valores?</li> <li>Dispers\u00e3o: Qu\u00e3o espalhados est\u00e3o os dados?</li> <li>Outliers: Valores extremos isolados?</li> </ul>"},{"location":"aulas/IA/lab01/dataframe%20copy/#box-plots-detectando-outliers-e-comparando-grupos","title":"Box Plots: Detectando Outliers e Comparando Grupos\u00b6","text":"<p>O Box Plot \u00e9 como um \"resumo de cinco n\u00fameros\" visual que mostra:</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#como-interpretar","title":"Como interpretar:\u00b6","text":"<ul> <li>Caixa: Cont\u00e9m 50% dos dados (Q1 a Q3)</li> <li>Linha central: Mediana (Q2)</li> <li>Bigodes: Extens\u00e3o dos dados (1.5 \u00d7 IQR)</li> <li>Pontos isolados: Outliers potenciais</li> </ul>"},{"location":"aulas/IA/lab01/dataframe%20copy/#exercicio-pratico-3-interpretacao-visual","title":"Exerc\u00edcio Pr\u00e1tico 3: Interpreta\u00e7\u00e3o Visual\u00b6","text":"<p>Com base nos histogramas e box plots gerados:</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#questoes-para-analise","title":"Quest\u00f5es para An\u00e1lise:\u00b6","text":"<ol> <li>Distribui\u00e7\u00f5es: Qual atributo tem a distribui\u00e7\u00e3o mais sim\u00e9trica?</li> <li>Separabilidade: Qual atributo melhor separa as esp\u00e9cies?</li> <li>Outliers: Existem valores extremos que merecem investiga\u00e7\u00e3o?</li> <li>Variabilidade: Qual esp\u00e9cie \u00e9 mais homog\u00eanea em suas caracter\u00edsticas?</li> </ol>"},{"location":"aulas/IA/lab01/dataframe%20copy/#dica-de-analise","title":"Dica de An\u00e1lise:\u00b6","text":"<p>Observe especialmente as p\u00e9talas vs s\u00e9palas - h\u00e1 uma diferen\u00e7a clara entre elas na capacidade de distin\u00e7\u00e3o das esp\u00e9cies?</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#scatter-plots-descobrindo-relacionamentos","title":"Scatter Plots: Descobrindo Relacionamentos\u00b6","text":"<p>Os Scatter Plots revelam relacionamentos entre vari\u00e1veis. S\u00e3o fundamentais para entender correla\u00e7\u00f5es e padr\u00f5es.</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#padroes-a-observar","title":"Padr\u00f5es a observar:\u00b6","text":"<ul> <li>Linear: Pontos seguem uma linha reta</li> <li>Curvil\u00edneo: Rela\u00e7\u00e3o n\u00e3o-linear</li> <li>Clusters: Grupos distintos de pontos</li> <li>Outliers: Pontos isolados</li> </ul>"},{"location":"aulas/IA/lab01/dataframe%20copy/#analise-de-correlacao","title":"An\u00e1lise de Correla\u00e7\u00e3o\u00b6","text":"<p>A correla\u00e7\u00e3o mede a for\u00e7a e dire\u00e7\u00e3o do relacionamento linear entre vari\u00e1veis.</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#escala-de-interpretacao","title":"\ud83d\udccf Escala de Interpreta\u00e7\u00e3o:\u00b6","text":"Valor Interpreta\u00e7\u00e3o 0.9 a 1.0 Correla\u00e7\u00e3o muito forte 0.7 a 0.9 Correla\u00e7\u00e3o forte 0.5 a 0.7 Correla\u00e7\u00e3o moderada 0.3 a 0.5 Correla\u00e7\u00e3o fraca 0.0 a 0.3 Correla\u00e7\u00e3o desprez\u00edvel"},{"location":"aulas/IA/lab01/dataframe%20copy/#importante","title":"Importante:\u00b6","text":"<p>Correla\u00e7\u00e3o \u2260 Causalidade</p> <ul> <li>Uma correla\u00e7\u00e3o alta n\u00e3o significa que uma vari\u00e1vel causa a outra</li> <li>Pode haver uma terceira vari\u00e1vel influenciando ambas</li> </ul>"},{"location":"aulas/IA/lab01/dataframe%20copy/#checkpoint-2-sintese-dos-insights","title":"\u2705 Checkpoint 2: S\u00edntese dos Insights\u00b6","text":"<p>Vamos consolidar o que descobrimos at\u00e9 agora:</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#principais-descobertas","title":"Principais Descobertas:\u00b6","text":"<ol> <li>Estrutura dos Dados: Dataset balanceado com 150 amostras</li> <li>Qualidade: Sem valores faltantes no dataset original</li> <li>Distribui\u00e7\u00f5es: Algumas caracter\u00edsticas s\u00e3o mais discriminativas</li> <li>Correla\u00e7\u00f5es: Existe forte correla\u00e7\u00e3o entre dimens\u00f5es das p\u00e9talas</li> <li>Separabilidade: As esp\u00e9cies s\u00e3o visualmente separ\u00e1veis</li> </ol>"},{"location":"aulas/IA/lab01/dataframe%20copy/#proximos-passos","title":"Pr\u00f3ximos Passos:\u00b6","text":"<ul> <li>Criar caracter\u00edsticas derivadas (feature engineering)</li> <li>Aplicar t\u00e9cnicas de classifica\u00e7\u00e3o</li> <li>Validar insights com modelos preditivos</li> </ul>"},{"location":"aulas/IA/lab01/dataframe%20copy/#desafio-analise-completa-de-novo-dataset","title":"Desafio: An\u00e1lise Completa de Novo Dataset\u00b6","text":"<p>Agora que voc\u00ea domina os conceitos, vamos aplic\u00e1-los em um dataset mais desafiador!</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#seu-desafio-breast-cancer-dataset","title":"Seu Desafio: Breast Cancer Dataset\u00b6","text":"<p>Aplique todo o conhecimento adquirido no Breast Cancer Wisconsin Dataset:</p>"},{"location":"aulas/IA/lab01/dataframe%20copy/#checklist-de-tarefas","title":"Checklist de Tarefas:\u00b6","text":"<ol> <li>Carregamento: Importe e configure o dataset</li> <li>Explora\u00e7\u00e3o inicial: Entenda estrutura e qualidade</li> <li>Limpeza: Trate valores faltantes se houver</li> <li>Estat\u00edsticas: Calcule e interprete estat\u00edsticas descritivas</li> <li>Visualiza\u00e7\u00f5es: Crie histogramas, box plots e scatter plots</li> <li>Correla\u00e7\u00f5es: Analise relacionamentos entre vari\u00e1veis</li> </ol>"},{"location":"aulas/IA/lab01/dataframe%20copy/#dataset-information","title":"Dataset Information:\u00b6","text":"<ul> <li>URL: <code>https://archive.ics.uci.edu/ml/datasets/breast+cancer</code></li> <li>Objetivo: Classificar tumores como malignos ou benignos</li> <li>Caracter\u00edsticas: 30 caracter\u00edsticas num\u00e9ricas</li> <li>Classes: 2 (Maligno/Benigno)</li> </ul>"},{"location":"aulas/IA/lab01/dataframe/","title":"Dataframe","text":"<p>Para instalar</p> <ul> <li>pandas: <code>pip install pandas</code></li> </ul> <p>LEIA A DOCUMENTA\u00c7\u00c3O: https://pandas.pydata.org/docs/index.html</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[\u00a0]: Copied! <pre># Caminho do arquivo\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n\n# L\u00ea e carrega o arquivo para a mem\u00f3ria\ndf = pd.read_csv(url)\n</pre> # Caminho do arquivo url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"  # L\u00ea e carrega o arquivo para a mem\u00f3ria df = pd.read_csv(url) In\u00a0[\u00a0]: Copied! <pre>df.head()\n</pre> df.head() Out[\u00a0]: 5.1 3.5 1.4 0.2 Iris-setosa 0 4.9 3.0 1.4 0.2 Iris-setosa 1 4.7 3.2 1.3 0.2 Iris-setosa 2 4.6 3.1 1.5 0.2 Iris-setosa 3 5.0 3.6 1.4 0.2 Iris-setosa 4 5.4 3.9 1.7 0.4 Iris-setosa <p>Note que a primeira linha n\u00e3o contem os nomes das colunas ou <code>atributos</code>(variaveis) e sim, dados (valores).</p> <p>Dependendo da base dados utilizada e como voc\u00ea carrega no pandas, os dados da primeira linha s\u00e3o importados como atributos.</p> <p>Vamos adicionar um cabe\u00e7ario ao nosso dataframe. Mas o que podemos adicionar???</p> <p>Vamos dar uma olhada no reposit\u00f3rio oficial onde dadas informa\u00e7\u00f5es sobre o dataset e \u00e9 dito que as variaveis s\u00e3o:</p> <pre><code>Attribute Information:\n\n1. sepal length in cm\n2. sepal width in cm\n3. petal length in cm\n4. petal width in cm\n5. class:\n-- Iris Setosa\n-- Iris Versicolour\n-- Iris Virginica\n</code></pre> In\u00a0[\u00a0]: Copied! <pre># Define o nome das colunas\nheader = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n# L\u00ea e carrega o arquivo para a mem\u00f3ria\ndf = pd.read_csv(url, header=None, names=header)\n</pre> # Define o nome das colunas header = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'] # L\u00ea e carrega o arquivo para a mem\u00f3ria df = pd.read_csv(url, header=None, names=header) In\u00a0[\u00a0]: Copied! <pre># Retorna um trecho com as 5 primeiras linhas do dataframe\ndf.head()\n</pre> # Retorna um trecho com as 5 primeiras linhas do dataframe df.head() Out[\u00a0]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa In\u00a0[\u00a0]: Copied! <pre># Mostra informa\u00e7\u00f5es sobre o dataframe em si\ndf.info()\n</pre> # Mostra informa\u00e7\u00f5es sobre o dataframe em si df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  150 non-null    float64\n 1   sepal_width   150 non-null    float64\n 2   petal_length  150 non-null    float64\n 3   petal_width   150 non-null    float64\n 4   species       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB\n</pre> In\u00a0[\u00a0]: Copied! <pre># exibe o shape (dimenso\u1ebds) do dataframe\ndf.shape\n</pre> # exibe o shape (dimenso\u1ebds) do dataframe df.shape Out[\u00a0]: <pre>(150, 5)</pre> In\u00a0[\u00a0]: Copied! <pre>## Suas respostas....\n</pre> ## Suas respostas....       <p>S\u00e3o 150 exemplares de flor de \u00edris, pertencentes a tr\u00eas esp\u00e9cies diferentes: setosa, versicolor e virginica, sendo 50 amostras de cada esp\u00e9cie.</p> <p>Os atributos de <code>largura e comprimento de s\u00e9pala</code> e <code>largura e comprimento de p\u00e9tala</code> de cada flor fooram medidos manualmente.</p> In\u00a0[\u00a0]: Copied! <pre>df.describe()\n</pre> df.describe() Out[\u00a0]: sepal_length sepal_width petal_length petal_width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.054000 3.758667 1.198667 std 0.828066 0.433594 1.764420 0.763161 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 <p>Note que o m\u00e9todo describe() n\u00e3o exibe a coluna species, pois se trata de uma coluna n\u00e3o-num\u00e9rica.</p> <p>Apenas as colunas num\u00e9ricas est\u00e3o presentes, o atributo species indica r\u00f3tulos - trata-se de dados categ\u00f3ricos.</p> In\u00a0[\u00a0]: Copied! <pre># retorna a quantiade de classes da coluna\n\ndf.species.unique()\n</pre> # retorna a quantiade de classes da coluna  df.species.unique() Out[\u00a0]: <pre>array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)</pre> In\u00a0[\u00a0]: Copied! <pre>## Suas respostas....\n</pre> ## Suas respostas....       In\u00a0[\u00a0]: Copied! <pre># agrupamento por m\u00e9dia\n\ndf.groupby('species').mean()\n</pre> # agrupamento por m\u00e9dia  df.groupby('species').mean() Out[\u00a0]: sepal_length sepal_width petal_length petal_width species Iris-setosa 5.006 3.418 1.464 0.244 Iris-versicolor 5.936 2.770 4.260 1.326 Iris-virginica 6.588 2.974 5.552 2.026 In\u00a0[\u00a0]: Copied! <pre>#  Quantidade de cada categoria\ndf.groupby('species').size()\n</pre> #  Quantidade de cada categoria df.groupby('species').size() Out[\u00a0]: <pre>species\nIris-setosa        50\nIris-versicolor    50\nIris-virginica     50\ndtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\n# c\u00f3pia de df\ndf_iris = df\n\n# Gera dados faltante no dataset\nfor col in df_iris.columns[:-1]:\n    df_iris.loc[np.random.choice(df_iris.index, 5), col] = np.nan\n</pre> import numpy as np  # c\u00f3pia de df df_iris = df  # Gera dados faltante no dataset for col in df_iris.columns[:-1]:     df_iris.loc[np.random.choice(df_iris.index, 5), col] = np.nan  In\u00a0[\u00a0]: Copied! <pre>df_iris.info()\n</pre> df_iris.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 131 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  121 non-null    float64\n 1   sepal_width   121 non-null    float64\n 2   petal_length  122 non-null    float64\n 3   petal_width   121 non-null    float64\n 4   species       131 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 10.2+ KB\n</pre> In\u00a0[\u00a0]: Copied! <pre># Verifica os valores ausentes\n\ndf_iris.isnull().sum()\n</pre> # Verifica os valores ausentes  df_iris.isnull().sum()  Out[\u00a0]: <pre>sepal_length    10\nsepal_width     10\npetal_length     9\npetal_width     10\nspecies          0\ndtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>df_iris.dropna(axis=0, inplace=True)\n</pre> df_iris.dropna(axis=0, inplace=True) In\u00a0[\u00a0]: Copied! <pre># Tratamento de valores faltantes: imputa\u00e7\u00e3o m\u00e9dia\n\nfor col in df_iris.columns[:-1]:  # a coluna de especie n\u00e3o entra\n    mean_val = df_iris[col].mean()\n    df_iris[col].fillna(mean_val, inplace=True)\n</pre> # Tratamento de valores faltantes: imputa\u00e7\u00e3o m\u00e9dia  for col in df_iris.columns[:-1]:  # a coluna de especie n\u00e3o entra     mean_val = df_iris[col].mean()     df_iris[col].fillna(mean_val, inplace=True) In\u00a0[\u00a0]: Copied! <pre>df_iris.info()\n</pre> df_iris.info()  <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 131 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  131 non-null    float64\n 1   sepal_width   131 non-null    float64\n 2   petal_length  131 non-null    float64\n 3   petal_width   131 non-null    float64\n 4   species       131 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 10.2+ KB\n</pre> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport seaborn as sns\n</pre> import matplotlib.pyplot as plt import seaborn as sns In\u00a0[\u00a0]: Copied! <pre># Gr\u00e1fico de linha para a m\u00e9dia do comprimento da s\u00e9pala de cada esp\u00e9cie\n\ngrouped = df_iris.groupby('species')['sepal_length'].mean()\ngrouped.plot(kind='line', marker='o')\n\n\nplt.title('M\u00e9dia do Comprimento da S\u00e9pala por Esp\u00e9cie')\nplt.ylabel('Comprimento da S\u00e9pala (cm)')\nplt.grid(True)\nplt.show()\n</pre> # Gr\u00e1fico de linha para a m\u00e9dia do comprimento da s\u00e9pala de cada esp\u00e9cie  grouped = df_iris.groupby('species')['sepal_length'].mean() grouped.plot(kind='line', marker='o')   plt.title('M\u00e9dia do Comprimento da S\u00e9pala por Esp\u00e9cie') plt.ylabel('Comprimento da S\u00e9pala (cm)') plt.grid(True) plt.show()  In\u00a0[\u00a0]: Copied! <pre># Gr\u00e1fico de Barras\n\nspecies_count = df['species'].value_counts()\nspecies_count.plot(kind='bar')\n\nplt.title('Contagem de Esp\u00e9cies')\nplt.xlabel('Esp\u00e9cie')\nplt.ylabel('Contagem')\nplt.show()\n</pre> # Gr\u00e1fico de Barras  species_count = df['species'].value_counts() species_count.plot(kind='bar')  plt.title('Contagem de Esp\u00e9cies') plt.xlabel('Esp\u00e9cie') plt.ylabel('Contagem') plt.show()  In\u00a0[\u00a0]: Copied! <pre># Lembra de histograma, que exibe uma gr\u00e1fico de frequ\u00eancia.\ndf.hist(bins=100, figsize=(15, 15))\nplt.show()\n</pre> # Lembra de histograma, que exibe uma gr\u00e1fico de frequ\u00eancia. df.hist(bins=100, figsize=(15, 15)) plt.show() In\u00a0[\u00a0]: Copied! <pre># Histograma apenas de um atributo\n\nplt.hist(df['sepal_length'], bins=100)\n\n\nplt.title('Histograma de Sepal Length')\nplt.xlabel('Sepal Length')\nplt.ylabel('Contagem')\nplt.show()\n</pre> # Histograma apenas de um atributo  plt.hist(df['sepal_length'], bins=100)   plt.title('Histograma de Sepal Length') plt.xlabel('Sepal Length') plt.ylabel('Contagem') plt.show()  In\u00a0[\u00a0]: Copied! <pre>### Suas respostas....\n</pre> ### Suas respostas....        In\u00a0[\u00a0]: Copied! <pre># box plot de todos \ndf.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False, figsize=(15, 15))\nplt.show()\n</pre> # box plot de todos  df.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False, figsize=(15, 15)) plt.show() In\u00a0[\u00a0]: Copied! <pre>### boxplot para uma especie especifica\n\n# Filtrar os dados para a esp\u00e9cie Iris-virginica \nvirginica = df[df['species'] == 'Iris-virginica']\n\nvirginica.plot(kind='box', y='petal_length', figsize=(6, 6))\nplt.title('Boxplot do Comprimento das P\u00e9talas de Iris-virginica')\nplt.ylabel('Comprimento das P\u00e9talas (cm)')\nplt.show()\n</pre> ### boxplot para uma especie especifica  # Filtrar os dados para a esp\u00e9cie Iris-virginica  virginica = df[df['species'] == 'Iris-virginica']  virginica.plot(kind='box', y='petal_length', figsize=(6, 6)) plt.title('Boxplot do Comprimento das P\u00e9talas de Iris-virginica') plt.ylabel('Comprimento das P\u00e9talas (cm)') plt.show()  In\u00a0[\u00a0]: Copied! <pre>### Suas respostas.......\n</pre> ### Suas respostas.......         In\u00a0[\u00a0]: Copied! <pre># Grafico de dispers\u00e3o\n\nplt.scatter(df_iris['sepal_length'], df_iris['petal_length'])\n\nplt.title('Sepal Length vs Petal Length')\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Petal Length (cm)')\nplt.show()\n</pre> # Grafico de dispers\u00e3o  plt.scatter(df_iris['sepal_length'], df_iris['petal_length'])  plt.title('Sepal Length vs Petal Length') plt.xlabel('Sepal Length (cm)') plt.ylabel('Petal Length (cm)') plt.show() In\u00a0[\u00a0]: Copied! <pre># Os mesmos dados mas agora cada classe de uma cor diferente\n\ncolors = {'Iris-setosa':'red', 'Iris-versicolor':'blue', 'Iris-virginica':'green'}\n\nplt.scatter(df['sepal_length'], df['petal_length'], c=df['species'].map(colors), label=colors)\n\nplt.title('Sepal Length vs Petal Length')\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Petal Length (cm)')\nplt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in colors.values()], labels=colors.keys())\nplt.show()\n</pre> # Os mesmos dados mas agora cada classe de uma cor diferente  colors = {'Iris-setosa':'red', 'Iris-versicolor':'blue', 'Iris-virginica':'green'}  plt.scatter(df['sepal_length'], df['petal_length'], c=df['species'].map(colors), label=colors)  plt.title('Sepal Length vs Petal Length') plt.xlabel('Sepal Length (cm)') plt.ylabel('Petal Length (cm)') plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in colors.values()], labels=colors.keys()) plt.show() In\u00a0[\u00a0]: Copied! <pre># scatter plot matrix\n# tudo junto e misturado\n\nfrom pandas.plotting import scatter_matrix\n\nscatter_matrix(df,figsize=(15, 15))\n\nplt.show()\n</pre> # scatter plot matrix # tudo junto e misturado  from pandas.plotting import scatter_matrix  scatter_matrix(df,figsize=(15, 15))  plt.show() <p>Vamos utilizar o <code>seaborn</code> para visualizar gr\u00e1ficos mais bonitos</p> In\u00a0[\u00a0]: Copied! <pre>import seaborn as sns\n\n# A cor vem do campo `species` do dataframe\n\nsns.pairplot(df, hue='species', height=5)\n\nplt.show()\n</pre> import seaborn as sns  # A cor vem do campo `species` do dataframe  sns.pairplot(df, hue='species', height=5)  plt.show() In\u00a0[\u00a0]: Copied! <pre>### Suas Respostas........\n</pre> ### Suas Respostas........          In\u00a0[\u00a0]: Copied! <pre># Violin plot\ng = sns.violinplot(y='species', x='sepal_length', data=df, inner='quartile')\nplt.show()\ng = sns.violinplot(y='species', x='sepal_width', data=df, inner='quartile')\nplt.show()\ng = sns.violinplot(y='species', x='petal_length', data=df, inner='quartile')\nplt.show()\ng = sns.violinplot(y='species', x='petal_width', data=df, inner='quartile')\nplt.show()\n</pre> # Violin plot g = sns.violinplot(y='species', x='sepal_length', data=df, inner='quartile') plt.show() g = sns.violinplot(y='species', x='sepal_width', data=df, inner='quartile') plt.show() g = sns.violinplot(y='species', x='petal_length', data=df, inner='quartile') plt.show() g = sns.violinplot(y='species', x='petal_width', data=df, inner='quartile') plt.show() In\u00a0[\u00a0]: Copied! <pre>cols = ['sepal_length', 'sepal_width', 'petal_length','petal_width']\ncorr_matx = df[cols].corr()\n\nheatmap = sns.heatmap(corr_matx,cbar=True,annot=True,square=True,fmt='.2f',annot_kws={'size': 15},yticklabels=cols,xticklabels=cols,cmap='Dark2')\n</pre> cols = ['sepal_length', 'sepal_width', 'petal_length','petal_width'] corr_matx = df[cols].corr()  heatmap = sns.heatmap(corr_matx,cbar=True,annot=True,square=True,fmt='.2f',annot_kws={'size': 15},yticklabels=cols,xticklabels=cols,cmap='Dark2') In\u00a0[\u00a0]: Copied! <pre>### Implemente sua solu\u00e7\u00e3o e apresente sua an\u00e1lise....\n</pre> ### Implemente sua solu\u00e7\u00e3o e apresente sua an\u00e1lise....     In\u00a0[\u00a0]: Copied! <pre>df['petal_length'].head()\n</pre> df['petal_length'].head() Out[\u00a0]: <pre>0    1.4\n1    1.4\n2    1.3\n3    1.5\n4    1.4\nName: petal_length, dtype: float64</pre> <p>Por outro lado, se dentro dos colchetes passamos uma lista de nomes de coluna, o reultado \u00e9 outro <code>DataFrame</code> contendo aquelas colunas. Isso vale inclusive para uma coluna simples:</p> In\u00a0[\u00a0]: Copied! <pre># Mostra apenas a coluna petal_len\ndf[['petal_length']].head()\n</pre> # Mostra apenas a coluna petal_len df[['petal_length']].head() Out[\u00a0]: petal_length 0 1.4 1 1.4 2 1.3 3 1.5 4 1.4 In\u00a0[\u00a0]: Copied! <pre># Mostra as colunas petal_length e petal_width\ndf[['petal_length', 'petal_width']].head()\n</pre> # Mostra as colunas petal_length e petal_width df[['petal_length', 'petal_width']].head() Out[\u00a0]: petal_length petal_width 0 1.4 0.2 1 1.4 0.2 2 1.3 0.2 3 1.5 0.2 4 1.4 0.2 In\u00a0[\u00a0]: Copied! <pre># Criando uma nova caracter\u00edstica: \u00e1rea da s\u00e9pala\n\ndf['sepal_area'] = df['sepal_length'] * df['sepal_width']\n\ndf.head()\n</pre> # Criando uma nova caracter\u00edstica: \u00e1rea da s\u00e9pala  df['sepal_area'] = df['sepal_length'] * df['sepal_width']  df.head()  Out[\u00a0]: sepal_length sepal_width petal_length petal_width species sepal_area 0 5.1 3.5 1.4 0.2 Iris-setosa 17.85 1 4.9 3.0 1.4 0.2 Iris-setosa 14.70 2 4.7 3.2 1.3 0.2 Iris-setosa 15.04 3 4.6 3.1 1.5 0.2 Iris-setosa 14.26 4 5.0 3.6 1.4 0.2 Iris-setosa 18.00 In\u00a0[\u00a0]: Copied! <pre>## suas respostas aqui.....\n</pre> ## suas respostas aqui.....      In\u00a0[\u00a0]: Copied! <pre>### Implemente sua sua solu\u00e7\u00e3o e an\u00e1lise de dados. :)\n</pre> ### Implemente sua sua solu\u00e7\u00e3o e an\u00e1lise de dados. :)"},{"location":"aulas/IA/lab01/dataframe/#objetivos","title":"Objetivos\u00b6","text":"<ul> <li>Apresentar e utilizar o pacote pandas</li> <li>Como carregar uma base dados</li> <li>Como visualizar os dados</li> <li>Intui\u00e7\u00e3o de an\u00e1lise explorat\u00f3ria de dados</li> </ul>"},{"location":"aulas/IA/lab01/dataframe/#introducao-a-analise-de-dados-usando-pandas","title":"Introdu\u00e7\u00e3o a an\u00e1lise de dados usando Pandas\u00b6","text":"<p>Vamos come\u00e7ar pelo come\u00e7o! Vamos escolher um dataset (conjunto de dados) para analisar.</p> <p>Vamos utilizar um pacote do python capaz de trabalhar com tabelas de dados chamada pandas, para essas tabelas chamamos de dataframe.</p> <p>Veja mais em: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html</p>"},{"location":"aulas/IA/lab01/dataframe/#hello-world-do-mundo-dos-dados","title":"Hello World! do mundo dos dados\u00b6","text":""},{"location":"aulas/IA/lab01/dataframe/#conjunto-de-dados-dataset","title":"Conjunto de dados <code>Dataset</code>\u00b6","text":"<p>Para trabalhar com an\u00e1lise de dados precisamos de.... <code>DADOS</code>.</p> <p>Podemos escolher qualquer base de dados disponivel na internet, ou at\u00e9 mesmo criar nosso proprio dataset.</p> <p>Vamos simplificar essa etapa e come\u00e7ar analisando uma base pequena e muito famosa chamada iris que esta disponivel em:</p> <p>ref: https://archive.ics.uci.edu/ml/datasets/Iris</p> <p>Conhe\u00e7a outros datasets: https://archive.ics.uci.edu/ml/datasets.php</p>"},{"location":"aulas/IA/lab01/dataframe/#conhecendo-os-dados","title":"Conhecendo os dados\u00b6","text":"<p>Essa etapa \u00e9 muito importante, CONHECER OS DADOS!</p> <p>Quanto mais voc\u00ea conhece a base de DADOS maior a possibilidade de extrair INFORMA\u00c7\u00d5ES \u00fateis para tomada de decis\u00e3o.</p>"},{"location":"aulas/IA/lab01/dataframe/#analisando-o-dataset","title":"an\u00e1lisando o dataset\u00b6","text":"<p>Agora que j\u00e1 carregamos o dataset corretamente, vamos come\u00e7ar a analisa-lo. o m\u00e9todo <code>info()</code> \u00e9 um bom ponto de partida para isso.</p>"},{"location":"aulas/IA/lab01/dataframe/#desafio-1","title":"Desafio 1\u00b6","text":"<p>Analisando as informa\u00e7\u00f5es do dataset iris, responda:</p> <ol> <li><p>Quantos dados existem nesse dataset?</p> </li> <li><p>Qual a quantidade de atributos?</p> </li> <li><p>Existe valores faltantes?</p> </li> <li><p>De que tipo s\u00e3o os dados (dtype)?</p> </li> </ol>"},{"location":"aulas/IA/lab01/dataframe/#analisando-os-dados-mais-a-fundo","title":"An\u00e1lisando os dados mais a fundo\u00b6","text":""},{"location":"aulas/IA/lab01/dataframe/#resumo-estatistico","title":"Resumo estat\u00edstico\u00b6","text":"<p>O m\u00e9todo <code>describe()</code> gera um resumo estat\u00edstico dos dados contidos em um DataFrame ou Series.</p> <p>Retorna estat\u00edsticas descritivas como <code>m\u00e9dia, desvio padr\u00e3o, valor m\u00ednimo, quartis e valor m\u00e1ximo</code> para as colunas num\u00e9ricas do DataFrame</p>"},{"location":"aulas/IA/lab01/dataframe/#desafio-2","title":"Desafio 2\u00b6","text":"<p>Analisando as informa\u00e7\u00f5es obtidas com o metodo describe(), responda:</p> <ol> <li><p>Qual \u00e9 a m\u00e9dia (mean) do comprimento das s\u00e9palas (sepal length) no conjunto de dados Iris?</p> </li> <li><p>Qual \u00e9 o desvio padr\u00e3o (std) da largura das p\u00e9talas (petal width)?</p> </li> <li><p>Interprete o valor do quartil 25% (25%) para o comprimento das p\u00e9talas (petal length). O que ele representa?</p> </li> <li><p>Compare as m\u00e9dias do comprimento das s\u00e9palas entre as diferentes esp\u00e9cies de Iris. Qual esp\u00e9cie tem o maior valor m\u00e9dio?</p> </li> <li><p>Com base nos resultados do describe(), qual esp\u00e9cie parece ter a maior varia\u00e7\u00e3o no tamanho das p\u00e9talas?</p> </li> <li><p>Se um novo registro de Iris tem um comprimento de s\u00e9pala de 7.5 cm, como esse valor se compara com a distribui\u00e7\u00e3o do comprimento das s\u00e9palas no conjunto de dados? Ele \u00e9 considerado um valor alto, baixo ou dentro da m\u00e9dia?</p> </li> </ol>"},{"location":"aulas/IA/lab01/dataframe/#agrupando-dados","title":"Agrupando dados\u00b6","text":"<p>O m\u00e9todo <code>groupby</code> \u00e9 usada para agrupar dados com base em valores espec\u00edficos de uma ou mais colunas.</p> <p>Permite realizar opera\u00e7\u00f5es em grupos de dados e \u00e9 muito \u00fatil e versatil para an\u00e1lise e agrega\u00e7\u00e3o de dado.</p>"},{"location":"aulas/IA/lab01/dataframe/#limpeza-de-dados","title":"Limpeza de Dados\u00b6","text":"<p>Base de dados do mundo real podem conter diversos problemas, \u00e9 muito comum o lidar com valores faltantes em um dataset.</p> <p>Como exemplo, vamos usar o conjunto de dados Iris, mas introduzimos algumas 'imperfei\u00e7\u00f5es' para fins de demonstra\u00e7\u00e3o.</p> <p>Para introduzir valores faltantes, podemos usar o seguinte c\u00f3digo:</p> <pre>## Gera dados faltante no dataset\nfor col in df_iris.columns[:-1]:\n    df_iris.loc[np.random.choice(df_iris.index, 5), col] = np.nan\n</pre>"},{"location":"aulas/IA/lab01/dataframe/#excluindo-linhas","title":"Excluindo linhas\u00b6","text":"<p>Podemos simplismente excluir as linhas ou colunas que contenham dados faltantes, basta usar a fun\u00e7\u00e3o dropna pandas, utilizando como par\u00e2metros o axis = 1 para dizer que queremos deletar a coluna ou axis = 0 para linha e inplace = True para aplicarmos no dataset e n\u00e3o criarmos uma c\u00f3pia deste:</p> <ul> <li><code>axis=0</code> --&gt; exclui linha</li> <li><code>axis=1</code> --&gt; exclui coluna</li> </ul> <p><code>Pense bemmmm!!!!</code> A decis\u00e3o por qual vai excluir depende do problema que voc\u00ea est\u00e1 atacando...</p>"},{"location":"aulas/IA/lab01/dataframe/#preenchimento-com-valores","title":"Preenchimento com valores\u00b6","text":"<p>Voc\u00ea pode preencher os valores faltantes com m\u00e9dias, medianas, modas ou outros valores relevantes.</p> <p>Isso ajuda a manter o tamanho do conjunto de dados, mas pode introduzir vi\u00e9s nos resultados.</p> <p><code>Pense bemmmm!!!!</code> A decis\u00e3o por qual valor preencher depende do problema que voc\u00ea est\u00e1 atacando...</p>"},{"location":"aulas/IA/lab01/dataframe/#analisando-informacoes-em-graficos","title":"Analisando informa\u00e7\u00f5es em gr\u00e1ficos\u00b6","text":"<p>Uma an\u00e1lise gr\u00e1fica pode ajudar a compreeder melhor os dados que estamos trabalhando....</p> <p>Vamos explorar diferentes tipos de visualiza\u00e7\u00f5es que podem nos ajudar a entender melhor nossos dados.</p> <p>Vamos usar o <code>matplotlib</code> e o  <code>seaborn</code> para nos ajudar.</p> <p>Se precisar instalar:</p> <ul> <li>pip install matplotlib seaborn</li> </ul>"},{"location":"aulas/IA/lab01/dataframe/#histograma","title":"histograma\u00b6","text":"<p>Um histograma \u00e9 um tipo de gr\u00e1fico usado para representar a distribui\u00e7\u00e3o de frequ\u00eancias de um conjunto de dados.</p> <p>Ele \u00e9 composto por barras, onde cada barra representa um intervalo de valores (tamb\u00e9m chamado de \"bin\").</p> <p>A altura de cada barra indica a frequ\u00eancia (ou quantidade) de dados que caem dentro desse intervalo.</p>"},{"location":"aulas/IA/lab01/dataframe/#desafio-3","title":"Desafio 3\u00b6","text":"<p>Com base nos histogramas, responda:</p> <ol> <li><p>Crie um histograma para o largura das s\u00e9palas (sepal width) para visualizar a distribui\u00e7\u00e3o dessa caracter\u00edstica no conjunto de dados.</p> </li> <li><p>Crie histogramas para diferentes caracter\u00edsticas (como largura das s\u00e9palas, largura das p\u00e9talas, etc.) e compare a variabilidade entre elas. Observe quais caracter\u00edsticas t\u00eam maior dispers\u00e3o.</p> </li> <li><p>Explore utilizar diferentes n\u00fameros de bins (intervalos) no histograma e observe como isso afeta a visualiza\u00e7\u00e3o da distribui\u00e7\u00e3o. Qual o n\u00famero adequado de bins para a interpreta\u00e7\u00e3o dos dados?</p> </li> </ol>"},{"location":"aulas/IA/lab01/dataframe/#boxplot","title":"Boxplot\u00b6","text":"<p>Um boxplot, tamb\u00e9m conhecido como diagrama de caixa, \u00e9 um gr\u00e1fico usado para representar a distribui\u00e7\u00e3o de um conjunto de dados atrav\u00e9s de cinco medidas resumidas: m\u00ednimo, primeiro quartil (Q1), mediana (Q2), terceiro quartil (Q3) e m\u00e1ximo.</p>"},{"location":"aulas/IA/lab01/dataframe/#desafio-4","title":"Desafio 4\u00b6","text":"<p>Com base nos boxplot, responda:</p> <ol> <li><p>Qual \u00e9 o valor do terceiro quartil (Q3) para o comprimento das p\u00e9talas (petal length) da esp\u00e9cie Iris-setosa?</p> </li> <li><p>Compare o primeiro quartil (Q1) da largura das s\u00e9palas (sepal width) entre as tr\u00eas esp\u00e9cies de Iris. Qual esp\u00e9cie tem o menor Q1?</p> </li> <li><p>Identifique se h\u00e1 outliers no boxplot do comprimento das s\u00e9palas (sepal length) para a esp\u00e9cie Iris-versicolor. Se sim, especifique o n\u00famero aproximado de outliers.</p> </li> <li><p>Explique como os outliers s\u00e3o determinados em um boxplot.</p> </li> <li><p>Compare os boxplots do comprimento das p\u00e9talas (petal length) e da largura das p\u00e9talas (petal width) para a esp\u00e9cie Iris-versicolor. Qual caracter\u00edstica tem maior variabilidade?</p> </li> <li><p>Observando o boxplot da largura das s\u00e9palas (sepal width) para a esp\u00e9cie Iris-setosa, a distribui\u00e7\u00e3o dos dados \u00e9 sim\u00e9trica ou assim\u00e9trica? Como voc\u00ea pode dizer isso a partir do boxplot?</p> </li> <li><p>Compare a simetria dos boxplots do comprimento das s\u00e9palas (sepal length) entre as tr\u00eas esp\u00e9cies de Iris. Qual esp\u00e9cie apresenta a distribui\u00e7\u00e3o mais sim\u00e9trica?</p> </li> </ol>"},{"location":"aulas/IA/lab01/dataframe/#scatter","title":"Scatter\u00b6","text":"<p>Um scatter plot, ou gr\u00e1fico de dispers\u00e3o, \u00e9 uma representa\u00e7\u00e3o gr\u00e1fica que utiliza pontos para mostrar a rela\u00e7\u00e3o entre duas vari\u00e1veis num\u00e9ricas.</p> <p>Cada ponto no gr\u00e1fico corresponde a um par de valores das duas vari\u00e1veis. Scatter plots s\u00e3o \u00fateis para identificar padr\u00f5es, tend\u00eancias, correla\u00e7\u00f5es ou poss\u00edveis associa\u00e7\u00f5es entre as vari\u00e1veis.</p>"},{"location":"aulas/IA/lab01/dataframe/#desafio-5","title":"Desafio 5\u00b6","text":"<p>Com base nos scatter plot, scatter matrix e pairplot, responda:</p> <ol> <li><p>Crie um scatter plot para o comprimento e a largura das s\u00e9palas (sepal length vs. sepal width). Existe alguma rela\u00e7\u00e3o aparente entre essas duas vari\u00e1veis?</p> </li> <li><p>Compare os scatter plots do comprimento das p\u00e9talas (petal length) versus largura das p\u00e9talas (petal width) para cada esp\u00e9cie de Iris. Qual esp\u00e9cie parece ter a rela\u00e7\u00e3o mais forte entre essas duas caracter\u00edsticas?</p> </li> <li><p>Observe o scatter plot do comprimento das s\u00e9palas (sepal length) versus comprimento das p\u00e9talas (petal length). Existe algum padr\u00e3o distinto que voc\u00ea pode identificar?</p> </li> <li><p>Com base no scatter plot da largura das s\u00e9palas (sepal width) versus largura das p\u00e9talas (petal width), existe alguma tend\u00eancia que possa ajudar na classifica\u00e7\u00e3o das esp\u00e9cies de Iris?</p> </li> <li><p>Como voc\u00ea usaria os padr\u00f5es identificados nos scatter plots para diferenciar entre as esp\u00e9cies de Iris?</p> </li> </ol>"},{"location":"aulas/IA/lab01/dataframe/#violin","title":"Violin\u00b6","text":"<p>O Violin plot \u00e9 similar ao box plot, exibe a distribui\u00e7\u00e3o de variaveis num\u00e9ricas em niveis, pode ser configurada de muitas formas e \u00e9 uma forma de visualiza\u00e7\u00e3o interessante de dados.</p> <p>Saiba mais em: https://seaborn.pydata.org/generated/seaborn.violinplot.html</p>"},{"location":"aulas/IA/lab01/dataframe/#correlacao-entre-atributos","title":"Correla\u00e7\u00e3o entre atributos\u00b6","text":"<p>A matriz de correla\u00e7\u00e3o  avalia a rela\u00e7\u00e3o entre duas ou mais variaveis (correla\u00e7\u00e3o).</p> <p>valores:</p> <ul> <li>0.9 a 1 positivo ou negativo indica uma correla\u00e7\u00e3o muito forte.</li> <li>0.7 a 0.9 positivo ou negativo indica uma correla\u00e7\u00e3o forte.</li> <li>0.5 a 0.7 positivo ou negativo indica uma correla\u00e7\u00e3o moderada.</li> <li>0.3 a 0.5 positivo ou negativo indica uma correla\u00e7\u00e3o fraca.</li> <li>0 a 0.3 positivo ou negativo indica uma correla\u00e7\u00e3o desprez\u00edvel.</li> </ul> <p>lembre-se que: alta correla\u00e7\u00e3o n\u00e3o implica em causa. (causa e consequ\u00eancia). Para entender melhor vale a pena dar uma olhada nesse site que mostra correla\u00e7\u00f5es absurdas...'spurious correlations'</p>"},{"location":"aulas/IA/lab01/dataframe/#desafio-6","title":"Desafio 6\u00b6","text":"<p>Analise os gr\u00e1ficos gerados at\u00e9 o momento para responder as quest\u00f5es abaixo:</p> <ol> <li><p>A especie que possui na m\u00e9dia a menor sepala \u00e9 a mesma que possui a menor petala?</p> </li> <li><p>Existe sobreposi\u00e7\u00e3o entre as medi\u00e7\u00f5es, ou seja, uma petala de tamanho x pode ser tanto da especie versicolor ou da virginica?</p> </li> <li><p>\u00c9 possivel classificar as especies de iris com base apenas em suas dimens\u00f5es?</p> </li> </ol>"},{"location":"aulas/IA/lab01/dataframe/#acessando-dados-de-um-dataframe","title":"Acessando dados de um Dataframe\u00b6","text":"<p>H\u00e1 v\u00e1rias maneiras de acessar o conte\u00fado de um <code>DataFrame</code>. Os mais simples s\u00e3o aqueles que usam a nota\u00e7\u00e3o de colchetes. Primeiramente, podemos acessar uma coluna atrav\u00e9s do seu \u00edndice, retornando uma <code>Series</code>, ou seja, uma coluna do dataframe.</p>"},{"location":"aulas/IA/lab01/dataframe/#desafios-7","title":"Desafios 7\u00b6","text":"<ol> <li>Limpeza de Dados: Introduza valores faltantes no dataset Iris. Tente usar diferentes m\u00e9todos para tratar essas imperfei\u00e7\u00f5es e compare os resultados.</li> <li>Manipula\u00e7\u00e3o de Dados: Use as fun\u00e7\u00f5es do pandas para responder \u00e0s seguintes perguntas sobre o dataset Iris:<ul> <li>Qual \u00e9 a m\u00e9dia da largura da s\u00e9pala para cada esp\u00e9cie?</li> <li>Quantas flores t\u00eam uma \u00e1rea da s\u00e9pala maior que 20 cm^2?</li> </ul> </li> <li>feature engineering: Pense em outras caracter\u00edsticas que podem ser criadas a partir do dataset Iris. Por exemplo, uma caracter\u00edstica que represente a propor\u00e7\u00e3o entre a largura e o comprimento da s\u00e9pala.</li> </ol>"},{"location":"aulas/IA/lab01/dataframe/#desafio-8","title":"Desafio 8\u00b6","text":"<p>Fa\u00e7a agora uma explora\u00e7\u00e3o de dados em outra base, conhe\u00e7a a base, e crie hipoteses de teste.</p> <ol> <li>Importar os dados do dataset <code>Breast Cancer Data Set</code> acesse o site:<code>https://archive.ics.uci.edu/ml/datasets/breast+cancer</code></li> <li>Nomear as colunas de acordo com o arquivo <code>breast-cancer.names</code></li> <li>Ralizar a An\u00e1lise Explorat\u00f3ria de Dados (EDA):</li> <li>Visualize a distribui\u00e7\u00e3o de cada caracter\u00edstica do conjunto de dados.</li> <li>Determine quais caracter\u00edsticas t\u00eam maior correla\u00e7\u00e3o com a classifica\u00e7\u00e3o de malignidade.</li> <li>Crie novas caracter\u00edsticas a partir das existentes.</li> <li>Crie representa\u00e7\u00e3oes gr\u00e1ficas do dataset para contribuir para sua an\u00e1lise</li> </ol>"},{"location":"aulas/IA/lab02/classificador-knn/","title":"Classificador knn","text":"<p>S\u00e3o 150 exemplares de flor de \u00edris, pertencentes a tr\u00eas esp\u00e9cies diferentes: setosa, versicolor e virginica, sendo 50 amostras de cada esp\u00e9cie. Os atributos de largura e comprimento de s\u00e9pala e largura e comprimento de p\u00e9tala de cada flor fooram medidos manualmente.</p> In\u00a0[\u00a0]: Copied! <pre>### Sua resposta.....\n</pre> ### Sua resposta.....   In\u00a0[\u00a0]: Copied! <pre># Inicializ\u00e7\u00e3o das bibliotecas\n%matplotlib inline\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n</pre> # Inicializ\u00e7\u00e3o das bibliotecas %matplotlib inline  import pandas as pd import matplotlib.pyplot as plt  In\u00a0[\u00a0]: Copied! <pre># Caminho do arquivo\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n# Define o nome das colunas\nheader = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n# L\u00ea e carrega o arquivo para a mem\u00f3ria\ndf = pd.read_csv(url, header=None, names=header)\n</pre> # Caminho do arquivo url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" # Define o nome das colunas header = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'] # L\u00ea e carrega o arquivo para a mem\u00f3ria df = pd.read_csv(url, header=None, names=header) In\u00a0[\u00a0]: Copied! <pre># Retorna um trecho com as 5 primeiras linhas do dataframe\ndf.head()\n</pre> # Retorna um trecho com as 5 primeiras linhas do dataframe df.head() Out[\u00a0]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa In\u00a0[\u00a0]: Copied! <pre>df.tail()\n</pre> df.tail() Out[\u00a0]: sepal_length sepal_width petal_length petal_width species 145 6.7 3.0 5.2 2.3 Iris-virginica 146 6.3 2.5 5.0 1.9 Iris-virginica 147 6.5 3.0 5.2 2.0 Iris-virginica 148 6.2 3.4 5.4 2.3 Iris-virginica 149 5.9 3.0 5.1 1.8 Iris-virginica In\u00a0[\u00a0]: Copied! <pre># Mostra informa\u00e7\u00f5es sobre o dataframe em si\ndf.info()\n</pre> # Mostra informa\u00e7\u00f5es sobre o dataframe em si df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  150 non-null    float64\n 1   sepal_width   150 non-null    float64\n 2   petal_length  150 non-null    float64\n 3   petal_width   150 non-null    float64\n 4   species       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB\n</pre> In\u00a0[\u00a0]: Copied! <pre># class distribution\nprint(df.groupby('species').size())\n</pre> # class distribution print(df.groupby('species').size()) <pre>species\nIris-setosa        50\nIris-versicolor    50\nIris-virginica     50\ndtype: int64\n</pre> In\u00a0[\u00a0]: Copied! <pre>## Sua resposta e seus gr\u00e1ficos para an\u00e1lisar..\n</pre> ## Sua resposta e seus gr\u00e1ficos para an\u00e1lisar..       In\u00a0[\u00a0]: Copied! <pre># Selecionando um sub-dataframe com os campos petal_length e petal_width, \n# e outro com a vari\u00e1vel de classes\nentradas = df[['petal_length', 'petal_width']]\nclasses = df['species']\nprint(f\"Formato das tabelas de dados {entradas.shape} e classes {classes.shape}\")\n</pre> # Selecionando um sub-dataframe com os campos petal_length e petal_width,  # e outro com a vari\u00e1vel de classes entradas = df[['petal_length', 'petal_width']] classes = df['species'] print(f\"Formato das tabelas de dados {entradas.shape} e classes {classes.shape}\") <pre>Formato das tabelas de dados (150, 2) e classes (150,)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Separamos 20% para o teste\nfrom sklearn.model_selection import train_test_split\n\nentradas_treino, entradas_teste, classes_treino, classes_teste = train_test_split(entradas, classes, test_size=0.2)\n\nprint(f\"Formato das tabelas de dados de treino {entradas_treino.shape} e teste {entradas_teste.shape}\")\n</pre> # Separamos 20% para o teste from sklearn.model_selection import train_test_split  entradas_treino, entradas_teste, classes_treino, classes_teste = train_test_split(entradas, classes, test_size=0.2)  print(f\"Formato das tabelas de dados de treino {entradas_treino.shape} e teste {entradas_teste.shape}\") <pre>Formato das tabelas de dados de treino (120, 2) e teste (30, 2)\n</pre> In\u00a0[\u00a0]: Copied! <pre>#Primeiras linhas do dataframe \nentradas_treino.head()\n</pre> #Primeiras linhas do dataframe  entradas_treino.head() Out[\u00a0]: petal_length petal_width 31 1.5 0.4 120 5.7 2.3 93 3.3 1.0 5 1.7 0.4 102 5.9 2.1 In\u00a0[\u00a0]: Copied! <pre>classes_treino.head()\n</pre> classes_treino.head() Out[\u00a0]: <pre>31         Iris-setosa\n120     Iris-virginica\n93     Iris-versicolor\n5          Iris-setosa\n102     Iris-virginica\nName: species, dtype: object</pre> In\u00a0[\u00a0]: Copied! <pre># Importa a biblioteca\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Cria o classificar KNN\nk = 9\nmodelo = KNeighborsClassifier(n_neighbors=k)\n\n# Cria o modelo de machine learning\nmodelo.fit(entradas_treino, classes_treino)\n</pre> # Importa a biblioteca from sklearn.neighbors import KNeighborsClassifier  # Cria o classificar KNN k = 9 modelo = KNeighborsClassifier(n_neighbors=k)  # Cria o modelo de machine learning modelo.fit(entradas_treino, classes_treino)    Out[\u00a0]: <pre>KNeighborsClassifier(n_neighbors=9)</pre> <p>Pronto!! bora testar se esta funcionando....</p> In\u00a0[\u00a0]: Copied! <pre># Para obter as previs\u00f5es, basta chamar o m\u00e9todo predict()\nclasses_encontradas = modelo.predict(entradas_teste)\nprint(\"Predi\u00e7\u00e3o: {}\".format(classes_encontradas))\n</pre> # Para obter as previs\u00f5es, basta chamar o m\u00e9todo predict() classes_encontradas = modelo.predict(entradas_teste) print(\"Predi\u00e7\u00e3o: {}\".format(classes_encontradas)) <pre>Predi\u00e7\u00e3o: ['Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n 'Iris-virginica' 'Iris-virginica' 'Iris-versicolor' 'Iris-virginica'\n 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n 'Iris-virginica' 'Iris-setosa' 'Iris-versicolor' 'Iris-setosa'\n 'Iris-versicolor' 'Iris-setosa' 'Iris-virginica' 'Iris-setosa'\n 'Iris-virginica' 'Iris-setosa' 'Iris-versicolor' 'Iris-versicolor'\n 'Iris-setosa' 'Iris-virginica' 'Iris-versicolor' 'Iris-virginica'\n 'Iris-versicolor' 'Iris-virginica']\n</pre> In\u00a0[\u00a0]: Copied! <pre># Para determinar a quantidade de acertos (acuracia)\n\nfrom sklearn.metrics import accuracy_score\nacertos = accuracy_score(classes_teste, classes_encontradas)\nprint(\"Acerto m\u00e9dio de classifica\u00e7\u00e3o: \", acertos)\n</pre> # Para determinar a quantidade de acertos (acuracia)  from sklearn.metrics import accuracy_score acertos = accuracy_score(classes_teste, classes_encontradas) print(\"Acerto m\u00e9dio de classifica\u00e7\u00e3o: \", acertos) <pre>Acerto m\u00e9dio de classifica\u00e7\u00e3o:  0.9666666666666667\n</pre> In\u00a0[\u00a0]: Copied! <pre># Criamos um modelo utilizando duas entradas e uma saida, logo temos que passar duas entradas para o modelo fa\u00e7a a predi\u00e7\u00e3o. \n\nmodelo.predict([[3.3, 3.2]])\n</pre> # Criamos um modelo utilizando duas entradas e uma saida, logo temos que passar duas entradas para o modelo fa\u00e7a a predi\u00e7\u00e3o.   modelo.predict([[3.3, 3.2]]) Out[\u00a0]: <pre>array(['Iris-versicolor'], dtype=object)</pre> In\u00a0[\u00a0]: Copied! <pre># Unificamos os dados de entrada e as classes de treino e teste em um daframe cada\ndf_treino = pd.concat((entradas_treino, classes_treino), axis=1)\n\nnovas_classes = pd.Series(classes_encontradas, name=\"species\", index=entradas_teste.index)\ndf_teste = pd.concat((entradas_teste, novas_classes), axis=1)\n</pre> # Unificamos os dados de entrada e as classes de treino e teste em um daframe cada df_treino = pd.concat((entradas_treino, classes_treino), axis=1)  novas_classes = pd.Series(classes_encontradas, name=\"species\", index=entradas_teste.index) df_teste = pd.concat((entradas_teste, novas_classes), axis=1) In\u00a0[\u00a0]: Copied! <pre>import seaborn as sns\n## Unificamos os dataframes de treinamento e teste em um novo DataFrame\n# indicando a origem dos dados\nnovo_df = pd.concat((df_treino, df_teste), keys=['train', 'test'])\nnovo_df['origin'] = ''\nnovo_df.loc['train','origin'] = 'Treino'\nnovo_df.loc['test','origin'] = 'Teste'\n\n# Usamos o scatterplot do seaborn, informando mudando o marcador de acordo com a origem do dado\nsns.scatterplot('petal_length', 'petal_width', hue='species', style='origin', data=novo_df)\n\nplt.show()\n</pre> import seaborn as sns ## Unificamos os dataframes de treinamento e teste em um novo DataFrame # indicando a origem dos dados novo_df = pd.concat((df_treino, df_teste), keys=['train', 'test']) novo_df['origin'] = '' novo_df.loc['train','origin'] = 'Treino' novo_df.loc['test','origin'] = 'Teste'  # Usamos o scatterplot do seaborn, informando mudando o marcador de acordo com a origem do dado sns.scatterplot('petal_length', 'petal_width', hue='species', style='origin', data=novo_df)  plt.show() <pre>c:\\Users\\junior\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n</pre> In\u00a0[\u00a0]: Copied! <pre>### Implemente sua sua solu\u00e7\u00e3o.....\n</pre> ### Implemente sua sua solu\u00e7\u00e3o.....       In\u00a0[\u00a0]: Copied! <pre>#### Resposta loop for para diferntes k\nk_range = list(range(1,26))\nacertos = []\nfor k in k_range:\n    modelo = KNeighborsClassifier(n_neighbors=k)\n    modelo.fit(entradas_treino, classes_treino)\n    classes_encontradas = modelo.predict(entradas_teste)\n    acertos.append(accuracy_score(classes_teste, classes_encontradas))\n  \n  \nplt.plot(k_range, acertos)\nplt.xlabel('Valor de k do KNN')\nplt.ylabel('Taxa de acertos')\nplt.title('Taxa de acertos x valor de k do KNN')\nplt.show()\n</pre> #### Resposta loop for para diferntes k k_range = list(range(1,26)) acertos = [] for k in k_range:     modelo = KNeighborsClassifier(n_neighbors=k)     modelo.fit(entradas_treino, classes_treino)     classes_encontradas = modelo.predict(entradas_teste)     acertos.append(accuracy_score(classes_teste, classes_encontradas))       plt.plot(k_range, acertos) plt.xlabel('Valor de k do KNN') plt.ylabel('Taxa de acertos') plt.title('Taxa de acertos x valor de k do KNN') plt.show()  In\u00a0[\u00a0]: Copied! <pre>## implemente sua sua solu\u00e7\u00e3o....\n</pre> ## implemente sua sua solu\u00e7\u00e3o....     In\u00a0[\u00a0]: Copied! <pre># implemente sua solu\u00e7\u00e3o......\n</pre> # implemente sua solu\u00e7\u00e3o......"},{"location":"aulas/IA/lab02/classificador-knn/#objetivos","title":"Objetivos\u00b6","text":"<ul> <li>Apresentar e utilizar o classificador k-nearest neighbours (kNN)</li> <li>Apresentar a t\u00e9cnica de separa\u00e7\u00e3o de dados (treino e teste)</li> <li>Avaliar Aprendizagem do modelo</li> </ul>"},{"location":"aulas/IA/lab02/classificador-knn/#comecando","title":"Come\u00e7ando\u00b6","text":"<p>Vamos dar continuidade ao nosso estudo de aprendizagem de m\u00e1quina, j\u00e1 vimos:</p> <ul> <li>Tudo come\u00e7a, conhecendo os dados dispon\u00edveis.</li> <li>Como carregar um data frame</li> <li>Como visualizar os dados em gr\u00e1ficos (histograma, box plot, violin plot, matriz de confus\u00e3o)</li> <li>Fizemos uma breve introdu\u00e7\u00e3o sobre an\u00e1lise explorat\u00f3ria buscando correlacionar os dados para gerar informa\u00e7\u00f5es.</li> </ul> <p>Hoje, vamos seguir nossa jornada e finalizar nosso estudo aplicando a t\u00e9cnica de KNN.</p>"},{"location":"aulas/IA/lab02/classificador-knn/#k-nearest-neighbors","title":"k-Nearest Neighbors\u00b6","text":"<p>O KNN(K vizinhos mais pr\u00f3ximos) \u00e9 considerado um dos algoritmos mais simples dentro da categoria de aprendizagem supervisionada sendo muito utilizado para problemas de classifica\u00e7\u00e3o, por\u00e9m tamb\u00e9m pode ser utilizado em problemas de regress\u00e3o.</p> <p>Problemas de classifica\u00e7\u00e3o = Vale lembrar que em problemas de classifica\u00e7\u00e3o n\u00e3o estamos interessados em valores exatos, queremos apenas saber se um dado pertence ou n\u00e3o a uma dada classe.</p>"},{"location":"aulas/IA/lab02/classificador-knn/#uma-intuicao-sobre-o-metodo","title":"Uma intui\u00e7\u00e3o sobre o m\u00e9todo\u00b6","text":"<p>Para realizar a classifica\u00e7\u00e3o o KNN calcula a dist\u00e2ncia objeto desconhecido (target) para todos os outros elementos, encontra os mais K vizinhos mais pr\u00f3ximos faz uma contagem dos r\u00f3tulos e considera que o objeto desconhecido pertence ao r\u00f3tulo de maior contagem.</p> <p>A imagem abaixo exemplifica o funcionamento, mas se ficou um pouco complicado de entender, rode o script python iknn.py e fa\u00e7a algumas simula\u00e7\u00f5es para compreender.</p> <p> </p>"},{"location":"aulas/IA/lab02/classificador-knn/#bora-la","title":"Bora l\u00e1!!\u00b6","text":"<p>Vamos juntos realizar nosso primeiro projeto, do come\u00e7o ao fim, de aprendizagem de m\u00e1quina.</p>"},{"location":"aulas/IA/lab02/classificador-knn/#definicao-do-problema","title":"Defini\u00e7\u00e3o do problema\u00b6","text":"<p>A primeira coisa que precisamos fazer \u00e9 a defini\u00e7\u00e3o do problema. Neste primeiro caso vamos trabalhar com o mesmo dataset da \u00faltima aula, dataset iris. Vamos desenvolver um sistema de machine learning capaz de classificar sua esp\u00e9cie com base nos dimensionais da p\u00e9tala.</p>"},{"location":"aulas/IA/lab02/classificador-knn/#desafio-1","title":"Desafio 1\u00b6","text":"<p>Do ponto de vista de machine learning, que problema \u00e9 esse:</p> <pre><code>Aprendizado supervisionado ou n\u00e3o-supervisionado?</code></pre> <p>R:</p> <pre><code>Classifica\u00e7\u00e3o ou regress\u00e3o?</code></pre> <p>R:</p>"},{"location":"aulas/IA/lab02/classificador-knn/#desafio-2","title":"Desafio 2\u00b6","text":"<p>Aplique os m\u00e9todos que achar conveniente (vimos algumas op\u00e7\u00f5es na \u00faltima aula) para visualizar os dados de forma gr\u00e1fica.</p>"},{"location":"aulas/IA/lab02/classificador-knn/#pare","title":"PARE!!!\u00b6","text":"<p>A an\u00e1lise feita no desafio 2 \u00e9 uma das etapas mais importantes. Caso voc\u00ea tenha pulado essa etapa, volte e fa\u00e7a suas an\u00e1lises.</p> <p>Com essa etapa conclu\u00edda, vamos criar um sub-dataset com os atributos que ser\u00e3o utilizados.</p>"},{"location":"aulas/IA/lab02/classificador-knn/#dividindo-os-dados-em-conjunto-de-treinamento-e-de-testes","title":"Dividindo os dados em conjunto de treinamento e de testes\u00b6","text":"<p>Dividir nosso dataset em dois conjuntos de dados.</p> <pre><code>Treinamento - Representa 80% das amostras do conjunto de dados original,\nTeste - com 20% das amostras</code></pre> <p>Vamos escolher aleatoriamente algumas amostras do conjunto original. Isto pode ser feito com Scikit-Learn usando a fun\u00e7\u00e3o train_test_split()</p> <p>scikit-learn: pip3 install scikit-learn</p>"},{"location":"aulas/IA/lab02/classificador-knn/#chegou-a-hora-de-aplicar-o-modelo-preditivo","title":"Chegou a hora de aplicar o modelo preditivo\u00b6","text":"<p>Treinar um modelo no python \u00e9 simples se usar o Scikit-Learn. Treinar um modelo no Scikit-Learn \u00e9 simples: basta criar o classificador, e chamar o m\u00e9todo fit().</p> <p>Uma observa\u00e7\u00e3o sobre a sintaxe dos classificadores do <code>scikit-learn</code></p> <ul> <li>O m\u00e9todo <code>fit(X,Y)</code> recebe uma matriz ou dataframe X onde cada linha \u00e9 uma amostra de aprendizado, e um array Y contendo as sa\u00eddas esperadas do classificador, seja na forma de texto ou de inteiros</li> <li>O m\u00e9todo <code>predict(X)</code> recebe uma matriz ou dataframe X onde cada linha \u00e9 uma amostra de teste, retornando um array de classes</li> </ul>"},{"location":"aulas/IA/lab02/classificador-knn/#utilizando-o-modelo-treinado-com-amostras-fora-do-dataset","title":"Utilizando o modelo treinado com amostras fora do dataset\u00b6","text":"<p>Vamos colocar alguns valores e ver a predi\u00e7\u00e3o do classificador.</p>"},{"location":"aulas/IA/lab02/classificador-knn/#visualizando-o-modelo-de-forma-grafica","title":"Visualizando o modelo de forma gr\u00e1fica\u00b6","text":""},{"location":"aulas/IA/lab02/classificador-knn/#desafio-3","title":"Desafio 3\u00b6","text":"<p>Fizemos o treinamento para k=3, mude o valor de k e an\u00e1lise a acur\u00e1cia do modelo.</p> <p>Dica: Fa\u00e7a um loop for que varre um range de k, a sa\u00edda pode ser armazenada em uma lista. No final do loop exiba em um gr\u00e1fico.</p>"},{"location":"aulas/IA/lab02/classificador-knn/#desafio-4","title":"Desafio 4\u00b6","text":"<p>Refa\u00e7a os notebook substituindo as entradas (variaveis independentes) e analise se o modelo obtido ficou melhor ou pior.</p>"},{"location":"aulas/IA/lab02/classificador-knn/#desafio-5","title":"Desafio 5\u00b6","text":"<p>Lembra o dataset 'breast_cancer', fa\u00e7a um modelo de predi\u00e7\u00e3o que informa se o c\u00e2ncer \u00e9 maligno ou n\u00e3o.</p>"},{"location":"aulas/IA/lab02/iknn/","title":"Iknn","text":"In\u00a0[\u00a0]: Copied! <pre># adaptado de: https://pysource.com/2018/05/22/k-nearest-neighbour-classification-opencv-3-4-with-python-3-tutorial-33/\nimport cv2\nimport numpy as np\n</pre> # adaptado de: https://pysource.com/2018/05/22/k-nearest-neighbour-classification-opencv-3-4-with-python-3-tutorial-33/ import cv2 import numpy as np In\u00a0[\u00a0]: Copied! <pre>def mouse_pos(event, x, y, flags, params):\n\tglobal squares, color, new_element\n\n\tif event == cv2.EVENT_LBUTTONDOWN:\n\t\tif color == \"b\":\n\t\t\tblue_squares.append([x, y])\n\t\telif color == \"r\":\n\t\t\tred_squares.append([x, y])\n\t\telse:\n\t\t\tnew_element = [x, y]\n</pre> def mouse_pos(event, x, y, flags, params): \tglobal squares, color, new_element  \tif event == cv2.EVENT_LBUTTONDOWN: \t\tif color == \"b\": \t\t\tblue_squares.append([x, y]) \t\telif color == \"r\": \t\t\tred_squares.append([x, y]) \t\telse: \t\t\tnew_element = [x, y] In\u00a0[\u00a0]: Copied! <pre># Create Window and Set mouse events\ncv2.namedWindow(\"KNN\")\ncv2.setMouseCallback(\"KNN\", mouse_pos)\n</pre> # Create Window and Set mouse events cv2.namedWindow(\"KNN\") cv2.setMouseCallback(\"KNN\", mouse_pos) In\u00a0[\u00a0]: Copied! <pre># Create an empty image\nimg = np.zeros([500, 700, 3], dtype=np.uint8)\nimg[:] = (255, 255, 255)\n</pre> # Create an empty image img = np.zeros([500, 700, 3], dtype=np.uint8) img[:] = (255, 255, 255) In\u00a0[\u00a0]: Copied! <pre># Load KNN algorythm\nknn = cv2.ml.KNearest_create()\n</pre> # Load KNN algorythm knn = cv2.ml.KNearest_create() In\u00a0[\u00a0]: Copied! <pre># Store all the elements\nblue_squares = []\nred_squares = []\nnew_element = []\nnew_comer = False\ncolor = \"b\"\n</pre> # Store all the elements blue_squares = [] red_squares = [] new_element = [] new_comer = False color = \"b\" In\u00a0[\u00a0]: Copied! <pre># Text Data\nfont = cv2.FONT_HERSHEY_SIMPLEX\nresult = \"None\"\nk = 1\nneighbours = \"None\"\ndist = \"None\"\nwhile True:\n\timg[:] = (255, 255, 255)\n    \n\tcv2.line(img, (0, 330),(700, 330),(0, 0, 0),2)\n\n\tcv2.putText(img, \"CALCULO KNN\", (10, 360), font, 1, (0, 0, 0), 2)\n\tcv2.putText(img, \"Resultado: \" + str(result), (10, 400), font, 1, (0, 0, 0), 2)\n\tcv2.putText(img, \"K: \" + str(k), (10, 440), font, 1, (0, 0, 0), 2)\n\tcv2.putText(img, \"Neighbours: \" + str(neighbours), (10, 470), font, 0.5, (0, 0, 0), 1)\n\tcv2.putText(img, \"Distance: \" + str(dist), (10, 490), font, 0.5, (0, 0, 0), 1)\n\n\tcv2.putText(img, \"Manual:\", (440, 350), font, 0.5, (0, 0, 0), 1)\n\tcv2.putText(img, \"B: Ponto AZUL\", (440, 370), font, 0.5, (0, 0, 0), 1)\n\tcv2.putText(img, \"R: Ponto Vermelho\", (440, 390), font, 0.5, (0, 0, 0), 1)\n\tcv2.putText(img, \"G: Ponto Verde\", (440, 410), font, 0.5, (0, 0, 0), 1)\n\tcv2.putText(img, \"1, 3, 5, 7, 9: muda K\", (440, 430), font, 0.5, (0, 0, 0), 1)\n\tcv2.putText(img, \"C: Calcula\", (440, 450), font, 0.5, (0, 0, 0), 1)\n\tcv2.putText(img, \"D: Deleta, limpa\", (440, 470), font, 0.5, (0, 0, 0), 1)\n\n\t# Show the Squares\n\tfor s in blue_squares:\n\t\tcv2.rectangle(img, (s[0] - 5, s[1] - 5), (s[0] + 5, s[1] + 5), (255, 0, 0), -1)\n\tfor s in red_squares:\n\t\tcv2.rectangle(img, (s[0] - 5, s[1] - 5), (s[0] + 5, s[1] + 5), (0, 0, 255), -1)\n\tif new_element != []:\n\t\tcv2.rectangle(img, (new_element[0] - 5, new_element[1] - 5),\n\t\t(new_element[0] + 5, new_element[1] + 5), (0, 255, 0), -1)\n\n\t# Create element to show\n\n\tcv2.imshow(\"KNN\", img)\n\n\t# Key events to break the loop and to select the color of the squares\n\tkey = cv2.waitKey(25)\n\tif key == 27 or key == ord(\"q\") :\n\t\tbreak\n\telif key == ord(\"b\"):\n\t\tcolor = \"b\"\n\telif key == ord(\"r\"):\n\t\tcolor = \"r\"\n\telif key == ord(\"g\"):\n\t\tcolor = \"g\"\n\t\tnew_comer = True\n\telif key == ord(\"1\"):\n\t\tk = 1\n\telif key == ord(\"2\"):\n\t\tk = 2\n\telif key == ord(\"3\"):\n\t\tk = 3\n\telif key == ord(\"4\"):\n\t\tk = 4\n\telif key == ord(\"5\"):\n\t\tk = 5\n\telif key == ord(\"6\"):\n\t\tk = 6\n\telif key == ord(\"7\"):\n\t\tk = 7\n\telif key == ord(\"8\"):\n\t\tk = 8\n\telif key == ord(\"9\"):\n\t\tk = 9\n\telif key == ord(\"d\"):\n\t\tblue_squares = []\n\t\tred_squares = []\n\t\tnew_element = []\n\telif key == ord(\"c\"):\n\t\ttraindata = np.array(blue_squares + red_squares, dtype=np.float32)\n\t\tblue_responses = np.zeros(len(blue_squares), dtype=np.float32)\n\t\tred_resposnes = np.ones(len(red_squares), dtype=np.float32)\n\t\tresponses = np.concatenate((blue_responses, red_resposnes))\n\n\t\tknn.train(traindata, cv2.ml.ROW_SAMPLE, responses)\n\t\tif new_comer:\n\t\t\tgreen_square = np.array([new_element], dtype=np.float32)\n\n\t\t\tret, results, neighbours, dist = knn.findNearest(green_square, k)\n\n\t\t\tprint(results[0][0])\n\t\t\tprint (results)\n\n\t\t\tif results[0][0] &gt; 0:\n\t\t\t\tresult = \"Red\"\n\t\t\telse:\n\t\t\t\tresult = \"Blue\"\n</pre> # Text Data font = cv2.FONT_HERSHEY_SIMPLEX result = \"None\" k = 1 neighbours = \"None\" dist = \"None\" while True: \timg[:] = (255, 255, 255)      \tcv2.line(img, (0, 330),(700, 330),(0, 0, 0),2)  \tcv2.putText(img, \"CALCULO KNN\", (10, 360), font, 1, (0, 0, 0), 2) \tcv2.putText(img, \"Resultado: \" + str(result), (10, 400), font, 1, (0, 0, 0), 2) \tcv2.putText(img, \"K: \" + str(k), (10, 440), font, 1, (0, 0, 0), 2) \tcv2.putText(img, \"Neighbours: \" + str(neighbours), (10, 470), font, 0.5, (0, 0, 0), 1) \tcv2.putText(img, \"Distance: \" + str(dist), (10, 490), font, 0.5, (0, 0, 0), 1)  \tcv2.putText(img, \"Manual:\", (440, 350), font, 0.5, (0, 0, 0), 1) \tcv2.putText(img, \"B: Ponto AZUL\", (440, 370), font, 0.5, (0, 0, 0), 1) \tcv2.putText(img, \"R: Ponto Vermelho\", (440, 390), font, 0.5, (0, 0, 0), 1) \tcv2.putText(img, \"G: Ponto Verde\", (440, 410), font, 0.5, (0, 0, 0), 1) \tcv2.putText(img, \"1, 3, 5, 7, 9: muda K\", (440, 430), font, 0.5, (0, 0, 0), 1) \tcv2.putText(img, \"C: Calcula\", (440, 450), font, 0.5, (0, 0, 0), 1) \tcv2.putText(img, \"D: Deleta, limpa\", (440, 470), font, 0.5, (0, 0, 0), 1)  \t# Show the Squares \tfor s in blue_squares: \t\tcv2.rectangle(img, (s[0] - 5, s[1] - 5), (s[0] + 5, s[1] + 5), (255, 0, 0), -1) \tfor s in red_squares: \t\tcv2.rectangle(img, (s[0] - 5, s[1] - 5), (s[0] + 5, s[1] + 5), (0, 0, 255), -1) \tif new_element != []: \t\tcv2.rectangle(img, (new_element[0] - 5, new_element[1] - 5), \t\t(new_element[0] + 5, new_element[1] + 5), (0, 255, 0), -1)  \t# Create element to show  \tcv2.imshow(\"KNN\", img)  \t# Key events to break the loop and to select the color of the squares \tkey = cv2.waitKey(25) \tif key == 27 or key == ord(\"q\") : \t\tbreak \telif key == ord(\"b\"): \t\tcolor = \"b\" \telif key == ord(\"r\"): \t\tcolor = \"r\" \telif key == ord(\"g\"): \t\tcolor = \"g\" \t\tnew_comer = True \telif key == ord(\"1\"): \t\tk = 1 \telif key == ord(\"2\"): \t\tk = 2 \telif key == ord(\"3\"): \t\tk = 3 \telif key == ord(\"4\"): \t\tk = 4 \telif key == ord(\"5\"): \t\tk = 5 \telif key == ord(\"6\"): \t\tk = 6 \telif key == ord(\"7\"): \t\tk = 7 \telif key == ord(\"8\"): \t\tk = 8 \telif key == ord(\"9\"): \t\tk = 9 \telif key == ord(\"d\"): \t\tblue_squares = [] \t\tred_squares = [] \t\tnew_element = [] \telif key == ord(\"c\"): \t\ttraindata = np.array(blue_squares + red_squares, dtype=np.float32) \t\tblue_responses = np.zeros(len(blue_squares), dtype=np.float32) \t\tred_resposnes = np.ones(len(red_squares), dtype=np.float32) \t\tresponses = np.concatenate((blue_responses, red_resposnes))  \t\tknn.train(traindata, cv2.ml.ROW_SAMPLE, responses) \t\tif new_comer: \t\t\tgreen_square = np.array([new_element], dtype=np.float32)  \t\t\tret, results, neighbours, dist = knn.findNearest(green_square, k)  \t\t\tprint(results[0][0]) \t\t\tprint (results)  \t\t\tif results[0][0] &gt; 0: \t\t\t\tresult = \"Red\" \t\t\telse: \t\t\t\tresult = \"Blue\" In\u00a0[\u00a0]: Copied! <pre>cv2.destroyAllWindows()\n</pre> cv2.destroyAllWindows()"},{"location":"aulas/IA/lab02/ml-classificador/","title":"Ml classificador","text":"In\u00a0[9]: Copied! <pre># Inicializ\u00e7\u00e3o das bibliotecas\n%matplotlib inline\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n</pre> # Inicializ\u00e7\u00e3o das bibliotecas %matplotlib inline  import pandas as pd import matplotlib.pyplot as plt  In\u00a0[10]: Copied! <pre># Caminho do arquivo\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n# Define o nome das colunas\nheader = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n# L\u00ea e carrega o arquivo para a mem\u00f3ria\ndf = pd.read_csv(url, header=None, names=header)\n</pre> # Caminho do arquivo url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" # Define o nome das colunas header = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'] # L\u00ea e carrega o arquivo para a mem\u00f3ria df = pd.read_csv(url, header=None, names=header) In\u00a0[11]: Copied! <pre># Retorna um trecho com as 5 primeiras linhas do dataframe\ndf.head()\n</pre> # Retorna um trecho com as 5 primeiras linhas do dataframe df.head() Out[11]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa In\u00a0[12]: Copied! <pre>df.tail()\n</pre> df.tail() Out[12]: sepal_length sepal_width petal_length petal_width species 145 6.7 3.0 5.2 2.3 Iris-virginica 146 6.3 2.5 5.0 1.9 Iris-virginica 147 6.5 3.0 5.2 2.0 Iris-virginica 148 6.2 3.4 5.4 2.3 Iris-virginica 149 5.9 3.0 5.1 1.8 Iris-virginica In\u00a0[13]: Copied! <pre># Mostra informa\u00e7\u00f5es sobre o dataframe em si\ndf.info()\n</pre> # Mostra informa\u00e7\u00f5es sobre o dataframe em si df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  150 non-null    float64\n 1   sepal_width   150 non-null    float64\n 2   petal_length  150 non-null    float64\n 3   petal_width   150 non-null    float64\n 4   species       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB\n</pre> In\u00a0[14]: Copied! <pre># class distribution\nprint(df.groupby('species').size())\n</pre> # class distribution print(df.groupby('species').size()) <pre>species\nIris-setosa        50\nIris-versicolor    50\nIris-virginica     50\ndtype: int64\n</pre> In\u00a0[7]: Copied! <pre>## Sua resposta e seus gr\u00e1ficos para an\u00e1lisar..\n</pre> ## Sua resposta e seus gr\u00e1ficos para an\u00e1lisar..       In\u00a0[15]: Copied! <pre># instalando a biblioteca scikit-learn\n\n!pip install scikit-learn\n</pre> # instalando a biblioteca scikit-learn  !pip install scikit-learn <pre>Collecting scikit-learn\n  Downloading scikit_learn-1.7.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\nRequirement already satisfied: numpy&gt;=1.22.0 in /Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages (from scikit-learn) (2.3.2)\nCollecting scipy&gt;=1.8.0 (from scikit-learn)\n  Using cached scipy-1.16.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (61 kB)\nCollecting joblib&gt;=1.2.0 (from scikit-learn)\n  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\nCollecting threadpoolctl&gt;=3.1.0 (from scikit-learn)\n  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\nDownloading scikit_learn-1.7.1-cp312-cp312-macosx_12_0_arm64.whl (8.6 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8.6/8.6 MB 14.2 MB/s  0:00:00 eta 0:00:01\nUsing cached joblib-1.5.1-py3-none-any.whl (307 kB)\nUsing cached scipy-1.16.1-cp312-cp312-macosx_14_0_arm64.whl (20.9 MB)\nUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4/4 [scikit-learn][0m [scikit-learn]\nSuccessfully installed joblib-1.5.1 scikit-learn-1.7.1 scipy-1.16.1 threadpoolctl-3.6.0\n</pre> In\u00a0[16]: Copied! <pre># Codificando os r\u00f3tulos de especies\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndf['species'] = le.fit_transform(df['species'])\n</pre> # Codificando os r\u00f3tulos de especies  from sklearn.preprocessing import LabelEncoder  le = LabelEncoder() df['species'] = le.fit_transform(df['species']) In\u00a0[18]: Copied! <pre># Verificando o dataframe ap\u00f3s a codifica\u00e7\u00e3o\ndf.head()\n</pre> # Verificando o dataframe ap\u00f3s a codifica\u00e7\u00e3o df.head() Out[18]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 3 4.6 3.1 1.5 0.2 0 4 5.0 3.6 1.4 0.2 0 In\u00a0[20]: Copied! <pre>df.groupby('species').size()\n</pre> df.groupby('species').size() Out[20]: <pre>species\n0    50\n1    50\n2    50\ndtype: int64</pre> In\u00a0[21]: Copied! <pre># Separamos 20% para o teste\nfrom sklearn.model_selection import train_test_split\n\n## define entradas de dados e o target\n\nX = df.iloc[:, :-1]\ny = df['species']\n\n# Separando os dados em conjunto de treinamento e teste\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n\nprint(f\"Formato das tabelas de dados de treino {X_train.shape} e teste {y_train.shape}\")\n</pre> # Separamos 20% para o teste from sklearn.model_selection import train_test_split  ## define entradas de dados e o target  X = df.iloc[:, :-1] y = df['species']  # Separando os dados em conjunto de treinamento e teste X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)  print(f\"Formato das tabelas de dados de treino {X_train.shape} e teste {y_train.shape}\") <pre>Formato das tabelas de dados de treino (120, 4) e teste (120,)\n</pre> In\u00a0[22]: Copied! <pre>#Primeiras linhas do dataframe de treino \nX_train.head()\n</pre> #Primeiras linhas do dataframe de treino  X_train.head() Out[22]: sepal_length sepal_width petal_length petal_width 22 4.6 3.6 1.0 0.2 15 5.7 4.4 1.5 0.4 65 6.7 3.1 4.4 1.4 11 4.8 3.4 1.6 0.2 42 4.4 3.2 1.3 0.2 In\u00a0[23]: Copied! <pre>y_train.tail()\n</pre> y_train.tail() Out[23]: <pre>71     1\n106    2\n14     0\n92     1\n102    2\nName: species, dtype: int64</pre> In\u00a0[24]: Copied! <pre># Importa a biblioteca\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Cria o classificar KNN\nk = 3\nknn = KNeighborsClassifier(n_neighbors=k)\n\n# Cria o modelo de machine learning\nknn.fit(X_train, y_train)\n</pre> # Importa a biblioteca from sklearn.neighbors import KNeighborsClassifier  # Cria o classificar KNN k = 3 knn = KNeighborsClassifier(n_neighbors=k)  # Cria o modelo de machine learning knn.fit(X_train, y_train) Out[24]: <pre>KNeighborsClassifier(n_neighbors=3)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted Parameters n_neighbors\u00a0 3 weights\u00a0 'uniform' algorithm\u00a0 'auto' leaf_size\u00a0 30 p\u00a0 2 metric\u00a0 'minkowski' metric_params\u00a0 None n_jobs\u00a0 None <p>Pronto!! bora testar se esta funcionando....</p> <p>Vamos fazer predi\u00e7\u00f5es para os dados do conjunto de teste</p> In\u00a0[27]: Copied! <pre># Realizando previs\u00f5es\n\ny_pred = knn.predict(X_test)\n\n# vendo as previs\u00f5es\nprint(y_pred)\n</pre> # Realizando previs\u00f5es  y_pred = knn.predict(X_test)  # vendo as previs\u00f5es print(y_pred)  <pre>[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n</pre> In\u00a0[34]: Copied! <pre># novas previs\u00f5es\nsepal_length = 5.1\nsepal_width = 3.5\npetal_length = 1.4\npetal_width = 0.2\n\ndado = [[sepal_length, sepal_width, petal_length, petal_width]]\n\nnova_previsao = knn.predict(dado)\n\nprint(\"Novas previs\u00f5es:\", nova_previsao)\n\n# com o r\u00f3tulo original\nprint(\"R\u00f3tulo original:\", le.inverse_transform(nova_previsao))  \n</pre> # novas previs\u00f5es sepal_length = 5.1 sepal_width = 3.5 petal_length = 1.4 petal_width = 0.2  dado = [[sepal_length, sepal_width, petal_length, petal_width]]  nova_previsao = knn.predict(dado)  print(\"Novas previs\u00f5es:\", nova_previsao)  # com o r\u00f3tulo original print(\"R\u00f3tulo original:\", le.inverse_transform(nova_previsao))   <pre>Novas previs\u00f5es: [0]\nR\u00f3tulo original: ['Iris-setosa']\n</pre> <pre>/Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n  warnings.warn(\n</pre> In\u00a0[35]: Copied! <pre># a probabilidade da predi\u00e7\u00e3o\nprob = knn.predict_proba(dado)\nprint(\"Probabilidade da predi\u00e7\u00e3o:\", prob)\n</pre> # a probabilidade da predi\u00e7\u00e3o prob = knn.predict_proba(dado) print(\"Probabilidade da predi\u00e7\u00e3o:\", prob) <pre>Probabilidade da predi\u00e7\u00e3o: [[1. 0. 0.]]\n</pre> <pre>/Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n  warnings.warn(\n</pre> In\u00a0[36]: Copied! <pre># Avaliando o modelo\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nprint(\"Acur\u00e1cia: \", accuracy_score(y_test, y_pred))\nprint(\"Precis\u00e3o: \", precision_score(y_test, y_pred, average='macro'))\nprint(\"Recall: \", recall_score(y_test, y_pred, average='macro'))\nprint(\"F1-score: \", f1_score(y_test, y_pred, average='macro'))\n\n## average='macro': m\u00e9trica \u00e9 calculada para cada classe individualmente e, em seguida, a m\u00e9dia n\u00e3o ponderada das m\u00e9tricas de cada classe \u00e9 retornada.\n##  Isso significa que todas as classes t\u00eam a mesma import\u00e2ncia no c\u00e1lculo da m\u00e9trica.\n</pre>  # Avaliando o modelo from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  print(\"Acur\u00e1cia: \", accuracy_score(y_test, y_pred)) print(\"Precis\u00e3o: \", precision_score(y_test, y_pred, average='macro')) print(\"Recall: \", recall_score(y_test, y_pred, average='macro')) print(\"F1-score: \", f1_score(y_test, y_pred, average='macro'))  ## average='macro': m\u00e9trica \u00e9 calculada para cada classe individualmente e, em seguida, a m\u00e9dia n\u00e3o ponderada das m\u00e9tricas de cada classe \u00e9 retornada. ##  Isso significa que todas as classes t\u00eam a mesma import\u00e2ncia no c\u00e1lculo da m\u00e9trica. <pre>Acur\u00e1cia:  1.0\nPrecis\u00e3o:  1.0\nRecall:  1.0\nF1-score:  1.0\n</pre> In\u00a0[37]: Copied! <pre>from sklearn.model_selection import GridSearchCV\n\n\n# Definindo os valores para os hiperpar\u00e2metros\nparam_grid = {'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance'], 'algorithm': ['ball_tree', 'kd_tree', 'brute'], 'p': [1, 2]}\n\n# Criando o objeto GridSearchCV\ngrid = GridSearchCV(KNeighborsClassifier(), param_grid, verbose=1, cv=5, n_jobs=-1)\n\n# Ajustando o modelo com os dados de treinamento\ngrid.fit(X_train, y_train)\n\n# Imprimindo os melhores hiperpar\u00e2metros encontrados\nprint(\"Melhores hiperpar\u00e2metros: \", grid.best_params_)\n\n# Realizando previs\u00f5es e avaliando o modelo com os melhores hiperpar\u00e2metros\ny_pred = grid.predict(X_test)\nprint(\"Acur\u00e1cia: \", accuracy_score(y_test, y_pred))\nprint(\"Precis\u00e3o: \", precision_score(y_test, y_pred, average='macro'))\nprint(\"Recall: \", recall_score(y_test, y_pred, average='macro'))\nprint(\"F1-score: \", f1_score(y_test, y_pred, average='macro'))\n</pre> from sklearn.model_selection import GridSearchCV   # Definindo os valores para os hiperpar\u00e2metros param_grid = {'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance'], 'algorithm': ['ball_tree', 'kd_tree', 'brute'], 'p': [1, 2]}  # Criando o objeto GridSearchCV grid = GridSearchCV(KNeighborsClassifier(), param_grid, verbose=1, cv=5, n_jobs=-1)  # Ajustando o modelo com os dados de treinamento grid.fit(X_train, y_train)  # Imprimindo os melhores hiperpar\u00e2metros encontrados print(\"Melhores hiperpar\u00e2metros: \", grid.best_params_)  # Realizando previs\u00f5es e avaliando o modelo com os melhores hiperpar\u00e2metros y_pred = grid.predict(X_test) print(\"Acur\u00e1cia: \", accuracy_score(y_test, y_pred)) print(\"Precis\u00e3o: \", precision_score(y_test, y_pred, average='macro')) print(\"Recall: \", recall_score(y_test, y_pred, average='macro')) print(\"F1-score: \", f1_score(y_test, y_pred, average='macro')) <pre>Fitting 5 folds for each of 48 candidates, totalling 240 fits\nMelhores hiperpar\u00e2metros:  {'algorithm': 'ball_tree', 'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}\nAcur\u00e1cia:  1.0\nPrecis\u00e3o:  1.0\nRecall:  1.0\nF1-score:  1.0\n</pre> In\u00a0[38]: Copied! <pre>from sklearn.tree import DecisionTreeClassifier\n\n# Definindo o modelo de \u00e1rvore de decis\u00e3o com hiperpar\u00e2metros padr\u00e3o\ntree = DecisionTreeClassifier(criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=42)\ntree.fit(X_train, y_train)\n\n# Realizando previs\u00f5es e avaliando o modelo\ny_pred = tree.predict(X_test)\nprint(\"Acur\u00e1cia: \", accuracy_score(y_test, y_pred))\nprint(\"Precis\u00e3o: \", precision_score(y_test, y_pred, average='macro'))\nprint(\"Recall: \", recall_score(y_test, y_pred, average='macro'))\nprint(\"F1-score: \", f1_score(y_test, y_pred, average='macro'))\n</pre> from sklearn.tree import DecisionTreeClassifier  # Definindo o modelo de \u00e1rvore de decis\u00e3o com hiperpar\u00e2metros padr\u00e3o tree = DecisionTreeClassifier(criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=42) tree.fit(X_train, y_train)  # Realizando previs\u00f5es e avaliando o modelo y_pred = tree.predict(X_test) print(\"Acur\u00e1cia: \", accuracy_score(y_test, y_pred)) print(\"Precis\u00e3o: \", precision_score(y_test, y_pred, average='macro')) print(\"Recall: \", recall_score(y_test, y_pred, average='macro')) print(\"F1-score: \", f1_score(y_test, y_pred, average='macro')) <pre>Acur\u00e1cia:  1.0\nPrecis\u00e3o:  1.0\nRecall:  1.0\nF1-score:  1.0\n</pre> In\u00a0[39]: Copied! <pre># Definindo os valores para os hiperpar\u00e2metros\nparam_grid = {'criterion': ['gini', 'entropy'], 'max_depth': [None, 3, 5, 7], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n\n# Criando o objeto GridSearchCV\ngrid = GridSearchCV(DecisionTreeClassifier(), param_grid, verbose=1, cv=5, n_jobs=-1)\n\n# Ajustando o modelo com os dados de treinamento\ngrid.fit(X_train, y_train)\n\n# Imprimindo os melhores hiperpar\u00e2metros encontrados\nprint(\"Melhores hiperpar\u00e2metros: \", grid.best_params_)\n\n# Realizando previs\u00f5es e avaliando o modelo com os melhores hiperpar\u00e2metros\ny_pred = grid.predict(X_test)\nprint(\"Acur\u00e1cia: \", accuracy_score(y_test, y_pred))\nprint(\"Precis\u00e3o: \", precision_score(y_test, y_pred, average='macro'))\nprint(\"Recall: \", recall_score(y_test, y_pred, average='macro'))\nprint(\"F1-score: \", f1_score(y_test, y_pred, average='macro'))\n</pre> # Definindo os valores para os hiperpar\u00e2metros param_grid = {'criterion': ['gini', 'entropy'], 'max_depth': [None, 3, 5, 7], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}  # Criando o objeto GridSearchCV grid = GridSearchCV(DecisionTreeClassifier(), param_grid, verbose=1, cv=5, n_jobs=-1)  # Ajustando o modelo com os dados de treinamento grid.fit(X_train, y_train)  # Imprimindo os melhores hiperpar\u00e2metros encontrados print(\"Melhores hiperpar\u00e2metros: \", grid.best_params_)  # Realizando previs\u00f5es e avaliando o modelo com os melhores hiperpar\u00e2metros y_pred = grid.predict(X_test) print(\"Acur\u00e1cia: \", accuracy_score(y_test, y_pred)) print(\"Precis\u00e3o: \", precision_score(y_test, y_pred, average='macro')) print(\"Recall: \", recall_score(y_test, y_pred, average='macro')) print(\"F1-score: \", f1_score(y_test, y_pred, average='macro')) <pre>Fitting 5 folds for each of 72 candidates, totalling 360 fits\nMelhores hiperpar\u00e2metros:  {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\nAcur\u00e1cia:  1.0\nPrecis\u00e3o:  1.0\nRecall:  1.0\nF1-score:  1.0\n</pre> In\u00a0[40]: Copied! <pre># Criando os modelos ( altere os parametros para os melhores hiperpar\u00e2metros encontrados)\nknn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='ball_tree', p=2) \ntree = DecisionTreeClassifier(criterion='entropy', max_depth=None, min_samples_split=2, min_samples_leaf=1)\n\n# Ajustando os modelos com os dados de treinamento\nknn.fit(X_train, y_train)\ntree.fit(X_train, y_train)\n\n# Realizando previs\u00f5es e avaliando os modelos com os dados de teste\nknn_pred = knn.predict(X_test)\ntree_pred = tree.predict(X_test)\n\nprint(\"KNN - Acur\u00e1cia: \", accuracy_score(y_test, knn_pred))\nprint(\"KNN - Precis\u00e3o: \", precision_score(y_test, knn_pred, average='macro'))\nprint(\"KNN - Recall: \", recall_score(y_test, knn_pred, average='macro'))\nprint(\"KNN - F1-score: \", f1_score(y_test, knn_pred, average='macro'))\n\nprint(\"\u00c1rvore de Decis\u00e3o - Acur\u00e1cia: \", accuracy_score(y_test, tree_pred))\nprint(\"\u00c1rvore de Decis\u00e3o - Precis\u00e3o: \", precision_score(y_test, tree_pred, average='macro'))\nprint(\"\u00c1rvore de Decis\u00e3o - Recall: \", recall_score(y_test, tree_pred, average='macro'))\nprint(\"\u00c1rvore de Decis\u00e3o - F1-score: \", f1_score(y_test, tree_pred, average='macro'))\n</pre> # Criando os modelos ( altere os parametros para os melhores hiperpar\u00e2metros encontrados) knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='ball_tree', p=2)  tree = DecisionTreeClassifier(criterion='entropy', max_depth=None, min_samples_split=2, min_samples_leaf=1)  # Ajustando os modelos com os dados de treinamento knn.fit(X_train, y_train) tree.fit(X_train, y_train)  # Realizando previs\u00f5es e avaliando os modelos com os dados de teste knn_pred = knn.predict(X_test) tree_pred = tree.predict(X_test)  print(\"KNN - Acur\u00e1cia: \", accuracy_score(y_test, knn_pred)) print(\"KNN - Precis\u00e3o: \", precision_score(y_test, knn_pred, average='macro')) print(\"KNN - Recall: \", recall_score(y_test, knn_pred, average='macro')) print(\"KNN - F1-score: \", f1_score(y_test, knn_pred, average='macro'))  print(\"\u00c1rvore de Decis\u00e3o - Acur\u00e1cia: \", accuracy_score(y_test, tree_pred)) print(\"\u00c1rvore de Decis\u00e3o - Precis\u00e3o: \", precision_score(y_test, tree_pred, average='macro')) print(\"\u00c1rvore de Decis\u00e3o - Recall: \", recall_score(y_test, tree_pred, average='macro')) print(\"\u00c1rvore de Decis\u00e3o - F1-score: \", f1_score(y_test, tree_pred, average='macro')) <pre>KNN - Acur\u00e1cia:  1.0\nKNN - Precis\u00e3o:  1.0\nKNN - Recall:  1.0\nKNN - F1-score:  1.0\n\u00c1rvore de Decis\u00e3o - Acur\u00e1cia:  1.0\n\u00c1rvore de Decis\u00e3o - Precis\u00e3o:  1.0\n\u00c1rvore de Decis\u00e3o - Recall:  1.0\n\u00c1rvore de Decis\u00e3o - F1-score:  1.0\n</pre> In\u00a0[41]: Copied! <pre>from sklearn.datasets import load_digits\n\ndigits = load_digits()\nX, y = digits.data, digits.target\n</pre> from sklearn.datasets import load_digits  digits = load_digits() X, y = digits.data, digits.target  In\u00a0[42]: Copied! <pre># Separando os dados em conjunto de treinamento e teste\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\nprint(f\"Formato das tabelas de dados de treino {X_train.shape} e teste {y_train.shape}\")\n</pre> # Separando os dados em conjunto de treinamento e teste X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)  print(f\"Formato das tabelas de dados de treino {X_train.shape} e teste {y_train.shape}\") <pre>Formato das tabelas de dados de treino (1437, 64) e teste (1437,)\n</pre> In\u00a0[49]: Copied! <pre># Criando os modelos ( altere os parametros para os melhores hiperpar\u00e2metros encontrados)\nknn = KNeighborsClassifier(n_neighbors=3, weights='uniform', algorithm='ball_tree', p=2) \ntree = DecisionTreeClassifier(criterion='entropy', max_depth=None, min_samples_split=2, min_samples_leaf=1)\n\n# Ajustando os modelos com os dados de treinamento\nknn.fit(X_train, y_train)\ntree.fit(X_train, y_train)\n\n# Realizando previs\u00f5es e avaliando os modelos com os dados de teste\nknn_pred = knn.predict(X_test)\ntree_pred = tree.predict(X_test)\n\nprint(\"KNN - Acur\u00e1cia: \", accuracy_score(y_test, knn_pred))\nprint(\"KNN - Precis\u00e3o: \", precision_score(y_test, knn_pred, average='macro'))\nprint(\"KNN - Recall: \", recall_score(y_test, knn_pred, average='macro'))\nprint(\"KNN - F1-score: \", f1_score(y_test, knn_pred, average='macro'))\n\nprint(\"\u00c1rvore de Decis\u00e3o - Acur\u00e1cia: \", accuracy_score(y_test, tree_pred))\nprint(\"\u00c1rvore de Decis\u00e3o - Precis\u00e3o: \", precision_score(y_test, tree_pred, average='macro'))\nprint(\"\u00c1rvore de Decis\u00e3o - Recall: \", recall_score(y_test, tree_pred, average='macro'))\nprint(\"\u00c1rvore de Decis\u00e3o - F1-score: \", f1_score(y_test, tree_pred, average='macro'))\n</pre> # Criando os modelos ( altere os parametros para os melhores hiperpar\u00e2metros encontrados) knn = KNeighborsClassifier(n_neighbors=3, weights='uniform', algorithm='ball_tree', p=2)  tree = DecisionTreeClassifier(criterion='entropy', max_depth=None, min_samples_split=2, min_samples_leaf=1)  # Ajustando os modelos com os dados de treinamento knn.fit(X_train, y_train) tree.fit(X_train, y_train)  # Realizando previs\u00f5es e avaliando os modelos com os dados de teste knn_pred = knn.predict(X_test) tree_pred = tree.predict(X_test)  print(\"KNN - Acur\u00e1cia: \", accuracy_score(y_test, knn_pred)) print(\"KNN - Precis\u00e3o: \", precision_score(y_test, knn_pred, average='macro')) print(\"KNN - Recall: \", recall_score(y_test, knn_pred, average='macro')) print(\"KNN - F1-score: \", f1_score(y_test, knn_pred, average='macro'))  print(\"\u00c1rvore de Decis\u00e3o - Acur\u00e1cia: \", accuracy_score(y_test, tree_pred)) print(\"\u00c1rvore de Decis\u00e3o - Precis\u00e3o: \", precision_score(y_test, tree_pred, average='macro')) print(\"\u00c1rvore de Decis\u00e3o - Recall: \", recall_score(y_test, tree_pred, average='macro')) print(\"\u00c1rvore de Decis\u00e3o - F1-score: \", f1_score(y_test, tree_pred, average='macro'))    <pre>KNN - Acur\u00e1cia:  0.9833333333333333\nKNN - Precis\u00e3o:  0.9829739828748038\nKNN - Recall:  0.9829214303222512\nKNN - F1-score:  0.982914400577056\n\u00c1rvore de Decis\u00e3o - Acur\u00e1cia:  0.8444444444444444\n\u00c1rvore de Decis\u00e3o - Precis\u00e3o:  0.8419715923353694\n\u00c1rvore de Decis\u00e3o - Recall:  0.8462637284492087\n\u00c1rvore de Decis\u00e3o - F1-score:  0.8429769564839328\n</pre> In\u00a0[50]: Copied! <pre>from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, knn_pred))\nprint(classification_report(y_test, tree_pred))\n</pre> from sklearn.metrics import classification_report  print(classification_report(y_test, knn_pred)) print(classification_report(y_test, tree_pred)) <pre>              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        39\n           1       0.97      1.00      0.99        39\n           2       1.00      1.00      1.00        36\n           3       0.98      0.98      0.98        43\n           4       1.00      1.00      1.00        29\n           5       0.97      0.97      0.97        34\n           6       1.00      1.00      1.00        37\n           7       1.00      1.00      1.00        39\n           8       0.96      0.96      0.96        27\n           9       0.94      0.92      0.93        37\n\n    accuracy                           0.98       360\n   macro avg       0.98      0.98      0.98       360\nweighted avg       0.98      0.98      0.98       360\n\n              precision    recall  f1-score   support\n\n           0       0.97      0.95      0.96        39\n           1       0.82      0.72      0.77        39\n           2       0.81      0.83      0.82        36\n           3       0.85      0.79      0.82        43\n           4       0.80      0.83      0.81        29\n           5       0.88      0.88      0.88        34\n           6       0.86      0.86      0.86        37\n           7       0.88      0.90      0.89        39\n           8       0.75      0.89      0.81        27\n           9       0.79      0.81      0.80        37\n\n    accuracy                           0.84       360\n   macro avg       0.84      0.85      0.84       360\nweighted avg       0.85      0.84      0.84       360\n\n</pre> In\u00a0[\u00a0]: Copied! <pre># sua resposta aqui...\n</pre> # sua resposta aqui..."},{"location":"aulas/IA/lab02/ml-classificador/#objetivos","title":"Objetivos\u00b6","text":"<ul> <li>Conhecer algoritmos de classifica\u00e7\u00e3o, K-Nearest Neighbors (KNN) e \u00c1rvore de Decis\u00e3o;</li> <li>Como aplicar o Grid Search para encontrar os melhores par\u00e2metros para esses modelos.</li> </ul>"},{"location":"aulas/IA/lab02/ml-classificador/#problemas-de-classicacao","title":"Problemas de classica\u00e7\u00e3o\u00b6","text":"<p>A classifica\u00e7\u00e3o \u00e9 uma das principais tarefas em aprendizado de m\u00e1quina e envolve a atribui\u00e7\u00e3o de um r\u00f3tulo ou categoria a um conjunto de dados. Por exemplo, podemos usar a classifica\u00e7\u00e3o para identificar se um e-mail \u00e9 spam ou n\u00e3o, se uma transa\u00e7\u00e3o financeira \u00e9 fraudulenta ou leg\u00edtima, ou para prever se um paciente desenvolver\u00e1 uma doen\u00e7a com base em seus dados m\u00e9dicos.</p> <p>Existem muitos desafios em problemas de classica\u00e7\u00e3o que o an\u00e1lista de dados deve levar em considera\u00e7\u00e3o, como a escolha do modelo de aprendizado de m\u00e1quina adequado, o ajuste dos hiperpar\u00e2metros do modelo, a sele\u00e7\u00e3o de features relevantes, o tratamento de dados adequado ao problema, o cuidado com overfitting e a avalia\u00e7\u00e3o correta do modelo.</p> <p>Al\u00e9m disso, diferentes modelos de classifica\u00e7\u00e3o t\u00eam seus pr\u00f3prios pontos fortes e fracos, e escolher o modelo certo para um problema espec\u00edfico pode ser dif\u00edcil.</p>"},{"location":"aulas/IA/lab02/ml-classificador/#resumo-sobre-knn-e-arvore-de-decisao","title":"Resumo sobre KNN e \u00c1rvore de Decis\u00e3o\u00b6","text":"Algoritmo Aplica\u00e7\u00e3o Vantagens Desvantagens Contexto de uso \u00c1rvores de Decis\u00e3o Classifica\u00e7\u00e3o/Regress\u00e3o F\u00e1cil interpretabilidade, lida bem com dados faltantes, captura rela\u00e7\u00f5es n\u00e3o lineares Tend\u00eancia ao overfitting, pode ser sens\u00edvel a ru\u00eddo Problemas de classifica\u00e7\u00e3o/regress\u00e3o com rela\u00e7\u00f5es n\u00e3o lineares K-Nearest Neighbors (KNN) Classifica\u00e7\u00e3o/Regress\u00e3o F\u00e1cil de entender e implementar, lida bem com dados com muitas vari\u00e1veis de entrada Requer muita mem\u00f3ria para grandes conjuntos de dados, sens\u00edvel a outliers Problemas de classifica\u00e7\u00e3o/regress\u00e3o com muitas vari\u00e1veis de entrada e poucas classes poss\u00edveis <p>O KNN (K-Nearest Neighbors) \u00e9 um algoritmo de classifica\u00e7\u00e3o simples e popular que usa a dist\u00e2ncia entre pontos para classificar novos dados. O algoritmo funciona encontrando os \"k\" pontos mais pr\u00f3ximos a uma nova inst\u00e2ncia de dados e, em seguida, atribuindo a essa inst\u00e2ncia a classe mais comum entre seus k vizinhos mais pr\u00f3ximos. O valor de k \u00e9 um hiperpar\u00e2metro que pode ser ajustado para melhorar a precis\u00e3o do modelo.</p> <p>J\u00e1 a \u00c1rvore de Decis\u00e3o \u00e9 um algoritmo de classifica\u00e7\u00e3o/regress\u00e3o que usa uma estrutura em forma de \u00e1rvore para representar poss\u00edveis decis\u00f5es e seus resultados. A \u00e1rvore \u00e9 constru\u00edda recursivamente, come\u00e7ando com um n\u00f3 raiz que cont\u00e9m todo o conjunto de dados. Em seguida, o algoritmo seleciona a melhor vari\u00e1vel para dividir o conjunto de dados em dois, com base em algum crit\u00e9rio de impureza, como a entropia ou o \u00edndice Gini. Esse processo \u00e9 repetido para cada subconjunto de dados resultante, criando um ramo de decis\u00e3o na \u00e1rvore. Quando a \u00e1rvore \u00e9 constru\u00edda, novos dados podem ser classificados percorrendo a \u00e1rvore de decis\u00e3o a partir da raiz at\u00e9 chegar a uma folha, onde a classe \u00e9 atribu\u00edda.</p>"},{"location":"aulas/IA/lab02/ml-classificador/#definicao-do-problema","title":"Defini\u00e7\u00e3o do problema\u00b6","text":"<p>A primeira coisa que precisamos fazer \u00e9 a defini\u00e7\u00e3o do problema. Neste primeiro caso vamos trabalhar com o mesmo dataset da \u00faltima aula, dataset iris. Vamos desenvolver um sistema de machine learning capaz de classificar a especie de flor Iris com base nos dimensionais da p\u00e9tala e sepala.</p>"},{"location":"aulas/IA/lab02/ml-classificador/#relembrando-o-dataset-iris","title":"Relembrando o dataset Iris\u00b6","text":"<p>Iris \u00e9 um dataset de flor com 150 linhas, divididos em tr\u00eas esp\u00e9cies diferentes: setosa, versicolor e virginica, sendo 50 amostras de cada esp\u00e9cie. Os atributos de largura e comprimento de s\u00e9pala e largura e comprimento de p\u00e9tala de cada flor foram anotados manualmente.</p>"},{"location":"aulas/IA/lab02/ml-classificador/#desafio-1","title":"Desafio 1\u00b6","text":"<p>Aplique os m\u00e9todos que achar conveniente (vimos algumas op\u00e7\u00f5es na \u00faltima aula) para visualizar os dados de forma gr\u00e1fica.</p>"},{"location":"aulas/IA/lab02/ml-classificador/#pare","title":"PARE!!!\u00b6","text":"<p>A an\u00e1lise feita no desafio 1 \u00e9 uma das etapas mais importantes. Caso voc\u00ea tenha pulado essa etapa, volte e fa\u00e7a suas an\u00e1lises.</p>"},{"location":"aulas/IA/lab02/ml-classificador/#scikit-learn","title":"scikit-learn\u00b6","text":"<p>Tamb\u00e9m conhecida como sklearn, \u00e9 uma biblioteca de c\u00f3digo aberto (open source) para a linguagem Python, essa biblioteca fornece uma ampla gama de ferramentas e algoritmos de aprendizado de m\u00e1quina de forma acess\u00edvel e eficiente.</p> <p></p> <p>Podemos dizer que o scikit-learn \u00e9 a ferramenta mais popular e utilizada em aprendizado de m\u00e1quina e existem alguns motivos para isso, tais como:</p> <ul> <li>Simplicidade e Consist\u00eancia: Todos os algoritmos no scikit-learn compartilham uma interface consistente, que segue um padr\u00e3o de m\u00e9todos como fit(), predict(), e transform(). Isso facilita a troca e experimenta\u00e7\u00e3o de diferentes algoritmos, promovendo um ambiente de desenvolvimento r\u00e1pido e eficiente.</li> <li>Compatibilidade e Integra\u00e7\u00e3o: O scikit-learn \u00e9 projetado para interoperabilidade com outras bibliotecas do ecossistema Python, como pandas para manipula\u00e7\u00e3o de dados, numpy para opera\u00e7\u00f5es num\u00e9ricas, e matplotlib para visualiza\u00e7\u00e3o. Essa integra\u00e7\u00e3o permite uma f\u00e1cil manipula\u00e7\u00e3o e transforma\u00e7\u00e3o de dados, bem como a visualiza\u00e7\u00e3o de resultados de modelos.</li> <li>Foco em Performance e Efici\u00eancia Computacional: Embora Python seja uma linguagem interpretada, a scikit-learn faz uso extensivo de opera\u00e7\u00f5es vetorizadas atrav\u00e9s de numpy e implementa\u00e7\u00f5es otimizadas em Cython (que \u00e9 uma linguagem de programa\u00e7\u00e3o que facilita a escrita de extens\u00f5es em C para Python) para acelerar o processamento de dados e a execu\u00e7\u00e3o de algoritmos. Isso proporciona uma execu\u00e7\u00e3o eficiente de tarefas complexas de machine learning, como treinamento de modelos e ajuste de hiperpar\u00e2metros.</li> </ul>"},{"location":"aulas/IA/lab02/ml-classificador/#instalacao","title":"Instala\u00e7\u00e3o\u00b6","text":"<pre>pip install scikit-learn\n</pre> <p>Saiba mais: site da documenta\u00e7\u00e3o oficial https://scikit-learn.org/stable/</p>"},{"location":"aulas/IA/lab02/ml-classificador/#codificando-os-dados","title":"Codificando os dados\u00b6","text":"<p>Com essa etapa conclu\u00edda, vamos codificar os r\u00f3tulos de especie para que possam ser usados pelos modelos que vamos treinar.</p>"},{"location":"aulas/IA/lab02/ml-classificador/#labelencoder","title":"LabelEncoder\u00b6","text":"<ul> <li>LabelEncoder: Converte cada categoria em um valor inteiro \u00fanico. Cada categoria recebe um n\u00famero inteiro diferente, e os valores num\u00e9ricos n\u00e3o t\u00eam rela\u00e7\u00e3o direta entre si al\u00e9m de representar categorias distintas. As categorias s\u00e3o ordenadas alfabeticamente e cada categoria recebe um valor correspondente.</li> </ul>"},{"location":"aulas/IA/lab02/ml-classificador/#dividindo-os-dados-em-conjunto-de-treinamento-e-de-testes","title":"Dividindo os dados em conjunto de treinamento e de testes\u00b6","text":"<p>Em qualquer projeto de aprendizado de m\u00e1quina, uma pr\u00e1tica essencial \u00e9 <code>separar</code> os dados em conjuntos de <code>treinamento e teste</code>. Isso permite treinar o modelo em um subconjunto dos dados e avali\u00e1-lo em outro subconjunto, garantindo que o modelo n\u00e3o seja avaliado nos mesmos dados em que foi treinado. A fun\u00e7\u00e3o train_test_split facilita esse processo, dividindo os dados em duas ou mais partes de forma aleat\u00f3ria ou estratificada. O par\u00e2metro test_size define a propor\u00e7\u00e3o dos dados reservada para o teste, e o par\u00e2metro random_state garante a reprodutibilidade do processo.</p> <p>Dividir nosso dataset em dois conjuntos de dados.</p> <ul> <li><code>Treinamento</code> - Representa <code>80%</code> das amostras do conjunto de dados original.</li> <li><code>Teste</code> - com <code>20%</code> das amostras</li> </ul>"},{"location":"aulas/IA/lab02/ml-classificador/#chegou-a-hora-de-treinar-os-modelos","title":"Chegou a hora de treinar os modelos\u00b6","text":"<p>Treinar um modelo no python \u00e9 simples se usar o Scikit-Learn.</p> <p></p> <p>Treinar um modelo no Scikit-Learn \u00e9 simples: basta criar o classificador, e chamar o m\u00e9todo fit().</p> <p>Uma observa\u00e7\u00e3o sobre a sintaxe dos classificadores do <code>scikit-learn</code></p> <ul> <li>O m\u00e9todo <code>fit(X,Y)</code> recebe uma matriz ou dataframe X onde cada linha \u00e9 uma amostra de aprendizado, e um array Y contendo as sa\u00eddas esperadas do classificador, seja na forma de texto ou de inteiros</li> <li>O m\u00e9todo <code>predict(X)</code> recebe uma matriz ou dataframe X onde cada linha \u00e9 uma amostra de teste, retornando um array de classes</li> </ul>"},{"location":"aulas/IA/lab02/ml-classificador/#treinamento-usando-algoritmo-knn","title":"Treinamento usando algoritmo KNN\u00b6","text":""},{"location":"aulas/IA/lab02/ml-classificador/#predicao-para-novos-dados","title":"Predi\u00e7\u00e3o para novos dados\u00b6","text":"<p>Vamos fazer a predi\u00e7\u00e3o para uma flor com as seguintes dimens\u00f5es:</p> <ul> <li>sepal_length = 5.1</li> <li>sepal_width = 3.5</li> <li>petal_length = 1.4</li> <li>petal_width = 0.2</li> </ul>"},{"location":"aulas/IA/lab02/ml-classificador/#avaliacao-do-modelo-treinado","title":"Avalia\u00e7\u00e3o do modelo treinado\u00b6","text":"<p>A avalia\u00e7\u00e3o de modelos \u00e9 uma etapa importante no pipeline de machine learning.</p> <p>Para cada tipo de tarefa (classifica\u00e7\u00e3o, regress\u00e3o ou clusteriza\u00e7\u00e3o), diferentes m\u00e9tricas s\u00e3o usadas para medir o qu\u00e3o bem o modelo est\u00e1 desempenhando. A seguir, exploramos as principais m\u00e9tricas oferecidas pelo scikit-learn.</p> <p>Acur\u00e1cia ou taxa de acerto \u00e9 a m\u00e9trica bastante popular em problemas de classifica\u00e7\u00e3o pois mede o n\u00famero de acertos do modelo dividido pelo n\u00famero total testado.</p> <p>Por exemplo, se o modelo possui uma acur\u00e1cia de 0.95 (95%) o seu modelo acerta 95 de 100 previs\u00f5es. Trata-se da m\u00e9trica mais simples e usada em datasets onde as classes s\u00e3o balanceadas, ou seja, possu\u00edmos propor\u00e7\u00f5es parecidas de amostras para cada uma das categorias.</p> <p>Outras t\u00e3o importante quanto s\u00e3o: F1-Score, Recall, Precision e Especificidade.</p>"},{"location":"aulas/IA/lab02/ml-classificador/#ajustando-os-hiperparametros-do-modelo-knn","title":"Ajustando os hiperpar\u00e2metros do modelo KNN\u00b6","text":"<p>Ao ajustar esses hiperpar\u00e2metros usando o Grid Search, podemos encontrar a combina\u00e7\u00e3o ideal de hiperpar\u00e2metros que leva \u00e0 melhor performance do modelo em um conjunto de dados espec\u00edfico.</p> <p>Existem v\u00e1rios hiperpar\u00e2metros do modelo K-Nearest Neighbors (KNN) que podem ser ajustados usando o Grid Search. Alguns dos hiperpar\u00e2metros mais comuns s\u00e3o:</p> <ul> <li><p><code>n_neighbors</code>: o n\u00famero de vizinhos mais pr\u00f3ximos a serem considerados no modelo.</p> </li> <li><p><code>weights</code>: como ponderar a contribui\u00e7\u00e3o dos vizinhos mais pr\u00f3ximos. Op\u00e7\u00f5es comuns s\u00e3o <code>uniform</code>, onde todos os vizinhos t\u00eam peso igual, e <code>distance</code>, onde o peso \u00e9 inversamente proporcional \u00e0 dist\u00e2ncia do ponto de consulta aos vizinhos.</p> </li> <li><p><code>p</code>: a m\u00e9trica de dist\u00e2ncia a ser usada. O valor padr\u00e3o \u00e9 <code>p=2</code>, que corresponde \u00e0 dist\u00e2ncia Euclidiana. Outras op\u00e7\u00f5es incluem <code>p=1</code>, que corresponde \u00e0 dist\u00e2ncia de Manhattan, e <code>p=inf</code>, que corresponde \u00e0 dist\u00e2ncia m\u00e1xima.</p> </li> <li><p><code>algorithm</code>: o algoritmo usado para calcular os vizinhos mais pr\u00f3ximos. As op\u00e7\u00f5es comuns s\u00e3o <code>brute</code>, que for\u00e7a uma busca exaustiva sobre todos os pontos de treinamento, e <code>kd_tree</code> e <code>ball_tree</code>, que usam estruturas de dados mais eficientes para acelerar a busca.</p> </li> <li><p><code>leaf_size</code>: o tamanho da folha para a \u00e1rvore de busca, que afeta a efici\u00eancia do algoritmo.</p> </li> </ul>"},{"location":"aulas/IA/lab02/ml-classificador/#treinamento-usando-algoritmo-arvore-de-decisao","title":"Treinamento usando algoritmo \u00c1rvore de Decis\u00e3o\u00b6","text":""},{"location":"aulas/IA/lab02/ml-classificador/#ajustando-os-hiperparametros-do-modelo-de-arvore-de-decisao","title":"Ajustando os hiperpar\u00e2metros do modelo de \u00e1rvore de decis\u00e3o\u00b6","text":"<p>Existem v\u00e1rios hiperpar\u00e2metros da \u00c1rvore de Decis\u00e3o que podem ser ajustados usando o Grid Search. Alguns dos hiperpar\u00e2metros mais comuns s\u00e3o:</p> <ul> <li><p><code>criterion</code>: a fun\u00e7\u00e3o usada para medir a qualidade da divis\u00e3o em cada n\u00f3 da \u00e1rvore. As op\u00e7\u00f5es comuns s\u00e3o <code>gini</code> e <code>entropy</code>.</p> </li> <li><p><code>splitter</code>: a estrat\u00e9gia usada para escolher a vari\u00e1vel que divide o conjunto de dados em cada n\u00f3. As op\u00e7\u00f5es comuns s\u00e3o <code>best</code>, que escolhe a melhor divis\u00e3o poss\u00edvel, e <code>random</code>, que escolhe uma divis\u00e3o aleat\u00f3ria.</p> </li> <li><p><code>max_depth</code>: a profundidade m\u00e1xima da \u00e1rvore. Se definido como <code>None</code>, os n\u00f3s ser\u00e3o expandidos at\u00e9 que todas as folhas contenham menos de min_samples_split amostras ou todas as amostras sejam classificadas.</p> </li> <li><p><code>min_samples_split</code>: o n\u00famero m\u00ednimo de amostras necess\u00e1rias para dividir um n\u00f3 interno.</p> </li> <li><p><code>min_samples_leaf</code>: o n\u00famero m\u00ednimo de amostras necess\u00e1rias para ser uma folha.</p> </li> <li><p><code>max_features</code>: o n\u00famero m\u00e1ximo de recursos que podem ser considerados em cada divis\u00e3o.</p> </li> <li><p><code>max_leaf_nodes</code>: o n\u00famero m\u00e1ximo de folhas permitidas na \u00e1rvore.</p> </li> </ul>"},{"location":"aulas/IA/lab02/ml-classificador/#comparando-os-melhores-modelos-treinados-de-cada-algoritmo","title":"Comparando os melhores modelos treinados de cada algoritmo\u00b6","text":"<p>Por fim, podemos comparar a performance dos modelos de KNN e \u00e1rvore de decis\u00e3o.</p>"},{"location":"aulas/IA/lab02/ml-classificador/#classificacao-de-digitos-0-9","title":"Classifica\u00e7\u00e3o de digitos 0-9\u00b6","text":""},{"location":"aulas/IA/lab02/ml-classificador/#desafio-1","title":"Desafio 1\u00b6","text":"<p>Tente ajustar os hiperpar\u00e2metros dos modelos de KNN e \u00e1rvore de decis\u00e3o para melhorar ainda mais a performance dos modelos.</p>"},{"location":"aulas/IA/lab02/ml-classificador/#desafio-2","title":"Desafio 2\u00b6","text":"<p>Pesquise sobre outros modelos de classifica\u00e7\u00e3o e compare-os com o KNN e \u00e1rvore de decis\u00e3o.</p> <p>Qual modelo \u00e9 o mais adequado para diferentes tipos de problemas de classifica\u00e7\u00e3o?</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa/","title":"\ud83d\udcca Regress\u00e3o em Machine Learning: Guia Completo","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa/#objetivos-de-aprendizagem","title":"\ud83c\udfaf Objetivos de Aprendizagem","text":"<p>Ao final desta aula, voc\u00ea ser\u00e1 capaz de:</p> <ul> <li>Compreender os fundamentos matem\u00e1ticos e conceituais da regress\u00e3o</li> <li>Distinguir entre diferentes tipos de problemas de regress\u00e3o</li> <li>Implementar modelos de regress\u00e3o linear simples e m\u00faltipla</li> <li>Aplicar t\u00e9cnicas de regress\u00e3o polinomial e regulariza\u00e7\u00e3o</li> <li>Avaliar modelos usando m\u00e9tricas apropriadas</li> <li>Interpretar resultados e diagnosticar problemas comuns</li> <li>Escolher o modelo mais adequado para diferentes cen\u00e1rios</li> </ul>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#indice","title":"\ud83d\udccb \u00cdndice","text":"<ol> <li>Conceitos Fundamentais</li> <li>Tipos de Regress\u00e3o</li> <li>Regress\u00e3o Linear</li> <li>An\u00e1lise Explorat\u00f3ria de Dados</li> <li>M\u00e9tricas de Avalia\u00e7\u00e3o</li> <li>Regress\u00e3o Polinomial</li> <li>Regulariza\u00e7\u00e3o</li> <li>Diagn\u00f3stico de Modelos</li> <li>Casos Pr\u00e1ticos</li> <li>Exerc\u00edcios e Projetos</li> </ol>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#conceitos-fundamentais","title":"\ud83c\udfd7\ufe0f Conceitos Fundamentais","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa/#o-que-e-regressao","title":"O que \u00e9 Regress\u00e3o?","text":"<p>A regress\u00e3o \u00e9 uma t\u00e9cnica de aprendizado supervisionado que visa predizer valores cont\u00ednuos (num\u00e9ricos) com base em vari\u00e1veis de entrada (features).</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#classificacao-vs-regressao","title":"\ud83c\udfad Classifica\u00e7\u00e3o vs Regress\u00e3o","text":"Aspecto Classifica\u00e7\u00e3o Regress\u00e3o Sa\u00edda Categ\u00f3rica/Discreta Num\u00e9rica/Cont\u00ednua Exemplos Spam/N\u00e3o-spam, Gato/Cachorro Pre\u00e7o, Temperatura, Altura M\u00e9tricas Acur\u00e1cia, Precis\u00e3o, Recall MSE, RMSE, R\u00b2 Algoritmos KNN, SVM, Random Forest Linear, Polinomial, Ridge"},{"location":"aulas/IA/lab03/regressao-ml-completa/#matematica-por-tras-da-regressao","title":"\ud83e\uddee Matem\u00e1tica por tr\u00e1s da Regress\u00e3o","text":"<p>A regress\u00e3o busca encontrar uma fun\u00e7\u00e3o que melhor relacione as vari\u00e1veis independentes (X) com a vari\u00e1vel dependente (y):</p> <pre><code>y = f(X) + \u03b5\n</code></pre> <p>Onde: - y: vari\u00e1vel dependente (target) - X: vari\u00e1veis independentes (features) - f(X): fun\u00e7\u00e3o que queremos aprender - \u03b5: erro aleat\u00f3rio</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#tipos-de-regressao","title":"\ud83d\udd22 Tipos de Regress\u00e3o","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa/#1-regressao-linear-simples","title":"1. Regress\u00e3o Linear Simples","text":"<ul> <li>Uma vari\u00e1vel independente</li> <li>Rela\u00e7\u00e3o linear entre X e y</li> <li>Equa\u00e7\u00e3o: <code>y = \u03b2\u2080 + \u03b2\u2081x + \u03b5</code></li> </ul>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#2-regressao-linear-multipla","title":"2. Regress\u00e3o Linear M\u00faltipla","text":"<ul> <li>M\u00faltiplas vari\u00e1veis independentes</li> <li>Equa\u00e7\u00e3o: <code>y = \u03b2\u2080 + \u03b2\u2081x\u2081 + \u03b2\u2082x\u2082 + ... + \u03b2\u2099x\u2099 + \u03b5</code></li> </ul>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#3-regressao-polinomial","title":"3. Regress\u00e3o Polinomial","text":"<ul> <li>Rela\u00e7\u00f5es n\u00e3o-lineares</li> <li>Equa\u00e7\u00e3o: <code>y = \u03b2\u2080 + \u03b2\u2081x + \u03b2\u2082x\u00b2 + ... + \u03b2\u2099x\u207f + \u03b5</code></li> </ul>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#4-regressao-regularizada","title":"4. Regress\u00e3o Regularizada","text":"<ul> <li>Ridge: penaliza coeficientes grandes</li> <li>Lasso: pode zerar coeficientes (sele\u00e7\u00e3o de features)</li> <li>Elastic Net: combina Ridge e Lasso</li> </ul>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#regressao-linear","title":"\ud83d\udcc8 Regress\u00e3o Linear","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa/#fundamentos-matematicos","title":"Fundamentos Matem\u00e1ticos","text":"<p>A regress\u00e3o linear busca encontrar a melhor reta que passa pelos dados, minimizando o erro entre os valores preditos e reais.</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#metodo-dos-minimos-quadrados","title":"M\u00e9todo dos M\u00ednimos Quadrados","text":"<p>O objetivo \u00e9 minimizar a Soma dos Quadrados dos Res\u00edduos (SSR):</p> <pre><code>SSR = \u03a3(y\u1d62 - \u0177\u1d62)\u00b2\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#formulas-dos-coeficientes","title":"F\u00f3rmulas dos Coeficientes","text":"<p>Para regress\u00e3o linear simples:</p> <pre><code>\u03b2\u2081 = \u03a3((x\u1d62 - x\u0304)(y\u1d62 - \u0233)) / \u03a3((x\u1d62 - x\u0304)\u00b2)\n\u03b2\u2080 = \u0233 - \u03b2\u2081x\u0304\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#pressupostos-da-regressao-linear","title":"Pressupostos da Regress\u00e3o Linear","text":"<ol> <li>Linearidade: rela\u00e7\u00e3o linear entre X e y</li> <li>Independ\u00eancia: observa\u00e7\u00f5es independentes</li> <li>Homocedasticidade: vari\u00e2ncia constante dos res\u00edduos</li> <li>Normalidade: res\u00edduos seguem distribui\u00e7\u00e3o normal</li> <li>Aus\u00eancia de multicolinearidade: features n\u00e3o correlacionadas</li> </ol>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#implementacao-em-python","title":"Implementa\u00e7\u00e3o em Python","text":"<pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Exemplo b\u00e1sico\n# Criando dados sint\u00e9ticos\nnp.random.seed(42)\nX = np.random.randn(100, 1)\ny = 2 + 3 * X.ravel() + np.random.randn(100)\n\n# Dividindo os dados\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Criando e treinando o modelo\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Fazendo predi\u00e7\u00f5es\ny_pred = model.predict(X_test)\n\n# Avaliando o modelo\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Coeficiente: {model.coef_[0]:.2f}\")\nprint(f\"Intercepto: {model.intercept_:.2f}\")\nprint(f\"MSE: {mse:.2f}\")\nprint(f\"R\u00b2: {r2:.2f}\")\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#metricas-de-avaliacao","title":"\ud83d\udcca M\u00e9tricas de Avalia\u00e7\u00e3o","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa/#metricas-principais","title":"M\u00e9tricas Principais","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa/#1-erro-quadratico-medio-mse","title":"1. Erro Quadr\u00e1tico M\u00e9dio (MSE)","text":"<p><pre><code>MSE = (1/n) \u00d7 \u03a3(y\u1d62 - \u0177\u1d62)\u00b2\n</code></pre> - Unidade: quadrado da unidade do target - Penaliza: erros grandes mais fortemente</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#2-raiz-do-erro-quadratico-medio-rmse","title":"2. Raiz do Erro Quadr\u00e1tico M\u00e9dio (RMSE)","text":"<p><pre><code>RMSE = \u221aMSE\n</code></pre> - Unidade: mesma do target - Interpreta\u00e7\u00e3o: erro m\u00e9dio em termos absolutos</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#3-erro-absoluto-medio-mae","title":"3. Erro Absoluto M\u00e9dio (MAE)","text":"<p><pre><code>MAE = (1/n) \u00d7 \u03a3|y\u1d62 - \u0177\u1d62|\n</code></pre> - Robustez: menos sens\u00edvel a outliers</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#4-coeficiente-de-determinacao-r2","title":"4. Coeficiente de Determina\u00e7\u00e3o (R\u00b2)","text":"<p><pre><code>R\u00b2 = 1 - (SSres/SStot)\n</code></pre> - Interpreta\u00e7\u00e3o: propor\u00e7\u00e3o da vari\u00e2ncia explicada - Faixa: 0 a 1 (quanto maior, melhor)</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#implementacao-em-python_1","title":"Implementa\u00e7\u00e3o em Python","text":"<pre><code>from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport numpy as np\n\ndef avaliar_modelo(y_true, y_pred, nome_modelo=\"Modelo\"):\n    \"\"\"\n    Fun\u00e7\u00e3o para avaliar um modelo de regress\u00e3o\n    \"\"\"\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n\n    # R\u00b2 ajustado (precisa do n\u00famero de features)\n    n = len(y_true)\n    p = 1  # n\u00famero de features (ajustar conforme necess\u00e1rio)\n    r2_adj = 1 - ((1 - r2) * (n - 1) / (n - p - 1))\n\n    print(f\"=== Avalia\u00e7\u00e3o do {nome_modelo} ===\")\n    print(f\"MSE: {mse:.4f}\")\n    print(f\"RMSE: {rmse:.4f}\")\n    print(f\"MAE: {mae:.4f}\")\n    print(f\"R\u00b2: {r2:.4f}\")\n    print(f\"R\u00b2 Ajustado: {r2_adj:.4f}\")\n\n    return {\n        'MSE': mse,\n        'RMSE': rmse,\n        'MAE': mae,\n        'R2': r2,\n        'R2_adj': r2_adj\n    }\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#regressao-polinomial","title":"\ud83c\udf00 Regress\u00e3o Polinomial","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa/#conceito","title":"Conceito","text":"<p>A regress\u00e3o polinomial estende a regress\u00e3o linear para capturar rela\u00e7\u00f5es n\u00e3o-lineares entre vari\u00e1veis.</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#matematicamente","title":"Matematicamente","text":"<p>Para um polin\u00f4mio de grau n: <pre><code>y = \u03b2\u2080 + \u03b2\u2081x + \u03b2\u2082x\u00b2 + \u03b2\u2083x\u00b3 + ... + \u03b2\u2099x\u207f + \u03b5\n</code></pre></p>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#implementacao","title":"Implementa\u00e7\u00e3o","text":"<pre><code>from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Gerando dados n\u00e3o-lineares\nnp.random.seed(42)\nX = np.linspace(-3, 3, 100).reshape(-1, 1)\ny = 0.5 * X.ravel()**3 - 2 * X.ravel()**2 + X.ravel() + np.random.normal(0, 1, 100)\n\n# Fun\u00e7\u00e3o para treinar modelo polinomial\ndef treinar_modelo_polinomial(X, y, grau):\n    # Criando pipeline\n    modelo = Pipeline([\n        ('poly', PolynomialFeatures(degree=grau)),\n        ('linear', LinearRegression())\n    ])\n\n    # Treinando\n    modelo.fit(X, y)\n\n    return modelo\n\n# Testando diferentes graus\ngraus = [1, 2, 3, 4, 8]\nfig, axes = plt.subplots(1, len(graus), figsize=(20, 4))\n\nfor i, grau in enumerate(graus):\n    # Treinando modelo\n    modelo = treinar_modelo_polinomial(X, y, grau)\n\n    # Predi\u00e7\u00f5es\n    X_plot = np.linspace(-3, 3, 300).reshape(-1, 1)\n    y_pred = modelo.predict(X_plot)\n\n    # Plotando\n    axes[i].scatter(X, y, alpha=0.6, label='Dados')\n    axes[i].plot(X_plot, y_pred, color='red', label=f'Grau {grau}')\n    axes[i].set_title(f'Polin\u00f4mio Grau {grau}')\n    axes[i].legend()\n    axes[i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#overfitting-vs-underfitting","title":"\u26a0\ufe0f Overfitting vs Underfitting","text":"<ul> <li>Underfitting (grau muito baixo): modelo muito simples</li> <li>Overfitting (grau muito alto): modelo muito complexo</li> <li>Solu\u00e7\u00e3o: valida\u00e7\u00e3o cruzada para escolher grau \u00f3timo</li> </ul> <pre><code>from sklearn.model_selection import cross_val_score\n\ndef encontrar_grau_otimo(X, y, max_grau=10):\n    graus = range(1, max_grau + 1)\n    scores = []\n\n    for grau in graus:\n        modelo = Pipeline([\n            ('poly', PolynomialFeatures(degree=grau)),\n            ('linear', LinearRegression())\n        ])\n\n        # Valida\u00e7\u00e3o cruzada\n        cv_scores = cross_val_score(modelo, X, y, cv=5, scoring='neg_mean_squared_error')\n        scores.append(-cv_scores.mean())\n\n    # Plotando\n    plt.figure(figsize=(10, 6))\n    plt.plot(graus, scores, marker='o')\n    plt.xlabel('Grau do Polin\u00f4mio')\n    plt.ylabel('MSE (Valida\u00e7\u00e3o Cruzada)')\n    plt.title('Escolha do Grau \u00d3timo')\n    plt.grid(True, alpha=0.3)\n\n    grau_otimo = graus[np.argmin(scores)]\n    plt.axvline(x=grau_otimo, color='red', linestyle='--', \n                label=f'Grau \u00d3timo: {grau_otimo}')\n    plt.legend()\n    plt.show()\n\n    return grau_otimo\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#regularizacao","title":"\ud83c\udfaf Regulariza\u00e7\u00e3o","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa/#por-que-regularizar","title":"Por que Regularizar?","text":"<ul> <li>Evitar overfitting</li> <li>Reduzir complexidade do modelo</li> <li>Melhorar generaliza\u00e7\u00e3o</li> <li>Lidar com multicolinearidade</li> </ul>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#ridge-regression-l2","title":"Ridge Regression (L2)","text":"<p>Adiciona penalidade proporcional ao quadrado dos coeficientes:</p> <pre><code>Custo = MSE + \u03b1 \u00d7 \u03a3\u03b2\u1d62\u00b2\n</code></pre> <pre><code>from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\n\n# Ridge Regression\nridge = Ridge()\n\n# Buscando melhor alpha\nalphas = np.logspace(-4, 4, 50)\nridge_cv = GridSearchCV(ridge, {'alpha': alphas}, cv=5, scoring='neg_mean_squared_error')\nridge_cv.fit(X_train, y_train)\n\nprint(f\"Melhor alpha (Ridge): {ridge_cv.best_params_['alpha']:.4f}\")\n\n# Modelo final\nridge_final = Ridge(alpha=ridge_cv.best_params_['alpha'])\nridge_final.fit(X_train, y_train)\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#lasso-regression-l1","title":"Lasso Regression (L1)","text":"<p>Adiciona penalidade proporcional ao valor absoluto dos coeficientes:</p> <pre><code>Custo = MSE + \u03b1 \u00d7 \u03a3|\u03b2\u1d62|\n</code></pre> <pre><code>from sklearn.linear_model import Lasso\n\n# Lasso Regression\nlasso = Lasso()\nlasso_cv = GridSearchCV(lasso, {'alpha': alphas}, cv=5, scoring='neg_mean_squared_error')\nlasso_cv.fit(X_train, y_train)\n\nprint(f\"Melhor alpha (Lasso): {lasso_cv.best_params_['alpha']:.4f}\")\n\n# Modelo final\nlasso_final = Lasso(alpha=lasso_cv.best_params_['alpha'])\nlasso_final.fit(X_train, y_train)\n\n# Verificando sele\u00e7\u00e3o de features\nfeatures_selecionadas = np.where(lasso_final.coef_ != 0)[0]\nprint(f\"Features selecionadas pelo Lasso: {len(features_selecionadas)}\")\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#elastic-net","title":"Elastic Net","text":"<p>Combina Ridge e Lasso:</p> <pre><code>Custo = MSE + \u03b1\u2081 \u00d7 \u03a3\u03b2\u1d62\u00b2 + \u03b1\u2082 \u00d7 \u03a3|\u03b2\u1d62|\n</code></pre> <pre><code>from sklearn.linear_model import ElasticNet\n\n# Elastic Net\nelastic = ElasticNet()\nparam_grid = {\n    'alpha': alphas,\n    'l1_ratio': np.linspace(0, 1, 11)  # 0 = Ridge, 1 = Lasso\n}\n\nelastic_cv = GridSearchCV(elastic, param_grid, cv=5, scoring='neg_mean_squared_error')\nelastic_cv.fit(X_train, y_train)\n\nprint(f\"Melhores par\u00e2metros (Elastic Net): {elastic_cv.best_params_}\")\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#diagnostico-de-modelos","title":"\ud83d\udd2c Diagn\u00f3stico de Modelos","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa/#analise-de-residuos","title":"An\u00e1lise de Res\u00edduos","text":"<pre><code>def analisar_residuos(modelo, X_test, y_test):\n    # Predi\u00e7\u00f5es\n    y_pred = modelo.predict(X_test)\n    residuos = y_test - y_pred\n\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n    # 1. Res\u00edduos vs Predi\u00e7\u00f5es\n    axes[0, 0].scatter(y_pred, residuos, alpha=0.6)\n    axes[0, 0].axhline(y=0, color='red', linestyle='--')\n    axes[0, 0].set_xlabel('Valores Preditos')\n    axes[0, 0].set_ylabel('Res\u00edduos')\n    axes[0, 0].set_title('Res\u00edduos vs Predi\u00e7\u00f5es')\n\n    # 2. Q-Q Plot (normalidade dos res\u00edduos)\n    from scipy import stats\n    stats.probplot(residuos, dist=\"norm\", plot=axes[0, 1])\n    axes[0, 1].set_title('Q-Q Plot')\n\n    # 3. Histograma dos res\u00edduos\n    axes[1, 0].hist(residuos, bins=30, alpha=0.7)\n    axes[1, 0].set_xlabel('Res\u00edduos')\n    axes[1, 0].set_ylabel('Frequ\u00eancia')\n    axes[1, 0].set_title('Distribui\u00e7\u00e3o dos Res\u00edduos')\n\n    # 4. Valores Reais vs Preditos\n    axes[1, 1].scatter(y_test, y_pred, alpha=0.6)\n    min_val = min(y_test.min(), y_pred.min())\n    max_val = max(y_test.max(), y_pred.max())\n    axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'red', linestyle='--')\n    axes[1, 1].set_xlabel('Valores Reais')\n    axes[1, 1].set_ylabel('Valores Preditos')\n    axes[1, 1].set_title('Reais vs Preditos')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Testes estat\u00edsticos\n    from scipy.stats import shapiro, jarque_bera\n\n    print(\"=== Testes de Diagn\u00f3stico ===\")\n\n    # Teste de normalidade\n    shapiro_stat, shapiro_p = shapiro(residuos[:5000])  # Limitando para performance\n    print(f\"Teste Shapiro-Wilk (normalidade): p-value = {shapiro_p:.6f}\")\n\n    # Teste Jarque-Bera\n    jb_stat, jb_p = jarque_bera(residuos)\n    print(f\"Teste Jarque-Bera (normalidade): p-value = {jb_p:.6f}\")\n\n    # Homocedasticidade (vari\u00e2ncia constante)\n    from scipy.stats import spearmanr\n    corr_stat, corr_p = spearmanr(np.abs(residuos), y_pred)\n    print(f\"Correla\u00e7\u00e3o |res\u00edduos| vs predi\u00e7\u00f5es: {corr_stat:.4f} (p = {corr_p:.6f})\")\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#deteccao-de-outliers","title":"Detec\u00e7\u00e3o de Outliers","text":"<pre><code>def detectar_outliers(X, y, modelo):\n    # Res\u00edduos padronizados\n    y_pred = modelo.predict(X)\n    residuos = y - y_pred\n    residuos_padronizados = residuos / np.std(residuos)\n\n    # Dist\u00e2ncia de Cook\n    n, p = X.shape\n    mse = np.mean(residuos**2)\n\n    # Leverage (alavancagem)\n    if hasattr(X, 'values'):\n        X_array = X.values\n    else:\n        X_array = X\n\n    # Adicionando coluna de intercepto\n    X_with_intercept = np.column_stack([np.ones(n), X_array])\n    H = X_with_intercept @ np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T\n    leverage = np.diag(H)\n\n    # Dist\u00e2ncia de Cook\n    cook_distance = (residuos_padronizados**2 / p) * (leverage / (1 - leverage)**2)\n\n    # Identificando outliers\n    outliers_residuos = np.abs(residuos_padronizados) &gt; 3\n    outliers_cook = cook_distance &gt; 4/n  # Regra pr\u00e1tica\n    outliers_leverage = leverage &gt; 2*p/n  # Regra pr\u00e1tica\n\n    print(f\"Outliers por res\u00edduos padronizados (|z| &gt; 3): {np.sum(outliers_residuos)}\")\n    print(f\"Outliers por dist\u00e2ncia de Cook (D &gt; 4/n): {np.sum(outliers_cook)}\")\n    print(f\"Outliers por leverage (h &gt; 2p/n): {np.sum(outliers_leverage)}\")\n\n    # Plotando\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    # Res\u00edduos padronizados\n    axes[0].scatter(range(len(residuos_padronizados)), residuos_padronizados, alpha=0.6)\n    axes[0].axhline(y=3, color='red', linestyle='--', label='\u00b13\u03c3')\n    axes[0].axhline(y=-3, color='red', linestyle='--')\n    axes[0].set_xlabel('\u00cdndice')\n    axes[0].set_ylabel('Res\u00edduos Padronizados')\n    axes[0].set_title('Res\u00edduos Padronizados')\n    axes[0].legend()\n\n    # Dist\u00e2ncia de Cook\n    axes[1].scatter(range(len(cook_distance)), cook_distance, alpha=0.6)\n    axes[1].axhline(y=4/n, color='red', linestyle='--', label=f'4/n = {4/n:.4f}')\n    axes[1].set_xlabel('\u00cdndice')\n    axes[1].set_ylabel('Dist\u00e2ncia de Cook')\n    axes[1].set_title('Dist\u00e2ncia de Cook')\n    axes[1].legend()\n\n    # Leverage\n    axes[2].scatter(range(len(leverage)), leverage, alpha=0.6)\n    axes[2].axhline(y=2*p/n, color='red', linestyle='--', label=f'2p/n = {2*p/n:.4f}')\n    axes[2].set_xlabel('\u00cdndice')\n    axes[2].set_ylabel('Leverage')\n    axes[2].set_title('Leverage')\n    axes[2].legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    return {\n        'outliers_residuos': np.where(outliers_residuos)[0],\n        'outliers_cook': np.where(outliers_cook)[0],\n        'outliers_leverage': np.where(outliers_leverage)[0],\n        'cook_distance': cook_distance,\n        'leverage': leverage,\n        'residuos_padronizados': residuos_padronizados\n    }\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa/#projeto-2-analise-de-tendencias-temporais","title":"Projeto 2: An\u00e1lise de Tend\u00eancias Temporais","text":"<pre><code># Exemplo com dados sint\u00e9ticos de vendas\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# Gerando dados temporais\nnp.random.seed(42)\ndates = pd.date_range('2020-01-01', periods=365*3, freq='D')\ntrend = np.linspace(100, 200, len(dates))\nseasonality = 20 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25)\nnoise = np.random.normal(0, 10, len(dates))\nsales = trend + seasonality + noise\n\n# Criando DataFrame\ndf_temporal = pd.DataFrame({\n    'date': dates,\n    'sales': sales\n})\n\n# Features temporais\ndf_temporal['year'] = df_temporal['date'].dt.year\ndf_temporal['month'] = df_temporal['date'].dt.month\ndf_temporal['day_of_year'] = df_temporal['date'].dt.dayofyear\ndf_temporal['day_of_week'] = df_temporal['date'].dt.dayofweek\n\n# Regress\u00e3o com features temporais\nX_temp = df_temporal[['year', 'month', 'day_of_year', 'day_of_week']]\ny_temp = df_temporal['sales']\n\n# Divis\u00e3o temporal (importante para s\u00e9ries temporais)\nsplit_date = '2022-06-01'\ntrain_mask = df_temporal['date'] &lt; split_date\ntest_mask = df_temporal['date'] &gt;= split_date\n\nX_train_temp = X_temp[train_mask]\nX_test_temp = X_temp[test_mask]\ny_train_temp = y_temp[train_mask]\ny_test_temp = y_temp[test_mask]\n\n# Modelo\nmodelo_temporal = LinearRegression()\nmodelo_temporal.fit(X_train_temp, y_train_temp)\ny_pred_temp = modelo_temporal.predict(X_test_temp)\n\n# Visualiza\u00e7\u00e3o\nplt.figure(figsize=(15, 6))\nplt.plot(df_temporal['date'][train_mask], y_train_temp, label='Treino', alpha=0.7)\nplt.plot(df_temporal['date'][test_mask], y_test_temp, label='Teste (Real)', alpha=0.7)\nplt.plot(df_temporal['date'][test_mask], y_pred_temp, label='Predi\u00e7\u00e3o', alpha=0.7)\nplt.axvline(x=pd.to_datetime(split_date), color='red', linestyle='--', label='Divis\u00e3o')\nplt.xlabel('Data')\nplt.ylabel('Vendas')\nplt.title('Predi\u00e7\u00e3o de Vendas Temporais')\nplt.legend()\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Avalia\u00e7\u00e3o\navaliar_modelo(y_test_temp, y_pred_temp, \"Modelo Temporal\")\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/","title":"\ud83d\udcca Regress\u00e3o em Machine Learning: Guia Completo","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa2/#objetivos-de-aprendizagem","title":"\ud83c\udfaf Objetivos de Aprendizagem","text":"<p>Ao final desta aula, voc\u00ea ser\u00e1 capaz de:</p> <ul> <li>Compreender os fundamentos matem\u00e1ticos e conceituais da regress\u00e3o</li> <li>Distinguir entre diferentes tipos de problemas de regress\u00e3o</li> <li>Implementar modelos de regress\u00e3o linear simples e m\u00faltipla</li> <li>Aplicar t\u00e9cnicas de regress\u00e3o polinomial e regulariza\u00e7\u00e3o</li> <li>Avaliar modelos usando m\u00e9tricas apropriadas</li> <li>Interpretar resultados e diagnosticar problemas comuns</li> <li>Escolher o modelo mais adequado para diferentes cen\u00e1rios</li> </ul>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#indice","title":"\ud83d\udccb \u00cdndice","text":"<ol> <li>Conceitos Fundamentais</li> <li>Tipos de Regress\u00e3o</li> <li>Regress\u00e3o Linear</li> <li>An\u00e1lise Explorat\u00f3ria de Dados</li> <li>M\u00e9tricas de Avalia\u00e7\u00e3o</li> <li>Regress\u00e3o Polinomial</li> <li>Regulariza\u00e7\u00e3o</li> <li>Diagn\u00f3stico de Modelos</li> <li>Casos Pr\u00e1ticos</li> <li>Exerc\u00edcios e Projetos</li> </ol>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#conceitos-fundamentais","title":"\ud83c\udfd7\ufe0f Conceitos Fundamentais","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa2/#o-que-e-regressao","title":"O que \u00e9 Regress\u00e3o?","text":"<p>A regress\u00e3o \u00e9 uma t\u00e9cnica de aprendizado supervisionado que visa predizer valores cont\u00ednuos (num\u00e9ricos) com base em vari\u00e1veis de entrada (features).</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#classificacao-vs-regressao","title":"\ud83c\udfad Classifica\u00e7\u00e3o vs Regress\u00e3o","text":"Aspecto Classifica\u00e7\u00e3o Regress\u00e3o Sa\u00edda Categ\u00f3rica/Discreta Num\u00e9rica/Cont\u00ednua Exemplos Spam/N\u00e3o-spam, Gato/Cachorro Pre\u00e7o, Temperatura, Altura M\u00e9tricas Acur\u00e1cia, Precis\u00e3o, Recall MSE, RMSE, R\u00b2 Algoritmos KNN, SVM, Random Forest Linear, Polinomial, Ridge"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#matematica-por-tras-da-regressao","title":"\ud83e\uddee Matem\u00e1tica por tr\u00e1s da Regress\u00e3o","text":"<p>A regress\u00e3o busca encontrar uma fun\u00e7\u00e3o que melhor relacione as vari\u00e1veis independentes (X) com a vari\u00e1vel dependente (y):</p> <pre><code>y = f(X) + \u03b5\n</code></pre> <p>Onde: - y: vari\u00e1vel dependente (target) - X: vari\u00e1veis independentes (features) - f(X): fun\u00e7\u00e3o que queremos aprender - \u03b5: erro aleat\u00f3rio</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#tipos-de-regressao","title":"\ud83d\udd22 Tipos de Regress\u00e3o","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa2/#1-regressao-linear-simples","title":"1. Regress\u00e3o Linear Simples","text":"<ul> <li>Uma vari\u00e1vel independente</li> <li>Rela\u00e7\u00e3o linear entre X e y</li> <li>Equa\u00e7\u00e3o: <code>y = \u03b2\u2080 + \u03b2\u2081x + \u03b5</code></li> </ul>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#2-regressao-linear-multipla","title":"2. Regress\u00e3o Linear M\u00faltipla","text":"<ul> <li>M\u00faltiplas vari\u00e1veis independentes</li> <li>Equa\u00e7\u00e3o: <code>y = \u03b2\u2080 + \u03b2\u2081x\u2081 + \u03b2\u2082x\u2082 + ... + \u03b2\u2099x\u2099 + \u03b5</code></li> </ul>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#3-regressao-polinomial","title":"3. Regress\u00e3o Polinomial","text":"<ul> <li>Rela\u00e7\u00f5es n\u00e3o-lineares</li> <li>Equa\u00e7\u00e3o: <code>y = \u03b2\u2080 + \u03b2\u2081x + \u03b2\u2082x\u00b2 + ... + \u03b2\u2099x\u207f + \u03b5</code></li> </ul>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#4-regressao-regularizada","title":"4. Regress\u00e3o Regularizada","text":"<ul> <li>Ridge: penaliza coeficientes grandes</li> <li>Lasso: pode zerar coeficientes (sele\u00e7\u00e3o de features)</li> <li>Elastic Net: combina Ridge e Lasso</li> </ul>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#regressao-linear","title":"\ud83d\udcc8 Regress\u00e3o Linear","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa2/#fundamentos-matematicos","title":"Fundamentos Matem\u00e1ticos","text":"<p>A regress\u00e3o linear busca encontrar a melhor reta que passa pelos dados, minimizando o erro entre os valores preditos e reais.</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#metodo-dos-minimos-quadrados","title":"M\u00e9todo dos M\u00ednimos Quadrados","text":"<p>O objetivo \u00e9 minimizar a Soma dos Quadrados dos Res\u00edduos (SSR):</p> <pre><code>SSR = \u03a3(y\u1d62 - \u0177\u1d62)\u00b2\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#formulas-dos-coeficientes","title":"F\u00f3rmulas dos Coeficientes","text":"<p>Para regress\u00e3o linear simples:</p> <pre><code>\u03b2\u2081 = \u03a3((x\u1d62 - x\u0304)(y\u1d62 - \u0233)) / \u03a3((x\u1d62 - x\u0304)\u00b2)\n\u03b2\u2080 = \u0233 - \u03b2\u2081x\u0304\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#pressupostos-da-regressao-linear","title":"Pressupostos da Regress\u00e3o Linear","text":"<ol> <li>Linearidade: rela\u00e7\u00e3o linear entre X e y</li> <li>Independ\u00eancia: observa\u00e7\u00f5es independentes</li> <li>Homocedasticidade: vari\u00e2ncia constante dos res\u00edduos</li> <li>Normalidade: res\u00edduos seguem distribui\u00e7\u00e3o normal</li> <li>Aus\u00eancia de multicolinearidade: features n\u00e3o correlacionadas</li> </ol>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#implementacao-em-python","title":"Implementa\u00e7\u00e3o em Python","text":"<pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Exemplo b\u00e1sico\n# Criando dados sint\u00e9ticos\nnp.random.seed(42)\nX = np.random.randn(100, 1)\ny = 2 + 3 * X.ravel() + np.random.randn(100)\n\n# Dividindo os dados\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Criando e treinando o modelo\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Fazendo predi\u00e7\u00f5es\ny_pred = model.predict(X_test)\n\n# Avaliando o modelo\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Coeficiente: {model.coef_[0]:.2f}\")\nprint(f\"Intercepto: {model.intercept_:.2f}\")\nprint(f\"MSE: {mse:.2f}\")\nprint(f\"R\u00b2: {r2:.2f}\")\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#analise-exploratoria-de-dados","title":"\ud83d\udd0d An\u00e1lise Explorat\u00f3ria de Dados","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa2/#etapas-essenciais","title":"Etapas Essenciais","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa2/#1-carregamento-e-inspecao-inicial","title":"1. Carregamento e Inspe\u00e7\u00e3o Inicial","text":"<pre><code># Exemplo com dataset de habita\u00e7\u00e3o da Calif\u00f3rnia\nfrom sklearn.datasets import fetch_california_housing\nimport pandas as pd\nimport seaborn as sns\n\n# Carregando os dados\nhousing = fetch_california_housing()\ndf = pd.DataFrame(housing.data, columns=housing.feature_names)\ndf['target'] = housing.target\n\n# Informa\u00e7\u00f5es b\u00e1sicas\nprint(df.info())\nprint(df.describe())\nprint(df.isnull().sum())\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#2-visualizacao-de-distribuicoes","title":"2. Visualiza\u00e7\u00e3o de Distribui\u00e7\u00f5es","text":"<pre><code># Histogramas das vari\u00e1veis\nfig, axes = plt.subplots(2, 4, figsize=(15, 8))\nfor i, column in enumerate(df.columns):\n    ax = axes[i//4, i%4]\n    df[column].hist(ax=ax, bins=30)\n    ax.set_title(column)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#3-matriz-de-correlacao","title":"3. Matriz de Correla\u00e7\u00e3o","text":"<pre><code># Calculando e visualizando correla\u00e7\u00f5es\ncorrelation_matrix = df.corr()\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Matriz de Correla\u00e7\u00e3o')\nplt.show()\n\n# Correla\u00e7\u00f5es com o target\ntarget_corr = correlation_matrix['target'].sort_values(ascending=False)\nprint(\"Correla\u00e7\u00f5es com o target:\")\nprint(target_corr)\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#4-graficos-de-dispersao","title":"4. Gr\u00e1ficos de Dispers\u00e3o","text":"<pre><code># Scatter plots das vari\u00e1veis mais correlacionadas\ntop_features = target_corr.abs().sort_values(ascending=False)[1:4].index\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nfor i, feature in enumerate(top_features):\n    df.plot.scatter(x=feature, y='target', ax=axes[i])\n    axes[i].set_title(f'{feature} vs Target')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#metricas-de-avaliacao","title":"\ud83d\udcca M\u00e9tricas de Avalia\u00e7\u00e3o","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa2/#metricas-principais","title":"M\u00e9tricas Principais","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa2/#1-erro-quadratico-medio-mse","title":"1. Erro Quadr\u00e1tico M\u00e9dio (MSE)","text":"<p><pre><code>MSE = (1/n) \u00d7 \u03a3(y\u1d62 - \u0177\u1d62)\u00b2\n</code></pre> - Unidade: quadrado da unidade do target - Penaliza: erros grandes mais fortemente</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#2-raiz-do-erro-quadratico-medio-rmse","title":"2. Raiz do Erro Quadr\u00e1tico M\u00e9dio (RMSE)","text":"<p><pre><code>RMSE = \u221aMSE\n</code></pre> - Unidade: mesma do target - Interpreta\u00e7\u00e3o: erro m\u00e9dio em termos absolutos</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#3-erro-absoluto-medio-mae","title":"3. Erro Absoluto M\u00e9dio (MAE)","text":"<p><pre><code>MAE = (1/n) \u00d7 \u03a3|y\u1d62 - \u0177\u1d62|\n</code></pre> - Robustez: menos sens\u00edvel a outliers</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#4-coeficiente-de-determinacao-r2","title":"4. Coeficiente de Determina\u00e7\u00e3o (R\u00b2)","text":"<p><pre><code>R\u00b2 = 1 - (SSres/SStot)\n</code></pre> - Interpreta\u00e7\u00e3o: propor\u00e7\u00e3o da vari\u00e2ncia explicada - Faixa: 0 a 1 (quanto maior, melhor)</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#5-r2-ajustado","title":"5. R\u00b2 Ajustado","text":"<p><pre><code>R\u00b2adj = 1 - [(1-R\u00b2)(n-1)/(n-p-1)]\n</code></pre> - Penaliza: modelos com muitas vari\u00e1veis</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#implementacao-pratica","title":"Implementa\u00e7\u00e3o Pr\u00e1tica","text":"<pre><code>from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport numpy as np\n\ndef avaliar_modelo(y_true, y_pred, nome_modelo=\"Modelo\"):\n    \"\"\"\n    Fun\u00e7\u00e3o para avaliar um modelo de regress\u00e3o\n    \"\"\"\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n\n    # R\u00b2 ajustado (precisa do n\u00famero de features)\n    n = len(y_true)\n    p = 1  # n\u00famero de features (ajustar conforme necess\u00e1rio)\n    r2_adj = 1 - ((1 - r2) * (n - 1) / (n - p - 1))\n\n    print(f\"=== Avalia\u00e7\u00e3o do {nome_modelo} ===\")\n    print(f\"MSE: {mse:.4f}\")\n    print(f\"RMSE: {rmse:.4f}\")\n    print(f\"MAE: {mae:.4f}\")\n    print(f\"R\u00b2: {r2:.4f}\")\n    print(f\"R\u00b2 Ajustado: {r2_adj:.4f}\")\n\n    return {\n        'MSE': mse,\n        'RMSE': rmse,\n        'MAE': mae,\n        'R2': r2,\n        'R2_adj': r2_adj\n    }\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#regressao-polinomial","title":"\ud83c\udf00 Regress\u00e3o Polinomial","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa2/#conceito","title":"Conceito","text":"<p>A regress\u00e3o polinomial estende a regress\u00e3o linear para capturar rela\u00e7\u00f5es n\u00e3o-lineares entre vari\u00e1veis.</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#matematicamente","title":"Matematicamente","text":"<p>Para um polin\u00f4mio de grau n: <pre><code>y = \u03b2\u2080 + \u03b2\u2081x + \u03b2\u2082x\u00b2 + \u03b2\u2083x\u00b3 + ... + \u03b2\u2099x\u207f + \u03b5\n</code></pre></p>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#implementacao","title":"Implementa\u00e7\u00e3o","text":"<pre><code>from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Gerando dados n\u00e3o-lineares\nnp.random.seed(42)\nX = np.linspace(-3, 3, 100).reshape(-1, 1)\ny = 0.5 * X.ravel()**3 - 2 * X.ravel()**2 + X.ravel() + np.random.normal(0, 1, 100)\n\n# Fun\u00e7\u00e3o para treinar modelo polinomial\ndef treinar_modelo_polinomial(X, y, grau):\n    # Criando pipeline\n    modelo = Pipeline([\n        ('poly', PolynomialFeatures(degree=grau)),\n        ('linear', LinearRegression())\n    ])\n\n    # Treinando\n    modelo.fit(X, y)\n\n    return modelo\n\n# Testando diferentes graus\ngraus = [1, 2, 3, 4, 8]\nfig, axes = plt.subplots(1, len(graus), figsize=(20, 4))\n\nfor i, grau in enumerate(graus):\n    # Treinando modelo\n    modelo = treinar_modelo_polinomial(X, y, grau)\n\n    # Predi\u00e7\u00f5es\n    X_plot = np.linspace(-3, 3, 300).reshape(-1, 1)\n    y_pred = modelo.predict(X_plot)\n\n    # Plotando\n    axes[i].scatter(X, y, alpha=0.6, label='Dados')\n    axes[i].plot(X_plot, y_pred, color='red', label=f'Grau {grau}')\n    axes[i].set_title(f'Polin\u00f4mio Grau {grau}')\n    axes[i].legend()\n    axes[i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#overfitting-vs-underfitting","title":"\u26a0\ufe0f Overfitting vs Underfitting","text":"<ul> <li>Underfitting (grau muito baixo): modelo muito simples</li> <li>Overfitting (grau muito alto): modelo muito complexo</li> <li>Solu\u00e7\u00e3o: valida\u00e7\u00e3o cruzada para escolher grau \u00f3timo</li> </ul> <pre><code>from sklearn.model_selection import cross_val_score\n\ndef encontrar_grau_otimo(X, y, max_grau=10):\n    graus = range(1, max_grau + 1)\n    scores = []\n\n    for grau in graus:\n        modelo = Pipeline([\n            ('poly', PolynomialFeatures(degree=grau)),\n            ('linear', LinearRegression())\n        ])\n\n        # Valida\u00e7\u00e3o cruzada\n        cv_scores = cross_val_score(modelo, X, y, cv=5, scoring='neg_mean_squared_error')\n        scores.append(-cv_scores.mean())\n\n    # Plotando\n    plt.figure(figsize=(10, 6))\n    plt.plot(graus, scores, marker='o')\n    plt.xlabel('Grau do Polin\u00f4mio')\n    plt.ylabel('MSE (Valida\u00e7\u00e3o Cruzada)')\n    plt.title('Escolha do Grau \u00d3timo')\n    plt.grid(True, alpha=0.3)\n\n    grau_otimo = graus[np.argmin(scores)]\n    plt.axvline(x=grau_otimo, color='red', linestyle='--', \n                label=f'Grau \u00d3timo: {grau_otimo}')\n    plt.legend()\n    plt.show()\n\n    return grau_otimo\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#regularizacao","title":"\ud83c\udfaf Regulariza\u00e7\u00e3o","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa2/#por-que-regularizar","title":"Por que Regularizar?","text":"<ul> <li>Evitar overfitting</li> <li>Reduzir complexidade do modelo</li> <li>Melhorar generaliza\u00e7\u00e3o</li> <li>Lidar com multicolinearidade</li> </ul>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#ridge-regression-l2","title":"Ridge Regression (L2)","text":"<p>Adiciona penalidade proporcional ao quadrado dos coeficientes:</p> <pre><code>Custo = MSE + \u03b1 \u00d7 \u03a3\u03b2\u1d62\u00b2\n</code></pre> <pre><code>from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\n\n# Ridge Regression\nridge = Ridge()\n\n# Buscando melhor alpha\nalphas = np.logspace(-4, 4, 50)\nridge_cv = GridSearchCV(ridge, {'alpha': alphas}, cv=5, scoring='neg_mean_squared_error')\nridge_cv.fit(X_train, y_train)\n\nprint(f\"Melhor alpha (Ridge): {ridge_cv.best_params_['alpha']:.4f}\")\n\n# Modelo final\nridge_final = Ridge(alpha=ridge_cv.best_params_['alpha'])\nridge_final.fit(X_train, y_train)\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#lasso-regression-l1","title":"Lasso Regression (L1)","text":"<p>Adiciona penalidade proporcional ao valor absoluto dos coeficientes:</p> <pre><code>Custo = MSE + \u03b1 \u00d7 \u03a3|\u03b2\u1d62|\n</code></pre> <pre><code>from sklearn.linear_model import Lasso\n\n# Lasso Regression\nlasso = Lasso()\nlasso_cv = GridSearchCV(lasso, {'alpha': alphas}, cv=5, scoring='neg_mean_squared_error')\nlasso_cv.fit(X_train, y_train)\n\nprint(f\"Melhor alpha (Lasso): {lasso_cv.best_params_['alpha']:.4f}\")\n\n# Modelo final\nlasso_final = Lasso(alpha=lasso_cv.best_params_['alpha'])\nlasso_final.fit(X_train, y_train)\n\n# Verificando sele\u00e7\u00e3o de features\nfeatures_selecionadas = np.where(lasso_final.coef_ != 0)[0]\nprint(f\"Features selecionadas pelo Lasso: {len(features_selecionadas)}\")\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#elastic-net","title":"Elastic Net","text":"<p>Combina Ridge e Lasso:</p> <pre><code>Custo = MSE + \u03b1\u2081 \u00d7 \u03a3\u03b2\u1d62\u00b2 + \u03b1\u2082 \u00d7 \u03a3|\u03b2\u1d62|\n</code></pre> <pre><code>from sklearn.linear_model import ElasticNet\n\n# Elastic Net\nelastic = ElasticNet()\nparam_grid = {\n    'alpha': alphas,\n    'l1_ratio': np.linspace(0, 1, 11)  # 0 = Ridge, 1 = Lasso\n}\n\nelastic_cv = GridSearchCV(elastic, param_grid, cv=5, scoring='neg_mean_squared_error')\nelastic_cv.fit(X_train, y_train)\n\nprint(f\"Melhores par\u00e2metros (Elastic Net): {elastic_cv.best_params_}\")\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#diagnostico-de-modelos","title":"\ud83d\udd2c Diagn\u00f3stico de Modelos","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa2/#analise-de-residuos","title":"An\u00e1lise de Res\u00edduos","text":"<pre><code>def analisar_residuos(modelo, X_test, y_test):\n    # Predi\u00e7\u00f5es\n    y_pred = modelo.predict(X_test)\n    residuos = y_test - y_pred\n\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n    # 1. Res\u00edduos vs Predi\u00e7\u00f5es\n    axes[0, 0].scatter(y_pred, residuos, alpha=0.6)\n    axes[0, 0].axhline(y=0, color='red', linestyle='--')\n    axes[0, 0].set_xlabel('Valores Preditos')\n    axes[0, 0].set_ylabel('Res\u00edduos')\n    axes[0, 0].set_title('Res\u00edduos vs Predi\u00e7\u00f5es')\n\n    # 2. Q-Q Plot (normalidade dos res\u00edduos)\n    from scipy import stats\n    stats.probplot(residuos, dist=\"norm\", plot=axes[0, 1])\n    axes[0, 1].set_title('Q-Q Plot')\n\n    # 3. Histograma dos res\u00edduos\n    axes[1, 0].hist(residuos, bins=30, alpha=0.7)\n    axes[1, 0].set_xlabel('Res\u00edduos')\n    axes[1, 0].set_ylabel('Frequ\u00eancia')\n    axes[1, 0].set_title('Distribui\u00e7\u00e3o dos Res\u00edduos')\n\n    # 4. Valores Reais vs Preditos\n    axes[1, 1].scatter(y_test, y_pred, alpha=0.6)\n    min_val = min(y_test.min(), y_pred.min())\n    max_val = max(y_test.max(), y_pred.max())\n    axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'red', linestyle='--')\n    axes[1, 1].set_xlabel('Valores Reais')\n    axes[1, 1].set_ylabel('Valores Preditos')\n    axes[1, 1].set_title('Reais vs Preditos')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Testes estat\u00edsticos\n    from scipy.stats import shapiro, jarque_bera\n\n    print(\"=== Testes de Diagn\u00f3stico ===\")\n\n    # Teste de normalidade\n    shapiro_stat, shapiro_p = shapiro(residuos[:5000])  # Limitando para performance\n    print(f\"Teste Shapiro-Wilk (normalidade): p-value = {shapiro_p:.6f}\")\n\n    # Teste Jarque-Bera\n    jb_stat, jb_p = jarque_bera(residuos)\n    print(f\"Teste Jarque-Bera (normalidade): p-value = {jb_p:.6f}\")\n\n    # Homocedasticidade (vari\u00e2ncia constante)\n    from scipy.stats import spearmanr\n    corr_stat, corr_p = spearmanr(np.abs(residuos), y_pred)\n    print(f\"Correla\u00e7\u00e3o |res\u00edduos| vs predi\u00e7\u00f5es: {corr_stat:.4f} (p = {corr_p:.6f})\")\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#deteccao-de-outliers","title":"Detec\u00e7\u00e3o de Outliers","text":"<pre><code>def detectar_outliers(X, y, modelo):\n    # Res\u00edduos padronizados\n    y_pred = modelo.predict(X)\n    residuos = y - y_pred\n    residuos_padronizados = residuos / np.std(residuos)\n\n    # Dist\u00e2ncia de Cook\n    n, p = X.shape\n    mse = np.mean(residuos**2)\n\n    # Leverage (alavancagem)\n    if hasattr(X, 'values'):\n        X_array = X.values\n    else:\n        X_array = X\n\n    # Adicionando coluna de intercepto\n    X_with_intercept = np.column_stack([np.ones(n), X_array])\n    H = X_with_intercept @ np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T\n    leverage = np.diag(H)\n\n    # Dist\u00e2ncia de Cook\n    cook_distance = (residuos_padronizados**2 / p) * (leverage / (1 - leverage)**2)\n\n    # Identificando outliers\n    outliers_residuos = np.abs(residuos_padronizados) &gt; 3\n    outliers_cook = cook_distance &gt; 4/n  # Regra pr\u00e1tica\n    outliers_leverage = leverage &gt; 2*p/n  # Regra pr\u00e1tica\n\n    print(f\"Outliers por res\u00edduos padronizados (|z| &gt; 3): {np.sum(outliers_residuos)}\")\n    print(f\"Outliers por dist\u00e2ncia de Cook (D &gt; 4/n): {np.sum(outliers_cook)}\")\n    print(f\"Outliers por leverage (h &gt; 2p/n): {np.sum(outliers_leverage)}\")\n\n    # Plotando\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    # Res\u00edduos padronizados\n    axes[0].scatter(range(len(residuos_padronizados)), residuos_padronizados, alpha=0.6)\n    axes[0].axhline(y=3, color='red', linestyle='--', label='\u00b13\u03c3')\n    axes[0].axhline(y=-3, color='red', linestyle='--')\n    axes[0].set_xlabel('\u00cdndice')\n    axes[0].set_ylabel('Res\u00edduos Padronizados')\n    axes[0].set_title('Res\u00edduos Padronizados')\n    axes[0].legend()\n\n    # Dist\u00e2ncia de Cook\n    axes[1].scatter(range(len(cook_distance)), cook_distance, alpha=0.6)\n    axes[1].axhline(y=4/n, color='red', linestyle='--', label=f'4/n = {4/n:.4f}')\n    axes[1].set_xlabel('\u00cdndice')\n    axes[1].set_ylabel('Dist\u00e2ncia de Cook')\n    axes[1].set_title('Dist\u00e2ncia de Cook')\n    axes[1].legend()\n\n    # Leverage\n    axes[2].scatter(range(len(leverage)), leverage, alpha=0.6)\n    axes[2].axhline(y=2*p/n, color='red', linestyle='--', label=f'2p/n = {2*p/n:.4f}')\n    axes[2].set_xlabel('\u00cdndice')\n    axes[2].set_ylabel('Leverage')\n    axes[2].set_title('Leverage')\n    axes[2].legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    return {\n        'outliers_residuos': np.where(outliers_residuos)[0],\n        'outliers_cook': np.where(outliers_cook)[0],\n        'outliers_leverage': np.where(outliers_leverage)[0],\n        'cook_distance': cook_distance,\n        'leverage': leverage,\n        'residuos_padronizados': residuos_padronizados\n    }\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#casos-praticos","title":"\ud83c\udfe0 Casos Pr\u00e1ticos","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa2/#projeto-1-predicao-de-precos-de-imoveis","title":"Projeto 1: Predi\u00e7\u00e3o de Pre\u00e7os de Im\u00f3veis","text":"<pre><code># Dataset: California Housing\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# Carregando dados\nhousing = fetch_california_housing()\nX = pd.DataFrame(housing.data, columns=housing.feature_names)\ny = housing.target\n\nprint(\"Features dispon\u00edveis:\")\nfor i, feature in enumerate(housing.feature_names):\n    print(f\"{i+1}. {feature}: {housing.feature_names[i]}\")\n\n# An\u00e1lise explorat\u00f3ria\nprint(f\"\\nShape dos dados: {X.shape}\")\nprint(f\"Target (pre\u00e7o m\u00e9dio): min={y.min():.2f}, max={y.max():.2f}, m\u00e9dia={y.mean():.2f}\")\n\n# Dividindo dados\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Comparando diferentes modelos\nmodelos = {\n    'Linear': LinearRegression(),\n    'Ridge': Ridge(alpha=1.0),\n    'Lasso': Lasso(alpha=0.1),\n    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n}\n\nresultados = {}\n\nfor nome, modelo in modelos.items():\n    # Treinamento\n    modelo.fit(X_train, y_train)\n\n    # Predi\u00e7\u00f5es\n    y_pred = modelo.predict(X_test)\n\n    # Avalia\u00e7\u00e3o\n\n    resultados[nome] = avaliar_modelo(y_test, y_pred, nome)\n    print()\n\n# Compara\u00e7\u00e3o visual\nimport matplotlib.pyplot as plt\n\nmetricas = ['MSE', 'RMSE', 'MAE', 'R2']\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\nfor i, metrica in enumerate(metricas):\n    ax = axes[i//2, i%2]\n    valores = [resultados[modelo][metrica] for modelo in modelos.keys()]\n    ax.bar(modelos.keys(), valores)\n    ax.set_title(f'{metrica}')\n    ax.tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#projeto-2-analise-de-tendencias-temporais","title":"Projeto 2: An\u00e1lise de Tend\u00eancias Temporais","text":"<pre><code># Exemplo com dados sint\u00e9ticos de vendas\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# Gerando dados temporais\nnp.random.seed(42)\ndates = pd.date_range('2020-01-01', periods=365*3, freq='D')\ntrend = np.linspace(100, 200, len(dates))\nseasonality = 20 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25)\nnoise = np.random.normal(0, 10, len(dates))\nsales = trend + seasonality + noise\n\n# Criando DataFrame\ndf_temporal = pd.DataFrame({\n    'date': dates,\n    'sales': sales\n})\n\n# Features temporais\ndf_temporal['year'] = df_temporal['date'].dt.year\ndf_temporal['month'] = df_temporal['date'].dt.month\ndf_temporal['day_of_year'] = df_temporal['date'].dt.dayofyear\ndf_temporal['day_of_week'] = df_temporal['date'].dt.dayofweek\n\n# Regress\u00e3o com features temporais\nX_temp = df_temporal[['year', 'month', 'day_of_year', 'day_of_week']]\ny_temp = df_temporal['sales']\n\n# Divis\u00e3o temporal (importante para s\u00e9ries temporais)\nsplit_date = '2022-06-01'\ntrain_mask = df_temporal['date'] &lt; split_date\ntest_mask = df_temporal['date'] &gt;= split_date\n\nX_train_temp = X_temp[train_mask]\nX_test_temp = X_temp[test_mask]\ny_train_temp = y_temp[train_mask]\ny_test_temp = y_temp[test_mask]\n\n# Modelo\nmodelo_temporal = LinearRegression()\nmodelo_temporal.fit(X_train_temp, y_train_temp)\ny_pred_temp = modelo_temporal.predict(X_test_temp)\n\n# Visualiza\u00e7\u00e3o\nplt.figure(figsize=(15, 6))\nplt.plot(df_temporal['date'][train_mask], y_train_temp, label='Treino', alpha=0.7)\nplt.plot(df_temporal['date'][test_mask], y_test_temp, label='Teste (Real)', alpha=0.7)\nplt.plot(df_temporal['date'][test_mask], y_pred_temp, label='Predi\u00e7\u00e3o', alpha=0.7)\nplt.axvline(x=pd.to_datetime(split_date), color='red', linestyle='--', label='Divis\u00e3o')\nplt.xlabel('Data')\nplt.ylabel('Vendas')\nplt.title('Predi\u00e7\u00e3o de Vendas Temporais')\nplt.legend()\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Avalia\u00e7\u00e3o\navaliar_modelo(y_test_temp, y_pred_temp, \"Modelo Temporal\")\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#exercicios-e-projetos","title":"\ud83c\udfaf Exerc\u00edcios e Projetos","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa2/#exercicio-1-implementacao-manual","title":"\ud83d\udcdd Exerc\u00edcio 1: Implementa\u00e7\u00e3o Manual","text":"<p>Objetivo: Implementar regress\u00e3o linear simples do zero.</p> <pre><code>def regressao_linear_manual(X, y):\n    \"\"\"\n    Implementa regress\u00e3o linear simples manualmente\n    \"\"\"\n    # Adicionando coluna de intercepto\n    n = len(X)\n    X_with_intercept = np.column_stack([np.ones(n), X])\n\n    # F\u00f3rmula matricial: \u03b2 = (X'X)^(-1)X'y\n    beta = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y\n\n    return beta\n\n# Teste com dados sint\u00e9ticos\nnp.random.seed(42)\nX_simples = np.random.randn(100, 1)\ny_simples = 2 + 3 * X_simples.ravel() + np.random.randn(100)\n\n# Implementa\u00e7\u00e3o manual\nbeta_manual = regressao_linear_manual(X_simples, y_simples)\nprint(f\"Coeficientes manuais: intercepto={beta_manual[0]:.4f}, slope={beta_manual[1]:.4f}\")\n\n# Compara\u00e7\u00e3o com sklearn\nmodelo_sklearn = LinearRegression()\nmodelo_sklearn.fit(X_simples, y_simples)\nprint(f\"Coeficientes sklearn: intercepto={modelo_sklearn.intercept_:.4f}, slope={modelo_sklearn.coef_[0]:.4f}\")\n</code></pre>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#exercicio-2-analise-completa","title":"\ud83d\udcca Exerc\u00edcio 2: An\u00e1lise Completa","text":"<p>Objetivo: Realizar an\u00e1lise completa de um dataset real.</p> <p>Etapas: 1. Carregar dataset (sugest\u00e3o: Boston Housing, Wine Quality, ou Auto MPG) 2. An\u00e1lise explorat\u00f3ria completa 3. Tratamento de dados faltantes e outliers 4. Sele\u00e7\u00e3o de features 5. Compara\u00e7\u00e3o de m\u00faltiplos modelos 6. Valida\u00e7\u00e3o cruzada 7. Interpreta\u00e7\u00e3o dos resultados</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#projeto-final-sistema-de-predicao","title":"\ud83c\udfc6 Projeto Final: Sistema de Predi\u00e7\u00e3o","text":"<p>Objetivo: Criar um sistema completo de predi\u00e7\u00e3o.</p> <p>Requisitos: - Interface amig\u00e1vel (Streamlit ou Gradio) - M\u00faltiplos modelos - Visualiza\u00e7\u00f5es interativas - Explicabilidade (SHAP ou LIME) - Deploy (opcional)</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#recursos-adicionais","title":"\ud83d\udcda Recursos Adicionais","text":""},{"location":"aulas/IA/lab03/regressao-ml-completa2/#bibliografia-recomendada","title":"\ud83d\udcd6 Bibliografia Recomendada","text":"<ol> <li>\"Introduction to Statistical Learning\" - James, Witten, Hastie, Tibshirani</li> <li>\"Pattern Recognition and Machine Learning\" - Christopher Bishop</li> <li>\"The Elements of Statistical Learning\" - Hastie, Tibshirani, Friedman</li> </ol>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#links-uteis","title":"\ud83c\udf10 Links \u00dateis","text":"<ul> <li>Scikit-learn Documentation</li> <li>Statsmodels Documentation</li> <li>Kaggle Learn - Machine Learning</li> </ul>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#ferramentas-complementares","title":"\ud83d\udd27 Ferramentas Complementares","text":"<ul> <li>Statsmodels: An\u00e1lises estat\u00edsticas detalhadas</li> <li>XGBoost/LightGBM: Gradient boosting</li> <li>SHAP: Explicabilidade de modelos</li> <li>Plotly: Visualiza\u00e7\u00f5es interativas</li> </ul>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#checklist-de-aprendizagem","title":"\ud83c\udfaf Checklist de Aprendizagem","text":"<p>Marque conforme avan\u00e7a nos estudos:</p> <p>Conceitos Fundamentais - [ ] Diferen\u00e7a entre classifica\u00e7\u00e3o e regress\u00e3o - [ ] Pressupostos da regress\u00e3o linear - [ ] M\u00e9todo dos m\u00ednimos quadrados</p> <p>Implementa\u00e7\u00e3o Pr\u00e1tica - [ ] Implementar regress\u00e3o linear do zero - [ ] Usar scikit-learn para modelos - [ ] Criar pipeline completo</p> <p>Avalia\u00e7\u00e3o de Modelos - [ ] Calcular e interpretar m\u00e9tricas - [ ] Realizar valida\u00e7\u00e3o cruzada - [ ] Diagnosticar problemas</p> <p>T\u00e9cnicas Avan\u00e7adas - [ ] Regress\u00e3o polinomial - [ ] Regulariza\u00e7\u00e3o (Ridge, Lasso, Elastic Net) - [ ] Sele\u00e7\u00e3o de features</p> <p>Projetos - [ ] Completar exerc\u00edcios pr\u00e1ticos - [ ] Desenvolver projeto final - [ ] Apresentar resultados</p>"},{"location":"aulas/IA/lab03/regressao-ml-completa2/#dicas-de-estudo","title":"\ud83d\udca1 Dicas de Estudo","text":"<ol> <li>Pratique com dados reais: Use datasets do Kaggle ou UCI</li> <li>Visualize sempre: Gr\u00e1ficos s\u00e3o fundamentais para entendimento</li> <li>Valide seus modelos: Nunca confie apenas nas m\u00e9tricas de treino</li> <li>Interprete os resultados: N\u00fameros sem contexto n\u00e3o t\u00eam valor</li> <li>Documente seu c\u00f3digo: Coment\u00e1rios facilitam revis\u00e3o</li> <li>Experimente diferentes abordagens: Compare m\u00faltiplos modelos</li> <li>Mantenha-se atualizado: Acompanhe novas t\u00e9cnicas e ferramentas</li> </ol> <p>\ud83c\udf93 Conclus\u00e3o: A regress\u00e3o \u00e9 uma ferramenta poderosa e vers\u00e1til em Machine Learning. Dominar seus conceitos e aplica\u00e7\u00f5es \u00e9 fundamental para qualquer cientista de dados. Continue praticando e explorando novos datasets!</p> <p>Autor: Material Did\u00e1tico de Ci\u00eancia de Dados Data: Agosto 2025 Vers\u00e3o: 2.0 - Atualizada e Expandida</p>"},{"location":"aulas/IA/lab03/regressao-pratica/","title":"\ud83d\udcc8 Regress\u00e3o em ML: Segunda Aula - Pr\u00e1tica","text":"In\u00a0[1]: Copied! <pre># \ud83d\udcca Para trabalhar com dados\nimport pandas as pd              # Tabelas de dados\nimport numpy as np              # C\u00e1lculos matem\u00e1ticos\n\n# \ud83d\udcc8 Para criar gr\u00e1ficos\nimport matplotlib.pyplot as plt  # Gr\u00e1ficos b\u00e1sicos\nimport seaborn as sns           # Gr\u00e1ficos mais bonitos\n\n# \ud83e\udd16 Machine Learning (nossa estrela!)\nfrom sklearn.datasets import fetch_california_housing  # Dataset de casas\nfrom sklearn.model_selection import train_test_split   # Dividir treino/teste\nfrom sklearn.linear_model import LinearRegression      # Regress\u00e3o Linear\nfrom sklearn.preprocessing import PolynomialFeatures   # Regress\u00e3o Polinomial\n\n# \ud83d\udccf Para avaliar nossos modelos\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# \ud83c\udfa8 Deixar gr\u00e1ficos bonitos\nplt.style.use('default')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\nprint(\"\u2705 Todas as bibliotecas carregadas!\")\nprint(\"\ud83d\ude80 Vamos come\u00e7ar nossa jornada com regress\u00e3o!\")\n</pre> # \ud83d\udcca Para trabalhar com dados import pandas as pd              # Tabelas de dados import numpy as np              # C\u00e1lculos matem\u00e1ticos  # \ud83d\udcc8 Para criar gr\u00e1ficos import matplotlib.pyplot as plt  # Gr\u00e1ficos b\u00e1sicos import seaborn as sns           # Gr\u00e1ficos mais bonitos  # \ud83e\udd16 Machine Learning (nossa estrela!) from sklearn.datasets import fetch_california_housing  # Dataset de casas from sklearn.model_selection import train_test_split   # Dividir treino/teste from sklearn.linear_model import LinearRegression      # Regress\u00e3o Linear from sklearn.preprocessing import PolynomialFeatures   # Regress\u00e3o Polinomial  # \ud83d\udccf Para avaliar nossos modelos from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score  # \ud83c\udfa8 Deixar gr\u00e1ficos bonitos plt.style.use('default') sns.set_palette(\"husl\") %matplotlib inline  print(\"\u2705 Todas as bibliotecas carregadas!\") print(\"\ud83d\ude80 Vamos come\u00e7ar nossa jornada com regress\u00e3o!\") <pre>\u2705 Todas as bibliotecas carregadas!\n\ud83d\ude80 Vamos come\u00e7ar nossa jornada com regress\u00e3o!\n</pre> In\u00a0[2]: Copied! <pre># \ud83d\udce6 Carregando o dataset\nprint(\"\ud83d\udce5 Carregando dados das casas da Calif\u00f3rnia...\")\nhousing = fetch_california_housing()\n\n# \ud83d\udccb Criando uma tabela (DataFrame) organizada\ndf = pd.DataFrame(housing.data, columns=housing.feature_names)\ndf['target'] = housing.target  # Esta \u00e9 a coluna que queremos predizer!\n\nprint(f\"\u2705 Dados carregados!\")\nprint(f\"\ud83d\udcca Temos {df.shape[0]} casas e {df.shape[1]} informa\u00e7\u00f5es sobre cada uma\")\nprint(\"\\n\ud83d\udd0d Vamos ver as primeiras linhas:\")\ndf.head()\n</pre> # \ud83d\udce6 Carregando o dataset print(\"\ud83d\udce5 Carregando dados das casas da Calif\u00f3rnia...\") housing = fetch_california_housing()  # \ud83d\udccb Criando uma tabela (DataFrame) organizada df = pd.DataFrame(housing.data, columns=housing.feature_names) df['target'] = housing.target  # Esta \u00e9 a coluna que queremos predizer!  print(f\"\u2705 Dados carregados!\") print(f\"\ud83d\udcca Temos {df.shape[0]} casas e {df.shape[1]} informa\u00e7\u00f5es sobre cada uma\") print(\"\\n\ud83d\udd0d Vamos ver as primeiras linhas:\") df.head() <pre>\ud83d\udce5 Carregando dados das casas da Calif\u00f3rnia...\n\u2705 Dados carregados!\n\ud83d\udcca Temos 20640 casas e 9 informa\u00e7\u00f5es sobre cada uma\n\n\ud83d\udd0d Vamos ver as primeiras linhas:\n</pre> Out[2]: MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude target 0 8.3252 41.0 6.984127 1.023810 322.0 2.555556 37.88 -122.23 4.526 1 8.3014 21.0 6.238137 0.971880 2401.0 2.109842 37.86 -122.22 3.585 2 7.2574 52.0 8.288136 1.073446 496.0 2.802260 37.85 -122.24 3.521 3 5.6431 52.0 5.817352 1.073059 558.0 2.547945 37.85 -122.25 3.413 4 3.8462 52.0 6.281853 1.081081 565.0 2.181467 37.85 -122.25 3.422 In\u00a0[3]: Copied! <pre># \ud83d\udccb Informa\u00e7\u00f5es gerais\nprint(\"\ud83d\udccb INFORMA\u00c7\u00d5ES B\u00c1SICAS\")\nprint(\"=\" * 30)\nprint(f\"N\u00famero de casas: {df.shape[0]:,}\")\nprint(f\"N\u00famero de caracter\u00edsticas: {df.shape[1]}\")\nprint(\"\\n\ud83d\udd0d Tipos de dados:\")\ndf.info()\n\nprint(\"\\n\ud83d\udcca ESTAT\u00cdSTICAS RESUMIDAS\")\nprint(\"=\" * 30)\ndf.describe().round(2)\n</pre> # \ud83d\udccb Informa\u00e7\u00f5es gerais print(\"\ud83d\udccb INFORMA\u00c7\u00d5ES B\u00c1SICAS\") print(\"=\" * 30) print(f\"N\u00famero de casas: {df.shape[0]:,}\") print(f\"N\u00famero de caracter\u00edsticas: {df.shape[1]}\") print(\"\\n\ud83d\udd0d Tipos de dados:\") df.info()  print(\"\\n\ud83d\udcca ESTAT\u00cdSTICAS RESUMIDAS\") print(\"=\" * 30) df.describe().round(2) <pre>\ud83d\udccb INFORMA\u00c7\u00d5ES B\u00c1SICAS\n==============================\nN\u00famero de casas: 20,640\nN\u00famero de caracter\u00edsticas: 9\n\n\ud83d\udd0d Tipos de dados:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 9 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   MedInc      20640 non-null  float64\n 1   HouseAge    20640 non-null  float64\n 2   AveRooms    20640 non-null  float64\n 3   AveBedrms   20640 non-null  float64\n 4   Population  20640 non-null  float64\n 5   AveOccup    20640 non-null  float64\n 6   Latitude    20640 non-null  float64\n 7   Longitude   20640 non-null  float64\n 8   target      20640 non-null  float64\ndtypes: float64(9)\nmemory usage: 1.4 MB\n\n\ud83d\udcca ESTAT\u00cdSTICAS RESUMIDAS\n==============================\n</pre> Out[3]: MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude target count 20640.00 20640.00 20640.00 20640.00 20640.00 20640.00 20640.00 20640.00 20640.00 mean 3.87 28.64 5.43 1.10 1425.48 3.07 35.63 -119.57 2.07 std 1.90 12.59 2.47 0.47 1132.46 10.39 2.14 2.00 1.15 min 0.50 1.00 0.85 0.33 3.00 0.69 32.54 -124.35 0.15 25% 2.56 18.00 4.44 1.01 787.00 2.43 33.93 -121.80 1.20 50% 3.53 29.00 5.23 1.05 1166.00 2.82 34.26 -118.49 1.80 75% 4.74 37.00 6.05 1.10 1725.00 3.28 37.71 -118.01 2.65 max 15.00 52.00 141.91 34.07 35682.00 1243.33 41.95 -114.31 5.00 In\u00a0[4]: Copied! <pre># \ud83d\udea8 Verificando dados faltantes (muito importante!)\nprint(\"\ud83d\udd0d VERIFICANDO DADOS FALTANTES\")\nprint(\"=\" * 30)\ndados_faltantes = df.isnull().sum()\nprint(dados_faltantes)\n\nif dados_faltantes.sum() == 0:\n    print(\"\\n\u2705 Excelente! N\u00e3o temos dados faltantes.\")\n    print(\"\ud83d\udcca Isso facilita muito nossa an\u00e1lise!\")\nelse:\n    print(f\"\\n\u26a0\ufe0f  Temos {dados_faltantes.sum()} dados faltantes para tratar.\")\n</pre> # \ud83d\udea8 Verificando dados faltantes (muito importante!) print(\"\ud83d\udd0d VERIFICANDO DADOS FALTANTES\") print(\"=\" * 30) dados_faltantes = df.isnull().sum() print(dados_faltantes)  if dados_faltantes.sum() == 0:     print(\"\\n\u2705 Excelente! N\u00e3o temos dados faltantes.\")     print(\"\ud83d\udcca Isso facilita muito nossa an\u00e1lise!\") else:     print(f\"\\n\u26a0\ufe0f  Temos {dados_faltantes.sum()} dados faltantes para tratar.\") <pre>\ud83d\udd0d VERIFICANDO DADOS FALTANTES\n==============================\nMedInc        0\nHouseAge      0\nAveRooms      0\nAveBedrms     0\nPopulation    0\nAveOccup      0\nLatitude      0\nLongitude     0\ntarget        0\ndtype: int64\n\n\u2705 Excelente! N\u00e3o temos dados faltantes.\n\ud83d\udcca Isso facilita muito nossa an\u00e1lise!\n</pre> In\u00a0[5]: Copied! <pre># \ud83d\udcca Visualizando a distribui\u00e7\u00e3o dos dados\nfig, axes = plt.subplots(3, 3, figsize=(15, 12))\nfig.suptitle('\ud83d\udcca Como os Dados Est\u00e3o Distribu\u00eddos', fontsize=16, fontweight='bold')\n\nfor i, coluna in enumerate(df.columns):\n    row = i // 3\n    col = i % 3\n    \n    axes[row, col].hist(df[coluna], bins=30, alpha=0.7, \n                       color=sns.color_palette()[i % len(sns.color_palette())])\n    axes[row, col].set_title(f'{coluna}', fontweight='bold')\n    axes[row, col].set_xlabel(coluna)\n    axes[row, col].set_ylabel('Frequ\u00eancia')\n    axes[row, col].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\ud83d\udca1 O que observar:\")\nprint(\"\u2022 As distribui\u00e7\u00f5es parecem 'normais' (formato de sino)?\")\nprint(\"\u2022 Existem valores muito extremos?\")\nprint(\"\u2022 Os dados est\u00e3o concentrados em alguma faixa?\")\n</pre> # \ud83d\udcca Visualizando a distribui\u00e7\u00e3o dos dados fig, axes = plt.subplots(3, 3, figsize=(15, 12)) fig.suptitle('\ud83d\udcca Como os Dados Est\u00e3o Distribu\u00eddos', fontsize=16, fontweight='bold')  for i, coluna in enumerate(df.columns):     row = i // 3     col = i % 3          axes[row, col].hist(df[coluna], bins=30, alpha=0.7,                         color=sns.color_palette()[i % len(sns.color_palette())])     axes[row, col].set_title(f'{coluna}', fontweight='bold')     axes[row, col].set_xlabel(coluna)     axes[row, col].set_ylabel('Frequ\u00eancia')     axes[row, col].grid(True, alpha=0.3)  plt.tight_layout() plt.show()  print(\"\ud83d\udca1 O que observar:\") print(\"\u2022 As distribui\u00e7\u00f5es parecem 'normais' (formato de sino)?\") print(\"\u2022 Existem valores muito extremos?\") print(\"\u2022 Os dados est\u00e3o concentrados em alguma faixa?\") <pre>/var/folders/lk/j2v60jvx5sdgx3dm_16q4nxm0000gn/T/ipykernel_76956/2766775749.py:16: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n</pre> <pre>\ud83d\udca1 O que observar:\n\u2022 As distribui\u00e7\u00f5es parecem 'normais' (formato de sino)?\n\u2022 Existem valores muito extremos?\n\u2022 Os dados est\u00e3o concentrados em alguma faixa?\n</pre> In\u00a0[25]: Copied! <pre>df.plot(kind=\"scatter\", x=\"Longitude\",y=\"Latitude\", c=\"target\", cmap=\"jet\", colorbar=True, legend=True, sharex=False, figsize=(10,7), s=df['Population']/100, label=\"population\", alpha=0.7)\nplt.show()\n</pre> df.plot(kind=\"scatter\", x=\"Longitude\",y=\"Latitude\", c=\"target\", cmap=\"jet\", colorbar=True, legend=True, sharex=False, figsize=(10,7), s=df['Population']/100, label=\"population\", alpha=0.7) plt.show() In\u00a0[6]: Copied! <pre># \ud83d\udd17 Calculando e visualizando correla\u00e7\u00f5es\nplt.figure(figsize=(10, 8))\ncorrelation_matrix = df.corr()\n\nsns.heatmap(correlation_matrix, \n            annot=True,           # Mostrar n\u00fameros\n            cmap='coolwarm',      # Cores: azul = negativo, vermelho = positivo\n            center=0,             # Zero no centro\n            square=True,          # C\u00e9lulas quadradas\n            linewidths=0.5,       # Linhas entre c\u00e9lulas\n            fmt='.2f')            # 2 casas decimais\n\nplt.title('\ud83d\udd17 Matriz de Correla\u00e7\u00e3o', fontsize=16, fontweight='bold')\nplt.show()\n\nprint(\"\ud83c\udfaf Correla\u00e7\u00f5es com o PRE\u00c7O (target):\")\ncorrelacoes_target = correlation_matrix['target'].sort_values(ascending=False)\nfor variavel, correlacao in correlacoes_target.items():\n    if variavel != 'target':\n        emoji = \"\ud83d\udcc8\" if correlacao &gt; 0 else \"\ud83d\udcc9\"\n        print(f\"{emoji} {variavel}: {correlacao:.3f}\")\n</pre> # \ud83d\udd17 Calculando e visualizando correla\u00e7\u00f5es plt.figure(figsize=(10, 8)) correlation_matrix = df.corr()  sns.heatmap(correlation_matrix,              annot=True,           # Mostrar n\u00fameros             cmap='coolwarm',      # Cores: azul = negativo, vermelho = positivo             center=0,             # Zero no centro             square=True,          # C\u00e9lulas quadradas             linewidths=0.5,       # Linhas entre c\u00e9lulas             fmt='.2f')            # 2 casas decimais  plt.title('\ud83d\udd17 Matriz de Correla\u00e7\u00e3o', fontsize=16, fontweight='bold') plt.show()  print(\"\ud83c\udfaf Correla\u00e7\u00f5es com o PRE\u00c7O (target):\") correlacoes_target = correlation_matrix['target'].sort_values(ascending=False) for variavel, correlacao in correlacoes_target.items():     if variavel != 'target':         emoji = \"\ud83d\udcc8\" if correlacao &gt; 0 else \"\ud83d\udcc9\"         print(f\"{emoji} {variavel}: {correlacao:.3f}\") <pre>/Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 128279 (\\N{LINK SYMBOL}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n</pre> <pre>\ud83c\udfaf Correla\u00e7\u00f5es com o PRE\u00c7O (target):\n\ud83d\udcc8 MedInc: 0.688\n\ud83d\udcc8 AveRooms: 0.152\n\ud83d\udcc8 HouseAge: 0.106\n\ud83d\udcc9 AveOccup: -0.024\n\ud83d\udcc9 Population: -0.025\n\ud83d\udcc9 Longitude: -0.046\n\ud83d\udcc9 AveBedrms: -0.047\n\ud83d\udcc9 Latitude: -0.144\n</pre> In\u00a0[7]: Copied! <pre># \ud83d\udcc8 Visualizando as rela\u00e7\u00f5es mais fortes\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nfig.suptitle('\ud83d\udcc8 Rela\u00e7\u00e3o das Vari\u00e1veis com o Pre\u00e7o', fontsize=16)\n\n# Selecionando 4 vari\u00e1veis mais interessantes\nvariaveis_importantes = ['MedInc', 'AveRooms', 'HouseAge', 'AveBedrms']\n\nfor i, variavel in enumerate(variaveis_importantes):\n    row = i // 2\n    col = i % 2\n    \n    # Pegando uma amostra para n\u00e3o sobrecarregar o gr\u00e1fico\n    sample = df.sample(2000, random_state=42)\n    \n    axes[row, col].scatter(sample[variavel], sample['target'], alpha=0.6, s=20)\n    axes[row, col].set_xlabel(variavel)\n    axes[row, col].set_ylabel('Pre\u00e7o')\n    axes[row, col].set_title(f'{variavel} vs Pre\u00e7o')\n    axes[row, col].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</pre> # \ud83d\udcc8 Visualizando as rela\u00e7\u00f5es mais fortes fig, axes = plt.subplots(2, 2, figsize=(12, 10)) fig.suptitle('\ud83d\udcc8 Rela\u00e7\u00e3o das Vari\u00e1veis com o Pre\u00e7o', fontsize=16)  # Selecionando 4 vari\u00e1veis mais interessantes variaveis_importantes = ['MedInc', 'AveRooms', 'HouseAge', 'AveBedrms']  for i, variavel in enumerate(variaveis_importantes):     row = i // 2     col = i % 2          # Pegando uma amostra para n\u00e3o sobrecarregar o gr\u00e1fico     sample = df.sample(2000, random_state=42)          axes[row, col].scatter(sample[variavel], sample['target'], alpha=0.6, s=20)     axes[row, col].set_xlabel(variavel)     axes[row, col].set_ylabel('Pre\u00e7o')     axes[row, col].set_title(f'{variavel} vs Pre\u00e7o')     axes[row, col].grid(True, alpha=0.3)  plt.tight_layout() plt.show() <pre>/var/folders/lk/j2v60jvx5sdgx3dm_16q4nxm0000gn/T/ipykernel_76956/1718699509.py:21: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n</pre> In\u00a0[8]: Copied! <pre># \ud83c\udfaf Vamos come\u00e7ar SIMPLES: usar s\u00f3 a RENDA para predizer PRE\u00c7O\nprint(\"\ud83c\udfaf PREPARANDO OS DADOS\")\nprint(\"=\" * 25)\n\n# X = features (caracter\u00edsticas) - o que usamos para fazer a predi\u00e7\u00e3o\nX_simples = df[['MedInc']]  # S\u00f3 a renda, por enquanto\n\n# y = target (alvo) - o que queremos predizer\ny = df['target']  # O pre\u00e7o das casas\n\nprint(f\"\u2705 Features (X): {X_simples.shape[0]} casas, {X_simples.shape[1]} caracter\u00edstica\")\nprint(f\"\u2705 Target (y): {y.shape[0]} pre\u00e7os\")\nprint(f\"\\n\ud83d\udd0d Exemplo dos dados:\")\nprint(f\"Renda m\u00e9dia: {X_simples.iloc[0, 0]:.2f} \u2192 Pre\u00e7o: ${y.iloc[0]:.1f}k\")\n</pre> # \ud83c\udfaf Vamos come\u00e7ar SIMPLES: usar s\u00f3 a RENDA para predizer PRE\u00c7O print(\"\ud83c\udfaf PREPARANDO OS DADOS\") print(\"=\" * 25)  # X = features (caracter\u00edsticas) - o que usamos para fazer a predi\u00e7\u00e3o X_simples = df[['MedInc']]  # S\u00f3 a renda, por enquanto  # y = target (alvo) - o que queremos predizer y = df['target']  # O pre\u00e7o das casas  print(f\"\u2705 Features (X): {X_simples.shape[0]} casas, {X_simples.shape[1]} caracter\u00edstica\") print(f\"\u2705 Target (y): {y.shape[0]} pre\u00e7os\") print(f\"\\n\ud83d\udd0d Exemplo dos dados:\") print(f\"Renda m\u00e9dia: {X_simples.iloc[0, 0]:.2f} \u2192 Pre\u00e7o: ${y.iloc[0]:.1f}k\") <pre>\ud83c\udfaf PREPARANDO OS DADOS\n=========================\n\u2705 Features (X): 20640 casas, 1 caracter\u00edstica\n\u2705 Target (y): 20640 pre\u00e7os\n\n\ud83d\udd0d Exemplo dos dados:\nRenda m\u00e9dia: 8.33 \u2192 Pre\u00e7o: $4.5k\n</pre> In\u00a0[9]: Copied! <pre># \u2702\ufe0f Dividindo os dados\nX_train, X_test, y_train, y_test = train_test_split(\n    X_simples, y,           # Nossos dados\n    test_size=0.2,          # 20% para teste\n    random_state=42         # Para resultados reproduz\u00edveis\n)\n\nprint(\"\u2702\ufe0f DIVIS\u00c3O DOS DADOS\")\nprint(\"=\" * 20)\nprint(f\"\ud83d\udcda Treino: {X_train.shape[0]} casas ({X_train.shape[0]/len(df)*100:.0f}%)\")\nprint(f\"\ud83d\udcdd Teste: {X_test.shape[0]} casas ({X_test.shape[0]/len(df)*100:.0f}%)\")\nprint(f\"\\n\ud83d\udca1 O modelo vai 'estudar' com {X_train.shape[0]} casas\")\nprint(f\"\ud83d\udca1 E vai ser 'testado' com {X_test.shape[0]} casas que ele nunca viu!\")\n</pre> # \u2702\ufe0f Dividindo os dados X_train, X_test, y_train, y_test = train_test_split(     X_simples, y,           # Nossos dados     test_size=0.2,          # 20% para teste     random_state=42         # Para resultados reproduz\u00edveis )  print(\"\u2702\ufe0f DIVIS\u00c3O DOS DADOS\") print(\"=\" * 20) print(f\"\ud83d\udcda Treino: {X_train.shape[0]} casas ({X_train.shape[0]/len(df)*100:.0f}%)\") print(f\"\ud83d\udcdd Teste: {X_test.shape[0]} casas ({X_test.shape[0]/len(df)*100:.0f}%)\") print(f\"\\n\ud83d\udca1 O modelo vai 'estudar' com {X_train.shape[0]} casas\") print(f\"\ud83d\udca1 E vai ser 'testado' com {X_test.shape[0]} casas que ele nunca viu!\") <pre>\u2702\ufe0f DIVIS\u00c3O DOS DADOS\n====================\n\ud83d\udcda Treino: 16512 casas (80%)\n\ud83d\udcdd Teste: 4128 casas (20%)\n\n\ud83d\udca1 O modelo vai 'estudar' com 16512 casas\n\ud83d\udca1 E vai ser 'testado' com 4128 casas que ele nunca viu!\n</pre> In\u00a0[10]: Copied! <pre># \ud83e\udd16 Criando o modelo\nprint(\"\ud83e\udd16 TREINANDO O MODELO\")\nprint(\"=\" * 20)\nprint(\"\ud83d\udcd6 Criando modelo de Regress\u00e3o Linear...\")\n\nmodelo = LinearRegression()\n\nprint(\"\ud83c\udf93 Treinando com os dados de treino...\")\nmodelo.fit(X_train, y_train)  # Aqui \u00e9 onde a m\u00e1gica acontece!\n\nprint(\"\u2705 Modelo treinado com sucesso!\")\nprint(\"\\n\ud83e\udde0 O modelo aprendeu a equa\u00e7\u00e3o:\")\nprint(f\"Pre\u00e7o = {modelo.intercept_:.4f} + {modelo.coef_[0]:.4f} \u00d7 Renda\")\nprint(f\"\\n\ud83d\udcca Interpreta\u00e7\u00e3o:\")\nprint(f\"\u2022 Se a renda = 0, o pre\u00e7o base seria ${modelo.intercept_:.1f}k\")\nprint(f\"\u2022 Para cada unidade de renda a mais, o pre\u00e7o sobe ${modelo.coef_[0]:.1f}k\")\n</pre> # \ud83e\udd16 Criando o modelo print(\"\ud83e\udd16 TREINANDO O MODELO\") print(\"=\" * 20) print(\"\ud83d\udcd6 Criando modelo de Regress\u00e3o Linear...\")  modelo = LinearRegression()  print(\"\ud83c\udf93 Treinando com os dados de treino...\") modelo.fit(X_train, y_train)  # Aqui \u00e9 onde a m\u00e1gica acontece!  print(\"\u2705 Modelo treinado com sucesso!\") print(\"\\n\ud83e\udde0 O modelo aprendeu a equa\u00e7\u00e3o:\") print(f\"Pre\u00e7o = {modelo.intercept_:.4f} + {modelo.coef_[0]:.4f} \u00d7 Renda\") print(f\"\\n\ud83d\udcca Interpreta\u00e7\u00e3o:\") print(f\"\u2022 Se a renda = 0, o pre\u00e7o base seria ${modelo.intercept_:.1f}k\") print(f\"\u2022 Para cada unidade de renda a mais, o pre\u00e7o sobe ${modelo.coef_[0]:.1f}k\") <pre>\ud83e\udd16 TREINANDO O MODELO\n====================\n\ud83d\udcd6 Criando modelo de Regress\u00e3o Linear...\n\ud83c\udf93 Treinando com os dados de treino...\n\u2705 Modelo treinado com sucesso!\n\n\ud83e\udde0 O modelo aprendeu a equa\u00e7\u00e3o:\nPre\u00e7o = 0.4446 + 0.4193 \u00d7 Renda\n\n\ud83d\udcca Interpreta\u00e7\u00e3o:\n\u2022 Se a renda = 0, o pre\u00e7o base seria $0.4k\n\u2022 Para cada unidade de renda a mais, o pre\u00e7o sobe $0.4k\n</pre> In\u00a0[11]: Copied! <pre># \ud83d\udd2e Fazendo predi\u00e7\u00f5es\nprint(\"\ud83d\udd2e FAZENDO PREDI\u00c7\u00d5ES\")\nprint(\"=\" * 20)\n\ny_pred = modelo.predict(X_test)\n\nprint(\"\ud83d\udcca Exemplos de predi\u00e7\u00f5es:\")\nprint(\"Renda \u2192 Real vs Predito\")\nprint(\"-\" * 30)\n\nfor i in range(8):\n    renda = X_test.iloc[i, 0]\n    preco_real = y_test.iloc[i]\n    preco_predito = y_pred[i]\n    diferenca = abs(preco_real - preco_predito)\n    \n    print(f\"{renda:.1f} \u2192 Real: ${preco_real:.1f}k, Predito: ${preco_predito:.1f}k (dif: ${diferenca:.1f}k)\")\n</pre> # \ud83d\udd2e Fazendo predi\u00e7\u00f5es print(\"\ud83d\udd2e FAZENDO PREDI\u00c7\u00d5ES\") print(\"=\" * 20)  y_pred = modelo.predict(X_test)  print(\"\ud83d\udcca Exemplos de predi\u00e7\u00f5es:\") print(\"Renda \u2192 Real vs Predito\") print(\"-\" * 30)  for i in range(8):     renda = X_test.iloc[i, 0]     preco_real = y_test.iloc[i]     preco_predito = y_pred[i]     diferenca = abs(preco_real - preco_predito)          print(f\"{renda:.1f} \u2192 Real: ${preco_real:.1f}k, Predito: ${preco_predito:.1f}k (dif: ${diferenca:.1f}k)\") <pre>\ud83d\udd2e FAZENDO PREDI\u00c7\u00d5ES\n====================\n\ud83d\udcca Exemplos de predi\u00e7\u00f5es:\nRenda \u2192 Real vs Predito\n------------------------------\n1.7 \u2192 Real: $0.5k, Predito: $1.1k (dif: $0.7k)\n2.5 \u2192 Real: $0.5k, Predito: $1.5k (dif: $1.0k)\n3.5 \u2192 Real: $5.0k, Predito: $1.9k (dif: $3.1k)\n5.7 \u2192 Real: $2.2k, Predito: $2.9k (dif: $0.7k)\n3.7 \u2192 Real: $2.8k, Predito: $2.0k (dif: $0.8k)\n4.7 \u2192 Real: $1.6k, Predito: $2.4k (dif: $0.8k)\n5.1 \u2192 Real: $2.0k, Predito: $2.6k (dif: $0.6k)\n3.7 \u2192 Real: $1.6k, Predito: $2.0k (dif: $0.4k)\n</pre> In\u00a0[13]: Copied! <pre># \ud83d\udccf Calculando m\u00e9tricas\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"\ud83d\udccf AVALIA\u00c7\u00c3O DO MODELO\")\nprint(\"=\" * 25)\nprint(f\"MSE (Erro Quadr\u00e1tico M\u00e9dio): {mse:.4f}\")\nprint(f\"RMSE (Raiz do MSE): {rmse:.4f}\")\nprint(f\"MAE (Erro Absoluto M\u00e9dio): {mae:.4f}\")\nprint(f\"R\u00b2 (Coeficiente de Determina\u00e7\u00e3o): {r2:.4f}\")\n\nprint(f\"\\n\ud83d\udca1 INTERPRETA\u00c7\u00c3O:\")\nprint(f\"\ud83d\udcca R\u00b2 = {r2:.1%} significa que nosso modelo explica {r2:.1%} da varia\u00e7\u00e3o nos pre\u00e7os\")\nprint(f\"\ud83d\udcca RMSE = ${rmse:.0f}k \u00e9 o erro m\u00e9dio das nossas predi\u00e7\u00f5es\")\nprint(f\"\ud83d\udcca Em m\u00e9dia, erramos por ${mae:.0f}k para mais ou para menos\")\n\n# Interpreta\u00e7\u00e3o do R\u00b2\nif r2 &gt; 0.8:\n    print(\"\ud83c\udf89 Modelo EXCELENTE!\")\nelif r2 &gt; 0.6:\n    print(\"\ud83d\ude0a Modelo BOM!\")\nelif r2 &gt; 0.4:\n    print(\"\ud83e\udd14 Modelo RAZO\u00c1VEL\")\nelse:\n    print(\"\ud83d\ude1e Modelo PRECISA MELHORAR\")\n</pre> # \ud83d\udccf Calculando m\u00e9tricas mse = mean_squared_error(y_test, y_pred) rmse = np.sqrt(mse) mae = mean_absolute_error(y_test, y_pred) r2 = r2_score(y_test, y_pred)  print(\"\ud83d\udccf AVALIA\u00c7\u00c3O DO MODELO\") print(\"=\" * 25) print(f\"MSE (Erro Quadr\u00e1tico M\u00e9dio): {mse:.4f}\") print(f\"RMSE (Raiz do MSE): {rmse:.4f}\") print(f\"MAE (Erro Absoluto M\u00e9dio): {mae:.4f}\") print(f\"R\u00b2 (Coeficiente de Determina\u00e7\u00e3o): {r2:.4f}\")  print(f\"\\n\ud83d\udca1 INTERPRETA\u00c7\u00c3O:\") print(f\"\ud83d\udcca R\u00b2 = {r2:.1%} significa que nosso modelo explica {r2:.1%} da varia\u00e7\u00e3o nos pre\u00e7os\") print(f\"\ud83d\udcca RMSE = ${rmse:.0f}k \u00e9 o erro m\u00e9dio das nossas predi\u00e7\u00f5es\") print(f\"\ud83d\udcca Em m\u00e9dia, erramos por ${mae:.0f}k para mais ou para menos\")  # Interpreta\u00e7\u00e3o do R\u00b2 if r2 &gt; 0.8:     print(\"\ud83c\udf89 Modelo EXCELENTE!\") elif r2 &gt; 0.6:     print(\"\ud83d\ude0a Modelo BOM!\") elif r2 &gt; 0.4:     print(\"\ud83e\udd14 Modelo RAZO\u00c1VEL\") else:     print(\"\ud83d\ude1e Modelo PRECISA MELHORAR\") <pre>\ud83d\udccf AVALIA\u00c7\u00c3O DO MODELO\n=========================\nMSE (Erro Quadr\u00e1tico M\u00e9dio): 0.7091\nRMSE (Raiz do MSE): 0.8421\nMAE (Erro Absoluto M\u00e9dio): 0.6299\nR\u00b2 (Coeficiente de Determina\u00e7\u00e3o): 0.4589\n\n\ud83d\udca1 INTERPRETA\u00c7\u00c3O:\n\ud83d\udcca R\u00b2 = 45.9% significa que nosso modelo explica 45.9% da varia\u00e7\u00e3o nos pre\u00e7os\n\ud83d\udcca RMSE = $1k \u00e9 o erro m\u00e9dio das nossas predi\u00e7\u00f5es\n\ud83d\udcca Em m\u00e9dia, erramos por $1k para mais ou para menos\n\ud83e\udd14 Modelo RAZO\u00c1VEL\n</pre> In\u00a0[14]: Copied! <pre># \ud83d\udcc8 Visualizando os resultados\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Gr\u00e1fico 1: Reta de regress\u00e3o\n# Pegando uma amostra para n\u00e3o sobrecarregar\nsample_size = 1000\nsample_idx = np.random.choice(len(X_test), sample_size, replace=False)\nX_sample = X_test.iloc[sample_idx]\ny_sample = y_test.iloc[sample_idx]\ny_pred_sample = y_pred[sample_idx]\n\naxes[0].scatter(X_sample, y_sample, alpha=0.6, s=20, label='Dados reais')\naxes[0].plot(X_sample, y_pred_sample, color='red', linewidth=2, label='Linha de Regress\u00e3o')\naxes[0].set_xlabel('Renda M\u00e9dia')\naxes[0].set_ylabel('Pre\u00e7o das Casas')\naxes[0].set_title('\ud83d\udcc8 Regress\u00e3o Linear')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Gr\u00e1fico 2: Real vs Predito\naxes[1].scatter(y_test, y_pred, alpha=0.6, s=20)\n# Linha diagonal perfeita\nmin_val = min(y_test.min(), y_pred.min())\nmax_val = max(y_test.max(), y_pred.max())\naxes[1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Predi\u00e7\u00e3o Perfeita')\naxes[1].set_xlabel('Pre\u00e7o Real')\naxes[1].set_ylabel('Pre\u00e7o Predito')\naxes[1].set_title('\ud83d\udcca Real vs Predito')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\ud83d\udca1 No gr\u00e1fico da direita:\")\nprint(\"\u2022 Pontos na linha vermelha = predi\u00e7\u00f5es perfeitas\")\nprint(\"\u2022 Pontos dispersos = modelo tem dificuldade\")\n</pre> # \ud83d\udcc8 Visualizando os resultados fig, axes = plt.subplots(1, 2, figsize=(14, 6))  # Gr\u00e1fico 1: Reta de regress\u00e3o # Pegando uma amostra para n\u00e3o sobrecarregar sample_size = 1000 sample_idx = np.random.choice(len(X_test), sample_size, replace=False) X_sample = X_test.iloc[sample_idx] y_sample = y_test.iloc[sample_idx] y_pred_sample = y_pred[sample_idx]  axes[0].scatter(X_sample, y_sample, alpha=0.6, s=20, label='Dados reais') axes[0].plot(X_sample, y_pred_sample, color='red', linewidth=2, label='Linha de Regress\u00e3o') axes[0].set_xlabel('Renda M\u00e9dia') axes[0].set_ylabel('Pre\u00e7o das Casas') axes[0].set_title('\ud83d\udcc8 Regress\u00e3o Linear') axes[0].legend() axes[0].grid(True, alpha=0.3)  # Gr\u00e1fico 2: Real vs Predito axes[1].scatter(y_test, y_pred, alpha=0.6, s=20) # Linha diagonal perfeita min_val = min(y_test.min(), y_pred.min()) max_val = max(y_test.max(), y_pred.max()) axes[1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Predi\u00e7\u00e3o Perfeita') axes[1].set_xlabel('Pre\u00e7o Real') axes[1].set_ylabel('Pre\u00e7o Predito') axes[1].set_title('\ud83d\udcca Real vs Predito') axes[1].legend() axes[1].grid(True, alpha=0.3)  plt.tight_layout() plt.show()  print(\"\ud83d\udca1 No gr\u00e1fico da direita:\") print(\"\u2022 Pontos na linha vermelha = predi\u00e7\u00f5es perfeitas\") print(\"\u2022 Pontos dispersos = modelo tem dificuldade\") <pre>/var/folders/lk/j2v60jvx5sdgx3dm_16q4nxm0000gn/T/ipykernel_76956/93097262.py:32: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/var/folders/lk/j2v60jvx5sdgx3dm_16q4nxm0000gn/T/ipykernel_76956/93097262.py:32: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n</pre> <pre>\ud83d\udca1 No gr\u00e1fico da direita:\n\u2022 Pontos na linha vermelha = predi\u00e7\u00f5es perfeitas\n\u2022 Pontos dispersos = modelo tem dificuldade\n</pre> In\u00a0[15]: Copied! <pre># \ud83c\udf1f Usando m\u00faltiplas vari\u00e1veis\nprint(\"\ud83c\udf1f MELHORANDO O MODELO\")\nprint(\"=\" * 25)\n\n# Selecionando as 4 vari\u00e1veis mais correlacionadas\nfeatures_importantes = ['MedInc', 'AveRooms', 'HouseAge', 'AveBedrms']\nX_multiplo = df[features_importantes]\n\nprint(f\"\ud83d\udcca Agora usando {len(features_importantes)} vari\u00e1veis: {features_importantes}\")\n\n# Dividindo novamente\nX_train_mult, X_test_mult, y_train_mult, y_test_mult = train_test_split(\n    X_multiplo, y, test_size=0.2, random_state=42\n)\n\n# Treinando modelo m\u00faltiplo\nmodelo_mult = LinearRegression()\nmodelo_mult.fit(X_train_mult, y_train_mult)\n\n# Predi\u00e7\u00f5es\ny_pred_mult = modelo_mult.predict(X_test_mult)\n\n# M\u00e9tricas do modelo m\u00faltiplo\nr2_mult = r2_score(y_test_mult, y_pred_mult)\nrmse_mult = np.sqrt(mean_squared_error(y_test_mult, y_pred_mult))\nmae_mult = mean_absolute_error(y_test_mult, y_pred_mult)\n\nprint(f\"\\n\ud83d\udcca COMPARA\u00c7\u00c3O DOS MODELOS\")\nprint(\"=\" * 30)\nprint(f\"Modelo Simples (1 vari\u00e1vel):\")\nprint(f\"  R\u00b2: {r2:.4f} | RMSE: ${rmse:.0f}k | MAE: ${mae:.0f}k\")\nprint(f\"Modelo M\u00faltiplo (4 vari\u00e1veis):\")\nprint(f\"  R\u00b2: {r2_mult:.4f} | RMSE: ${rmse_mult:.0f}k | MAE: ${mae_mult:.0f}k\")\n\n# Calculando melhoria\nmelhoria_r2 = ((r2_mult - r2) / r2) * 100\nmelhoria_rmse = ((rmse - rmse_mult) / rmse) * 100\n\nprint(f\"\\n\ud83d\ude80 MELHORIA:\")\nprint(f\"\ud83d\udcc8 R\u00b2 melhorou {melhoria_r2:.1f}%\")\nprint(f\"\ud83d\udcc9 RMSE melhorou {melhoria_rmse:.1f}%\")\n\nprint(f\"\\n\ud83d\udcca Import\u00e2ncia das vari\u00e1veis (coeficientes):\")\nfor feature, coef in zip(features_importantes, modelo_mult.coef_):\n    print(f\"\u2022 {feature}: {coef:.4f}\")\n</pre> # \ud83c\udf1f Usando m\u00faltiplas vari\u00e1veis print(\"\ud83c\udf1f MELHORANDO O MODELO\") print(\"=\" * 25)  # Selecionando as 4 vari\u00e1veis mais correlacionadas features_importantes = ['MedInc', 'AveRooms', 'HouseAge', 'AveBedrms'] X_multiplo = df[features_importantes]  print(f\"\ud83d\udcca Agora usando {len(features_importantes)} vari\u00e1veis: {features_importantes}\")  # Dividindo novamente X_train_mult, X_test_mult, y_train_mult, y_test_mult = train_test_split(     X_multiplo, y, test_size=0.2, random_state=42 )  # Treinando modelo m\u00faltiplo modelo_mult = LinearRegression() modelo_mult.fit(X_train_mult, y_train_mult)  # Predi\u00e7\u00f5es y_pred_mult = modelo_mult.predict(X_test_mult)  # M\u00e9tricas do modelo m\u00faltiplo r2_mult = r2_score(y_test_mult, y_pred_mult) rmse_mult = np.sqrt(mean_squared_error(y_test_mult, y_pred_mult)) mae_mult = mean_absolute_error(y_test_mult, y_pred_mult)  print(f\"\\n\ud83d\udcca COMPARA\u00c7\u00c3O DOS MODELOS\") print(\"=\" * 30) print(f\"Modelo Simples (1 vari\u00e1vel):\") print(f\"  R\u00b2: {r2:.4f} | RMSE: ${rmse:.0f}k | MAE: ${mae:.0f}k\") print(f\"Modelo M\u00faltiplo (4 vari\u00e1veis):\") print(f\"  R\u00b2: {r2_mult:.4f} | RMSE: ${rmse_mult:.0f}k | MAE: ${mae_mult:.0f}k\")  # Calculando melhoria melhoria_r2 = ((r2_mult - r2) / r2) * 100 melhoria_rmse = ((rmse - rmse_mult) / rmse) * 100  print(f\"\\n\ud83d\ude80 MELHORIA:\") print(f\"\ud83d\udcc8 R\u00b2 melhorou {melhoria_r2:.1f}%\") print(f\"\ud83d\udcc9 RMSE melhorou {melhoria_rmse:.1f}%\")  print(f\"\\n\ud83d\udcca Import\u00e2ncia das vari\u00e1veis (coeficientes):\") for feature, coef in zip(features_importantes, modelo_mult.coef_):     print(f\"\u2022 {feature}: {coef:.4f}\") <pre>\ud83c\udf1f MELHORANDO O MODELO\n=========================\n\ud83d\udcca Agora usando 4 vari\u00e1veis: ['MedInc', 'AveRooms', 'HouseAge', 'AveBedrms']\n\n\ud83d\udcca COMPARA\u00c7\u00c3O DOS MODELOS\n==============================\nModelo Simples (1 vari\u00e1vel):\n  R\u00b2: 0.4589 | RMSE: $1k | MAE: $1k\nModelo M\u00faltiplo (4 vari\u00e1veis):\n  R\u00b2: 0.5089 | RMSE: $1k | MAE: $1k\n\n\ud83d\ude80 MELHORIA:\n\ud83d\udcc8 R\u00b2 melhorou 10.9%\n\ud83d\udcc9 RMSE melhorou 4.7%\n\n\ud83d\udcca Import\u00e2ncia das vari\u00e1veis (coeficientes):\n\u2022 MedInc: 0.5453\n\u2022 AveRooms: -0.2246\n\u2022 HouseAge: 0.0161\n\u2022 AveBedrms: 1.1130\n</pre> In\u00a0[16]: Copied! <pre># \ud83d\udd04 Explorando Regress\u00e3o Polinomial\nprint(\"\ud83d\udd04 REGRESS\u00c3O POLINOMIAL\")\nprint(\"=\" * 25)\nprint(\"\ud83d\udca1 Vamos testar se curvas funcionam melhor que retas!\")\n\n# Voltando para 1 vari\u00e1vel para visualizar melhor\nX_poly_base = df[['MedInc']]\ny_poly = df['target']\n\n# Testando diferentes graus de polin\u00f4mio\ngraus = [1, 2, 3, 4, 5]\nresultados_poly = {}\n\nprint(f\"\\n\ud83e\uddea Testando graus polinomiais: {graus}\")\n\nfor grau in graus:\n    print(f\"\\n\ud83d\udcca Testando grau {grau}...\")\n    \n    # Transforma\u00e7\u00e3o polinomial (aqui a m\u00e1gica acontece!)\n    poly_features = PolynomialFeatures(degree=grau, include_bias=False)\n    X_poly = poly_features.fit_transform(X_poly_base)\n    \n    # Divis\u00e3o treino/teste\n    X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(\n        X_poly, y_poly, test_size=0.2, random_state=42\n    )\n    \n    # Treinamento\n    modelo_poly = LinearRegression()\n    modelo_poly.fit(X_train_poly, y_train_poly)\n    \n    # Predi\u00e7\u00f5es\n    y_pred_poly = modelo_poly.predict(X_test_poly)\n    \n    # M\u00e9tricas\n    r2_poly = r2_score(y_test_poly, y_pred_poly)\n    rmse_poly = np.sqrt(mean_squared_error(y_test_poly, y_pred_poly))\n    \n    resultados_poly[grau] = {\n        'r2': r2_poly, \n        'rmse': rmse_poly,\n        'modelo': modelo_poly,\n        'poly_features': poly_features\n    }\n    \n    print(f\"  R\u00b2: {r2_poly:.4f} | RMSE: {rmse_poly:.4f}\")\n\nprint(f\"\\n\ud83d\udcca RESUMO DOS RESULTADOS:\")\nprint(\"Grau | R\u00b2     | RMSE\")\nprint(\"-\" * 20)\nfor grau, metricas in resultados_poly.items():\n    print(f\"{grau:4d} | {metricas['r2']:.4f} | {metricas['rmse']:.4f}\")\n</pre> # \ud83d\udd04 Explorando Regress\u00e3o Polinomial print(\"\ud83d\udd04 REGRESS\u00c3O POLINOMIAL\") print(\"=\" * 25) print(\"\ud83d\udca1 Vamos testar se curvas funcionam melhor que retas!\")  # Voltando para 1 vari\u00e1vel para visualizar melhor X_poly_base = df[['MedInc']] y_poly = df['target']  # Testando diferentes graus de polin\u00f4mio graus = [1, 2, 3, 4, 5] resultados_poly = {}  print(f\"\\n\ud83e\uddea Testando graus polinomiais: {graus}\")  for grau in graus:     print(f\"\\n\ud83d\udcca Testando grau {grau}...\")          # Transforma\u00e7\u00e3o polinomial (aqui a m\u00e1gica acontece!)     poly_features = PolynomialFeatures(degree=grau, include_bias=False)     X_poly = poly_features.fit_transform(X_poly_base)          # Divis\u00e3o treino/teste     X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(         X_poly, y_poly, test_size=0.2, random_state=42     )          # Treinamento     modelo_poly = LinearRegression()     modelo_poly.fit(X_train_poly, y_train_poly)          # Predi\u00e7\u00f5es     y_pred_poly = modelo_poly.predict(X_test_poly)          # M\u00e9tricas     r2_poly = r2_score(y_test_poly, y_pred_poly)     rmse_poly = np.sqrt(mean_squared_error(y_test_poly, y_pred_poly))          resultados_poly[grau] = {         'r2': r2_poly,          'rmse': rmse_poly,         'modelo': modelo_poly,         'poly_features': poly_features     }          print(f\"  R\u00b2: {r2_poly:.4f} | RMSE: {rmse_poly:.4f}\")  print(f\"\\n\ud83d\udcca RESUMO DOS RESULTADOS:\") print(\"Grau | R\u00b2     | RMSE\") print(\"-\" * 20) for grau, metricas in resultados_poly.items():     print(f\"{grau:4d} | {metricas['r2']:.4f} | {metricas['rmse']:.4f}\") <pre>\ud83d\udd04 REGRESS\u00c3O POLINOMIAL\n=========================\n\ud83d\udca1 Vamos testar se curvas funcionam melhor que retas!\n\n\ud83e\uddea Testando graus polinomiais: [1, 2, 3, 4, 5]\n\n\ud83d\udcca Testando grau 1...\n  R\u00b2: 0.4589 | RMSE: 0.8421\n\n\ud83d\udcca Testando grau 2...\n  R\u00b2: 0.4633 | RMSE: 0.8386\n\n\ud83d\udcca Testando grau 3...\n  R\u00b2: 0.4671 | RMSE: 0.8356\n\n\ud83d\udcca Testando grau 4...\n  R\u00b2: 0.4673 | RMSE: 0.8355\n\n\ud83d\udcca Testando grau 5...\n  R\u00b2: 0.4668 | RMSE: 0.8359\n\n\ud83d\udcca RESUMO DOS RESULTADOS:\nGrau | R\u00b2     | RMSE\n--------------------\n   1 | 0.4589 | 0.8421\n   2 | 0.4633 | 0.8386\n   3 | 0.4671 | 0.8356\n   4 | 0.4673 | 0.8355\n   5 | 0.4668 | 0.8359\n</pre> In\u00a0[17]: Copied! <pre># \ud83d\udcca Visualizando diferentes graus polinomiais\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('\ud83d\udd04 Regress\u00e3o Polinomial: Diferentes Graus', fontsize=16, fontweight='bold')\n\n# Sample dos dados para n\u00e3o sobrecarregar\nsample_data = df.sample(2000, random_state=42)\nX_sample = sample_data[['MedInc']]\ny_sample = sample_data['target']\n\nfor i, grau in enumerate(graus):\n    row = i // 3\n    col = i % 3\n    \n    # Dados para plotar a curva suave\n    X_plot = np.linspace(X_poly_base.min().values[0], X_poly_base.max().values[0], 300).reshape(-1, 1)\n    poly_features = resultados_poly[grau]['poly_features']\n    X_plot_poly = poly_features.transform(X_plot)\n    y_plot = resultados_poly[grau]['modelo'].predict(X_plot_poly)\n    \n    # Plot\n    axes[row, col].scatter(X_sample, y_sample, alpha=0.4, s=10, color='lightblue')\n    axes[row, col].plot(X_plot, y_plot, color='red', linewidth=3)\n    axes[row, col].set_title(f'Grau {grau} (R\u00b2={resultados_poly[grau][\"r2\"]:.3f})', fontweight='bold')\n    axes[row, col].set_xlabel('Renda M\u00e9dia')\n    axes[row, col].set_ylabel('Pre\u00e7o')\n    axes[row, col].grid(True, alpha=0.3)\n\n# Gr\u00e1fico de compara\u00e7\u00e3o\naxes[1, 2].plot(graus, [resultados_poly[g]['r2'] for g in graus], 'o-', linewidth=3, markersize=8)\naxes[1, 2].set_xlabel('Grau do Polin\u00f4mio')\naxes[1, 2].set_ylabel('R\u00b2 Score')\naxes[1, 2].set_title('\ud83d\udcc8 Performance vs Complexidade', fontweight='bold')\naxes[1, 2].grid(True, alpha=0.3)\naxes[1, 2].set_xticks(graus)\n\nplt.tight_layout()\nplt.show()\n\n# Encontrando o melhor grau\nmelhor_grau = max(resultados_poly.keys(), key=lambda x: resultados_poly[x]['r2'])\nprint(f\"\ud83c\udfc6 Melhor grau: {melhor_grau} (R\u00b2 = {resultados_poly[melhor_grau]['r2']:.4f})\")\n</pre> # \ud83d\udcca Visualizando diferentes graus polinomiais fig, axes = plt.subplots(2, 3, figsize=(18, 12)) fig.suptitle('\ud83d\udd04 Regress\u00e3o Polinomial: Diferentes Graus', fontsize=16, fontweight='bold')  # Sample dos dados para n\u00e3o sobrecarregar sample_data = df.sample(2000, random_state=42) X_sample = sample_data[['MedInc']] y_sample = sample_data['target']  for i, grau in enumerate(graus):     row = i // 3     col = i % 3          # Dados para plotar a curva suave     X_plot = np.linspace(X_poly_base.min().values[0], X_poly_base.max().values[0], 300).reshape(-1, 1)     poly_features = resultados_poly[grau]['poly_features']     X_plot_poly = poly_features.transform(X_plot)     y_plot = resultados_poly[grau]['modelo'].predict(X_plot_poly)          # Plot     axes[row, col].scatter(X_sample, y_sample, alpha=0.4, s=10, color='lightblue')     axes[row, col].plot(X_plot, y_plot, color='red', linewidth=3)     axes[row, col].set_title(f'Grau {grau} (R\u00b2={resultados_poly[grau][\"r2\"]:.3f})', fontweight='bold')     axes[row, col].set_xlabel('Renda M\u00e9dia')     axes[row, col].set_ylabel('Pre\u00e7o')     axes[row, col].grid(True, alpha=0.3)  # Gr\u00e1fico de compara\u00e7\u00e3o axes[1, 2].plot(graus, [resultados_poly[g]['r2'] for g in graus], 'o-', linewidth=3, markersize=8) axes[1, 2].set_xlabel('Grau do Polin\u00f4mio') axes[1, 2].set_ylabel('R\u00b2 Score') axes[1, 2].set_title('\ud83d\udcc8 Performance vs Complexidade', fontweight='bold') axes[1, 2].grid(True, alpha=0.3) axes[1, 2].set_xticks(graus)  plt.tight_layout() plt.show()  # Encontrando o melhor grau melhor_grau = max(resultados_poly.keys(), key=lambda x: resultados_poly[x]['r2']) print(f\"\ud83c\udfc6 Melhor grau: {melhor_grau} (R\u00b2 = {resultados_poly[melhor_grau]['r2']:.4f})\") <pre>/Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but PolynomialFeatures was fitted with feature names\n  warnings.warn(\n/Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but PolynomialFeatures was fitted with feature names\n  warnings.warn(\n/Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but PolynomialFeatures was fitted with feature names\n  warnings.warn(\n/Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but PolynomialFeatures was fitted with feature names\n  warnings.warn(\n/Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but PolynomialFeatures was fitted with feature names\n  warnings.warn(\n/var/folders/lk/j2v60jvx5sdgx3dm_16q4nxm0000gn/T/ipykernel_76956/1768836301.py:36: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/var/folders/lk/j2v60jvx5sdgx3dm_16q4nxm0000gn/T/ipykernel_76956/1768836301.py:36: UserWarning: Glyph 128260 (\\N{ANTICLOCKWISE DOWNWARDS AND UPWARDS OPEN CIRCLE ARROWS}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 128260 (\\N{ANTICLOCKWISE DOWNWARDS AND UPWARDS OPEN CIRCLE ARROWS}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 128260 (\\N{ANTICLOCKWISE DOWNWARDS AND UPWARDS OPEN CIRCLE ARROWS}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n</pre> <pre>\ud83c\udfc6 Melhor grau: 4 (R\u00b2 = 0.4673)\n</pre> In\u00a0[18]: Copied! <pre># \u26a0\ufe0f Testando overfitting com graus muito altos\nprint(\"\u26a0\ufe0f  TESTANDO OVERFITTING\")\nprint(\"=\" * 25)\n\ngraus_extremos = [1, 2, 5, 10, 15]\nresultados_overfitting = {}\n\nfor grau in graus_extremos:\n    # Prepara\u00e7\u00e3o\n    poly_features = PolynomialFeatures(degree=grau, include_bias=False)\n    X_poly = poly_features.fit_transform(X_poly_base)\n    \n    X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(\n        X_poly, y_poly, test_size=0.2, random_state=42\n    )\n    \n    # Treinamento\n    modelo_poly = LinearRegression()\n    modelo_poly.fit(X_train_poly, y_train_poly)\n    \n    # Avalia\u00e7\u00e3o no TREINO e no TESTE\n    y_train_pred = modelo_poly.predict(X_train_poly)\n    y_test_pred = modelo_poly.predict(X_test_poly)\n    \n    r2_train = r2_score(y_train_poly, y_train_pred)\n    r2_test = r2_score(y_test_poly, y_test_pred)\n    \n    resultados_overfitting[grau] = {\n        'r2_train': r2_train,\n        'r2_test': r2_test,\n        'diferenca': r2_train - r2_test\n    }\n\nprint(\"Grau | R\u00b2 Treino | R\u00b2 Teste | Diferen\u00e7a | Status\")\nprint(\"-\" * 55)\n\nfor grau, resultado in resultados_overfitting.items():\n    r2_train = resultado['r2_train']\n    r2_test = resultado['r2_test']\n    diff = resultado['diferenca']\n    \n    if diff &gt; 0.1:\n        status = \"\ud83d\udd34 OVERFITTING\"\n    elif r2_test &lt; 0.3:\n        status = \"\ud83d\udd35 UNDERFITTING\"\n    else:\n        status = \"\ud83d\udfe2 BOM\"\n    \n    print(f\"{grau:4d} | {r2_train:9.4f} | {r2_test:8.4f} | {diff:9.4f} | {status}\")\n\nprint(f\"\\n\ud83d\udca1 INTERPRETA\u00c7\u00c3O:\")\nprint(f\"\u2022 \ud83d\udfe2 BOM: R\u00b2 similar no treino e teste\")\nprint(f\"\u2022 \ud83d\udd34 OVERFITTING: R\u00b2 muito maior no treino que no teste\")\nprint(f\"\u2022 \ud83d\udd35 UNDERFITTING: R\u00b2 baixo tanto no treino quanto no teste\")\n</pre> # \u26a0\ufe0f Testando overfitting com graus muito altos print(\"\u26a0\ufe0f  TESTANDO OVERFITTING\") print(\"=\" * 25)  graus_extremos = [1, 2, 5, 10, 15] resultados_overfitting = {}  for grau in graus_extremos:     # Prepara\u00e7\u00e3o     poly_features = PolynomialFeatures(degree=grau, include_bias=False)     X_poly = poly_features.fit_transform(X_poly_base)          X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(         X_poly, y_poly, test_size=0.2, random_state=42     )          # Treinamento     modelo_poly = LinearRegression()     modelo_poly.fit(X_train_poly, y_train_poly)          # Avalia\u00e7\u00e3o no TREINO e no TESTE     y_train_pred = modelo_poly.predict(X_train_poly)     y_test_pred = modelo_poly.predict(X_test_poly)          r2_train = r2_score(y_train_poly, y_train_pred)     r2_test = r2_score(y_test_poly, y_test_pred)          resultados_overfitting[grau] = {         'r2_train': r2_train,         'r2_test': r2_test,         'diferenca': r2_train - r2_test     }  print(\"Grau | R\u00b2 Treino | R\u00b2 Teste | Diferen\u00e7a | Status\") print(\"-\" * 55)  for grau, resultado in resultados_overfitting.items():     r2_train = resultado['r2_train']     r2_test = resultado['r2_test']     diff = resultado['diferenca']          if diff &gt; 0.1:         status = \"\ud83d\udd34 OVERFITTING\"     elif r2_test &lt; 0.3:         status = \"\ud83d\udd35 UNDERFITTING\"     else:         status = \"\ud83d\udfe2 BOM\"          print(f\"{grau:4d} | {r2_train:9.4f} | {r2_test:8.4f} | {diff:9.4f} | {status}\")  print(f\"\\n\ud83d\udca1 INTERPRETA\u00c7\u00c3O:\") print(f\"\u2022 \ud83d\udfe2 BOM: R\u00b2 similar no treino e teste\") print(f\"\u2022 \ud83d\udd34 OVERFITTING: R\u00b2 muito maior no treino que no teste\") print(f\"\u2022 \ud83d\udd35 UNDERFITTING: R\u00b2 baixo tanto no treino quanto no teste\") <pre>\u26a0\ufe0f  TESTANDO OVERFITTING\n=========================\nGrau | R\u00b2 Treino | R\u00b2 Teste | Diferen\u00e7a | Status\n-------------------------------------------------------\n   1 |    0.4770 |   0.4589 |    0.0181 | \ud83d\udfe2 BOM\n   2 |    0.4816 |   0.4633 |    0.0183 | \ud83d\udfe2 BOM\n   5 |    0.4910 |   0.4668 |    0.0242 | \ud83d\udfe2 BOM\n  10 |    0.4929 |   0.4690 |    0.0239 | \ud83d\udfe2 BOM\n  15 |    0.3982 |   0.3615 |    0.0367 | \ud83d\udfe2 BOM\n\n\ud83d\udca1 INTERPRETA\u00c7\u00c3O:\n\u2022 \ud83d\udfe2 BOM: R\u00b2 similar no treino e teste\n\u2022 \ud83d\udd34 OVERFITTING: R\u00b2 muito maior no treino que no teste\n\u2022 \ud83d\udd35 UNDERFITTING: R\u00b2 baixo tanto no treino quanto no teste\n</pre> In\u00a0[19]: Copied! <pre># \ud83c\udfaf Resumo final\nprint(\"\ud83c\udfaf RESUMO DO QUE APRENDEMOS\")\nprint(\"=\" * 35)\nprint(\"\u2705 Diferen\u00e7a entre classifica\u00e7\u00e3o e regress\u00e3o\")\nprint(\"\u2705 Como implementar regress\u00e3o linear no scikit-learn\")\nprint(\"\u2705 Import\u00e2ncia de dividir dados em treino/teste\")\nprint(\"\u2705 Como avaliar modelos com m\u00e9tricas (R\u00b2, RMSE, MAE)\")\nprint(\"\u2705 Regress\u00e3o polinomial para rela\u00e7\u00f5es n\u00e3o-lineares\")\nprint(\"\u2705 Problemas de overfitting e underfitting\")\n\nprint(f\"\\n\ud83d\ude80 PR\u00d3XIMOS PASSOS:\")\nprint(f\"\u2022 Experimentar outros algoritmos (Random Forest, SVM)\")\nprint(f\"\u2022 Aprender sobre valida\u00e7\u00e3o cruzada\")\nprint(f\"\u2022 Feature engineering (criar novas vari\u00e1veis)\")\nprint(f\"\u2022 Regulariza\u00e7\u00e3o (Ridge, Lasso)\")\nprint(f\"\u2022 Ensemble methods (combinar modelos)\")\n\nprint(f\"\\n\ud83d\udcda CONCEITOS-CHAVE PARA LEMBRAR:\")\nprint(f\"\u2022 R\u00b2: % da varia\u00e7\u00e3o explicada (0-1, maior = melhor)\")\nprint(f\"\u2022 RMSE: Erro m\u00e9dio em unidades originais (menor = melhor)\")\nprint(f\"\u2022 Overfitting: Modelo decora em vez de aprender\")\nprint(f\"\u2022 Sempre dividir dados para testar generaliza\u00e7\u00e3o\")\n\nprint(f\"\\n\ud83c\udf89 PARAB\u00c9NS! Voc\u00ea agora sabe os fundamentos da regress\u00e3o!\")\nprint(f\"\ud83c\udfaf Continue praticando com datasets diferentes!\")\n</pre> # \ud83c\udfaf Resumo final print(\"\ud83c\udfaf RESUMO DO QUE APRENDEMOS\") print(\"=\" * 35) print(\"\u2705 Diferen\u00e7a entre classifica\u00e7\u00e3o e regress\u00e3o\") print(\"\u2705 Como implementar regress\u00e3o linear no scikit-learn\") print(\"\u2705 Import\u00e2ncia de dividir dados em treino/teste\") print(\"\u2705 Como avaliar modelos com m\u00e9tricas (R\u00b2, RMSE, MAE)\") print(\"\u2705 Regress\u00e3o polinomial para rela\u00e7\u00f5es n\u00e3o-lineares\") print(\"\u2705 Problemas de overfitting e underfitting\")  print(f\"\\n\ud83d\ude80 PR\u00d3XIMOS PASSOS:\") print(f\"\u2022 Experimentar outros algoritmos (Random Forest, SVM)\") print(f\"\u2022 Aprender sobre valida\u00e7\u00e3o cruzada\") print(f\"\u2022 Feature engineering (criar novas vari\u00e1veis)\") print(f\"\u2022 Regulariza\u00e7\u00e3o (Ridge, Lasso)\") print(f\"\u2022 Ensemble methods (combinar modelos)\")  print(f\"\\n\ud83d\udcda CONCEITOS-CHAVE PARA LEMBRAR:\") print(f\"\u2022 R\u00b2: % da varia\u00e7\u00e3o explicada (0-1, maior = melhor)\") print(f\"\u2022 RMSE: Erro m\u00e9dio em unidades originais (menor = melhor)\") print(f\"\u2022 Overfitting: Modelo decora em vez de aprender\") print(f\"\u2022 Sempre dividir dados para testar generaliza\u00e7\u00e3o\")  print(f\"\\n\ud83c\udf89 PARAB\u00c9NS! Voc\u00ea agora sabe os fundamentos da regress\u00e3o!\") print(f\"\ud83c\udfaf Continue praticando com datasets diferentes!\") <pre>\ud83c\udfaf RESUMO DO QUE APRENDEMOS\n===================================\n\u2705 Diferen\u00e7a entre classifica\u00e7\u00e3o e regress\u00e3o\n\u2705 Como implementar regress\u00e3o linear no scikit-learn\n\u2705 Import\u00e2ncia de dividir dados em treino/teste\n\u2705 Como avaliar modelos com m\u00e9tricas (R\u00b2, RMSE, MAE)\n\u2705 Regress\u00e3o polinomial para rela\u00e7\u00f5es n\u00e3o-lineares\n\u2705 Problemas de overfitting e underfitting\n\n\ud83d\ude80 PR\u00d3XIMOS PASSOS:\n\u2022 Experimentar outros algoritmos (Random Forest, SVM)\n\u2022 Aprender sobre valida\u00e7\u00e3o cruzada\n\u2022 Feature engineering (criar novas vari\u00e1veis)\n\u2022 Regulariza\u00e7\u00e3o (Ridge, Lasso)\n\u2022 Ensemble methods (combinar modelos)\n\n\ud83d\udcda CONCEITOS-CHAVE PARA LEMBRAR:\n\u2022 R\u00b2: % da varia\u00e7\u00e3o explicada (0-1, maior = melhor)\n\u2022 RMSE: Erro m\u00e9dio em unidades originais (menor = melhor)\n\u2022 Overfitting: Modelo decora em vez de aprender\n\u2022 Sempre dividir dados para testar generaliza\u00e7\u00e3o\n\n\ud83c\udf89 PARAB\u00c9NS! Voc\u00ea agora sabe os fundamentos da regress\u00e3o!\n\ud83c\udfaf Continue praticando com datasets diferentes!\n</pre>"},{"location":"aulas/IA/lab03/regressao-pratica/#regressao-em-ml-segunda-aula-pratica","title":"\ud83d\udcc8 Regress\u00e3o em ML: Segunda Aula - Pr\u00e1tica\u00b6","text":""},{"location":"aulas/IA/lab03/regressao-pratica/#o-que-voce-vai-aprender-hoje","title":"\ud83c\udfaf O que voc\u00ea vai aprender hoje:\u00b6","text":"<ul> <li>\u2705 Diferen\u00e7a entre classifica\u00e7\u00e3o e regress\u00e3o</li> <li>\u2705 Implementar regress\u00e3o linear no scikit-learn</li> <li>\u2705 Avaliar modelos com m\u00e9tricas apropriadas</li> <li>\u2705 Explorar regress\u00e3o polinomial</li> <li>\u2705 Entender overfitting vs underfitting</li> </ul>"},{"location":"aulas/IA/lab03/regressao-pratica/#1-regressao-vs-classificacao","title":"\ud83d\udd0d 1. Regress\u00e3o vs Classifica\u00e7\u00e3o\u00b6","text":""},{"location":"aulas/IA/lab03/regressao-pratica/#qual-a-diferenca","title":"\ud83e\udd14 Qual a diferen\u00e7a?\u00b6","text":"Classifica\u00e7\u00e3o Regress\u00e3o Prediz categorias Prediz n\u00fameros \"Gato\" ou \"Cachorro\" Pre\u00e7o: $250.000 \"Spam\" ou \"Normal\" Temperatura: 23.5\u00b0C Discreto Cont\u00ednuo"},{"location":"aulas/IA/lab03/regressao-pratica/#nosso-problema-hoje-preco-de-casas","title":"\ud83c\udfe0 Nosso problema hoje: Pre\u00e7o de Casas\u00b6","text":"<p>Entrada: Caracter\u00edsticas da casa (renda da regi\u00e3o, idade, quartos...) Sa\u00edda: Pre\u00e7o da casa (valor num\u00e9rico)</p> <p>\u2705 \u00c9 REGRESS\u00c3O porque queremos predizer um n\u00famero!</p>"},{"location":"aulas/IA/lab03/regressao-pratica/#2-importando-as-bibliotecas","title":"\ud83d\udcda 2. Importando as Bibliotecas\u00b6","text":"<p>N\u00e3o se preocupe em decorar! Vamos explicar cada uma conforme usamos.</p>"},{"location":"aulas/IA/lab03/regressao-pratica/#3-carregando-os-dados-casas-da-california","title":"\ud83c\udfe0 3. Carregando os Dados: Casas da Calif\u00f3rnia\u00b6","text":"<p>Vamos usar um dataset real sobre pre\u00e7os de casas. \u00c9 perfeito para aprender regress\u00e3o!</p>"},{"location":"aulas/IA/lab03/regressao-pratica/#o-que-significa-cada-coluna","title":"\ud83d\udcd6 O que significa cada coluna?\u00b6","text":"Coluna Significado Exemplo MedInc Renda m\u00e9dia da regi\u00e3o 8.32 (dezenas de milhares) HouseAge Idade m\u00e9dia das casas 41.0 anos AveRooms N\u00famero m\u00e9dio de quartos 6.98 AveBedrms Quartos de dormir m\u00e9dios 1.02 Population Popula\u00e7\u00e3o da \u00e1rea 322 pessoas AveOccup Ocupa\u00e7\u00e3o m\u00e9dia 2.55 pessoas/casa Latitude Coordenada geogr\u00e1fica 37.88 Longitude Coordenada geogr\u00e1fica -122.23 target \ud83c\udfaf PRE\u00c7O (nosso objetivo!) 4.526 (centenas de milhares) <p>\ud83d\udca1 Importante: O pre\u00e7o est\u00e1 em centenas de milhares. 4.526 = $452,600</p>"},{"location":"aulas/IA/lab03/regressao-pratica/#4-explorando-os-dados-eda","title":"\ud83d\udd0d 4. Explorando os Dados (EDA)\u00b6","text":"<p>Antes de treinar qualquer modelo, precisamos conhecer nossos dados!</p>"},{"location":"aulas/IA/lab03/regressao-pratica/#5-matriz-de-correlacao-quem-se-relaciona","title":"\ud83d\udd17 5. Matriz de Correla\u00e7\u00e3o: Quem se Relaciona?\u00b6","text":"<p>Correla\u00e7\u00e3o mede se duas vari\u00e1veis \"andam juntas\":</p> <ul> <li>+1: Correla\u00e7\u00e3o positiva perfeita (uma sobe \u2192 outra sobe)</li> <li>0: Sem correla\u00e7\u00e3o (s\u00e3o independentes)</li> <li>-1: Correla\u00e7\u00e3o negativa perfeita (uma sobe \u2192 outra desce)</li> </ul>"},{"location":"aulas/IA/lab03/regressao-pratica/#6-preparando-os-dados-para-o-modelo","title":"\ud83c\udfaf 6. Preparando os Dados para o Modelo\u00b6","text":"<p>Agora vamos preparar nossos dados para treinar o modelo!</p>"},{"location":"aulas/IA/lab03/regressao-pratica/#7-dividindo-em-treino-e-teste","title":"\u2702\ufe0f 7. Dividindo em Treino e Teste\u00b6","text":"<p>Por que dividir?</p> <ul> <li>Treino (80%): Para o modelo aprender</li> <li>Teste (20%): Para verificar se realmente aprendeu</li> </ul> <p>\u00c9 como estudar para uma prova e depois fazer a prova com quest\u00f5es novas!</p>"},{"location":"aulas/IA/lab03/regressao-pratica/#8-treinando-nosso-primeiro-modelo","title":"\ud83e\udd16 8. Treinando Nosso Primeiro Modelo\u00b6","text":"<p>Finalmente! Vamos treinar um modelo de Regress\u00e3o Linear.</p>"},{"location":"aulas/IA/lab03/regressao-pratica/#9-fazendo-predicoes","title":"\ud83d\udd2e 9. Fazendo Predi\u00e7\u00f5es\u00b6","text":"<p>Agora vamos ver como nosso modelo se sai com dados que ele nunca viu!</p>"},{"location":"aulas/IA/lab03/regressao-pratica/#10-avaliando-o-modelo","title":"\ud83d\udccf 10. Avaliando o Modelo\u00b6","text":"<p>Como saber se nosso modelo \u00e9 bom? Vamos usar m\u00e9tricas!</p>"},{"location":"aulas/IA/lab03/regressao-pratica/#11-melhorando-o-modelo-multiplas-variaveis","title":"\ud83c\udf1f 11. Melhorando o Modelo: M\u00faltiplas Vari\u00e1veis\u00b6","text":"<p>Usar s\u00f3 a renda \u00e9 limitado. Vamos usar mais vari\u00e1veis!</p>"},{"location":"aulas/IA/lab03/regressao-pratica/#12-regressao-polinomial-capturando-nao-linearidade","title":"\ud83d\udd04 12. Regress\u00e3o Polinomial: Capturando N\u00e3o-Linearidade\u00b6","text":"<p>E se a rela\u00e7\u00e3o n\u00e3o for uma reta? Vamos tentar curvas!</p>"},{"location":"aulas/IA/lab03/regressao-pratica/#13-overfitting-vs-underfitting","title":"\u26a0\ufe0f 13. Overfitting vs Underfitting\u00b6","text":"<p>Overfitting: Modelo muito complexo (decora em vez de aprender) Underfitting: Modelo muito simples (n\u00e3o consegue aprender)</p> <p>Vamos testar isso!</p>"},{"location":"aulas/IA/lab03/regressao-pratica/#14-conclusoes-e-proximos-passos","title":"\ud83c\udfaf 14. Conclus\u00f5es e Pr\u00f3ximos Passos\u00b6","text":"<p>Parab\u00e9ns! Voc\u00ea completou sua introdu\u00e7\u00e3o \u00e0 regress\u00e3o!</p>"},{"location":"aulas/IA/lab03/regressao-pratica/#exercicios-para-praticar","title":"\ud83d\udcdd Exerc\u00edcios para Praticar\u00b6","text":""},{"location":"aulas/IA/lab03/regressao-pratica/#exercicio-1-experimentar-outras-variaveis","title":"\ud83c\udfaf Exerc\u00edcio 1: Experimentar outras vari\u00e1veis\u00b6","text":"<ol> <li>Tente usar apenas <code>HouseAge</code> para predizer pre\u00e7o</li> <li>Compare o R\u00b2 com o modelo que usa <code>MedInc</code></li> <li>Qual vari\u00e1vel sozinha \u00e9 melhor preditora?</li> </ol>"},{"location":"aulas/IA/lab03/regressao-pratica/#exercicio-2-modelo-com-todas-as-variaveis","title":"\ud83c\udfaf Exerc\u00edcio 2: Modelo com todas as vari\u00e1veis\u00b6","text":"<ol> <li>Crie um modelo usando todas as 8 vari\u00e1veis</li> <li>Compare com o modelo de 4 vari\u00e1veis</li> <li>Houve melhoria significativa?</li> </ol>"},{"location":"aulas/IA/lab03/regressao-pratica/#exercicio-3-testar-outros-graus-polinomiais","title":"\ud83c\udfaf Exerc\u00edcio 3: Testar outros graus polinomiais\u00b6","text":"<ol> <li>Teste graus 6 a 10 na regress\u00e3o polinomial</li> <li>Identifique sinais de overfitting</li> <li>Qual seria o grau ideal?</li> </ol>"},{"location":"aulas/IA/lab03/regressao-pratica/#exercicio-4-analise-de-residuos","title":"\ud83c\udfaf Exerc\u00edcio 4: An\u00e1lise de res\u00edduos\u00b6","text":"<ol> <li>Calcule os res\u00edduos (diferen\u00e7a entre real e predito)</li> <li>Fa\u00e7a um histograma dos res\u00edduos</li> <li>Os res\u00edduos seguem uma distribui\u00e7\u00e3o normal?</li> </ol> <p>\ud83c\udf93 Boa sorte e continue aprendendo!</p>"},{"location":"aulas/IA/lab03/regressao/","title":"Regressao","text":"In\u00a0[1]: Copied! <pre># Inicializ\u00e7\u00e3o das bibliotecas\n%matplotlib inline\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n</pre> # Inicializ\u00e7\u00e3o das bibliotecas %matplotlib inline  import pandas as pd import matplotlib.pyplot as plt  <p>O scikit-learn possui diversos dataset em seu banco de dados, um deles \u00e9 o dataset que vamos utilizar hoje.</p> <p>fa\u00e7a o import direto usando sklearn.datasets</p> <p>caso queira, voc\u00ea pode fazer o downlod do dataset direto do site e importar em seu projeto.</p> In\u00a0[2]: Copied! <pre>from sklearn.datasets import fetch_california_housing \n\ncalifornia_dataset = fetch_california_housing()\n\n#para conhecer o que foi importado do dataset \ncalifornia_dataset.keys()\n</pre> from sklearn.datasets import fetch_california_housing   california_dataset = fetch_california_housing()  #para conhecer o que foi importado do dataset  california_dataset.keys()  Out[2]: <pre>dict_keys(['data', 'target', 'frame', 'target_names', 'feature_names', 'DESCR'])</pre> In\u00a0[3]: Copied! <pre># vamos carregar no pandas apenas data com os dados e \"feature_names\" com os nomes dos atributos\n\ndf = pd.DataFrame(california_dataset.data, columns=california_dataset.feature_names)\ndf.head()\n</pre> # vamos carregar no pandas apenas data com os dados e \"feature_names\" com os nomes dos atributos  df = pd.DataFrame(california_dataset.data, columns=california_dataset.feature_names) df.head() Out[3]: MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude 0 8.3252 41.0 6.984127 1.023810 322.0 2.555556 37.88 -122.23 1 8.3014 21.0 6.238137 0.971880 2401.0 2.109842 37.86 -122.22 2 7.2574 52.0 8.288136 1.073446 496.0 2.802260 37.85 -122.24 3 5.6431 52.0 5.817352 1.073059 558.0 2.547945 37.85 -122.25 4 3.8462 52.0 6.281853 1.081081 565.0 2.181467 37.85 -122.25 In\u00a0[4]: Copied! <pre>#vamos adicionar mais uma coluna ao nosso dataframe com o target (alvo que vamos fazer a predi\u00e7\u00e3o)\ndf['MEDV'] = california_dataset.target\n</pre> #vamos adicionar mais uma coluna ao nosso dataframe com o target (alvo que vamos fazer a predi\u00e7\u00e3o) df['MEDV'] = california_dataset.target <p>Pronto!! agora o nosso dataset est\u00e1 completamente carregado e podemos come\u00e7ar a an\u00e1lise de dados.</p> In\u00a0[5]: Copied! <pre>df.head()\n</pre> df.head() Out[5]: MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude MEDV 0 8.3252 41.0 6.984127 1.023810 322.0 2.555556 37.88 -122.23 4.526 1 8.3014 21.0 6.238137 0.971880 2401.0 2.109842 37.86 -122.22 3.585 2 7.2574 52.0 8.288136 1.073446 496.0 2.802260 37.85 -122.24 3.521 3 5.6431 52.0 5.817352 1.073059 558.0 2.547945 37.85 -122.25 3.413 4 3.8462 52.0 6.281853 1.081081 565.0 2.181467 37.85 -122.25 3.422 In\u00a0[6]: Copied! <pre># Mostra informa\u00e7\u00f5es sobre o dataframe em si\ndf.info()\n</pre> # Mostra informa\u00e7\u00f5es sobre o dataframe em si df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 9 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   MedInc      20640 non-null  float64\n 1   HouseAge    20640 non-null  float64\n 2   AveRooms    20640 non-null  float64\n 3   AveBedrms   20640 non-null  float64\n 4   Population  20640 non-null  float64\n 5   AveOccup    20640 non-null  float64\n 6   Latitude    20640 non-null  float64\n 7   Longitude   20640 non-null  float64\n 8   MEDV        20640 non-null  float64\ndtypes: float64(9)\nmemory usage: 1.4 MB\n</pre> In\u00a0[7]: Copied! <pre>df.describe()\n</pre> df.describe() Out[7]: MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude MEDV count 20640.000000 20640.000000 20640.000000 20640.000000 20640.000000 20640.000000 20640.000000 20640.000000 20640.000000 mean 3.870671 28.639486 5.429000 1.096675 1425.476744 3.070655 35.631861 -119.569704 2.068558 std 1.899822 12.585558 2.474173 0.473911 1132.462122 10.386050 2.135952 2.003532 1.153956 min 0.499900 1.000000 0.846154 0.333333 3.000000 0.692308 32.540000 -124.350000 0.149990 25% 2.563400 18.000000 4.440716 1.006079 787.000000 2.429741 33.930000 -121.800000 1.196000 50% 3.534800 29.000000 5.229129 1.048780 1166.000000 2.818116 34.260000 -118.490000 1.797000 75% 4.743250 37.000000 6.052381 1.099526 1725.000000 3.282261 37.710000 -118.010000 2.647250 max 15.000100 52.000000 141.909091 34.066667 35682.000000 1243.333333 41.950000 -114.310000 5.000010 In\u00a0[\u00a0]: Copied! <pre>## Sua resposta e seus gr\u00e1ficos para an\u00e1lisar..\n</pre> ## Sua resposta e seus gr\u00e1ficos para an\u00e1lisar..       In\u00a0[\u00a0]: Copied! <pre># Vamos visualizar a rela\u00e7\u00e3o entre as vari\u00e1veis\n# Longitude e Latitude em rela\u00e7\u00e3o ao pre\u00e7o m\u00e9dio das casas\n\ndf.plot(kind=\"scatter\", x=\"Longitude\",y=\"Latitude\", c=\"MEDV\", cmap=\"jet\", colorbar=True, legend=True, sharex=False, figsize=(10,7), s=df['Population']/100, label=\"population\", alpha=0.7)\nplt.show()\n</pre> # Vamos visualizar a rela\u00e7\u00e3o entre as vari\u00e1veis # Longitude e Latitude em rela\u00e7\u00e3o ao pre\u00e7o m\u00e9dio das casas  df.plot(kind=\"scatter\", x=\"Longitude\",y=\"Latitude\", c=\"MEDV\", cmap=\"jet\", colorbar=True, legend=True, sharex=False, figsize=(10,7), s=df['Population']/100, label=\"population\", alpha=0.7) plt.show()  In\u00a0[8]: Copied! <pre>#Vamos explorar um pouco uma matrix de correla\u00e7\u00e3o\n\nimport seaborn as sns \ncorrelation_matrix = df.corr().round(2)\n\nfig, ax = plt.subplots(figsize=(10,10))    \nsns.heatmap(data=correlation_matrix, annot=True, linewidths=.5, ax=ax)\n</pre> #Vamos explorar um pouco uma matrix de correla\u00e7\u00e3o  import seaborn as sns  correlation_matrix = df.corr().round(2)  fig, ax = plt.subplots(figsize=(10,10))     sns.heatmap(data=correlation_matrix, annot=True, linewidths=.5, ax=ax) Out[8]: <pre>&lt;Axes: &gt;</pre> In\u00a0[23]: Copied! <pre># Vamos treinar nosso modelo com 2 dois atributos independentes\n# para predizer o valor de saida\n# X = df[['MedInc', 'HouseAge']]   ### teste com duas entradas\n\nX = df[['MedInc']]            ### teste com uma entrada\n\n#X = df.drop(['MEDV'], axis=1)     ### teste com todas as entradas\n\nY = df['MEDV']             \nprint(f\"Formato das tabelas de dados {X.shape} e saidas {Y.shape}\")\n</pre> # Vamos treinar nosso modelo com 2 dois atributos independentes # para predizer o valor de saida # X = df[['MedInc', 'HouseAge']]   ### teste com duas entradas  X = df[['MedInc']]            ### teste com uma entrada  #X = df.drop(['MEDV'], axis=1)     ### teste com todas as entradas  Y = df['MEDV']              print(f\"Formato das tabelas de dados {X.shape} e saidas {Y.shape}\") <pre>Formato das tabelas de dados (20640, 1) e saidas (20640,)\n</pre> In\u00a0[24]: Copied! <pre># Separamos 20% para o teste\nfrom sklearn.model_selection import train_test_split\n\nX_treino, X_teste, Y_treino, Y_teste = train_test_split(X, Y, test_size=0.2)\n\nprint(X_treino.shape)\nprint(X_teste.shape)\nprint(Y_treino.shape)\nprint(Y_teste.shape)\n</pre> # Separamos 20% para o teste from sklearn.model_selection import train_test_split  X_treino, X_teste, Y_treino, Y_teste = train_test_split(X, Y, test_size=0.2)  print(X_treino.shape) print(X_teste.shape) print(Y_treino.shape) print(Y_teste.shape) <pre>(16512, 1)\n(4128, 1)\n(16512,)\n(4128,)\n</pre> In\u00a0[32]: Copied! <pre># Importa a biblioteca\nfrom sklearn.linear_model import LinearRegression\n\n\n# Cria o modelo de regress\u00e3o \nlin_model = LinearRegression()\n\n# Cria o modelo de machine learning\nlin_model.fit(X_treino, Y_treino)\n</pre> # Importa a biblioteca from sklearn.linear_model import LinearRegression   # Cria o modelo de regress\u00e3o  lin_model = LinearRegression()  # Cria o modelo de machine learning lin_model.fit(X_treino, Y_treino)    Out[32]: <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted Parameters fit_intercept\u00a0 True copy_X\u00a0 True tol\u00a0 1e-06 n_jobs\u00a0 None positive\u00a0 False <p>Pronto!! bora testar se esta funcionando....</p> In\u00a0[33]: Copied! <pre># Para obter as previs\u00f5es, basta chamar o m\u00e9todo predict()\ny_teste_predito = lin_model.predict(X_teste)\nprint(\"Predi\u00e7\u00e3o usando regress\u00e3o, retorna valores continuos: {}\".format(y_teste_predito))\n</pre> # Para obter as previs\u00f5es, basta chamar o m\u00e9todo predict() y_teste_predito = lin_model.predict(X_teste) print(\"Predi\u00e7\u00e3o usando regress\u00e3o, retorna valores continuos: {}\".format(y_teste_predito))  <pre>Predi\u00e7\u00e3o usando regress\u00e3o, retorna valores continuos: [1.22201031 1.20622349 0.8628707  ... 1.27463304 1.3798785  1.49131239]\n</pre> In\u00a0[37]: Copied! <pre># vamos avaliar os parametros do nosso modelo\nprint('(A) Intercepto: ', lin_model.intercept_)\nprint('(B) Inclina\u00e7\u00e3o: ', lin_model.coef_)\n\nprint('(C) Equa\u00e7\u00e3o: Y = {} + {} * X'.format(lin_model.intercept_, lin_model.coef_[0]))\n</pre> # vamos avaliar os parametros do nosso modelo print('(A) Intercepto: ', lin_model.intercept_) print('(B) Inclina\u00e7\u00e3o: ', lin_model.coef_)  print('(C) Equa\u00e7\u00e3o: Y = {} + {} * X'.format(lin_model.intercept_, lin_model.coef_[0]))  <pre>(A) Intercepto:  0.432669361569749\n(B) Inclina\u00e7\u00e3o:  [0.42098184]\n(C) Equa\u00e7\u00e3o: Y = 0.432669361569749 + 0.4209818390411239 * X\n</pre> In\u00a0[49]: Copied! <pre>plt.scatter(Y_teste,y_teste_predito, alpha=0.2)\nplt.xlabel('Valor Real')\nplt.ylabel('Valor Predito')\n</pre> plt.scatter(Y_teste,y_teste_predito, alpha=0.2) plt.xlabel('Valor Real') plt.ylabel('Valor Predito') Out[49]: <pre>Text(0, 0.5, 'Valor Predito')</pre> In\u00a0[52]: Copied! <pre>from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\nimport numpy as np\n\nprint(\"Soma dos Erros ao Quadrado (SSE): %2.f \" % np.sum((y_teste_predito - Y_teste)**2))\nprint(\"Erro Quadr\u00e1tico M\u00e9dio (MSE): %.2f\" % mean_squared_error(Y_teste, y_teste_predito))\nprint(\"Erro M\u00e9dio Absoluto (MAE): %.2f\" % mean_absolute_error(Y_teste, y_teste_predito))\nprint (\"Raiz do Erro Quadr\u00e1tico M\u00e9dio (RMSE): %.2f \" % np.sqrt(mean_squared_error(Y_teste, y_teste_predito)))\nprint(\"R2-score: %.2f\" % r2_score(Y_teste, y_teste_predito))\n</pre> from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error import numpy as np  print(\"Soma dos Erros ao Quadrado (SSE): %2.f \" % np.sum((y_teste_predito - Y_teste)**2)) print(\"Erro Quadr\u00e1tico M\u00e9dio (MSE): %.2f\" % mean_squared_error(Y_teste, y_teste_predito)) print(\"Erro M\u00e9dio Absoluto (MAE): %.2f\" % mean_absolute_error(Y_teste, y_teste_predito)) print (\"Raiz do Erro Quadr\u00e1tico M\u00e9dio (RMSE): %.2f \" % np.sqrt(mean_squared_error(Y_teste, y_teste_predito))) print(\"R2-score: %.2f\" % r2_score(Y_teste, y_teste_predito)) <pre>Soma dos Erros ao Quadrado (SSE): 3014 \nErro Quadr\u00e1tico M\u00e9dio (MSE): 0.73\nErro M\u00e9dio Absoluto (MAE): 0.63\nRaiz do Erro Quadr\u00e1tico M\u00e9dio (RMSE): 0.85 \nR2-score: 0.45\n</pre> In\u00a0[\u00a0]: Copied! <pre>## implemente sua sua solu\u00e7\u00e3o....\n\n# testa o random forest\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nmodel = GradientBoostingRegressor(n_estimators=100)\nmodel.fit(X_treino, Y_treino)\ny_teste_predito_gb = model.predict(X_teste)\n\n\ny_teste_predito_rf = model.predict(X_teste)\n\nprint(\"Soma dos Erros ao Quadrado (SSE): %2.f \" % np.sum((y_teste_predito_rf - Y_teste)**2))\nprint(\"Erro Quadr\u00e1tico M\u00e9dio (MSE): %.2f\" % mean_squared_error(Y_teste, y_teste_predito_rf))\nprint(\"Erro M\u00e9dio Absoluto (MAE): %.2f\" % mean_absolute_error(Y_teste, y_teste_predito_rf))\nprint (\"Raiz do Erro Quadr\u00e1tico M\u00e9dio (RMSE): %.2f \" % np.sqrt(mean_squared_error(Y_teste, y_teste_predito_rf)))\nprint(\"R2-score: %.2f\" % r2_score(Y_teste, y_teste_predito_rf))\n</pre> ## implemente sua sua solu\u00e7\u00e3o....  # testa o random forest  from sklearn.ensemble import GradientBoostingRegressor  model = GradientBoostingRegressor(n_estimators=100) model.fit(X_treino, Y_treino) y_teste_predito_gb = model.predict(X_teste)   y_teste_predito_rf = model.predict(X_teste)  print(\"Soma dos Erros ao Quadrado (SSE): %2.f \" % np.sum((y_teste_predito_rf - Y_teste)**2)) print(\"Erro Quadr\u00e1tico M\u00e9dio (MSE): %.2f\" % mean_squared_error(Y_teste, y_teste_predito_rf)) print(\"Erro M\u00e9dio Absoluto (MAE): %.2f\" % mean_absolute_error(Y_teste, y_teste_predito_rf)) print (\"Raiz do Erro Quadr\u00e1tico M\u00e9dio (RMSE): %.2f \" % np.sqrt(mean_squared_error(Y_teste, y_teste_predito_rf))) print(\"R2-score: %.2f\" % r2_score(Y_teste, y_teste_predito_rf))  <pre>Soma dos Erros ao Quadrado (SSE): 2914 \nErro Quadr\u00e1tico M\u00e9dio (MSE): 0.71\nErro M\u00e9dio Absoluto (MAE): 0.62\nRaiz do Erro Quadr\u00e1tico M\u00e9dio (RMSE): 0.84 \nR2-score: 0.47\n</pre> In\u00a0[\u00a0]: Copied! <pre>## implemente sua sua solu\u00e7\u00e3o....\n## implemente sua sua solu\u00e7\u00e3o....\n## implemente sua sua solu\u00e7\u00e3o....\n## implemente sua sua solu\u00e7\u00e3o....\n</pre> ## implemente sua sua solu\u00e7\u00e3o.... ## implemente sua sua solu\u00e7\u00e3o.... ## implemente sua sua solu\u00e7\u00e3o.... ## implemente sua sua solu\u00e7\u00e3o...."},{"location":"aulas/IA/lab03/regressao/#objetivos","title":"Objetivos\u00b6","text":"<ul> <li>Apresentar o conceito de Regress\u00e3o</li> <li>Apresentar e utilizar algoritmo de Regress\u00e3o linear</li> <li>Apresentar e utilizar Regress\u00e3o Polinomial</li> <li>Apresentar e discutir a matriz de correla\u00e7\u00e3o</li> <li>Apresentar uma intui\u00e7\u00e3o sobre m\u00e9tricas de avalia\u00e7\u00e3o (MSE, RMSE e $ R\u00b2 $ )</li> </ul>"},{"location":"aulas/IA/lab03/regressao/#comecando","title":"Come\u00e7ando\u00b6","text":"<p>Sabemos que dentro de aprendizado supervisionado vamos trabalhar com dois tipos de problemas:</p> <ul> <li>Classifica\u00e7\u00e3o - (J\u00e1 conhecemos o KNN)</li> <li>Regress\u00e3o - (Objetivo de hoje)</li> </ul>"},{"location":"aulas/IA/lab03/regressao/#uma-intuicao-sobre-problemas-que-envolvem-cada-um-deles","title":"Uma intui\u00e7\u00e3o sobre problemas que envolvem cada um deles:\u00b6","text":"<pre><code>    Classifica\u00e7\u00e3o --&gt; Resultados discretos (categ\u00f3ricos).\n    Regress\u00e3o --&gt; Resultados num\u00e9ricos e cont\u00ednuos.</code></pre>"},{"location":"aulas/IA/lab03/regressao/#regressao-linear","title":"Regress\u00e3o linear\u00b6","text":"<p>\u00c9 uma t\u00e9cnica que consiste em representar um conjunto de dados por meio de uma reta.</p> <pre><code>Na matem\u00e1tica aprendemos que a equa\u00e7\u00e3o de uma reta \u00e9:</code></pre> <p>$$ Y = A + BX \\\\ $$ A e B s\u00e3o constantes que determinam a posi\u00e7\u00e3o e inclina\u00e7\u00e3o da reta. Para cada valor de X temos um Y associado.</p> <pre><code>Em machine learning aprendemos que uma Regress\u00e3o linear \u00e9:</code></pre> <p>$$ Y_{predito} = \\beta_o + \\beta_1X \\\\ $$</p> <p>$ \\beta_o $ e $ \\beta_1 $ s\u00e3o par\u00e2metros que determinam o peso e bias da rede. Para cada entrada $ X $ temos um $ Y_{predito} $ aproximado predito.</p> <p></p> <p>Essa ideia se estende para mais de um par\u00e2metro independente, mas nesse caso n\u00e3o estamos associando a uma reta e sim a um plano ou hiperplano:</p> <p>$$ Y_{predito} = \\beta_o + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n\\\\ $$</p> <p> </p> <p>Em outras palavras, modelos de regress\u00e3o linear s\u00e3o intuitivos, f\u00e1ceis de interpretar e se ajustam aos dados razoavelmente bem em muitos problemas.</p>"},{"location":"aulas/IA/lab03/regressao/#bora-la","title":"Bora l\u00e1!!\u00b6","text":"<p>Vamos juntos realizar um projeto, do come\u00e7o ao fim, usando regress\u00e3o.</p>"},{"location":"aulas/IA/lab03/regressao/#definicao-do-problema","title":"Defini\u00e7\u00e3o do problema\u00b6","text":""},{"location":"aulas/IA/lab03/regressao/#preco-de-casas","title":"Pre\u00e7o de Casas\u00b6","text":"<ul> <li>Entrada: Caracter\u00edsticas da casa (renda da regi\u00e3o, idade, quartos...)</li> <li>Sa\u00edda: Pre\u00e7o da casa (valor num\u00e9rico)</li> </ul>"},{"location":"aulas/IA/lab03/regressao/#desafio-1","title":"Desafio 1\u00b6","text":"<p>Do ponto de vista de machine learning, que problema \u00e9 esse:</p> <pre><code>Aprendizado supervisionado, n\u00e3o-supervisionado ou aprendizado por refor\u00e7o?</code></pre> <p>R:</p> <pre><code>Classifica\u00e7\u00e3o, regress\u00e3o ou clusteriza\u00e7\u00e3o?</code></pre> <p>R:</p>"},{"location":"aulas/IA/lab03/regressao/#o-que-significa-cada-coluna","title":"O que significa cada coluna?\u00b6","text":"Coluna Significado Exemplo MedInc Renda m\u00e9dia da regi\u00e3o 8.32 (dezenas de milhares) HouseAge Idade m\u00e9dia das casas 41.0 anos AveRooms N\u00famero m\u00e9dio de quartos 6.98 AveBedrms Quartos de dormir m\u00e9dios 1.02 Population Popula\u00e7\u00e3o da \u00e1rea 322 pessoas AveOccup Ocupa\u00e7\u00e3o m\u00e9dia 2.55 pessoas/casa Latitude Coordenada geogr\u00e1fica 37.88 Longitude Coordenada geogr\u00e1fica -122.23 MEDV Pre\u00e7o m\u00e9dio das casas 4.526 (centenas de milhares) <p>Importante: O pre\u00e7o est\u00e1 em centenas de milhares. 4.526 = $452,600</p>"},{"location":"aulas/IA/lab03/regressao/#desafio-2","title":"Desafio 2\u00b6","text":"<p>Use os metodos info() e describe() para exibir as informa\u00e7\u00f5es do dataframe e responda:</p> <p>Existe dados faltantes?</p> <p>Qual o tamanho do dataset, quantas linhas e quantas colunas?</p>"},{"location":"aulas/IA/lab03/regressao/#desafio-3","title":"Desafio 3\u00b6","text":"<p>Aplique os m\u00e9todos que achar conveniente (vimos algumas op\u00e7\u00f5es na \u00faltima aula) para visualizar os dados de forma gr\u00e1fica.</p>"},{"location":"aulas/IA/lab03/regressao/#pare","title":"PARE!!!\u00b6","text":"<p>A an\u00e1lise feita no desafio 2 e 3 \u00e9 uma das etapas mais importantes. Caso voc\u00ea tenha pulado essa etapa, volte e fa\u00e7a suas an\u00e1lises.</p> <p>Com essa etapa conclu\u00edda, vamos criar um sub-dataset com os atributos que ser\u00e3o utilizados.</p>"},{"location":"aulas/IA/lab03/regressao/#dividindo-os-dados-em-conjunto-de-treinamento-e-de-testes","title":"Dividindo os dados em conjunto de treinamento e de testes\u00b6","text":"<p>Dividir nosso dataset em dois conjuntos de dados.</p> <pre><code>Treinamento - Representa 80% das amostras do conjunto de dados original,\nTeste - com 20% das amostras</code></pre> <p>Vamos escolher aleatoriamente algumas amostras do conjunto original. Isto pode ser feito com Scikit-Learn usando a fun\u00e7\u00e3o train_test_split()</p> <p>scikit-learn Caso ainda n\u00e3o tenha instalado, no terminal digite:</p> <ul> <li>pip install scikit-learn</li> </ul>"},{"location":"aulas/IA/lab03/regressao/#treinando-nosso-primeiro-modelo","title":"Treinando Nosso Primeiro Modelo\u00b6","text":"<p>Treinar um modelo no python \u00e9 simples se usar o Scikit-Learn.</p> <p>Treinar um modelo no Scikit-Learn \u00e9 simples: basta criar o regressor, e chamar o m\u00e9todo fit().</p> <p>Uma observa\u00e7\u00e3o sobre a sintaxe dos classificadores do <code>scikit-learn</code></p> <ul> <li><p>O m\u00e9todo <code>fit(X,Y)</code> recebe uma matriz ou dataframe X onde cada linha \u00e9 uma amostra de aprendizado, e um array Y contendo as sa\u00eddas esperadas do classificador, seja na forma de texto ou de inteiros</p> </li> <li><p>O m\u00e9todo <code>predict(X)</code> recebe uma matriz ou dataframe X onde cada linha \u00e9 uma amostra de teste, retornando um array de classes</p> </li> </ul>"},{"location":"aulas/IA/lab03/regressao/#avaliando-o-modelo-treinado","title":"Avaliando o modelo treinado\u00b6","text":"<p>Vamos colocar alguns valores e ver a predi\u00e7\u00e3o do classificador.</p>"},{"location":"aulas/IA/lab03/regressao/#algoritmos-de-ml-para-regressao","title":"Algoritmos de ML para regress\u00e3o\u00b6","text":"<p>Sugest\u00e3o de alguns algoritmos de ML para problemas de regress\u00e3o:</p> Nome Vantagem Desvantagem Exemplo sklearn Regress\u00e3o Linear F\u00e1cil de entender e implementar Pode n\u00e3o ser adequado para problemas mais complexos from sklearn.linear_model import LinearRegressionmodel = LinearRegression()model.fit(X, y)prediction = model.predict([X_teste]) \u00c1rvores de decis\u00e3o F\u00e1cil de entender e visualizar Pode levar a overfitting se a \u00e1rvore for muito grande from sklearn.tree import DecisionTreeRegressormodel = DecisionTreeRegressor()model.fit(X, y)prediction = model.predict([X_teste]) Random Forest Mais robusto e geralmente mais preciso do que uma \u00fanica \u00e1rvore de decis\u00e3o Pode ser mais lento e mais dif\u00edcil de ajustar from sklearn.ensemble import RandomForestRegressormodel = RandomForestRegressor(n_estimators=100)model.fit(X, y)prediction = model.predict([X_teste]) Support Vector Regression (SVR) Lida bem com dados multidimensionais e n\u00e3o lineares Pode ser dif\u00edcil de escolher o kernel correto e ajustar os hiperpar\u00e2metros from sklearn.svm import SVRmodel = SVR(kernel='rbf')model.fit(X, y)prediction = model.predict([X_teste]) Gradient Boosting Preciso e lida bem com dados multidimensionais e n\u00e3o lineares Pode ser mais lento e mais dif\u00edcil de ajustar from sklearn.ensemble import GradientBoostingRegressormodel = GradientBoostingRegressor(n_estimators=100)model.fit(X, y)prediction = model.predict([X_teste])"},{"location":"aulas/IA/lab03/regressao/#exercicio-1-experimentar-outras-variaveis","title":"Exerc\u00edcio 1: Experimentar outras vari\u00e1veis\u00b6","text":"<ol> <li>Tente usar apenas <code>HouseAge</code> para predizer pre\u00e7o</li> <li>Compare o R\u00b2 com o modelo que usa <code>MedInc</code></li> <li>Qual vari\u00e1vel sozinha \u00e9 melhor preditora?</li> </ol>"},{"location":"aulas/IA/lab03/regressao/#exercicio-2-modelo-com-todas-as-variaveis","title":"Exerc\u00edcio 2: Modelo com todas as vari\u00e1veis\u00b6","text":"<ol> <li>Crie um modelo usando todas as 8 vari\u00e1veis</li> <li>Compare com o modelo de 4 vari\u00e1veis</li> <li>Houve melhoria significativa?</li> </ol>"},{"location":"aulas/IA/lab03/regressao2/","title":"Regressao2","text":"In\u00a0[55]: Copied! <pre>import operator\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n# importa feature polinomial\nfrom sklearn.preprocessing import PolynomialFeatures\n\n#####----------- vou gerar alguns numeros aleat\u00f3rios ------------------\n\n#gerando numeros aleatorios, apenas para este exemplo\nnp.random.seed(42)\nx = 2 - 3 * np.random.normal(0, 1, 30)\ny = x - 3 * (x ** 2) + 0.8 * (x ** 3)+ 0.2 * (x ** 4) + np.random.normal(-20, 20, 30)\n\n# ajuste nos dados, pois estamos trabalhando com a numpy \nx = x[:, np.newaxis]\ny = y[:, np.newaxis]\n####---------------pronto j\u00e1 temos os dados para treinar -------------\n\n\n#----\u00c9 aqui que o seu c\u00f3digo muda ------------------------------------\n\n# Chama a fun\u00e7\u00e3o definindo o grau do polinomio e aplica o modelo\n\ngrau_poly = 1\npolynomial_features= PolynomialFeatures(degree = grau_poly)\nx_poly = polynomial_features.fit_transform(x)\n\n#----Pronto agora \u00e9 tudo como era antes, com regress\u00e3o linear\n\n\nmodel = LinearRegression()\nmodel.fit(x_poly, y)\ny_poly_pred = model.predict(x_poly)\n\n# M\u00e9trica de avalia\u00e7\u00e3o do modelo\nprint(\"Soma dos Erros ao Quadrado (SSE): %2.f \" % np.sum((y_poly_pred - y)**2))\nprint(\"Erro Quadr\u00e1tico M\u00e9dio (MSE): %.2f\" % mean_squared_error(y,y_poly_pred))\nprint(\"Erro M\u00e9dio Absoluto (MAE): %.2f\" % mean_absolute_error(y, y_poly_pred))\nprint (\"Raiz do Erro Quadr\u00e1tico M\u00e9dio (RMSE): %.2f \" % np.sqrt(mean_squared_error(y, y_poly_pred)))\nprint(\"R2-score: %.2f\" % r2_score(y,y_poly_pred) )\n\n\nplt.scatter(x, y, s=10)\n# ordena os valores de x antes de plotar\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)\nx, y_poly_pred = zip(*sorted_zip)\n\nplt.plot(x, y_poly_pred, color='r')\nplt.show()\n</pre> import operator  import numpy as np import matplotlib.pyplot as plt  from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error  # importa feature polinomial from sklearn.preprocessing import PolynomialFeatures  #####----------- vou gerar alguns numeros aleat\u00f3rios ------------------  #gerando numeros aleatorios, apenas para este exemplo np.random.seed(42) x = 2 - 3 * np.random.normal(0, 1, 30) y = x - 3 * (x ** 2) + 0.8 * (x ** 3)+ 0.2 * (x ** 4) + np.random.normal(-20, 20, 30)  # ajuste nos dados, pois estamos trabalhando com a numpy  x = x[:, np.newaxis] y = y[:, np.newaxis] ####---------------pronto j\u00e1 temos os dados para treinar -------------   #----\u00c9 aqui que o seu c\u00f3digo muda ------------------------------------  # Chama a fun\u00e7\u00e3o definindo o grau do polinomio e aplica o modelo  grau_poly = 1 polynomial_features= PolynomialFeatures(degree = grau_poly) x_poly = polynomial_features.fit_transform(x)  #----Pronto agora \u00e9 tudo como era antes, com regress\u00e3o linear   model = LinearRegression() model.fit(x_poly, y) y_poly_pred = model.predict(x_poly)  # M\u00e9trica de avalia\u00e7\u00e3o do modelo print(\"Soma dos Erros ao Quadrado (SSE): %2.f \" % np.sum((y_poly_pred - y)**2)) print(\"Erro Quadr\u00e1tico M\u00e9dio (MSE): %.2f\" % mean_squared_error(y,y_poly_pred)) print(\"Erro M\u00e9dio Absoluto (MAE): %.2f\" % mean_absolute_error(y, y_poly_pred)) print (\"Raiz do Erro Quadr\u00e1tico M\u00e9dio (RMSE): %.2f \" % np.sqrt(mean_squared_error(y, y_poly_pred))) print(\"R2-score: %.2f\" % r2_score(y,y_poly_pred) )   plt.scatter(x, y, s=10) # ordena os valores de x antes de plotar sort_axis = operator.itemgetter(0) sorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis) x, y_poly_pred = zip(*sorted_zip)  plt.plot(x, y_poly_pred, color='r') plt.show() <pre>Soma dos Erros ao Quadrado (SSE): 602124 \nErro Quadr\u00e1tico M\u00e9dio (MSE): 20070.81\nErro M\u00e9dio Absoluto (MAE): 104.66\nRaiz do Erro Quadr\u00e1tico M\u00e9dio (RMSE): 141.67 \nR2-score: 0.55\n</pre> In\u00a0[\u00a0]: Copied! <pre># instala\u00e7\u00e3o do XGBoost\n!pip install xgboost\n</pre> # instala\u00e7\u00e3o do XGBoost !pip install xgboost <pre>Collecting xgboost\n  Downloading xgboost-3.0.4-py3-none-macosx_12_0_arm64.whl.metadata (2.1 kB)\nRequirement already satisfied: numpy in /Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages (from xgboost) (2.3.2)\nRequirement already satisfied: scipy in /Users/arnaldoalvesvianajunior/DisruptiveArchitectures/.venv/lib/python3.12/site-packages (from xgboost) (1.16.1)\nDownloading xgboost-3.0.4-py3-none-macosx_12_0_arm64.whl (2.0 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 7.6 MB/s  0:00:00 eta 0:00:01\nInstalling collected packages: xgboost\nSuccessfully installed xgboost-3.0.4\n</pre> In\u00a0[5]: Copied! <pre># Importar bibliotecas\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\n\nfrom xgboost import XGBRegressor\n\n# 1. Carregar o dataset novamente, j\u00e1 fizemos isso antes\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\n\n# 2. Dividir os dados em treino (80%) e teste (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 3. Criar e treinar o modelo XGBoost\nmodel = XGBRegressor(\n    n_estimators=100,      # N\u00famero de \u00e1rvores\n    learning_rate=0.1,     # Taxa de aprendizado\n    max_depth=6,           # Profundidade m\u00e1xima de cada \u00e1rvore\n    random_state=42        # Para reprodutibilidade\n)\nmodel.fit(X_train, y_train)\n\n\n# 4. Avaliar no conjunto de teste\n\ny_teste_predito = model.predict(X_test)\n\nprint(\"Soma dos Erros ao Quadrado (SSE): %2.f \" % np.sum((y_teste_predito - y_test)**2))\nprint(\"Erro Quadr\u00e1tico M\u00e9dio (MSE): %.2f\" % mean_squared_error(y_test, y_teste_predito))\nprint(\"Erro M\u00e9dio Absoluto (MAE): %.2f\" % mean_absolute_error(y_test, y_teste_predito))\nprint (\"Raiz do Erro Quadr\u00e1tico M\u00e9dio (RMSE): %.2f \" % np.sqrt(mean_squared_error(y_test, y_teste_predito)))\nprint(\"R2-score: %.2f\" % r2_score(y_test, y_teste_predito))\n\n\n# 5. Exibir a import\u00e2ncia das features\nimportances = model.feature_importances_\nfeature_names = data.feature_names\nfor name, importance in zip(feature_names, importances):\n    print(f\"Atributo: {name}, Import\u00e2ncia: {importance:.3f}\")\n</pre> # Importar bibliotecas from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error  from xgboost import XGBRegressor  # 1. Carregar o dataset novamente, j\u00e1 fizemos isso antes data = fetch_california_housing() X, y = data.data, data.target   # 2. Dividir os dados em treino (80%) e teste (20%) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 3. Criar e treinar o modelo XGBoost model = XGBRegressor(     n_estimators=100,      # N\u00famero de \u00e1rvores     learning_rate=0.1,     # Taxa de aprendizado     max_depth=6,           # Profundidade m\u00e1xima de cada \u00e1rvore     random_state=42        # Para reprodutibilidade ) model.fit(X_train, y_train)   # 4. Avaliar no conjunto de teste  y_teste_predito = model.predict(X_test)  print(\"Soma dos Erros ao Quadrado (SSE): %2.f \" % np.sum((y_teste_predito - y_test)**2)) print(\"Erro Quadr\u00e1tico M\u00e9dio (MSE): %.2f\" % mean_squared_error(y_test, y_teste_predito)) print(\"Erro M\u00e9dio Absoluto (MAE): %.2f\" % mean_absolute_error(y_test, y_teste_predito)) print (\"Raiz do Erro Quadr\u00e1tico M\u00e9dio (RMSE): %.2f \" % np.sqrt(mean_squared_error(y_test, y_teste_predito))) print(\"R2-score: %.2f\" % r2_score(y_test, y_teste_predito))   # 5. Exibir a import\u00e2ncia das features importances = model.feature_importances_ feature_names = data.feature_names for name, importance in zip(feature_names, importances):     print(f\"Atributo: {name}, Import\u00e2ncia: {importance:.3f}\") <pre>Soma dos Erros ao Quadrado (SSE): 938 \nErro Quadr\u00e1tico M\u00e9dio (MSE): 0.23\nErro M\u00e9dio Absoluto (MAE): 0.32\nRaiz do Erro Quadr\u00e1tico M\u00e9dio (RMSE): 0.48 \nR2-score: 0.83\nAtributo: MedInc, Import\u00e2ncia: 0.546\nAtributo: HouseAge, Import\u00e2ncia: 0.065\nAtributo: AveRooms, Import\u00e2ncia: 0.039\nAtributo: AveBedrms, Import\u00e2ncia: 0.021\nAtributo: Population, Import\u00e2ncia: 0.021\nAtributo: AveOccup, Import\u00e2ncia: 0.149\nAtributo: Latitude, Import\u00e2ncia: 0.074\nAtributo: Longitude, Import\u00e2ncia: 0.084\n</pre>"},{"location":"aulas/IA/lab03/regressao2/#objetivo-da-aula","title":"Objetivo da Aula\u00b6","text":"<ul> <li>Apresentar e usar a regress\u00e3o polinomial</li> <li>Entender o funcionamento do XGBoost (eXtreme Gradient Boosting) e sua aplica\u00e7\u00e3o em problemas de regress\u00e3o.</li> <li>Aplicar o XGBoost para prever o valor.</li> <li>Aprender boas pr\u00e1ticas de pr\u00e9-processamento, treinamento e avalia\u00e7\u00e3o de modelos de machine learning.</li> </ul>"},{"location":"aulas/IA/lab03/regressao2/#regressao-polinomial","title":"Regress\u00e3o Polinomial\u00b6","text":"<p>$$ Y = A + BX + C X\u00b2 \\\\ $$ A, B e C s\u00e3o constantes que determinam a posi\u00e7\u00e3o e inclina\u00e7\u00e3o da curva, o 2 indica o grau do polin\u00f4mio. Para cada valor de X temos um Y associado.</p> <pre><code>Em machine learning aprendemos que uma Regress\u00e3o Polinomial \u00e9:</code></pre> <p>$$ Y_{predito} = \\beta_o + \\beta_1X + \\beta_2X\u00b2 \\\\ $$</p> <p>$ \\beta_o $ , $ \\beta_1 $ e $ \\beta_2 $ s\u00e3o par\u00e2metros que determinam o peso da rede. Para cada entrada $ X $ temos um $ Y_{predito} $ aproximado predito.</p> <p>Essa ideia se estende para polin\u00f4mio de graus maiores:</p> <p>$$ Y_{predito} = \\beta_o + \\beta_1X + \\beta_2X\u00b2 + ... + \\beta_nX^n\\\\ $$</p>"},{"location":"aulas/IA/lab03/regressao2/#xgboost","title":"XGBoost\u00b6","text":"<p>O XGBoost (eXtreme Gradient Boosting) \u00e9 um algoritmo de machine learning baseado em gradient boosting, amplamente utilizado em tarefas de regress\u00e3o, classifica\u00e7\u00e3o e ranqueamento. Ele \u00e9 conhecido por sua alta performance, escalabilidade e capacidade de lidar com datasets complexos.</p>"},{"location":"aulas/IA/lab03/regressao2/#conceitos-fundamentais","title":"Conceitos Fundamentais\u00b6","text":"<ol> <li>Boosting:</li> </ol> <ul> <li>Boosting \u00e9 uma t\u00e9cnica de ensemble que combina v\u00e1rios modelos fracos (geralmente \u00e1rvores de decis\u00e3o) para criar um modelo forte.</li> <li>No boosting, os modelos s\u00e3o treinados sequencialmente, onde cada modelo tenta corrigir os erros dos anteriores, dando mais peso \u00e0s inst\u00e2ncias mal previstas.</li> </ul> <ol> <li>Gradient Boosting:</li> </ol> <ul> <li>O gradient boosting usa o gradiente descendente para minimizar uma fun\u00e7\u00e3o de perda (ex.: erro quadr\u00e1tico m\u00e9dio para regress\u00e3o).</li> <li>Cada \u00e1rvore \u00e9 ajustada para prever o res\u00edduo (erro) do modelo anterior, reduzindo gradualmente o erro total.</li> </ul> <ol> <li>Caracter\u00edsticas do XGBoost:</li> </ol> <ul> <li>Regulariza\u00e7\u00e3o: Inclui penalidades L1 (Lasso) e L2 (Ridge) para evitar overfitting, tornando o modelo mais robusto.</li> <li>Paralelismo: Otimiza o treinamento usando m\u00faltiplos n\u00facleos de CPU, tornando-o r\u00e1pido mesmo em datasets grandes.</li> <li>Tratamento de Missing Values: Lida automaticamente com valores ausentes, decidindo a melhor dire\u00e7\u00e3o para cada n\u00f3 da \u00e1rvore.</li> <li>Import\u00e2ncia de Features: Fornece m\u00e9tricas para avaliar a relev\u00e2ncia de cada vari\u00e1vel no modelo.</li> <li>Flexibilidade: Suporta diferentes fun\u00e7\u00f5es de perda e pode ser ajustado com hiperpar\u00e2metros como taxa de aprendizado, n\u00famero de \u00e1rvores e profundidade.</li> </ul>"},{"location":"aulas/IA/lab03/regressao2/#funcionamento","title":"Funcionamento\u00b6","text":"<ol> <li>Inicializa\u00e7\u00e3o: Come\u00e7a com uma previs\u00e3o inicial (ex.: m\u00e9dia dos valores alvo para regress\u00e3o).</li> <li>Constru\u00e7\u00e3o de \u00c1rvores:</li> </ol> <ul> <li>Cada \u00e1rvore \u00e9 constru\u00edda para prever os res\u00edduos (erros) do modelo anterior.</li> <li>A fun\u00e7\u00e3o de perda (ex.: MSE) \u00e9 otimizada usando gradientes e hessianos (derivadas de primeira e segunda ordem).</li> </ul> <ol> <li>Atualiza\u00e7\u00e3o do Modelo: As previs\u00f5es s\u00e3o atualizadas somando as contribui\u00e7\u00f5es de cada \u00e1rvore, ponderadas por uma taxa de aprendizado.</li> <li>Regulariza\u00e7\u00e3o: Penalidades s\u00e3o aplicadas para limitar a complexidade das \u00e1rvores, evitando overfitting.</li> </ol>"},{"location":"aulas/IA/lab04/dicasDePreprocessamento/","title":"dicasDePreprocessamento","text":"<p>Exemplo: Vamos criar um lista com 3 atributos e vamos normalizar.</p> <ul> <li>Atributos:<ul> <li>altura (cm)</li> <li>massa (Kg)</li> <li>idade (anos)</li> </ul> </li> </ul> In\u00a0[47]: Copied! <pre>import numpy as np\nimport pandas as pd\n\ncols = ['altura (cm)', 'massa (kg)', 'idade (anos)']\ndf = pd.DataFrame(np.array([\n                            [170, 90, 20], # altura (cm), massa (Kg), idade (anos)\n                            [168, 55, 33],\n                            [173, 84, 57],\n                            [189, 98, 41]\n                        ]),columns=cols)\n\ndf.head()                  \n</pre> import numpy as np import pandas as pd  cols = ['altura (cm)', 'massa (kg)', 'idade (anos)'] df = pd.DataFrame(np.array([                             [170, 90, 20], # altura (cm), massa (Kg), idade (anos)                             [168, 55, 33],                             [173, 84, 57],                             [189, 98, 41]                         ]),columns=cols)  df.head()                   Out[47]: altura (cm) massa (kg) idade (anos) 0 170 90 20 1 168 55 33 2 173 84 57 3 189 98 41 <p>No nosso exemplo vamos usar o m\u00e9todo min-m\u00e1x:</p> <p>$$ valor_{normalizado}=\\dfrac{valor - min_{valores}}{(max_{valores} - min_{valores})}(max_{feature_range} - min_{feature_range}) + min_{feature_range} $$</p> In\u00a0[48]: Copied! <pre>from sklearn.preprocessing import MinMaxScaler\n\n#Cria o objeto da classe MinMaxScaler\nscaler_minmax = MinMaxScaler(feature_range=(0,1))\n\nscaled_data = scaler_minmax.fit_transform(df[cols])\n\nprint(scaled_data)\n</pre> from sklearn.preprocessing import MinMaxScaler  #Cria o objeto da classe MinMaxScaler scaler_minmax = MinMaxScaler(feature_range=(0,1))  scaled_data = scaler_minmax.fit_transform(df[cols])  print(scaled_data) <pre>[[0.0952381  0.81395349 0.        ]\n [0.         0.         0.35135135]\n [0.23809524 0.6744186  1.        ]\n [1.         1.         0.56756757]]\n</pre> <p>Agora que os dados j\u00e1 est\u00e3o normalizados, podemos aplicar est\u00e1 transforma\u00e7\u00e3o em novos dados inseridos pelos usuario. Para isso, basta usar a fun\u00e7\u00e3o <code>transform()</code> do scaler j\u00e1 treinado.</p> In\u00a0[49]: Copied! <pre># Dados definidos pelo usu\u00e1rio\nnovos_dados = pd.DataFrame({'altura (cm)': [180, 90, 30],\n                            'massa (kg)': [185, 80, 40],\n                            'idade (anos)': [165, 57, 25]})\n# Aplicando o scaler nos novos dados\nscaled_novos_dados = scaler_minmax.transform(novos_dados)\n\nprint(scaled_novos_dados)\n</pre> # Dados definidos pelo usu\u00e1rio novos_dados = pd.DataFrame({'altura (cm)': [180, 90, 30],                             'massa (kg)': [185, 80, 40],                             'idade (anos)': [165, 57, 25]}) # Aplicando o scaler nos novos dados scaled_novos_dados = scaler_minmax.transform(novos_dados)  print(scaled_novos_dados) <pre>[[ 0.57142857  3.02325581  3.91891892]\n [-3.71428571  0.58139535  1.        ]\n [-6.57142857 -0.34883721  0.13513514]]\n</pre> <p>Outra t\u00e9cnica que podemos utilizar \u00e9 o <code>StandardScaler</code> que normaliza valores com m\u00e9dia 0 e desvio padr\u00e3o igual a 1.</p> In\u00a0[50]: Copied! <pre>cols = ['altura (cm)', 'massa (kg)', 'idade (anos)']\ndf = pd.DataFrame(np.array([\n                            [170, 90, 20], # altura (cm), massa (Kg), idade (anos)\n                            [168, 55, 33],\n                            [173, 84, 57],\n                            [189, 98, 41]\n                        ]),columns=cols)\n\ndf.head() \n\nfrom sklearn.preprocessing import StandardScaler\n\n#Cria o objeto da classe standardScaler\nscaler_standard = StandardScaler()\n\nscaled_data = scaler_standard.fit_transform(df[cols])\n\nscaled_data\n</pre> cols = ['altura (cm)', 'massa (kg)', 'idade (anos)'] df = pd.DataFrame(np.array([                             [170, 90, 20], # altura (cm), massa (Kg), idade (anos)                             [168, 55, 33],                             [173, 84, 57],                             [189, 98, 41]                         ]),columns=cols)  df.head()   from sklearn.preprocessing import StandardScaler  #Cria o objeto da classe standardScaler scaler_standard = StandardScaler()  scaled_data = scaler_standard.fit_transform(df[cols])  scaled_data Out[50]: <pre>array([[-0.60412209,  0.50853555, -1.32415683],\n       [-0.84577093, -1.648888  , -0.35435183],\n       [-0.24164884,  0.13869151,  1.4360574 ],\n       [ 1.69154186,  1.00166093,  0.24245125]])</pre> <p>Da mesma forma que o anterior, podemos aplicar est\u00e1 transforma\u00e7\u00e3o em novos dados inseridos pelos usuario. Para isso, basta usar a fun\u00e7\u00e3o <code>transform()</code> do scaler j\u00e1 treinado.</p> In\u00a0[51]: Copied! <pre># Dados definidos pelo usu\u00e1rio\nnovos_dados2 = pd.DataFrame({'altura (cm)': [180, 90, 30],\n                            'massa (kg)': [185, 80, 40],\n                            'idade (anos)': [165, 57, 25]})\n# Aplicando o scaler nos novos dados\nscaled_novos_dados2 = scaler_standard.transform(novos_dados2)\n\nprint(scaled_novos_dados2)\n</pre> # Dados definidos pelo usu\u00e1rio novos_dados2 = pd.DataFrame({'altura (cm)': [180, 90, 30],                             'massa (kg)': [185, 80, 40],                             'idade (anos)': [165, 57, 25]}) # Aplicando o scaler nos novos dados scaled_novos_dados2 = scaler_standard.transform(novos_dados2)  print(scaled_novos_dados2)   <pre>[[  0.60412209   6.36439947   9.49289895]\n [-10.27007559  -0.10787118   1.4360574 ]\n [-17.51954071  -2.57349809  -0.9511549 ]]\n</pre> In\u00a0[52]: Copied! <pre># transforma\u00e7\u00f5es diferentes em no mesmo dataset\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\n\ncols = ['altura', 'massa', 'idade']\ndf = pd.DataFrame(np.array([\n                            [170, 90, 20], # altura (cm), massa (Kg), idade (anos)\n                            [168, 55, 33],\n                            [173, 84, 57],\n                            [189, 98, 41]\n                        ]),columns=cols)\n\n#Cria o objeto da classe standardScaler e MinMaxScaler\nscaler_standard = StandardScaler()\nscaler_minmax = MinMaxScaler(feature_range=(0,1))\n\n# Aplica as transforma\u00e7\u00f5es\nscaled_standard = scaler_standard.fit_transform(df[['altura']])\nscaled_minmax = scaler_minmax.fit_transform(df[['massa']])\n\n# atualiza o valor e mostra o resultado\ndf[['altura']] = scaled_standard\ndf[['massa']] = scaled_minmax\n\n\nprint(\"Dados transformados:\")\nprint(df.head())\n\n# Reverte as transforma\u00e7\u00f5es\ndf[['altura']] = scaler_standard.inverse_transform(df[['altura']])\ndf[['massa']] = scaler_minmax.inverse_transform(df[['massa']])\n\nprint(\"\\nDados revertidos para a escala original:\")\nprint(df.head())\n</pre> # transforma\u00e7\u00f5es diferentes em no mesmo dataset from sklearn.preprocessing import MinMaxScaler from sklearn.preprocessing import StandardScaler   cols = ['altura', 'massa', 'idade'] df = pd.DataFrame(np.array([                             [170, 90, 20], # altura (cm), massa (Kg), idade (anos)                             [168, 55, 33],                             [173, 84, 57],                             [189, 98, 41]                         ]),columns=cols)  #Cria o objeto da classe standardScaler e MinMaxScaler scaler_standard = StandardScaler() scaler_minmax = MinMaxScaler(feature_range=(0,1))  # Aplica as transforma\u00e7\u00f5es scaled_standard = scaler_standard.fit_transform(df[['altura']]) scaled_minmax = scaler_minmax.fit_transform(df[['massa']])  # atualiza o valor e mostra o resultado df[['altura']] = scaled_standard df[['massa']] = scaled_minmax   print(\"Dados transformados:\") print(df.head())  # Reverte as transforma\u00e7\u00f5es df[['altura']] = scaler_standard.inverse_transform(df[['altura']]) df[['massa']] = scaler_minmax.inverse_transform(df[['massa']])  print(\"\\nDados revertidos para a escala original:\") print(df.head()) <pre>Dados transformados:\n     altura     massa  idade\n0 -0.604122  0.813953     20\n1 -0.845771  0.000000     33\n2 -0.241649  0.674419     57\n3  1.691542  1.000000     41\n\nDados revertidos para a escala original:\n   altura  massa  idade\n0   170.0   90.0     20\n1   168.0   55.0     33\n2   173.0   84.0     57\n3   189.0   98.0     41\n</pre> In\u00a0[1]: Copied! <pre>import pandas as pd\n\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\nwine_data = pd.read_csv(url, delimiter=\";\")\n\nwine_data.head()\n\n### seu c\u00f3digo aqui.....\n</pre> import pandas as pd  url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\" wine_data = pd.read_csv(url, delimiter=\";\")  wine_data.head()  ### seu c\u00f3digo aqui.....   Out[1]: fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality 0 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5 1 7.8 0.88 0.00 2.6 0.098 25.0 67.0 0.9968 3.20 0.68 9.8 5 2 7.8 0.76 0.04 2.3 0.092 15.0 54.0 0.9970 3.26 0.65 9.8 5 3 11.2 0.28 0.56 1.9 0.075 17.0 60.0 0.9980 3.16 0.58 9.8 6 4 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5 In\u00a0[39]: Copied! <pre>import numpy as np\nimport pandas as pd\n\ncols = ['altura(cm)', 'massa(kg)', 'qualificacao']\ndf = pd.DataFrame(np.array([\n    [170, 90, \"junior\"],\n    [168, 55, \"pleno\"],\n    [173, 84, \"junior\"],\n    [189, 98, \"senior\"]\n]), columns=cols)\n\ndf.head()\n</pre> import numpy as np import pandas as pd  cols = ['altura(cm)', 'massa(kg)', 'qualificacao'] df = pd.DataFrame(np.array([     [170, 90, \"junior\"],     [168, 55, \"pleno\"],     [173, 84, \"junior\"],     [189, 98, \"senior\"] ]), columns=cols)  df.head() Out[39]: altura(cm) massa(kg) qualificacao 0 170 90 junior 1 168 55 pleno 2 173 84 junior 3 189 98 senior In\u00a0[40]: Copied! <pre>from sklearn.preprocessing import LabelEncoder\n\n# Cria o objeto LabelEncoder\nlabelencoder = LabelEncoder()\n\n# Converte a coluna 'qualificacao' para string e ajusta os dados categ\u00f3ricos\ndf['qualificacao'] = df['qualificacao'].astype(str)\nlabelencoder.fit(df['qualificacao'])\n\n# Aplica a transforma\u00e7\u00e3o dos dados categ\u00f3ricos\ndf['qualificacao'] = labelencoder.transform(df['qualificacao'])\n\n### uma alternativa \u00e9 usar o comando abaixo\n# Ajusta e transforma os dados categ\u00f3ricos no mesmo comando\n# df['qualificacao'] = df['qualificacao'].astype(str)\n# df['qualificacao'] = labelencoder.fit_transform(df['qualificacao'])\n\ndf.head()\n</pre> from sklearn.preprocessing import LabelEncoder  # Cria o objeto LabelEncoder labelencoder = LabelEncoder()  # Converte a coluna 'qualificacao' para string e ajusta os dados categ\u00f3ricos df['qualificacao'] = df['qualificacao'].astype(str) labelencoder.fit(df['qualificacao'])  # Aplica a transforma\u00e7\u00e3o dos dados categ\u00f3ricos df['qualificacao'] = labelencoder.transform(df['qualificacao'])  ### uma alternativa \u00e9 usar o comando abaixo # Ajusta e transforma os dados categ\u00f3ricos no mesmo comando # df['qualificacao'] = df['qualificacao'].astype(str) # df['qualificacao'] = labelencoder.fit_transform(df['qualificacao'])  df.head() Out[40]: altura(cm) massa(kg) qualificacao 0 170 90 0 1 168 55 1 2 173 84 0 3 189 98 2 In\u00a0[41]: Copied! <pre># Novos dados para codifica\u00e7\u00e3o\nnovos_dados2 = pd.DataFrame({'qualificacao': [\"junior\", \"pleno\", \"junior\", \"senior\", \"senior\"]})\nencoded_data = labelencoder.transform(novos_dados2['qualificacao'])\n\nprint('Qualifica\u00e7\u00e3o codificada:', encoded_data)\n\n# Para voltar aos atributos originais\nprint('Qualifica\u00e7\u00e3o decodificada:', labelencoder.inverse_transform(encoded_data))\n</pre> # Novos dados para codifica\u00e7\u00e3o novos_dados2 = pd.DataFrame({'qualificacao': [\"junior\", \"pleno\", \"junior\", \"senior\", \"senior\"]}) encoded_data = labelencoder.transform(novos_dados2['qualificacao'])  print('Qualifica\u00e7\u00e3o codificada:', encoded_data)  # Para voltar aos atributos originais print('Qualifica\u00e7\u00e3o decodificada:', labelencoder.inverse_transform(encoded_data))  <pre>Qualifica\u00e7\u00e3o codificada: [0 1 0 2 2]\nQualifica\u00e7\u00e3o decodificada: ['junior' 'pleno' 'junior' 'senior' 'senior']\n</pre> In\u00a0[53]: Copied! <pre>import numpy as np\nimport pandas as pd\n\ncols = ['altura (cm)', 'massa (kg)', 'qualificacao']\ndf = pd.DataFrame(np.array([\n                            [170, 90, \"junior\"], # altura (cm), massa (Kg), g\u00eanero (f/m)\n                            [168, 55, \"pleno\"],\n                            [173, 84, \"junior\"],\n                            [189, 98, \"senior\"]\n                        ]),columns=cols)\n\ndf.head()\n</pre> import numpy as np import pandas as pd  cols = ['altura (cm)', 'massa (kg)', 'qualificacao'] df = pd.DataFrame(np.array([                             [170, 90, \"junior\"], # altura (cm), massa (Kg), g\u00eanero (f/m)                             [168, 55, \"pleno\"],                             [173, 84, \"junior\"],                             [189, 98, \"senior\"]                         ]),columns=cols)  df.head() Out[53]: altura (cm) massa (kg) qualificacao 0 170 90 junior 1 168 55 pleno 2 173 84 junior 3 189 98 senior In\u00a0[54]: Copied! <pre>from sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder()\n\nponte_ohe = ohe.fit_transform(df[['qualificacao']]).toarray()\n\nponte_ohe\n</pre> from sklearn.preprocessing import OneHotEncoder  ohe = OneHotEncoder()  ponte_ohe = ohe.fit_transform(df[['qualificacao']]).toarray()  ponte_ohe Out[54]: <pre>array([[1., 0., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.]])</pre> In\u00a0[55]: Copied! <pre>#transforma o o np.arry em um dataframe\nponte_ohe = pd.DataFrame(ponte_ohe,columns=[\"qualificacao\"+str(int(i)) for i in range(df.shape[1])])\n\n#adiciona as novas colunas ao dataframe original\ndf = pd.concat([df,ponte_ohe], axis=1)\n\ndf.head()\n</pre> #transforma o o np.arry em um dataframe ponte_ohe = pd.DataFrame(ponte_ohe,columns=[\"qualificacao\"+str(int(i)) for i in range(df.shape[1])])  #adiciona as novas colunas ao dataframe original df = pd.concat([df,ponte_ohe], axis=1)  df.head() Out[55]: altura (cm) massa (kg) qualificacao qualificacao0 qualificacao1 qualificacao2 0 170 90 junior 1.0 0.0 0.0 1 168 55 pleno 0.0 1.0 0.0 2 173 84 junior 1.0 0.0 0.0 3 189 98 senior 0.0 0.0 1.0 In\u00a0[56]: Copied! <pre># faz o drop da coluna original qualificacao\n\ndf = df.drop([\"qualificacao\"], axis=1)\n\ndf.head()\n</pre> # faz o drop da coluna original qualificacao  df = df.drop([\"qualificacao\"], axis=1)  df.head() Out[56]: altura (cm) massa (kg) qualificacao0 qualificacao1 qualificacao2 0 170 90 1.0 0.0 0.0 1 168 55 0.0 1.0 0.0 2 173 84 1.0 0.0 0.0 3 189 98 0.0 0.0 1.0 <p>Note que agora temos a adi\u00e7\u00e3o de 3 novas colunas, onde cada uma corresponde a uma classifica\u00e7\u00e3o do atributo (\"junior\", \"pleno\", \"senior\")</p> In\u00a0[5]: Copied! <pre>##### seu c\u00f3digo aqui........\n</pre> ##### seu c\u00f3digo aqui........   <ul> <li><p>O PCA \u00e9 uma transforma\u00e7\u00e3o linear, ou seja, multiplica os vetores de atributos de entradas de N posi\u00e7\u00f5es por uma matriz com MxN, com M \u2264 N, resultando em um novo vetor de N dimens\u00f5es</p> </li> <li><p>O elemento que se destaca \u00e9 a da vari\u00e2ncia</p> </li> <li><p>Essa transforma\u00e7\u00e3o \u00e9 obtida por meio dos vetores de treinamento. A redu\u00e7\u00e3o na dimensionalidade \u00e9 controlada pelo par\u00e2metro que define a porcentagem de variabilidade que ser\u00e1 mantida nos novos dados</p> </li> <li><p>No Python, fazemos:</p> </li> </ul> In\u00a0[57]: Copied! <pre>from sklearn.decomposition import PCA\n\nmedidas_pca = PCA(0.5).fit_transform(df) # Mantem 50% de variabilidade\nprint(medidas_pca)\n\nprint(\"shape original: \" , df.shape, \"shape PCA: \" ,  medidas_pca.shape)\n</pre> from sklearn.decomposition import PCA  medidas_pca = PCA(0.5).fit_transform(df) # Mantem 50% de variabilidade print(medidas_pca)  print(\"shape original: \" , df.shape, \"shape PCA: \" ,  medidas_pca.shape)   <pre>[[ -5.81469106]\n [ 27.45356145]\n [ -1.35358402]\n [-20.28528637]]\nshape original:  (4, 5) shape PCA:  (4, 1)\n</pre> <ul> <li>Cada linha nessa matriz corresponde a uma vetor com N dimens\u00f5es, com um significado especial, denominado de auto-vetor</li> <li>No caso dos vetores serem imagens, essas \u201cauto-imagens\u201d guardam caracter\u00edsticas que ser\u00e3o usadas para identificar as imagens de teste</li> </ul> In\u00a0[61]: Copied! <pre>#Instale os pacotes e fa\u00e7a o download dos arquivos, se ja estiver na pasta n\u00e3o precisa rodar essa celula.\n\n# se `pip3` n\u00e3o funcionar, tente apenas `pip`\n!pip3 install --user python-mnist\n!pip3 install wget\n\n\nimport wget\nwget.download('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz')\nwget.download('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz')\nwget.download('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz')\nwget.download('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz')\n</pre> #Instale os pacotes e fa\u00e7a o download dos arquivos, se ja estiver na pasta n\u00e3o precisa rodar essa celula.  # se `pip3` n\u00e3o funcionar, tente apenas `pip` !pip3 install --user python-mnist !pip3 install wget   import wget wget.download('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz') wget.download('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz') wget.download('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz') wget.download('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz') <pre>Collecting python-mnist\n  Downloading python_mnist-0.7-py2.py3-none-any.whl.metadata (3.5 kB)\nDownloading python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\nInstalling collected packages: python-mnist\nSuccessfully installed python-mnist-0.7\n\n[notice] A new release of pip is available: 23.3.1 -&gt; 24.0\n[notice] To update, run: /Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: wget in /Users/arnaldoalvesvianajunior/Library/Python/3.9/lib/python/site-packages (3.2)\n\n[notice] A new release of pip is available: 23.3.1 -&gt; 24.0\n[notice] To update, run: /Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\n</pre> Out[61]: <pre>'t10k-labels-idx1-ubyte (1).gz'</pre> In\u00a0[62]: Copied! <pre>import time\nimport numpy as np\n# API MNIST\nfrom mnist import MNIST\n\nt0 = time.time()\n\n# Importa os dados do dieret\u00f3rio local\nmndata = MNIST('.')\n# Habilita abrir arquivos compactados\nmndata.gz = True \n\n# Carrega os dados de treinamento\nentradas_treino, classes_treino = mndata.load_training()\n# Carrega os dados de treinamento\nentradas_teste, classes_teste = mndata.load_testing()\n\n#Transformando em array do numpy\nentradas_treino = np.array(entradas_treino)\nclasses_treino = np.array(classes_treino)\nentradas_teste = np.array(entradas_teste)\nclasses_teste = np.array(classes_teste)\n\ndados_reduzidos = False\n\nprint(\"Tempo para carregamento das imagens: {}s\".format(time.time()-t0))\n\nprint(\"Dimens\u00f5es da matriz dos dados de treinamento: \", entradas_treino.shape)\nprint(\"Dimens\u00f5es da matriz dos dados de teste: \", entradas_teste.shape)\n</pre> import time import numpy as np # API MNIST from mnist import MNIST  t0 = time.time()  # Importa os dados do dieret\u00f3rio local mndata = MNIST('.') # Habilita abrir arquivos compactados mndata.gz = True   # Carrega os dados de treinamento entradas_treino, classes_treino = mndata.load_training() # Carrega os dados de treinamento entradas_teste, classes_teste = mndata.load_testing()  #Transformando em array do numpy entradas_treino = np.array(entradas_treino) classes_treino = np.array(classes_treino) entradas_teste = np.array(entradas_teste) classes_teste = np.array(classes_teste)  dados_reduzidos = False  print(\"Tempo para carregamento das imagens: {}s\".format(time.time()-t0))  print(\"Dimens\u00f5es da matriz dos dados de treinamento: \", entradas_treino.shape) print(\"Dimens\u00f5es da matriz dos dados de teste: \", entradas_teste.shape)  <pre>Tempo para carregamento das imagens: 4.33591890335083s\nDimens\u00f5es da matriz dos dados de treinamento:  (60000, 784)\nDimens\u00f5es da matriz dos dados de teste:  (10000, 784)\n</pre> In\u00a0[63]: Copied! <pre># Fun\u00e7\u00e3o que visualiza a linha lin da matriz\nimport matplotlib.pyplot as plt\nimport math\ndef visualiza_linha_mnist(matriz, lin):\n  size = int(math.sqrt(matriz.shape[1]))\n  img = np.reshape(matriz[lin], (size, size))\n  plt.imshow(img, cmap=\"gray\")\n  \n# Visualiza\u00e7\u00e3o da linha 0\nvisualiza_linha_mnist(entradas_treino, 0)\nplt.show()\n</pre> # Fun\u00e7\u00e3o que visualiza a linha lin da matriz import matplotlib.pyplot as plt import math def visualiza_linha_mnist(matriz, lin):   size = int(math.sqrt(matriz.shape[1]))   img = np.reshape(matriz[lin], (size, size))   plt.imshow(img, cmap=\"gray\")    # Visualiza\u00e7\u00e3o da linha 0 visualiza_linha_mnist(entradas_treino, 0) plt.show() In\u00a0[64]: Copied! <pre>from sklearn.preprocessing import StandardScaler\n# PCA\nfrom sklearn.decomposition import PCA\n\nt0 = time.time()\n\nnormalizador = StandardScaler()\nredutor_dim = PCA(0.85)   # 85% de variabilidade\n\nentradas_treino_norm = normalizador.fit_transform(entradas_treino)\nentradas_treino_norm = redutor_dim.fit_transform(entradas_treino_norm)\n\nentradas_teste_norm = normalizador.transform(entradas_teste)\nentradas_teste_norm = redutor_dim.transform(entradas_teste_norm)\n\nprint(\"Tempo para o processamento (normaliza\u00e7\u00e3o + PCA) das imagens: {}s\".format(time.time()-t0))\nprint(\"Novas dimensoes das matrizes de dados e classes (labels) de treinamento\")\nprint(entradas_treino_norm.shape, entradas_teste_norm.shape)\n</pre> from sklearn.preprocessing import StandardScaler # PCA from sklearn.decomposition import PCA  t0 = time.time()  normalizador = StandardScaler() redutor_dim = PCA(0.85)   # 85% de variabilidade  entradas_treino_norm = normalizador.fit_transform(entradas_treino) entradas_treino_norm = redutor_dim.fit_transform(entradas_treino_norm)  entradas_teste_norm = normalizador.transform(entradas_teste) entradas_teste_norm = redutor_dim.transform(entradas_teste_norm)  print(\"Tempo para o processamento (normaliza\u00e7\u00e3o + PCA) das imagens: {}s\".format(time.time()-t0)) print(\"Novas dimensoes das matrizes de dados e classes (labels) de treinamento\") print(entradas_treino_norm.shape, entradas_teste_norm.shape) <pre>Tempo para o processamento (normaliza\u00e7\u00e3o + PCA) das imagens: 8.599159002304077s\nNovas dimensoes das matrizes de dados e classes (labels) de treinamento\n(60000, 185) (10000, 185)\n</pre> In\u00a0[65]: Copied! <pre>print(\"Dimens\u00f5es da matriz dos dados de treinamento: \", entradas_treino.shape)\nprint(\"Dimens\u00f5es da matriz dos dados de teste: \", entradas_teste.shape)\nprint(28*28)\n</pre> print(\"Dimens\u00f5es da matriz dos dados de treinamento: \", entradas_treino.shape) print(\"Dimens\u00f5es da matriz dos dados de teste: \", entradas_teste.shape) print(28*28)  <pre>Dimens\u00f5es da matriz dos dados de treinamento:  (60000, 784)\nDimens\u00f5es da matriz dos dados de teste:  (10000, 784)\n784\n</pre> In\u00a0[66]: Copied! <pre>##Avalia\u00e7\u00e3o PCA\n\nprint (len(entradas_treino_norm), len(entradas_treino_norm))\n\nprint (\"taxa de variancia explicada: \" , len(redutor_dim.explained_variance_ratio_), redutor_dim.explained_variance_ratio_)\nprint (\"valores de cada um dos componentes: \", len(redutor_dim.singular_values_), redutor_dim.singular_values_)\n</pre> ##Avalia\u00e7\u00e3o PCA  print (len(entradas_treino_norm), len(entradas_treino_norm))  print (\"taxa de variancia explicada: \" , len(redutor_dim.explained_variance_ratio_), redutor_dim.explained_variance_ratio_) print (\"valores de cada um dos componentes: \", len(redutor_dim.singular_values_), redutor_dim.singular_values_) <pre>60000 60000\ntaxa de variancia explicada:  185 [0.05646717 0.04078272 0.0373938  0.02885115 0.02521109 0.0219427\n 0.01923344 0.01745799 0.01535092 0.0140172  0.01341743 0.01203742\n 0.0111457  0.01089924 0.01028649 0.00994487 0.00936383 0.00921046\n 0.00893437 0.00869913 0.00827363 0.00803417 0.00764846 0.00741772\n 0.00715293 0.00691847 0.00684136 0.00656675 0.00631677 0.0061292\n 0.00596255 0.00587716 0.00571592 0.00562307 0.00554682 0.00538418\n 0.00531182 0.00519606 0.00508211 0.00480006 0.00476456 0.00469139\n 0.00454349 0.00451346 0.00446963 0.00443383 0.00438215 0.00430382\n 0.00426878 0.00423647 0.00404696 0.00399447 0.00397456 0.00393821\n 0.00385814 0.00379043 0.00375403 0.00370776 0.00364944 0.00359301\n 0.00352382 0.00347794 0.00344411 0.00339868 0.00335955 0.00334886\n 0.00331864 0.00323026 0.00316277 0.00313244 0.00310731 0.00307243\n 0.00304914 0.00302717 0.00299485 0.00297761 0.00295052 0.00290438\n 0.00286856 0.00285678 0.00283398 0.00282627 0.00279551 0.00279305\n 0.00278519 0.00277455 0.00275901 0.00274227 0.00271411 0.00269263\n 0.00266484 0.00263581 0.00262962 0.00261034 0.00258827 0.00256176\n 0.00253846 0.00250447 0.00247829 0.00245034 0.00242347 0.00242064\n 0.00238875 0.00237455 0.00235608 0.00233053 0.0022798  0.00226174\n 0.00222832 0.00222442 0.00218169 0.00217257 0.00214277 0.00211938\n 0.00210972 0.0020733  0.00204761 0.00204368 0.00202409 0.00200462\n 0.00198822 0.00195216 0.00193737 0.00192103 0.00191716 0.00189802\n 0.00187089 0.00186536 0.0018132  0.00180005 0.00179194 0.00178973\n 0.0017695  0.00176158 0.00174797 0.00172985 0.00172017 0.00168727\n 0.00168517 0.00166842 0.00164718 0.00164575 0.00164294 0.00161486\n 0.0016049  0.00158912 0.0015749  0.00155918 0.00155638 0.00154666\n 0.00154043 0.00151605 0.00150272 0.00148761 0.00147505 0.0014682\n 0.00145803 0.00145568 0.00144737 0.00142895 0.00141058 0.00139939\n 0.00139709 0.00139533 0.00139355 0.00139225 0.00138773 0.0013834\n 0.00137816 0.00136845 0.00136165 0.00135822 0.00133701 0.00132905\n 0.00131059 0.00130293 0.00129324 0.00128241 0.00127407 0.00126822\n 0.00125322 0.00124045 0.00122984 0.00121618 0.00121506]\nvalores de cada um dos componentes:  185 [1558.59475775 1324.56506425 1268.33806904 1114.08096949 1041.4321537\n  971.58372712  909.62781125  866.6272717   812.64796157  776.54347762\n  759.74854205  719.61780297  692.45059052  684.75186297  665.2254475\n  654.08571283  634.6905444   629.47108378  619.96491977  611.74864821\n  596.60000904  587.90318277  573.61706238  564.89867585  554.72424844\n  545.55706108  542.50833328  531.50859803  521.29389656  513.4959735\n  506.46720335  502.82760665  495.88178934  491.83803286  488.4917578\n  481.27703518  478.03201127  472.79417281  467.58152416  454.42094643\n  452.73755506  449.24798572  442.10962544  440.64606814  438.50160223\n  436.74183847  434.18923818  430.29086602  428.53573113  426.91093544\n  417.25324612  414.53862665  413.50407786  411.60868344  407.40275713\n  403.81203369  401.86842781  399.38423688  396.23108492  393.15532131\n  389.35177902  386.80851333  384.92303762  382.37581508  380.16791664\n  379.56282447  377.84621102  372.78112287  368.86630886  367.09355234\n  365.61819099  363.56002954  362.17963536  360.87245819  358.94092281\n  357.90621176  356.27402823  353.47770091  351.29123623  350.56891845\n  349.16718716  348.69212323  346.78936634  346.6369362   346.14874915\n  345.48710725  344.51781742  343.47138797  341.70285667  340.34823393\n  338.58759852  336.73828279  336.34220011  335.10707121  333.68749917\n  331.97454025  330.46102411  328.24139239  326.52136861  324.67435089\n  322.88936082  322.70086024  320.56858096  319.61385191  318.36887157\n  316.63780238  313.1725175   311.92971598  309.61609571  309.34544735\n  306.360028    305.71862309  303.61468514  301.95298194  301.26427292\n  298.6527374   296.79663011  296.51171699  295.08666108  293.66447678\n  292.46061956  289.79603176  288.69671405  287.47662373  287.18664393\n  285.74965273  283.70035923  283.28015727  279.29169961  278.27731866\n  277.64955632  277.47843311  275.90592833  275.28720761  274.22175433\n  272.7966822   272.03227791  269.41865042  269.25082545  267.90897203\n  266.19893845  266.08295896  265.85540571  263.57386832  262.7601788\n  261.46521492  260.29213116  258.9899899   258.75750421  257.94798876\n  257.42815671  255.38269599  254.25740548  252.97587879  251.90640352\n  251.32057934  250.44831234  250.24628968  249.53113324  247.93848294\n  246.33934447  245.36012105  245.15894614  245.00431129  244.84794458\n  244.73387616  244.33626317  243.95435627  243.49224383  242.63262475\n  242.0297457   241.72463554  239.82975985  239.11486781  237.44845681\n  236.75327822  235.87142391  234.8811359   234.11618726  233.57844421\n  232.19261652  231.00662861  230.01633731  228.73575127  228.63068847]\n</pre> In\u00a0[67]: Copied! <pre>plt.figure(figsize=(10,10))\nplt.subplot(4,4,1)\nfor i in range(4):\n  for j in range(4):\n    plt.subplot(4,4,i*4+j+1)\n    visualiza_linha_mnist(redutor_dim.components_, i*4 + j)\nplt.show()\n</pre> plt.figure(figsize=(10,10)) plt.subplot(4,4,1) for i in range(4):   for j in range(4):     plt.subplot(4,4,i*4+j+1)     visualiza_linha_mnist(redutor_dim.components_, i*4 + j) plt.show() In\u00a0[70]: Copied! <pre>## Seu c\u00f3digo aqui.....\n</pre> ## Seu c\u00f3digo aqui....."},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#2-aprendizagem-de-maquina","title":"2. Aprendizagem de m\u00e1quina\u00b6","text":""},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#objetivos","title":"Objetivos\u00b6","text":"<ul> <li>Dicas de pr\u00e9-processamento de dados</li> <li>Entender e praticar a normaliza\u00e7\u00e3o dos dados</li> <li>Entender e praticar codifica\u00e7\u00e3o Label Encoder e One Hot Encoder.</li> <li>Redu\u00e7\u00e3o de dimensionalidade: PCA</li> </ul>"},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#pre-processamento","title":"Pr\u00e9-processamento\u00b6","text":"<p>Nesta etapa estamos interessados em tratar os dados que servir\u00e3o de entrada do nosso modelo de Machine Learning seja ele predi\u00e7\u00e3o, agrupamento ou classifica\u00e7\u00e3o. Existem diversas t\u00e9cnicas que podem (e devem) ser aplicadas, j\u00e1 conhecemos algumas e hoje veremos outras t\u00e9cnicas.</p>"},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#normalizacao-de-dados","title":"Normaliza\u00e7\u00e3o de dados\u00b6","text":"<p>O conceito de normaliza\u00e7\u00e3o \u00e9 simples, a id\u00e9ia \u00e9 deixar os dados todos na mesma ordem de grandeza, desta forma evita-se gerar discrep\u00e2ncias entre os atributos (colunas).</p> <p>Os m\u00e9todos mais populares s\u00e3o:</p> <ul> <li>Normaliza\u00e7\u00e3o Min-M\u00e1x: transforma os dados em um escala linear entre 0 e 1</li> <li>Normaliza\u00e7\u00e3o de pontua\u00e7\u00e3o Z: Escala de dados com base na m\u00e9dia e desvio padr\u00e3o (tambem chamado de padroniza\u00e7\u00e3o)</li> <li>Dimensionamento decimal: Dimensiona os dados movendo o ponto decimal do atributo (muito utilizado em sistemas embarcados).</li> </ul>"},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#aplicando-transformacoes-e-revertendo-a-transformacao-dos-dados","title":"Aplicando transforma\u00e7\u00f5es e revertendo a transforma\u00e7\u00e3o dos dados\u00b6","text":"<p>Podemos aplicar a transforma\u00e7\u00e3o nos dados e o m\u00e9todo <code>inverse_transform</code> \u00e9 usado para reverter os dados de volta \u00e0 sua escala original.</p>"},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#desafio-1","title":"Desafio 1\u00b6","text":"<p>Vamos praticar com o dataset 'wine' outro cl\u00e1ssico do mundo de ml. Aplique essas transforma\u00e7\u00f5es necess\u00e1rias aprendidas nos atributos num\u00e9ricos.</p>"},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#codificacao-de-atributos-categoricos-label-encoder-e-one-hot-encoder","title":"Codifica\u00e7\u00e3o de Atributos Categ\u00f3ricos: <code>Label Encoder</code> e <code>One Hot Encoder</code>\u00b6","text":"<p>Em diversas situa\u00e7\u00f5es, nos deparamos com atributos categ\u00f3ricos em nossos conjuntos de dados, ou seja, aqueles que cont\u00eam texto em vez de n\u00fameros.</p> <p>Considere os seguintes exemplos:</p> <ul> <li>O atributo <code>cidade</code>, que pode conter valores como <code>[\"cotia\",\"S\u00e3o Paulo\",\"Pouso Alegre\"]</code> que s\u00e3o exemplos de nomes de cidades;</li> <li>O atributo <code>qualifica\u00e7\u00e3o profissional</code>, que pode ter valores como <code>[\"junior\",\"Pleno\",\"Senior\"]</code> indicando n\u00edveis hier\u00e1rquicos de cargos.</li> </ul> <p>Para trabalhar com aprendizado de m\u00e1quina, manter esses atributos em formato de texto pode n\u00e3o ser a abordagem mais adequada. Por isso, uma alternativa \u00e9 converter esses textos em valores num\u00e9ricos. Vamos explorar algumas t\u00e9cnicas para realizar essa convers\u00e3o.</p>"},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#label-encoder","title":"Label Encoder\u00b6","text":"<p>O Label Encoder \u00e9 uma t\u00e9cnica simples para converter atributos categ\u00f3ricos de texto para n\u00fameros, associando um valor num\u00e9rico \u00fanico a cada texto distinto do atributo.</p> <ul> <li><p>Exemplo:</p> <p>[\"cotia\",\"S\u00e3o Paulo\",\"Pouso Alegre\"] == [0,2,1]</p> <p>[\"junior\",\"Pleno\",\"Senior\"] == [0,1,2]</p> </li> </ul> <p>Observa\u00e7\u00e3o: Note que os indices est\u00e3o em <code>ordem alfab\u00e9tica</code>.</p>"},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#dicas","title":"Dicas\u00b6","text":"<p>O problema surge quando diferentes n\u00fameros na mesma coluna levam o modelo a interpretar erroneamente os dados como se estivessem em uma ordem espec\u00edfica, por exemplo, 0 &lt; 1 &lt; 2.</p> <p>Como desenvolvedor, \u00e9 crucial estar ciente dessa quest\u00e3o e avaliar se a ordena\u00e7\u00e3o faz sentido para o contexto. Por exemplo, para o atributo qualifica\u00e7\u00e3o, a ordena\u00e7\u00e3o pode n\u00e3o ser problem\u00e1tica. No entanto, para atributos como g\u00eanero ou estado, onde n\u00e3o existe uma ordem natural, a utiliza\u00e7\u00e3o do Label Encoder pode ser inadequada.</p> <p>Nesses casos, o One Hot Encoder pode ser uma alternativa \u00fatil, pois cria vari\u00e1veis bin\u00e1rias para cada categoria, evitando assim a implica\u00e7\u00e3o de uma ordem entre elas.</p>"},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#one-hot-encoder","title":"One Hot Encoder\u00b6","text":"<p>Podemos associar cada valor de um atribuco como uma nova coluna e preencher com 0 ou 1 o valor desta coluna, \u00e9 desta forma que o one hot encoder funciona.</p> <ul> <li>Exemplo:</li> </ul>"},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#desafio-2","title":"Desafio 2\u00b6","text":"<p>Vamos avaliar o efeito de transforma\u00e7\u00e3o de variavel no treinamento de um dataset, para isso:</p> <p>Treine e avalie <code>duas vezes</code> o classificador kNN para o dataset Wine.</p> <p>Considere o sequinte:</p> <pre>X = wine_data.drop(['quality'], axis=1)\ny = wine_data['quality'] #Variavel para ser predita\n</pre> <p>Compare o efeito da normaliza\u00e7\u00e3o na avalia\u00e7\u00e3o do classificador. Use k = 5.</p>"},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#reducao-de-dimensionalidade","title":"Redu\u00e7\u00e3o de dimensionalidade\u00b6","text":"<ul> <li>Para o bom desempenho da tarefa de classifica\u00e7\u00e3o \u00e9 importante um conjunto suficientemente grande de atributos.<ul> <li>Em muitos casos, especialmente quando se trabalha diretamente com os pixels das imagens, a informa\u00e7\u00e3o necess\u00e1ria para a classifica\u00e7\u00e3o de padr\u00f5es est\u00e1 espalhada por praticamente todos os atributos</li> </ul> </li> <li>No entanto, um n\u00famero muito grande de atributos atrapalha o desempenho dos classificadores, num efeito conhecido como a maldi\u00e7\u00e3o da dimensionalidade, curse of dimensionality.</li> <li>Frequentemente um n\u00famero grande de atributos est\u00e1 associado \u00e0 redund\u00e2ncia da informa\u00e7\u00e3o, ou seja, os valores dos tributos est\u00e3o fortemente ligados entre si.<ul> <li>Por exemplo, nas imagens de d\u00edgitos, pixels pr\u00f3ximos tendem a ter tonalidades semelhantes</li> </ul> </li> <li>Uma sa\u00edda para aproveitar a maior parte da informa\u00e7\u00e3o espalhada pelos atributos \u00e9 encontrar uma transforma\u00e7\u00e3o dos dados que use atributos o t\u00e3o independentes quanto poss\u00edvel.<ul> <li>Dessa forma, alguns atributos ter\u00e3o mais relev\u00e2ncia do que outros, pois ao desfazer a interdepend\u00eancia, conseguimos \u201cseparar\u201d a informa\u00e7\u00e3o relevante da informa\u00e7\u00e3o redundante</li> </ul> </li> </ul>"},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#pca-principal-component-analysis","title":"PCA : Principal Component Analysis\u00b6","text":"<p>(An\u00e1lise de Componentes Principais)</p>"},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#1-gera-a-matriz-de-dados-de-entradas-e-o-vetor-de-classes-alvo-para-treinamento","title":"1 - Gera a matriz de dados de entradas e o vetor de classes alvo para treinamento\u00b6","text":"<p>Cada linha da matriz de entradas (atributos) cont\u00e9m os pixels da  imagem.</p> <p>Cada posi\u00e7\u00e3o do array de r\u00f3tulos (labels) cont\u00e9m a classe alvo da imgem.</p> <p>No caso deste dataset, as imagens de trenamento e de teste j\u00e1 est\u00e3o separadas, e vamos adotar a separa\u00e7\u00e3o sugerida pelo autor da base de dados.</p>"},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#11-visualizcao-de-uma-imagem","title":"1.1 Visualiz\u00e7\u00e3o de uma imagem\u00b6","text":"<p>Neste dataset cada imagem est\u00e1 armazenada como uma linha da matriz de entrada. Para visualizar a imagem que est\u00e1 na linha <code>i</code> da matriz, temos que convert\u00ea-la novamente em uma matriz quadrada, e usar a biblioteca <code>matplotlib</code></p>"},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#2-faz-a-normalizacao-e-a-reducao-da-dimensionalidade-com-pca","title":"2 - Faz a normaliza\u00e7\u00e3o e a redu\u00e7\u00e3o da dimensionalidade com PCA\u00b6","text":"<p>Instancia o modelo PCA de forma que 85% da variabilidade de dados seja mantida. O m\u00e9todo <code>fit_transform(X)</code> treina o PCA e j\u00e1 traz os dados <code>X</code> transformados. Para reaproveitar o mesmo modelo PCA sem trein\u00e1-o novamente, usamos <code>transform()</code>.</p> <p>Uma vez que os dados de treinamento e teste j\u00e1 est\u00e3o separados,treinamos o normalizador e o PCA com os dados de treinamento, e apenas aplicamos a transforma\u00e7\u00e3o nos dados de teste.</p>"},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#21-visualizacao-das-16-primairas-imagens-principais","title":"2.1 - Visualiza\u00e7\u00e3o das 16 primairas imagens principais\u00b6","text":"<p>O PCA neste caso transforma um array (entrada ou linha da matriz) composta por um conjunto de pixels em um outro array que indica a composi\u00e7\u00e3o da imagem em termos de \"imagens principais\"</p>"},{"location":"aulas/IA/lab04/dicasDePreprocessamento/#desafio-3","title":"Desafio 3\u00b6","text":"<p>Agora \u00e9 com voc\u00ea termine a implementa\u00e7ao deste classificador de digitos usando o KNN. Usar as novas matrizes no treinamento, teste e avalia\u00e7\u00e3o do classificador</p>"},{"location":"aulas/IA/lab05/validacaocruzada/","title":"Validacaocruzada","text":"In\u00a0[1]: Copied! <pre>from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics                 \n</pre> from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn import metrics                  In\u00a0[2]: Copied! <pre># importa o dataset iris\niris = load_iris()\n\n# separa os dados em atributos (x) e alvo (y)\nX = iris.data\ny = iris.target\n</pre> # importa o dataset iris iris = load_iris()  # separa os dados em atributos (x) e alvo (y) X = iris.data y = iris.target In\u00a0[3]: Copied! <pre># divide os dados em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n\n# treina o modelo com knn=15\nknn = KNeighborsClassifier(n_neighbors=15)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\n# resultado da acuracia\nmetrics.accuracy_score(y_test, y_pred)\n\nprint(\"Acuracia: \", metrics.accuracy_score(y_test, y_pred))\n</pre> # divide os dados em treino e teste X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)  # treina o modelo com knn=15 knn = KNeighborsClassifier(n_neighbors=15) knn.fit(X_train, y_train) y_pred = knn.predict(X_test)  # resultado da acuracia metrics.accuracy_score(y_test, y_pred)  print(\"Acuracia: \", metrics.accuracy_score(y_test, y_pred))  <pre>Acuracia:  0.9\n</pre> In\u00a0[4]: Copied! <pre>from sklearn.model_selection import KFold\n\ncrossvalidation = KFold(n_splits=10,shuffle=True, random_state=7)\n\nknn = KNeighborsClassifier(n_neighbors=5)\n</pre> from sklearn.model_selection import KFold  crossvalidation = KFold(n_splits=10,shuffle=True, random_state=7)  knn = KNeighborsClassifier(n_neighbors=5) In\u00a0[6]: Copied! <pre>from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(knn, X, y, cv=crossvalidation, scoring='accuracy')\nprint(\"Array do kfold com os resultados: \",scores)\n</pre> from sklearn.model_selection import cross_val_score  scores = cross_val_score(knn, X, y, cv=crossvalidation, scoring='accuracy') print(\"Array do kfold com os resultados: \",scores) <pre>Array do kfold com os resultados:  [0.86666667 0.86666667 1.         1.         1.         1.\n 1.         0.93333333 0.93333333 0.93333333]\n</pre> In\u00a0[7]: Copied! <pre>print(\"Acuracia m\u00e9dia com kfold: \",scores.mean())\n</pre> print(\"Acuracia m\u00e9dia com kfold: \",scores.mean()) <pre>Acuracia m\u00e9dia com kfold:  0.9533333333333334\n</pre> In\u00a0[88]: Copied! <pre>## Sua resposta aqui....\n</pre> ## Sua resposta aqui...."},{"location":"aulas/IA/lab05/validacaocruzada/#validacao-cruzada","title":"Valida\u00e7\u00e3o Cruzada\u00b6","text":""},{"location":"aulas/IA/lab05/validacaocruzada/#objetivos","title":"Objetivos\u00b6","text":"<ul> <li>Entender e praticar valida\u00e7\u00e3o cruzada: kfold.</li> </ul> <p>A t\u00e9cnica de valida\u00e7\u00e3o cruzada consiste em dividir em partes pequenas (fold) a base de dados e realizar diversos treinamentos e valida\u00e7\u00f5es com partes diferente de treinamento e teste, ao final \u00e9 feita a m\u00e9dia e o desvio padr\u00e3o do aprendizado.</p> <p>Pr\u00f3s:</p> <ul> <li>Normalmente aumenta a performance do modelo.</li> <li>Reduz aleatoriedade, reduz viez.</li> </ul> <p>Contra:</p> <ul> <li>Mais processamento computacional.</li> </ul> <p>Dicas:</p> <ul> <li>A escolha do <code>k</code> numero de folds \u00e9 determinada tipicamente como sendo 5 ou 10.</li> </ul>"},{"location":"aulas/IA/lab05/validacaocruzada/#diagrama-do-kfold","title":"Diagrama do kfold\u00b6","text":""},{"location":"aulas/IA/lab05/validacaocruzada/#melhorando-o-modelo","title":"Melhorando o modelo\u00b6","text":"<p>At\u00e9 aqui, sem novidades! Mas... como ficaria o resultado se os grupos de teste e treino fossem alterados? vamos descobrir usando o kfold e crossvalidation.</p>"},{"location":"aulas/IA/lab05/validacaocruzada/#desafio","title":"Desafio\u00b6","text":"<p>Pergunta: O Resultado foi praticamente o mesmo, por que?</p>"},{"location":"aulas/IA/lab05/validacaocruzada/#bonus-outras-tecnicas-de-avaliacao-de-modelo","title":"Bonus: Outras t\u00e9cnicas de avalia\u00e7\u00e3o de modelo\u00b6","text":"<ul> <li><p><code>StratifiedKFold</code> = Lida melhor com dados desbalanceados, ou seja, possui uma difer\u00e7a grande entre as frequencias das classes, pois tentar manter a mesma propor\u00e7\u00e3o em todos os folds.</p> </li> <li><p><code>ShuffleSplit</code> = Gera folds aleatorios de treino e teste a cada itera\u00e7\u00e3o. Um cuidado, pode ser que entre uma itera\u00e7\u00e3o e outra os mesmos dados sejam selecionados</p> </li> </ul>"},{"location":"aulas/IA/lab06/redesneurais%20copy/","title":"Redes Neurais Artificiais: Guia Completo","text":""},{"location":"aulas/IA/lab06/redesneurais%20copy/#sumario","title":"Sum\u00e1rio","text":"<ol> <li>Introdu\u00e7\u00e3o</li> <li>Fundamentos Biol\u00f3gicos</li> <li>Neur\u00f4nio Artificial</li> <li>Perceptron</li> <li>Multilayer Perceptron (MLP)</li> <li>Algoritmos de Treinamento</li> <li>Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o</li> <li>Regulariza\u00e7\u00e3o e Otimiza\u00e7\u00e3o</li> <li>Aplica\u00e7\u00f5es Pr\u00e1ticas</li> <li>Exerc\u00edcios e Projetos</li> </ol>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>As Redes Neurais Artificiais (RNA) s\u00e3o modelos computacionais inspirados no funcionamento do sistema nervoso biol\u00f3gico. Elas representam uma das abordagens mais poderosas e vers\u00e1teis do aprendizado de m\u00e1quina, capazes de:</p> <ul> <li>Aprender padr\u00f5es complexos em dados</li> <li>Aproximar fun\u00e7\u00f5es n\u00e3o-lineares arbitr\u00e1rias</li> <li>Resolver problemas de classifica\u00e7\u00e3o, regress\u00e3o e clustering</li> <li>Processar diferentes tipos de dados (imagens, texto, s\u00e9ries temporais)</li> </ul>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#por-que-estudar-redes-neurais","title":"Por que estudar Redes Neurais?","text":"<ol> <li>Versatilidade: Aplicam-se a diversos dom\u00ednios</li> <li>Poder expressivo: Podem modelar rela\u00e7\u00f5es complexas</li> <li>Evolu\u00e7\u00e3o cont\u00ednua: Base para Deep Learning e IA moderna</li> <li>Resultados pr\u00e1ticos: Solucionam problemas reais</li> </ol>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#fundamentos-biologicos","title":"Fundamentos Biol\u00f3gicos","text":""},{"location":"aulas/IA/lab06/redesneurais%20copy/#o-neuronio-biologico","title":"O Neur\u00f4nio Biol\u00f3gico","text":"<p>O neur\u00f4nio \u00e9 a unidade fundamental do sistema nervoso, composto por:</p> <p></p> <pre><code>Dendritos \u2192 Soma \u2192 Ax\u00f4nio \u2192 Sinapses\n    \u2191        \u2191       \u2191        \u2191\n  Entrada  Processamento  Transmiss\u00e3o  Sa\u00edda\n</code></pre>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#componentes-principais","title":"Componentes principais:","text":"<ul> <li>Dendritos: Recebem sinais de outros neur\u00f4nios</li> <li>Soma (corpo celular): Integra e processa os sinais</li> <li>Ax\u00f4nio: Transmite o sinal processado</li> <li>Sinapses: Conex\u00f5es com outros neur\u00f4nios</li> </ul>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#processo-de-comunicacao-neural","title":"Processo de Comunica\u00e7\u00e3o Neural","text":"<ol> <li>Recep\u00e7\u00e3o: Dendritos captam neurotransmissores</li> <li>Integra\u00e7\u00e3o: Soma pondera e combina os sinais</li> <li>Limiar: Se o potencial excede um limiar, o neur\u00f4nio \"dispara\"</li> <li>Transmiss\u00e3o: Sinal el\u00e9trico percorre o ax\u00f4nio</li> <li>Libera\u00e7\u00e3o: Neurotransmissores s\u00e3o liberados nas sinapses</li> </ol>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#neuronio-artificial","title":"Neur\u00f4nio Artificial","text":""},{"location":"aulas/IA/lab06/redesneurais%20copy/#modelo-matematico","title":"Modelo Matem\u00e1tico","text":"<p>O neur\u00f4nio artificial \u00e9 uma abstra\u00e7\u00e3o matem\u00e1tica do neur\u00f4nio biol\u00f3gico:</p> <p></p> <pre><code>x\u2081 \u2500\u2500w\u2081\u2500\u2500\u2510\nx\u2082 \u2500\u2500w\u2082\u2500\u2500\u2524\n    ...  \u251c\u2500\u2192 \u03a3 \u2500\u2500\u2192 f(net) \u2500\u2500\u2192 y\nx\u2099 \u2500\u2500w\u2099\u2500\u2500\u2518\n    b \u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#equacoes-fundamentais","title":"Equa\u00e7\u00f5es fundamentais:","text":"<p>Net Input (Entrada l\u00edquida): <pre><code>net = \u03a3(w\u1d62 \u00d7 x\u1d62) + b = w\u2081x\u2081 + w\u2082x\u2082 + ... + w\u2099x\u2099 + b\n</code></pre></p> <p>Sa\u00edda: <pre><code>y = f(net)\n</code></pre></p> <p>Onde: - <code>x\u1d62</code>: Entradas do neur\u00f4nio - <code>w\u1d62</code>: Pesos sin\u00e1pticos - <code>b</code>: Bias (limiar) - <code>f</code>: Fun\u00e7\u00e3o de ativa\u00e7\u00e3o - <code>y</code>: Sa\u00edda do neur\u00f4nio</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#funcoes-de-ativacao-classicas","title":"Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o Cl\u00e1ssicas","text":""},{"location":"aulas/IA/lab06/redesneurais%20copy/#1-funcao-degrau-step-function","title":"1. Fun\u00e7\u00e3o Degrau (Step Function)","text":"<p><pre><code>f(x) = { 1, se x \u2265 0\n       { 0, se x &lt; 0\n</code></pre> - Uso: Perceptron cl\u00e1ssico - Caracter\u00edstica: Sa\u00edda bin\u00e1ria</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#2-funcao-sigmoide","title":"2. Fun\u00e7\u00e3o Sigm\u00f3ide","text":"<p><pre><code>f(x) = 1 / (1 + e^(-x))\n</code></pre> - Intervalo: (0, 1) - Caracter\u00edstica: Diferenci\u00e1vel, suave - Problema: Satura\u00e7\u00e3o dos gradientes</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#3-funcao-tangente-hiperbolica-tanh","title":"3. Fun\u00e7\u00e3o Tangente Hiperb\u00f3lica (tanh)","text":"<p><pre><code>f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n</code></pre> - Intervalo: (-1, 1) - Vantagem: Centrada em zero</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#4-funcao-relu-rectified-linear-unit","title":"4. Fun\u00e7\u00e3o ReLU (Rectified Linear Unit)","text":"<p><pre><code>f(x) = max(0, x)\n</code></pre> - Vantagem: Resolve o problema de gradientes - Uso: Redes profundas modernas</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#perceptron","title":"Perceptron","text":""},{"location":"aulas/IA/lab06/redesneurais%20copy/#conceito-e-historia","title":"Conceito e Hist\u00f3ria","text":"<p>O Perceptron, desenvolvido por Frank Rosenblatt em 1957, foi o primeiro algoritmo de aprendizado para redes neurais que garantia converg\u00eancia para problemas linearmente separ\u00e1veis.</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#arquitetura-do-perceptron","title":"Arquitetura do Perceptron","text":"<pre><code>Entrada \u2192 Pesos \u2192 Soma \u2192 Ativa\u00e7\u00e3o \u2192 Sa\u00edda\nx\u2081,x\u2082,...,x\u2099 \u2192 w\u2081,w\u2082,...,w\u2099 \u2192 \u03a3 \u2192 f \u2192 y\n</code></pre>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#algoritmo-de-treinamento","title":"Algoritmo de Treinamento","text":"<p>Pseudoc\u00f3digo: <pre><code>1. Inicializar pesos aleatoriamente\n2. Para cada \u00e9poca:\n   a. Para cada amostra (x, d):\n      - Calcular sa\u00edda: y = f(\u03a3w\u1d62x\u1d62 + b)\n      - Calcular erro: e = d - y\n      - Atualizar pesos: w\u1d62 = w\u1d62 + \u03b7 \u00d7 e \u00d7 x\u1d62\n      - Atualizar bias: b = b + \u03b7 \u00d7 e\n3. Repetir at\u00e9 converg\u00eancia\n</code></pre></p> <p>Par\u00e2metros: - <code>\u03b7</code> (eta): Taxa de aprendizado - <code>d</code>: Sa\u00edda desejada - <code>e</code>: Erro</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#teorema-da-convergencia","title":"Teorema da Converg\u00eancia","text":"<p>Teorema: Se os dados s\u00e3o linearmente separ\u00e1veis, o algoritmo Perceptron converge em um n\u00famero finito de itera\u00e7\u00f5es.</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#limitacoes-do-perceptron","title":"Limita\u00e7\u00f5es do Perceptron","text":"<ol> <li>Separabilidade linear: S\u00f3 resolve problemas linearmente separ\u00e1veis</li> <li>Problema XOR: N\u00e3o consegue resolver o XOR</li> <li>Fun\u00e7\u00e3o de ativa\u00e7\u00e3o: Limitado a fun\u00e7\u00f5es lineares por partes</li> </ol>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#multilayer-perceptron-mlp","title":"Multilayer Perceptron (MLP)","text":""},{"location":"aulas/IA/lab06/redesneurais%20copy/#superando-as-limitacoes","title":"Superando as Limita\u00e7\u00f5es","text":"<p>O MLP resolve as limita\u00e7\u00f5es do Perceptron atrav\u00e9s de:</p> <ol> <li>Camadas ocultas: Permitem n\u00e3o-linearidade</li> <li>M\u00faltiplas camadas: Aumentam poder expressivo</li> <li>Backpropagation: Algoritmo de treinamento eficiente</li> </ol>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#arquitetura-mlp","title":"Arquitetura MLP","text":"<pre><code>Camada de    Camada(s)      Camada de\nEntrada   \u2192   Oculta(s)   \u2192   Sa\u00edda\n\nx\u2081 \u2500\u2500\u2500\u2500\u2500\u2500\u2510   h\u2081 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   y\u2081\nx\u2082 \u2500\u2500\u2500\u2500\u2500\u2500\u2524   h\u2082 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   y\u2082\n  ...    \u251c\u2500\u2192  ... \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2192  ...\nx\u2099 \u2500\u2500\u2500\u2500\u2500\u2500\u2518   h\u2098 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   y\u2096\n</code></pre>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#teorema-da-aproximacao-universal","title":"Teorema da Aproxima\u00e7\u00e3o Universal","text":"<p>Teorema: Uma rede neural com uma \u00fanica camada oculta e um n\u00famero suficiente de neur\u00f4nios pode aproximar qualquer fun\u00e7\u00e3o cont\u00ednua com precis\u00e3o arbitr\u00e1ria.</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#regras-praticas-para-arquitetura","title":"Regras Pr\u00e1ticas para Arquitetura","text":"<p>Historicamente, quando redes neurais eram menores e o custo computacional mais alto, surgiram algumas heur\u00edsticas para estimar o tamanho inicial da camada oculta em redes totalmente conectadas (MLPs). Essas regras n\u00e3o s\u00e3o leis fixas, mas podem servir como ponto de partida:</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#numero-de-neuronios-na-camada-oculta","title":"N\u00famero de neur\u00f4nios na camada oculta:","text":"<ul> <li> <ol> <li>Regra dos \u2154: $$ \\text{neur\u00f4nios ocultos} \\approx \\frac{2}{3} \\times (\\text{neur\u00f4nios de entrada}) + \\text{neur\u00f4nios de sa\u00edda} $$</li> </ol> </li> </ul> <p>Ideia: reduzir a dimensionalidade da entrada mantendo espa\u00e7o para representar as sa\u00eddas.</p> <ul> <li> <ol> <li>M\u00e9dia geom\u00e9trica</li> </ol> </li> </ul> \\[ \\text{neur\u00f4nios ocultos} \\approx \\sqrt{\\text{entradas} \\times \\text{sa\u00eddas}} \\] <p>Ideia: buscar um equil\u00edbrio proporcional entre o tamanho da entrada e o tamanho da sa\u00edda.</p> <ul> <li> <ol> <li> <p>Experimenta\u00e7\u00e3o incremental (abordagem mais usada atualmente)</p> </li> <li> <p>Comece com uma rede pequena.  </p> </li> <li>Monitore as m\u00e9tricas de treinamento e valida\u00e7\u00e3o.  </li> <li>Aumente gradualmente a quantidade de neur\u00f4nios at\u00e9 atingir bom desempenho sem sobreajuste (overfitting).  </li> <li>Utilize t\u00e9cnicas de regulariza\u00e7\u00e3o (dropout, L2, batch normalization) para manter a generaliza\u00e7\u00e3o.</li> </ol> </li> </ul> <p>Tip</p> <p>Essas regras n\u00e3o consideram fatores como complexidade do problema, qualidade dos dados ou arquiteturas modernas (CNNs, RNNs, Transformers). </p> <p>Hoje, a pr\u00e1tica recomendada \u00e9 combinar um chute inicial com ajuste via valida\u00e7\u00e3o cruzada e ferramentas de busca de hiperpar\u00e2metros.</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#numero-de-camadas-ocultas","title":"N\u00famero de camadas ocultas:","text":"<p>O n\u00famero de camadas ocultas em uma rede neural influencia diretamente a capacidade de representa\u00e7\u00e3o do modelo. De forma geral, temos:</p> <ul> <li> <p>1 camada oculta:   Indicada para problemas que se tornam linearmente separ\u00e1veis ap\u00f3s uma transforma\u00e7\u00e3o n\u00e3o linear.   Exemplo: classifica\u00e7\u00e3o simples com fronteiras suaves.</p> </li> <li> <p>2 camadas ocultas:   Capaz de aproximar qualquer fun\u00e7\u00e3o cont\u00ednua arbitr\u00e1ria (Teorema da Aproxima\u00e7\u00e3o Universal).   \u00datil quando h\u00e1 rela\u00e7\u00f5es mais complexas entre entrada e sa\u00edda.</p> </li> <li> <p>3 ou mais camadas ocultas:   Necess\u00e1rias para representar fun\u00e7\u00f5es descont\u00ednuas ou padr\u00f5es muito complexos.   Base do Deep Learning, permitindo a extra\u00e7\u00e3o de caracter\u00edsticas em m\u00faltiplos n\u00edveis.</p> </li> </ul> <p>\ud83d\udca1 Observa\u00e7\u00e3o: Embora mais camadas aumentem a capacidade do modelo, tamb\u00e9m elevam o risco de overfitting e a necessidade de mais dados e regulariza\u00e7\u00e3o.</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#algoritmos-de-treinamento","title":"Algoritmos de Treinamento","text":""},{"location":"aulas/IA/lab06/redesneurais%20copy/#backpropagation","title":"Backpropagation","text":"<p>O algoritmo de retropropaga\u00e7\u00e3o \u00e9 o m\u00e9todo padr\u00e3o para treinar MLPs.</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#fases-do-algoritmo","title":"Fases do Algoritmo:","text":"<p>1. Forward Pass (Propaga\u00e7\u00e3o Direta): <pre><code># Para cada camada l\nfor l in range(1, L):\n    z[l] = W[l] @ a[l-1] + b[l]  # Linear combination\n    a[l] = activation(z[l])       # Activation function\n</code></pre></p> <p>2. Backward Pass (Retropropaga\u00e7\u00e3o): <pre><code># Calcular erro da sa\u00edda\ndelta[L] = (a[L] - y) * activation_derivative(z[L])\n\n# Propagar erro para tr\u00e1s\nfor l in range(L-1, 0, -1):\n    delta[l] = (W[l+1].T @ delta[l+1]) * activation_derivative(z[l])\n</code></pre></p> <p>3. Atualiza\u00e7\u00e3o dos Pesos: <pre><code># Para cada camada\nfor l in range(1, L):\n    W[l] -= learning_rate * (delta[l] @ a[l-1].T)\n    b[l] -= learning_rate * delta[l]\n</code></pre></p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#variacoes-do-gradient-descent","title":"Varia\u00e7\u00f5es do Gradient Descent","text":""},{"location":"aulas/IA/lab06/redesneurais%20copy/#1-batch-gradient-descent","title":"1. Batch Gradient Descent","text":"<ul> <li>Caracter\u00edstica: Usa todo o dataset por itera\u00e7\u00e3o</li> <li>Vantagem: Converg\u00eancia est\u00e1vel</li> <li>Desvantagem: Lento para grandes datasets</li> </ul>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#2-stochastic-gradient-descent-sgd","title":"2. Stochastic Gradient Descent (SGD)","text":"<ul> <li>Caracter\u00edstica: Uma amostra por vez</li> <li>Vantagem: R\u00e1pido, pode escapar de m\u00ednimos locais</li> <li>Desvantagem: Converg\u00eancia ruidosa</li> </ul>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#3-mini-batch-gradient-descent","title":"3. Mini-batch Gradient Descent","text":"<ul> <li>Caracter\u00edstica: Pequenos grupos de amostras</li> <li>Vantagem: Balanceia velocidade e estabilidade</li> <li>Uso: Mais comum na pr\u00e1tica</li> </ul>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#funcoes-de-ativacao-modernas","title":"Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o Modernas","text":""},{"location":"aulas/IA/lab06/redesneurais%20copy/#relu-e-variacoes","title":"ReLU e Varia\u00e7\u00f5es","text":""},{"location":"aulas/IA/lab06/redesneurais%20copy/#relu-rectified-linear-unit","title":"ReLU (Rectified Linear Unit)","text":"<p><pre><code>def relu(x):\n    return np.maximum(0, x)\n</code></pre> Vantagens: - Computacionalmente eficiente - Resolve gradientes que desvanecem - Induz esparsidade</p> <p>Desvantagens: - Neur\u00f4nios podem \"morrer\" - N\u00e3o diferenci\u00e1vel em zero</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#leaky-relu","title":"Leaky ReLU","text":"<p><pre><code>def leaky_relu(x, alpha=0.01):\n    return np.where(x &gt; 0, x, alpha * x)\n</code></pre> Vantagem: Evita neur\u00f4nios mortos</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#elu-exponential-linear-unit","title":"ELU (Exponential Linear Unit)","text":"<p><pre><code>def elu(x, alpha=1.0):\n    return np.where(x &gt; 0, x, alpha * (np.exp(x) - 1))\n</code></pre> Vantagem: Suave em toda parte</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#funcoes-de-ativacao-para-saida","title":"Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o para Sa\u00edda","text":""},{"location":"aulas/IA/lab06/redesneurais%20copy/#softmax-classificacao-multiclasse","title":"Softmax (Classifica\u00e7\u00e3o Multiclasse)","text":"<pre><code>def softmax(x):\n    exp_x = np.exp(x - np.max(x))\n    return exp_x / np.sum(exp_x)\n</code></pre>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#linear-regressao","title":"Linear (Regress\u00e3o)","text":"<pre><code>def linear(x):\n    return x\n</code></pre>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#regularizacao-e-otimizacao","title":"Regulariza\u00e7\u00e3o e Otimiza\u00e7\u00e3o","text":""},{"location":"aulas/IA/lab06/redesneurais%20copy/#tecnicas-de-regularizacao","title":"T\u00e9cnicas de Regulariza\u00e7\u00e3o","text":""},{"location":"aulas/IA/lab06/redesneurais%20copy/#1-l1-regularization-lasso","title":"1. L1 Regularization (Lasso)","text":"<p><pre><code>Loss = MSE + \u03bb\u2081 \u00d7 \u03a3|w\u1d62|\n</code></pre> - Efeito: Induz esparsidade</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#2-l2-regularization-ridge","title":"2. L2 Regularization (Ridge)","text":"<p><pre><code>Loss = MSE + \u03bb\u2082 \u00d7 \u03a3w\u1d62\u00b2\n</code></pre> - Efeito: Reduz magnitude dos pesos</p>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#3-dropout","title":"3. Dropout","text":"<ul> <li>Mecanismo: Desativa neur\u00f4nios aleatoriamente durante treinamento</li> <li>Vantagem: Reduz overfitting</li> </ul>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#4-early-stopping","title":"4. Early Stopping","text":"<ul> <li>Mecanismo: Para treinamento quando valida\u00e7\u00e3o para de melhorar</li> <li>Implementa\u00e7\u00e3o: Monitora loss de valida\u00e7\u00e3o</li> </ul>"},{"location":"aulas/IA/lab06/redesneurais%20copy/#otimizadores-avancados","title":"Otimizadores Avan\u00e7ados","text":""},{"location":"aulas/IA/lab06/redesneurais%20copy/#adam-adaptive-moment-estimation","title":"Adam (Adaptive Moment Estimation)","text":"<pre><code># Par\u00e2metros adaptativos\nm = \u03b2\u2081 \u00d7 m + (1 - \u03b2\u2081) \u00d7 gradient\nv = \u03b2\u2082 \u00d7 v + (1 - \u03b2\u2082) \u00d7 gradient\u00b2\n\n# Corre\u00e7\u00e3o de vi\u00e9s\nm_hat = m / (1 - \u03b2\u2081\u1d57)\nv_hat = v / (1 - \u03b2\u2082\u1d57)\n\n# Atualiza\u00e7\u00e3o\nweights -= learning_rate \u00d7 m_hat / (\u221av_hat + \u03b5)\n</code></pre>"},{"location":"aulas/IA/lab06/redesneurais/","title":"Redes Neurais Artificiais","text":""},{"location":"aulas/IA/lab06/redesneurais/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>As Redes Neurais Artificiais (RNA) s\u00e3o modelos computacionais inspirados no funcionamento do sistema nervoso biol\u00f3gico. Elas representam uma das abordagens mais poderosas e vers\u00e1teis do aprendizado de m\u00e1quina, capazes de:</p> <ul> <li>Aprender padr\u00f5es n\u00e3o lineares e intera\u00e7\u00f5es entre vari\u00e1veis dif\u00edceis de modelar com t\u00e9cnicas lineares</li> <li>Aproximar fun\u00e7\u00f5es n\u00e3o-lineares arbitr\u00e1rias (quando bem dimensionadas e treinadas), gra\u00e7as ao seu poder de representa\u00e7\u00e3o</li> <li>Resolver problemas de classifica\u00e7\u00e3o, regress\u00e3o e clustering</li> <li>Processar m\u00faltiplas modalidades: como dados de dados de imagens, texto e s\u00e9ries temporais...</li> </ul>"},{"location":"aulas/IA/lab06/redesneurais/#quando-considerar-rnas-regra-pratica","title":"Quando considerar RNAs (regra pr\u00e1tica):","text":"<ul> <li>Rela\u00e7\u00f5es claramente \"n\u00e3o lineares\" entre entradas e sa\u00eddas.</li> <li>\"Muitos atributos\" (alta dimensionalidade) e grande volume de dados.</li> <li>Necessidade de aprender representa\u00e7\u00f5es (extra\u00e7\u00e3o autom\u00e1tica de caracter\u00edsticas).</li> </ul>"},{"location":"aulas/IA/lab06/redesneurais/#quando-suspeitar-que-rnas-nao-sao-a-melhor-1a-escolha","title":"Quando suspeitar que RNAs n\u00e3o s\u00e3o a melhor 1\u00aa escolha:","text":"<ul> <li>Poucos dados rotulados e problema simples \u2192 tente modelos lineares, kNN ou \u00e1rvores antes.</li> <li>Forte necessidade de interpretabilidade imediata \u2192 prefira modelos explic\u00e1veis (\u00e1rvores, regress\u00f5es com regulariza\u00e7\u00e3o, regras).</li> </ul>"},{"location":"aulas/IA/lab06/redesneurais/#intuicao-basica","title":"Intui\u00e7\u00e3o b\u00e1sica","text":""},{"location":"aulas/IA/lab06/redesneurais/#o-neuronio-biologico","title":"O Neur\u00f4nio Biol\u00f3gico","text":"<p>O neur\u00f4nio \u00e9 a unidade fundamental do sistema nervoso, composto por:</p> <p></p> <pre><code>Dendritos \u2192 Soma \u2192 Ax\u00f4nio \u2192 Sinapses\n    \u2191        \u2191       \u2191        \u2191\n  Entrada  Processamento  Transmiss\u00e3o  Sa\u00edda\n</code></pre>"},{"location":"aulas/IA/lab06/redesneurais/#componentes-principais","title":"Componentes principais:","text":"<ul> <li>Dendritos: Recebem sinais de outros neur\u00f4nios</li> <li>Soma (corpo celular): Integra e processa os sinais</li> <li>Ax\u00f4nio: Transmite o sinal processado</li> <li>Sinapses: Conex\u00f5es com outros neur\u00f4nios</li> </ul>"},{"location":"aulas/IA/lab06/redesneurais/#processo-de-comunicacao-neural","title":"Processo de Comunica\u00e7\u00e3o Neural","text":"<ol> <li>Recep\u00e7\u00e3o: Dendritos captam neurotransmissores</li> <li>Integra\u00e7\u00e3o: Soma pondera e combina os sinais</li> <li>Limiar: Se o potencial excede um limiar, o neur\u00f4nio \"dispara\"</li> <li>Transmiss\u00e3o: Sinal el\u00e9trico percorre o ax\u00f4nio</li> <li>Libera\u00e7\u00e3o: Neurotransmissores s\u00e3o liberados nas sinapses</li> </ol>"},{"location":"aulas/IA/lab06/redesneurais/#neuronio-artificial","title":"Neur\u00f4nio Artificial","text":""},{"location":"aulas/IA/lab06/redesneurais/#modelo-matematico","title":"Modelo Matem\u00e1tico","text":"<p>O neur\u00f4nio artificial \u00e9 uma abstra\u00e7\u00e3o matem\u00e1tica do neur\u00f4nio biol\u00f3gico:</p> <p></p> <pre><code>x\u2081 \u2500\u2500w\u2081\u2500\u2500\u2510\nx\u2082 \u2500\u2500w\u2082\u2500\u2500\u2524\n    ...  \u251c\u2500\u2192 \u03a3 \u2500\u2500\u2192 f(net) \u2500\u2500\u2192 y\nx\u2099 \u2500\u2500w\u2099\u2500\u2500\u2518\n    b \u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aulas/IA/lab06/redesneurais/#equacoes-fundamentais","title":"Equa\u00e7\u00f5es fundamentais:","text":"<p>Net Input (Entrada l\u00edquida): <pre><code>net = \u03a3(w\u1d62 \u00d7 x\u1d62) + b = w\u2081x\u2081 + w\u2082x\u2082 + ... + w\u2099x\u2099 + b\n</code></pre></p> <p>Sa\u00edda: <pre><code>y = f(net)\n</code></pre></p> <p>Onde: - <code>x\u1d62</code>: Entradas do neur\u00f4nio - <code>w\u1d62</code>: Pesos sin\u00e1pticos - <code>b</code>: Bias (limiar) - <code>f</code>: Fun\u00e7\u00e3o de ativa\u00e7\u00e3o - <code>y</code>: Sa\u00edda do neur\u00f4nio</p>"},{"location":"aulas/IA/lab06/redesneurais/#funcoes-de-ativacao-classicas","title":"Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o Cl\u00e1ssicas","text":""},{"location":"aulas/IA/lab06/redesneurais/#1-funcao-degrau-step-function","title":"1. Fun\u00e7\u00e3o Degrau (Step Function)","text":"<p><pre><code>f(x) = { 1, se x \u2265 0\n       { 0, se x &lt; 0\n</code></pre> - Uso: Perceptron cl\u00e1ssico - Caracter\u00edstica: Sa\u00edda bin\u00e1ria</p>"},{"location":"aulas/IA/lab06/redesneurais/#2-funcao-sigmoide","title":"2. Fun\u00e7\u00e3o Sigm\u00f3ide","text":"<p><pre><code>f(x) = 1 / (1 + e^(-x))\n</code></pre> - Intervalo: (0, 1) - Caracter\u00edstica: Diferenci\u00e1vel, suave - Problema: Satura\u00e7\u00e3o dos gradientes</p>"},{"location":"aulas/IA/lab06/redesneurais/#3-funcao-tangente-hiperbolica-tanh","title":"3. Fun\u00e7\u00e3o Tangente Hiperb\u00f3lica (tanh)","text":"<p><pre><code>f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n</code></pre> - Intervalo: (-1, 1) - Vantagem: Centrada em zero</p>"},{"location":"aulas/IA/lab06/redesneurais/#4-funcao-relu-rectified-linear-unit","title":"4. Fun\u00e7\u00e3o ReLU (Rectified Linear Unit)","text":"<p><pre><code>f(x) = max(0, x)\n</code></pre> - Vantagem: Resolve o problema de gradientes - Uso: Redes profundas modernas</p>"},{"location":"aulas/IA/lab06/redesneurais/#perceptron","title":"Perceptron","text":""},{"location":"aulas/IA/lab06/redesneurais/#conceito-e-historia","title":"Conceito e Hist\u00f3ria","text":"<p>O Perceptron, desenvolvido por Frank Rosenblatt em 1957, foi o primeiro algoritmo de aprendizado para redes neurais que garantia converg\u00eancia para problemas linearmente separ\u00e1veis.</p>"},{"location":"aulas/IA/lab06/redesneurais/#arquitetura-do-perceptron","title":"Arquitetura do Perceptron","text":"<pre><code>Entrada \u2192 Pesos \u2192 Soma \u2192 Ativa\u00e7\u00e3o \u2192 Sa\u00edda\nx\u2081,x\u2082,...,x\u2099 \u2192 w\u2081,w\u2082,...,w\u2099 \u2192 \u03a3 \u2192 f \u2192 y\n</code></pre>"},{"location":"aulas/IA/lab06/redesneurais/#algoritmo-de-treinamento","title":"Algoritmo de Treinamento","text":"<p>Pseudoc\u00f3digo: <pre><code>1. Inicializar pesos aleatoriamente\n2. Para cada \u00e9poca:\n   a. Para cada amostra (x, d):\n      - Calcular sa\u00edda: y = f(\u03a3w\u1d62x\u1d62 + b)\n      - Calcular erro: e = d - y\n      - Atualizar pesos: w\u1d62 = w\u1d62 + \u03b7 \u00d7 e \u00d7 x\u1d62\n      - Atualizar bias: b = b + \u03b7 \u00d7 e\n3. Repetir at\u00e9 converg\u00eancia\n</code></pre></p> <p>Par\u00e2metros: - <code>\u03b7</code> (eta): Taxa de aprendizado - <code>d</code>: Sa\u00edda desejada - <code>e</code>: Erro</p>"},{"location":"aulas/IA/lab06/redesneurais/#teorema-da-convergencia","title":"Teorema da Converg\u00eancia","text":"<p>Teorema: Se os dados s\u00e3o linearmente separ\u00e1veis, o algoritmo Perceptron converge em um n\u00famero finito de itera\u00e7\u00f5es.</p>"},{"location":"aulas/IA/lab06/redesneurais/#limitacoes-do-perceptron","title":"Limita\u00e7\u00f5es do Perceptron","text":"<ol> <li>Separabilidade linear: S\u00f3 resolve problemas linearmente separ\u00e1veis</li> <li>Problema XOR: N\u00e3o consegue resolver o XOR</li> <li>Fun\u00e7\u00e3o de ativa\u00e7\u00e3o: Limitado a fun\u00e7\u00f5es lineares por partes</li> </ol>"},{"location":"aulas/IA/lab06/redesneurais/#multilayer-perceptron-mlp","title":"Multilayer Perceptron (MLP)","text":""},{"location":"aulas/IA/lab06/redesneurais/#superando-as-limitacoes","title":"Superando as Limita\u00e7\u00f5es","text":"<p>O MLP resolve as limita\u00e7\u00f5es do Perceptron atrav\u00e9s de:</p> <ol> <li>Camadas ocultas: Permitem n\u00e3o-linearidade</li> <li>M\u00faltiplas camadas: Aumentam poder expressivo</li> <li>Backpropagation: Algoritmo de treinamento eficiente</li> </ol>"},{"location":"aulas/IA/lab06/redesneurais/#arquitetura-mlp","title":"Arquitetura MLP","text":"<pre><code>Camada de    Camada(s)      Camada de\nEntrada   \u2192   Oculta(s)   \u2192   Sa\u00edda\n\nx\u2081 \u2500\u2500\u2500\u2500\u2500\u2500\u2510   h\u2081 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   y\u2081\nx\u2082 \u2500\u2500\u2500\u2500\u2500\u2500\u2524   h\u2082 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   y\u2082\n  ...    \u251c\u2500\u2192  ... \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2192  ...\nx\u2099 \u2500\u2500\u2500\u2500\u2500\u2500\u2518   h\u2098 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   y\u2096\n</code></pre>"},{"location":"aulas/IA/lab06/redesneurais/#teorema-da-aproximacao-universal","title":"Teorema da Aproxima\u00e7\u00e3o Universal","text":"<p>Teorema: Uma rede neural com uma \u00fanica camada oculta e um n\u00famero suficiente de neur\u00f4nios pode aproximar qualquer fun\u00e7\u00e3o cont\u00ednua com precis\u00e3o arbitr\u00e1ria.</p>"},{"location":"aulas/IA/lab06/redesneurais/#regras-praticas-para-arquitetura","title":"Regras Pr\u00e1ticas para Arquitetura","text":"<p>Historicamente, quando redes neurais eram menores e o custo computacional mais alto, surgiram algumas heur\u00edsticas para estimar o tamanho inicial da camada oculta em redes totalmente conectadas (MLPs). Essas regras n\u00e3o s\u00e3o leis fixas, mas podem servir como ponto de partida:</p>"},{"location":"aulas/IA/lab06/redesneurais/#numero-de-neuronios-na-camada-oculta","title":"N\u00famero de neur\u00f4nios na camada oculta:","text":"<ul> <li> <ol> <li>Regra dos \u2154: $$ \\text{neur\u00f4nios ocultos} \\approx \\frac{2}{3} \\times (\\text{neur\u00f4nios de entrada}) + \\text{neur\u00f4nios de sa\u00edda} $$</li> </ol> </li> </ul> <p>Ideia: reduzir a dimensionalidade da entrada mantendo espa\u00e7o para representar as sa\u00eddas.</p> <ul> <li> <ol> <li>M\u00e9dia geom\u00e9trica</li> </ol> </li> </ul> \\[ \\text{neur\u00f4nios ocultos} \\approx \\sqrt{\\text{entradas} \\times \\text{sa\u00eddas}} \\] <p>Ideia: buscar um equil\u00edbrio proporcional entre o tamanho da entrada e o tamanho da sa\u00edda.</p> <ul> <li> <ol> <li> <p>Experimenta\u00e7\u00e3o incremental (abordagem mais usada atualmente)</p> </li> <li> <p>Comece com uma rede pequena.  </p> </li> <li>Monitore as m\u00e9tricas de treinamento e valida\u00e7\u00e3o.  </li> <li>Aumente gradualmente a quantidade de neur\u00f4nios at\u00e9 atingir bom desempenho sem sobreajuste (overfitting).  </li> <li>Utilize t\u00e9cnicas de regulariza\u00e7\u00e3o (dropout, L2, batch normalization) para manter a generaliza\u00e7\u00e3o.</li> </ol> </li> </ul> <p>Tip</p> <p>Essas regras n\u00e3o consideram fatores como complexidade do problema, qualidade dos dados ou arquiteturas modernas (CNNs, RNNs, Transformers). </p> <p>Hoje, a pr\u00e1tica recomendada \u00e9 combinar um chute inicial com ajuste via valida\u00e7\u00e3o cruzada e ferramentas de busca de hiperpar\u00e2metros.</p>"},{"location":"aulas/IA/lab06/redesneurais/#numero-de-camadas-ocultas","title":"N\u00famero de camadas ocultas:","text":"<p>O n\u00famero de camadas ocultas em uma rede neural influencia diretamente a capacidade de representa\u00e7\u00e3o do modelo. De forma geral, temos:</p> <ul> <li> <p>1 camada oculta:   Indicada para problemas que se tornam linearmente separ\u00e1veis ap\u00f3s uma transforma\u00e7\u00e3o n\u00e3o linear.   Exemplo: classifica\u00e7\u00e3o simples com fronteiras suaves.</p> </li> <li> <p>2 camadas ocultas:   Capaz de aproximar qualquer fun\u00e7\u00e3o cont\u00ednua arbitr\u00e1ria (Teorema da Aproxima\u00e7\u00e3o Universal).   \u00datil quando h\u00e1 rela\u00e7\u00f5es mais complexas entre entrada e sa\u00edda.</p> </li> <li> <p>3 ou mais camadas ocultas:   Necess\u00e1rias para representar fun\u00e7\u00f5es descont\u00ednuas ou padr\u00f5es muito complexos.   Base do Deep Learning, permitindo a extra\u00e7\u00e3o de caracter\u00edsticas em m\u00faltiplos n\u00edveis.</p> </li> </ul> <p>\ud83d\udca1 Observa\u00e7\u00e3o: Embora mais camadas aumentem a capacidade do modelo, tamb\u00e9m elevam o risco de overfitting e a necessidade de mais dados e regulariza\u00e7\u00e3o.</p>"},{"location":"aulas/IA/lab06/rna/","title":"Rna","text":"<p>A rede Perceptron possui um algoritmo de aprendizado supervisionado que consegue definir um classificador que encontra a superf\u00edcie de separa\u00e7\u00e3o entre quaisquer duas classes linearmente separ\u00e1veis</p> In\u00a0[1]: Copied! <pre>import pandas as pd\n\n\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\nheader = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\ndf = pd.read_csv(url, header=None, names=header)\n</pre> import pandas as pd   url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" header = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'] df = pd.read_csv(url, header=None, names=header) In\u00a0[2]: Copied! <pre># Selecionando um sub-dataframe com os campos petal_length e petal_width, \n# e outro com a vari\u00e1vel de classes\nentradas = df[['petal_length', 'petal_width']]\nclasses = df['species']\nprint(f\"Formato das tabelas de dados {entradas.shape} e classes {classes.shape}\")\n</pre> # Selecionando um sub-dataframe com os campos petal_length e petal_width,  # e outro com a vari\u00e1vel de classes entradas = df[['petal_length', 'petal_width']] classes = df['species'] print(f\"Formato das tabelas de dados {entradas.shape} e classes {classes.shape}\") <pre>Formato das tabelas de dados (150, 2) e classes (150,)\n</pre> In\u00a0[3]: Copied! <pre># Separamos 20 % para o teste\nfrom sklearn.model_selection import train_test_split\nentradas_treino, entradas_teste, classes_treino, classes_teste = train_test_split(entradas, classes, test_size=0.2)\nprint(f\"Formato das tabelas de dados de treino {entradas_treino.shape} e teste {entradas_teste.shape}\")\n</pre> # Separamos 20 % para o teste from sklearn.model_selection import train_test_split entradas_treino, entradas_teste, classes_treino, classes_teste = train_test_split(entradas, classes, test_size=0.2) print(f\"Formato das tabelas de dados de treino {entradas_treino.shape} e teste {entradas_teste.shape}\") <pre>Formato das tabelas de dados de treino (120, 2) e teste (30, 2)\n</pre> In\u00a0[4]: Copied! <pre>from sklearn.linear_model import Perceptron\n\n\nmodelo = Perceptron(tol=1.7)\nmodelo.fit(entradas_treino, classes_treino)\n</pre> from sklearn.linear_model import Perceptron   modelo = Perceptron(tol=1.7) modelo.fit(entradas_treino, classes_treino) Out[4]: <pre>Perceptron(tol=1.7)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Perceptron<pre>Perceptron(tol=1.7)</pre> In\u00a0[5]: Copied! <pre>classes_encontradas = modelo.predict(entradas_teste)\n</pre> classes_encontradas = modelo.predict(entradas_teste) In\u00a0[6]: Copied! <pre>from sklearn.metrics import accuracy_score\n\nclasses_encontradas_train = modelo.predict(entradas_treino)\nprint(\"Acerto m\u00e9dio de classifica\u00e7\u00e3o treino: \",accuracy_score(classes_encontradas_train, classes_treino))\n\nclasses_encontradas = modelo.predict(entradas_teste)\nprint(\"Acerto m\u00e9dio de classifica\u00e7\u00e3o teste: \",accuracy_score(classes_encontradas, classes_teste))\n</pre> from sklearn.metrics import accuracy_score  classes_encontradas_train = modelo.predict(entradas_treino) print(\"Acerto m\u00e9dio de classifica\u00e7\u00e3o treino: \",accuracy_score(classes_encontradas_train, classes_treino))  classes_encontradas = modelo.predict(entradas_teste) print(\"Acerto m\u00e9dio de classifica\u00e7\u00e3o teste: \",accuracy_score(classes_encontradas, classes_teste)) <pre>Acerto m\u00e9dio de classifica\u00e7\u00e3o treino:  0.675\nAcerto m\u00e9dio de classifica\u00e7\u00e3o teste:  0.6333333333333333\n</pre> In\u00a0[7]: Copied! <pre>from sklearn.metrics import classification_report\n\nprint(classification_report(classes_encontradas, classes_teste))\n</pre>  from sklearn.metrics import classification_report  print(classification_report(classes_encontradas, classes_teste)) <pre>                 precision    recall  f1-score   support\n\n    Iris-setosa       1.00      0.77      0.87        13\nIris-versicolor       0.00      0.00      0.00         0\n Iris-virginica       1.00      0.53      0.69        17\n\n       accuracy                           0.63        30\n      macro avg       0.67      0.43      0.52        30\n   weighted avg       1.00      0.63      0.77        30\n\n</pre> <pre>/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n</pre> In\u00a0[8]: Copied! <pre>from sklearn.neural_network import MLPClassifier\n\ncamadas = [4,3]\nepocas = 1000\nbatch_size = 10\nativacao = 'relu' # Escolha dentre 'logistic', 'tanh' ou 'relu'\n\nmodelo = MLPClassifier(hidden_layer_sizes=camadas,\n                    batch_size=batch_size,\n                    activation=ativacao,\n                    max_iter=epocas)\n</pre> from sklearn.neural_network import MLPClassifier  camadas = [4,3] epocas = 1000 batch_size = 10 ativacao = 'relu' # Escolha dentre 'logistic', 'tanh' ou 'relu'  modelo = MLPClassifier(hidden_layer_sizes=camadas,                     batch_size=batch_size,                     activation=ativacao,                     max_iter=epocas) In\u00a0[9]: Copied! <pre>modelo.fit(entradas_treino, classes_treino)\n</pre>  modelo.fit(entradas_treino, classes_treino) Out[9]: <pre>MLPClassifier(batch_size=10, hidden_layer_sizes=[4, 3], max_iter=1000)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MLPClassifier<pre>MLPClassifier(batch_size=10, hidden_layer_sizes=[4, 3], max_iter=1000)</pre> In\u00a0[10]: Copied! <pre>from sklearn.metrics import accuracy_score\n\n\nclasses_encontradas_train = modelo.predict(entradas_treino)\nprint(\"Acerto m\u00e9dio de classifica\u00e7\u00e3o treino: \",accuracy_score(classes_encontradas_train, classes_treino))\n\nclasses_encontradas = modelo.predict(entradas_teste)\nprint(\"Acerto m\u00e9dio de classifica\u00e7\u00e3o teste: \",accuracy_score(classes_encontradas, classes_teste))\n</pre> from sklearn.metrics import accuracy_score   classes_encontradas_train = modelo.predict(entradas_treino) print(\"Acerto m\u00e9dio de classifica\u00e7\u00e3o treino: \",accuracy_score(classes_encontradas_train, classes_treino))  classes_encontradas = modelo.predict(entradas_teste) print(\"Acerto m\u00e9dio de classifica\u00e7\u00e3o teste: \",accuracy_score(classes_encontradas, classes_teste))  <pre>Acerto m\u00e9dio de classifica\u00e7\u00e3o treino:  0.9666666666666667\nAcerto m\u00e9dio de classifica\u00e7\u00e3o teste:  0.9666666666666667\n</pre> In\u00a0[11]: Copied! <pre>from sklearn.metrics import classification_report\n\nprint(classification_report(classes_encontradas, classes_teste))\n</pre> from sklearn.metrics import classification_report  print(classification_report(classes_encontradas, classes_teste)) <pre>                 precision    recall  f1-score   support\n\n    Iris-setosa       1.00      1.00      1.00        10\nIris-versicolor       1.00      0.92      0.96        12\n Iris-virginica       0.89      1.00      0.94         8\n\n       accuracy                           0.97        30\n      macro avg       0.96      0.97      0.97        30\n   weighted avg       0.97      0.97      0.97        30\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>## Seu c\u00f3digo aqui.......\n</pre> ## Seu c\u00f3digo aqui......."},{"location":"aulas/IA/lab06/rna/#2-aprendizagem-de-maquina","title":"2. Aprendizagem de m\u00e1quina\u00b6","text":""},{"location":"aulas/IA/lab06/rna/#objetivos","title":"Objetivos\u00b6","text":"<ul> <li>Conhecer uma intui\u00e7\u00e3o sobre Redes Neurais Artificiais RNA</li> <li>Praticar os algoritmos Perceptron e multilayer Perceptron (MLP)</li> </ul>"},{"location":"aulas/IA/lab06/rna/#redes-neurais-artificiais","title":"Redes Neurais Artificiais\u00b6","text":"<p>As redes neurais s\u00e3o modelos computacionais inspirados pelo sistema nervoso de um animal capazes de realizar o aprendizado de m\u00e1quina bem como o reconhecimento de padr\u00f5es.</p> <p>Tais modelos s\u00e3o muitas vezes utilizados para a tarefa de classifica\u00e7\u00e3o de padr\u00f5es, podendo gerar classificadores com caracter\u00edsticas variadas.</p> <p>As redes neurais artificiais possuem em comum o fato de serem constitu\u00eddas por neur\u00f4nios que se conectam entre si atrav\u00e9s de atrav\u00e9s de sinapses. A rede neural mais conhecida s\u00e3o as baseadas em Perceptron multicamada (MLP) embora existam outras redes como rede de Kohonem, as redes de base radial e a rede de Hopfield.</p>"},{"location":"aulas/IA/lab06/rna/#os-principais-componentes-dos-neuronios-sao","title":"Os principais componentes dos neur\u00f4nios s\u00e3o:\u00b6","text":"<ul> <li>Os <code>dendritos</code>, que t\u00eam por fun\u00e7\u00e3o receber os est\u00edmulos transmitidos pelos outros neur\u00f4nios;</li> <li>O <code>corpo</code> de neur\u00f4nio, tamb\u00e9m chamado de soma, que \u00e9 respons\u00e1vel por coletar e combinar informa\u00e7\u00f5es vindas de outros neur\u00f4nios;</li> <li>O <code>ax\u00f4nio</code>, que \u00e9 constitu\u00eddo de uma fibra tubular que pode alcan\u00e7ar at\u00e9 alguns metros, e \u00e9 respons\u00e1vel por transmitir os est\u00edmulos para outras c\u00e9lulas.</li> </ul>"},{"location":"aulas/IA/lab06/rna/#perceptron","title":"Perceptron\u00b6","text":"<p>O classificador Perceptron foi o primeiro classificador baseado em redes neurais que empregou uma regra de aprendizado capaz de garantir a correta separa\u00e7\u00e3o de classes linearmente separ\u00e1veis.</p> <p>No in\u00edcio do treinamento, os pesos dos neur\u00f4nios recebem valores aleat\u00f3rios. Ent\u00e3o, para cada amostra de treinamento com erro de classifica\u00e7\u00e3o, os pesos dos neur\u00f4nios s\u00e3o ajustados de modo a tentar corrigir a classe.</p> <p>Ap\u00f3s o treinamento, cada neur\u00f4nio na camada de sa\u00edda testa a pertin\u00eancia da amostra a uma classe. No caso de mais de um neur\u00f4nio fornecer resposta positiva a amostra, a classe correspondente ao neur\u00f4nio de maior resposta vence.</p>"},{"location":"aulas/IA/lab06/rna/#instanciar-o-classificador-e-treina-lo-com-as-amostras-de-treinamento","title":"Instanciar o classificador e trein\u00e1-lo com as amostras de treinamento\u00b6","text":"<p>https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html</p>"},{"location":"aulas/IA/lab06/rna/#analise-f1-score","title":"An\u00e1lise F1-Score\u00b6","text":"<p>A pontua\u00e7\u00e3o F1 pode ser interpretada como uma m\u00e9dia ponderada da precision e recall.</p> <ul> <li>Melhor valor = 1</li> <li>Pior valor = 0</li> </ul> <p>A contribui\u00e7\u00e3o relativa de precision e recall para a pontua\u00e7\u00e3o F1 s\u00e3o iguais. A f\u00f3rmula para a pontua\u00e7\u00e3o F1 \u00e9:</p> <p>F1 = 2 * (precision * recall) / (precision + recall)</p>"},{"location":"aulas/IA/lab06/rna/#multilayer-perceptron-mlp","title":"Multilayer Perceptron (MLP)\u00b6","text":"<ul> <li>O acr\u00e9scimo de uma nova camada de neur\u00f4nios, denominada camada oculta, permite criar superf\u00edcies de separa\u00e7\u00e3o n\u00e3o lineares, permitindo a classifica\u00e7\u00e3o de classes n\u00e3o-linearmente separ\u00e1veis</li> <li>A rede MLP \u00e9 considerada uma rede do tipo feed-forward, j\u00e1 que as sa\u00eddas dos neur\u00f4nios das camadas posteriores dependem apenas dos neur\u00f4nios das camadas anteriores</li> <li>Em uma rede MLP, n\u00e3o h\u00e1 regra para o n\u00famero de neur\u00f4nios a ser usado na camada oculta, e nem h\u00e1 limites para o n\u00famero de camadas ocultas a serem usadas</li> <li>Aparentemente, um bom chute inicial \u00e9 considerar o dobro de neur\u00f4nios na camada oculta com rela\u00e7\u00e3o ao tamanho da entrada</li> <li>\u00c9 conhecido que com uma \u00fanica camada oculta com um n\u00famero suficientemente grande de n\u00f3s \u00e9 poss\u00edvel representar qualquer fun\u00e7\u00e3o cont\u00ednua, e por isso essa estrutura \u00e9 conhecida como aproximador universal</li> </ul> <p>https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html</p>"},{"location":"aulas/IA/lab06/rna/#desafio1","title":"Desafio1\u00b6","text":"<p>Escolha uma dos exemplos dados em sala de aula e implemente um MLP com pelo menos 10 neuronios na camada escondida:</p> <p>OBS: S\u00f3 n\u00e3o vale o data da Iris, pois acabamos de usar...</p>"},{"location":"aulas/IA/lab07/","title":"Deep Learning e Vis\u00e3o Computacional usando TensorFlow","text":"<p>Este tutorial \u00e9 um guia r\u00e1pido de consulta para o mundo do deep learning. </p> <p>Aqui voc\u00ea encontra por meio de exemplos um pouco de teoria para comprees\u00e3o e aplica\u00e7\u00e3o dos conceitos.</p>"},{"location":"aulas/IA/lab07/#objetivos","title":"Objetivos","text":"<ol> <li>Introdu\u00e7\u00e3o</li> <li>Configura\u00e7\u00e3o e Instala\u00e7\u00e3o</li> <li>Entendendo os Conceitos B\u00e1sicos</li> <li>Perceptron</li> <li>Perceptron de M\u00faltiplas Camadas (MLP)</li> <li>Retropropaga\u00e7\u00e3o</li> <li>Otimizadores</li> <li>Redes Neurais Convolucionais (CNN)</li> <li>Implementando uma MLP Simples</li> <li>Treinando o Modelo</li> <li>Avalia\u00e7\u00e3o e Conclus\u00e3o</li> </ol>"},{"location":"aulas/IA/lab07/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Deep learning \u00e9 um subconjunto de aprendizado de m\u00e1quina onde redes neurais artificiais, algoritmos inspirados no c\u00e9rebro humano, aprendem a partir de grandes quantidades de dados. Da mesma forma, vis\u00e3o computacional \u00e9 um campo da intelig\u00eancia artificial que treina computadores para interpretar e compreender o mundo visual.</p>"},{"location":"aulas/IA/lab07/#configuracao-e-instalacao","title":"Configura\u00e7\u00e3o e Instala\u00e7\u00e3o","text":"<p>Primeiro, certifique-se de que Python e TensorFlow est\u00e3o instalados em seu sistema. Voc\u00ea pode instalar o TensorFlow com o seguinte comando:</p> <pre><code>pip install tensorflow\n</code></pre>"},{"location":"aulas/IA/lab07/#entendendo-os-conceitos-basicos","title":"Entendendo os Conceitos B\u00e1sicos","text":""},{"location":"aulas/IA/lab07/#perceptron","title":"Perceptron","text":"<p>O Perceptron \u00e9 um modelo de rede neural simples, geralmente utilizado para classifica\u00e7\u00e3o bin\u00e1ria. Consiste em uma \u00fanica camada de neur\u00f4nios, e sua opera\u00e7\u00e3o pode ser descrita pela f\u00f3rmula:</p> <p></p> <p>Podemos calcular o valor de um neuronio da seguinte forma:</p> <p>output = activation_function(weighted_sum + bias)</p> <p>O perceptron \u00e9 uma das formas mais simples de uma rede neural, utilizado para classifica\u00e7\u00e3o bin\u00e1ria. A sa\u00edda de um neur\u00f4nio no perceptron pode ser calculada usando a seguinte f\u00f3rmula:</p> <pre><code>output = activation_function(weighted_sum + bias)\n</code></pre> <p>Onde: </p> <ul> <li><code>weighted_sum</code>: \u00e9 a soma ponderada das entradas (inputs) multiplicadas pelos respectivos pesos (weights).</li> <li><code>activation_function</code>: refere-se \u00e0 fun\u00e7\u00e3o que define o limiar de ativa\u00e7\u00e3o do neur\u00f4nio, como sigmoid, relu ou softmax.</li> <li><code>bias</code>: \u00e9 um termo adicional que permite ajustar a sa\u00edda ao longo da fun\u00e7\u00e3o de ativa\u00e7\u00e3o para melhor adapta\u00e7\u00e3o dos dados</li> </ul>"},{"location":"aulas/IA/lab07/#perceptron-de-multiplas-camadas-mlp","title":"Perceptron de M\u00faltiplas Camadas (MLP)","text":"<p>O <code>MLP (Multilayer Perceptron)</code> \u00e9 uma extens\u00e3o do modelo perceptron que inclui m\u00faltiplas camadas. A estrutura t\u00edpica de um MLP consiste em:</p> <ul> <li><code>Uma camada de entrada</code>, que recebe os dados.</li> <li><code>Uma ou mais camadas ocultas</code>, que transformam os dados de entrada atrav\u00e9s de pesos, biases e fun\u00e7\u00f5es de ativa\u00e7\u00e3o.</li> <li><code>Uma camada de sa\u00edda</code>, que produz a previs\u00e3o final do modelo.</li> </ul> <p>Cada camada \u00e9 <code>totalmente conectada</code> \u00e0 pr\u00f3xima, o que significa que cada neur\u00f4nio em uma camada est\u00e1 conectado a todos os neur\u00f4nios na camada seguinte. O MLP \u00e9 capaz de aprender representa\u00e7\u00f5es n\u00e3o-lineares dos dados, o que o torna adequado para problemas complexos de classifica\u00e7\u00e3o e regress\u00e3o.</p> <p></p>"},{"location":"aulas/IA/lab07/#funcoes-de-ativacao","title":"Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o","text":"<p>As fun\u00e7\u00f5es de ativa\u00e7\u00e3o s\u00e3o componentes essenciais nas redes neurais, respons\u00e1veis por introduzir n\u00e3o-linearidades no modelo. Sem elas, a rede seria essencialmente um modelo linear e incapaz de aprender e representar dados complexos que requerem n\u00e3o-linearidade para sua modelagem. As fun\u00e7\u00f5es de ativa\u00e7\u00e3o decidem se um neur\u00f4nio deve ser ativado ou n\u00e3o, com base no valor da soma ponderada de suas entradas.</p>"},{"location":"aulas/IA/lab07/#exemplos-comuns","title":"Exemplos Comuns:","text":"<ul> <li><code>ReLU (Rectified Linear Unit)</code>: Oferece uma resposta linear para todos os valores positivos e zero para valores negativos. \u00c9 a mais usada devido \u00e0 sua efici\u00eancia computacional e \u00e0 capacidade de mitigar o problema do desaparecimento do gradiente em redes profundas.</li> <li><code>Sigmoid</code>: Transforma os valores em uma faixa entre 0 e 1, \u00fatil especialmente para modelos onde precisamos de uma probabilidade como sa\u00edda; contudo, \u00e9 menos usada em camadas ocultas devido a problemas de desaparecimento de gradiente.</li> <li><code>Tanh (Tangente Hiperb\u00f3lica)</code>: Semelhante \u00e0 sigmoid, mas transforma os valores em uma faixa entre -1 e 1, centrando os dados e, portanto, melhorando a efici\u00eancia do aprendizado nas camadas ocultas.</li> </ul>"},{"location":"aulas/IA/lab07/#retropropagacao-backpropagation","title":"Retropropaga\u00e7\u00e3o (Backpropagation)","text":"<p>A Retropropaga\u00e7\u00e3o \u00e9 uma t\u00e9cnica para treinar redes neurais, permitindo o ajuste dos pesos de conex\u00e3o ap\u00f3s cada itera\u00e7\u00e3o de treinamento. Este m\u00e9todo utiliza o <code>c\u00e1lculo do gradiente</code> da <code>fun\u00e7\u00e3o de perda</code> em rela\u00e7\u00e3o a cada peso, propagando o erro de sa\u00edda de volta pela rede para atualizar os pesos. Isso minimiza a fun\u00e7\u00e3o de perda ao longo do tempo, melhorando a precis\u00e3o do modelo ao ajust\u00e1-lo mais eficazmente aos dados de treinamento.</p>"},{"location":"aulas/IA/lab07/#funcionamento","title":"Funcionamento:","text":"<ul> <li>Calcula-se o gradiente da fun\u00e7\u00e3o de perda para determinar a dire\u00e7\u00e3o na qual os pesos devem ser ajustados para minimizar o erro.</li> <li>Os pesos s\u00e3o atualizados utilizando este gradiente, geralmente com a ajuda de um otimizador como SGD, Adam, entre outros.</li> <li>Este processo \u00e9 repetido para cada lote de dados (batch) durante v\u00e1rias \u00e9pocas, ajustando progressivamente os pesos para melhorar o desempenho do modelo.</li> </ul> <p>Esses ajustes permitem que a rede aprenda de forma eficiente, refinando seus pesos para reduzir o erro total e aumentar a precis\u00e3o nas tarefas de classifica\u00e7\u00e3o ou regress\u00e3o.</p>"},{"location":"aulas/IA/lab07/#otimizadores","title":"Otimizadores","text":"<p>Otimizadores s\u00e3o algoritmos projetados para otimizar o processo de treinamento de uma rede neural, ajustando os pesos e a taxa de aprendizagem. \u00c9 importante para determinar a rapidez e efic\u00e1cia com que uma rede neural aprende. </p> <p>Alguns exemplos comuns incluem:</p> <ul> <li>SGD (Descida do Gradiente Estoc\u00e1stico)</li> <li>Adam</li> <li>RMSprop</li> <li>Dentre outros, cada um com suas pr\u00f3prias caracter\u00edsticas e adequa\u00e7\u00f5es a diferentes tipos de problemas e conjuntos de dados.</li> </ul> <p></p>"},{"location":"aulas/IA/lab07/#redes-neurais-convolucionais-cnn","title":"Redes Neurais Convolucionais (CNN)","text":"<p>Redes Neurais Convolucionais (CNNs) s\u00e3o uma classe especializada de redes neurais profundas que s\u00e3o particularmente para tarefas de processamento de imagem e v\u00eddeo. Utilizam o processo de convolu\u00e7\u00e3o para capturar caracter\u00edsticas visuais importantes como bordas, texturas e padr\u00f5es mais complexos sem a necessidade de interven\u00e7\u00e3o ou extra\u00e7\u00e3o manual de caracter\u00edsticas.</p>"},{"location":"aulas/IA/lab07/#principais-componentes","title":"Principais Componentes","text":"<ul> <li>Camadas Convolucionais: O cora\u00e7\u00e3o de uma CNN. Estas camadas utilizam filtros que realizam a convolu\u00e7\u00e3o sobre a entrada para criar mapas de caracter\u00edsticas que resumem as presen\u00e7as de caracter\u00edsticas espec\u00edficas na entrada.</li> <li>Fun\u00e7\u00e3o de Ativa\u00e7\u00e3o: Normalmente, uma fun\u00e7\u00e3o ReLU \u00e9 aplicada ap\u00f3s cada convolu\u00e7\u00e3o para introduzir n\u00e3o-linearidades ao modelo, ajudando-o a aprender mais complexidades.</li> <li>Camadas de Pooling: Seguem as camadas convolucionais e s\u00e3o usadas para reduzir as dimens\u00f5es dos mapas de caracter\u00edsticas, o que ajuda a diminuir o c\u00e1lculo necess\u00e1rio e tamb\u00e9m controla o overfitting.</li> <li>Camadas Densas (Fully Connected Layers): Ap\u00f3s v\u00e1rias camadas convolucionais e de pooling, a rede utiliza uma ou mais camadas densas onde a classifica\u00e7\u00e3o final \u00e9 realizada baseada nas caracter\u00edsticas extra\u00eddas anteriormente.</li> </ul>"},{"location":"aulas/IA/lab07/#funcionamento_1","title":"Funcionamento","text":"<p>Uma CNN recebe uma imagem como entrada, que \u00e9 passada atrav\u00e9s de uma s\u00e9rie de camadas convolucionais com filtros (kernels), camadas de pooling, e eventualmente camadas densas para produzir uma sa\u00edda, que pode ser uma classifica\u00e7\u00e3o ou outra interpreta\u00e7\u00e3o da imagem. Em cada camada convolucional, a rede aprende a identificar caracter\u00edsticas cada vez mais complexas. \u00c0 medida que os dados avan\u00e7am pela rede, a \"vis\u00e3o\" da rede torna-se cada vez mais abstrata, permitindo que ela reconhe\u00e7a grandes padr\u00f5es compostos por caracter\u00edsticas menores capturadas nas primeiras camadas.</p>"},{"location":"aulas/IA/lab07/#aplicacoes","title":"Aplica\u00e7\u00f5es","text":"<p>As CNNs t\u00eam sido usadas com grande sucesso em uma variedade de aplica\u00e7\u00f5es de vis\u00e3o computacional:</p> <ul> <li><code>Reconhecimento de Imagens</code>: Identifica\u00e7\u00e3o de objetos, pessoas, cenas e atividades em imagens.</li> <li><code>Classifica\u00e7\u00e3o de Imagens</code>: Classificar imagens em categorias pr\u00e9-definidas.</li> <li><code>Detec\u00e7\u00e3o de Objetos</code>: Localizar e identificar m\u00faltiplos objetos dentro de uma \u00fanica imagem.</li> <li><code>Segmenta\u00e7\u00e3o Sem\u00e2ntica</code>: Classificar cada pixel de uma imagem em uma categoria de objeto, permitindo uma compreens\u00e3o detalhada da cena.</li> </ul>"},{"location":"aulas/IA/lab07/#implementando-um-mlp-simples","title":"Implementando um MLP Simples","text":"<p>Vamos implementar um exemplo simples de uma rede neural multicamadas (MLP) usando TensorFlow, aplicada ao conjunto de dados <code>MNIST</code>, que consiste em <code>imagens de d\u00edgitos escritos \u00e0 m\u00e3o</code>:</p> <p></p>"},{"location":"aulas/IA/lab07/#importando-o-dataset-e-bibliotecas","title":"Importando o dataset e bibliotecas","text":"<p>Come\u00e7amos importando a biblioteca <code>tensorflow</code> e o dataset mninst. Esse dataset \u00e9 bem famoso e faz parte dos do da biblioteca tensorflow. Precisamos fazer uma transforma\u00e7\u00e3o simples nas imagens de entrada para normalizar entre 0 e 1.</p> <pre><code>import tensorflow as tf\n\n# Carregando o conjunto de dados MNIST\nmnist = tf.keras.datasets.mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Normalizando os dados\nx_train, x_test = x_train / 255.0, x_test / 255.0\n</code></pre>"},{"location":"aulas/IA/lab07/#criando-a-rede-neural","title":"Criando a Rede Neural","text":"<p>Para construir nossa rede neural, utilizaremos a classe Sequential do TensorFlow, que permite compor modelos camada por camada de maneira simples e direta. Aqui est\u00e1 o detalhamento de cada componente utilizado:</p> <p>Warning</p> <p>Substitua os campos <code>&lt;QUANTIDADE_NEURONIOS&gt;</code>e <code>&lt;FUNCAO_ATIVACAO&gt;</code> nos lugares indicados para configurar o n\u00famero de neur\u00f4nios e as fun\u00e7\u00f5es de ativa\u00e7\u00e3o.</p> <p>Tip</p> <p>A quantidade de neur\u00f4nios na <code>camada de sa\u00edda</code> deve ser igual \u00e0 quantidade de classes a serem preditas no MNIST, que s\u00e3o 10 (d\u00edgitos de 0 a 9).</p> <p>Tip</p> <p>Utilize a fun\u00e7\u00e3o de ativa\u00e7\u00e3o <code>relu</code> para as camadas intermedi\u00e1rias, pois ajuda a resolver o problema do desaparecimento do gradiente em redes profundas. Para a camada de sa\u00edda, use <code>softmax</code> para converter as sa\u00eddas em probabilidades de pertencimento \u00e0s classes.</p>"},{"location":"aulas/IA/lab07/#componentes-do-modelo","title":"Componentes do Modelo","text":"<ul> <li><code>Sequential</code>: Esta \u00e9 a classe base para definir uma pilha de camadas de rede neural. Voc\u00ea come\u00e7a com uma lista vazia e adiciona camadas usando a nota\u00e7\u00e3o de lista. Cada camada adicionada \u00e9 empilhada sobre a anterior, o que facilita a modelagem de um fluxo de dados direto (feedforward).</li> <li><code>Flatten</code>: Transforma a matriz 2D de entrada (28x28 pixels da imagem) em um vetor 1D. Isso \u00e9 necess\u00e1rio porque a primeira camada densa (Dense) espera um vetor como entrada.</li> <li><code>Dense</code>: \u00c9 a camada de neur\u00f4nios densamente conectados ou completamente conectados. Cada neur\u00f4nio recebe entrada de todos os neur\u00f4nios da camada anterior, mantendo uma conex\u00e3o densa.</li> <li>: Aqui voc\u00ea define quantos neur\u00f4nios deseja nesta camada. <li>: Define a fun\u00e7\u00e3o de ativa\u00e7\u00e3o a ser usada. A fun\u00e7\u00e3o relu \u00e9 comum para camadas internas. <pre><code># Criando o modelo MLP\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(&lt;QUANTIDADE_NEURONIOS&gt;, activation=&lt;FUNCAO_ATIVACAO&gt;),\n  tf.keras.layers.Dense(&lt;QUANTIDADE_NEURONIOS&gt;, activation=&lt;FUNCAO_ATIVACAO&gt;)\n])\n</code></pre>"},{"location":"aulas/IA/lab07/#compilando-o-modelo","title":"Compilando o Modelo","text":"<p>Agora, vamos compilar o modelo utilizando o <code>otimizador Adam</code> e a fun\u00e7\u00e3o de perda <code>sparse_categorical_crossentropy</code>, adequada para classifica\u00e7\u00e3o de m\u00faltiplas classes onde as classes s\u00e3o fornecidas como n\u00fameros inteiros:</p> <p>Warning</p> <p>Substitua os campos <code>&lt;KEY&gt;</code> por valores convenientes. Para <code>&lt;METRICA&gt;</code> utilize <code>accuracy</code>.</p> <pre><code>model.compile(optimizer=&lt;OTIMIZADOR&gt;, \n              loss=&lt;LOSS&gt;, \n              metrics=[&lt;METRICA&gt;])\n</code></pre>"},{"location":"aulas/IA/lab07/#compilando-o-modelo_1","title":"Compilando o Modelo","text":"<p>Agora, vamos compilar o modelo passando os conjuntos de dados <code>x_train</code> e <code>y_train</code>, definindo a quantidade \u00e9pocas de treinamento e ajustando um subset de 20% para valida\u00e7\u00e3o. </p> <p>Warning</p> <p>Substitua os campos <code>&lt;KEY&gt;</code> por valores convenientes. Defina 30 \u00e9pocas e 0.2 de vali\u00e7ao.</p> <pre><code># Treinando o modelo\nepocas_hist = model.fit(&lt;X_TREINO&gt;, &lt;Y_TREINO&gt;, epochs=&lt;EPOCAS&gt;, validation_split=&lt;VALIDACAO&gt;)\n</code></pre>"},{"location":"aulas/IA/lab07/#visualizando-o-historico-de-treinamento","title":"Visualizando o historico de treinamento","text":"<p>O historico de treinamento \u00e9 salvo na variavel <code>epocas_hist</code>, podemos visualizar utlizando o pandas</p> <pre><code>import pandas as pd\n\ndf_historico = pd.DataFrame(epocas_hist.history)\ndf_historico.info()\n\ndf_historico[['loss','val_loss']].plot(); plt.show();\ndf_historico[['accuracy','val_accuracy']].plot(); plt.show();\n</code></pre>"},{"location":"aulas/IA/lab07/#avaliacao-e-conclusao","title":"Avalia\u00e7\u00e3o e Conclus\u00e3o","text":"<p>Ap\u00f3s o treinamento, avaliamos o modelo no conjunto de teste para verificar sua acur\u00e1cia e a perda:</p> <p>Warning</p> <p>Substitua os campos <code>&lt;KEY&gt;</code> por valores convenientes. </p> <pre><code>test_loss, test_acc = model.evaluate(&lt;X_TESTE&gt;, &lt;Y_TESTE&gt;)\nprint(f\"Teste Acur\u00e1cia: {test_acc:.3f}, Teste Loss: {test_loss:.3f}\")\n</code></pre>"},{"location":"aulas/IA/lab07/#salvando-o-modelo","title":"Salvando o modelo","text":"<pre><code># Salvando o modelo treinado\nmodel.save('mnist_mlp_model.h5')\n</code></pre>"},{"location":"aulas/IA/lab07/#importando-uma-imagem-e-usando-com-o-modelo-treinado","title":"Importando uma Imagem e Usando com o Modelo Treinado","text":"<p>Para usar uma imagem pr\u00f3pria e verificar como o modelo MLP prev\u00ea o d\u00edgito, precisamos garantir que a imagem seja processada de maneira semelhante ao conjunto de dados MNIST. A imagem deve ser em escala de cinza, de tamanho 28x28 pixels, e normalizada.</p> <pre><code>import tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Carregando o modelo treinado (certifique-se de que o modelo est\u00e1 dispon\u00edvel)\nmodel = tf.keras.models.load_model('mnist_mlp_model.h5')\n\n# Fun\u00e7\u00e3o para carregar e preparar a imagem\ndef load_and_prepare_image(filepath):\n    img = image.load_img(filepath, color_mode='grayscale', target_size=(28, 28))\n    img = image.img_to_array(img)\n    img = img.reshape(1, 28, 28)\n    img = img.astype('float32')\n    img /= 255.0\n    return img\n\n# Caminho para a sua imagem\nfilepath = 'path_to_your_image.png' # escolha uma imagem com um numero simples \n\n# Carregando e preparando a imagem\nimg = load_and_prepare_image(filepath)\n\n# Fazendo a previs\u00e3o\npredictions = model.predict(img)\npredicted_digit = np.argmax(predictions)\n\n# Mostrando a imagem e a previs\u00e3o\nplt.imshow(img.reshape(28, 28), cmap='gray')\nplt.title(f'Previs\u00e3o: {predicted_digit}')\nplt.show()\n</code></pre>"},{"location":"aulas/IA/lab07/mlp/","title":"Mlp","text":"In\u00a0[1]: Copied! <pre>import tensorflow as tf\nfrom tensorflow import keras\n</pre> import tensorflow as tf from tensorflow import keras  In\u00a0[11]: Copied! <pre>from tensorflow.keras import layers\n\n\nmodel = keras.Sequential([\n    layers.Dense(units=1, input_shape=[1])\n])\n\nmodel.summary()\n</pre> from tensorflow.keras import layers   model = keras.Sequential([     layers.Dense(units=1, input_shape=[1]) ])  model.summary()  <pre>Model: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 2 (8.00 Byte)\nTrainable params: 2 (8.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n</pre> In\u00a0[\u00a0]: Copied! <pre>## Sua resposta aqui...\n</pre> ## Sua resposta aqui...       In\u00a0[14]: Copied! <pre>model.compile(optimizer=tf.keras.optimizers.Adam(0.1), loss = 'mse')\n</pre> model.compile(optimizer=tf.keras.optimizers.Adam(0.1), loss = 'mse') In\u00a0[4]: Copied! <pre>!wget https://raw.githubusercontent.com/arnaldojr/disruptivearchitectures/master/material/aulas/IA/lab07/SalesData.csv /content\n</pre> !wget https://raw.githubusercontent.com/arnaldojr/disruptivearchitectures/master/material/aulas/IA/lab07/SalesData.csv /content  <pre>--2024-04-01 10:12:46--  https://raw.githubusercontent.com/arnaldojr/disruptivearchitectures/master/material/aulas/IA/lab07/SalesData.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 11884 (12K) [text/plain]\nSaving to: \u2018SalesData.csv\u2019\n\nSalesData.csv       100%[===================&gt;]  11.61K  --.-KB/s    in 0s      \n\n2024-04-01 10:12:46 (53.2 MB/s) - \u2018SalesData.csv\u2019 saved [11884/11884]\n\n/content: Scheme missing.\nFINISHED --2024-04-01 10:12:46--\nTotal wall clock time: 0.2s\nDownloaded: 1 files, 12K in 0s (53.2 MB/s)\n</pre> In\u00a0[5]: Copied! <pre>import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('SalesData.csv')\ndf.info()\n\n# Separa os dados em X e y\nX_train = df['Temperature']\ny_train = df['Revenue']\n</pre> import pandas as pd import numpy as np  df = pd.read_csv('SalesData.csv') df.info()  # Separa os dados em X e y X_train = df['Temperature'] y_train = df['Revenue']  <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 500 entries, 0 to 499\nData columns (total 2 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Temperature  500 non-null    float64\n 1   Revenue      500 non-null    float64\ndtypes: float64(2)\nmemory usage: 7.9 KB\n</pre> In\u00a0[8]: Copied! <pre>import seaborn as sns\n\nsns.scatterplot(x=X_train, y=y_train);\n</pre> import seaborn as sns  sns.scatterplot(x=X_train, y=y_train); In\u00a0[15]: Copied! <pre>### Tente fazer o treinamento, se der erro! fa\u00e7a o ajustes necess\u00e1rios na camada sequencial em layer.dense().\n\n\n\nepochs_hist = model.fit(X_train, y_train, epochs=10)\n</pre> ### Tente fazer o treinamento, se der erro! fa\u00e7a o ajustes necess\u00e1rios na camada sequencial em layer.dense().    epochs_hist = model.fit(X_train, y_train, epochs=10) <pre>Epoch 1/10\n16/16 [==============================] - 1s 3ms/step - loss: 315555.9688\nEpoch 2/10\n16/16 [==============================] - 0s 2ms/step - loss: 273533.7812\nEpoch 3/10\n16/16 [==============================] - 0s 2ms/step - loss: 235616.6250\nEpoch 4/10\n16/16 [==============================] - 0s 3ms/step - loss: 201430.7500\nEpoch 5/10\n16/16 [==============================] - 0s 3ms/step - loss: 171413.8906\nEpoch 6/10\n16/16 [==============================] - 0s 3ms/step - loss: 144605.4844\nEpoch 7/10\n16/16 [==============================] - 0s 2ms/step - loss: 121690.9766\nEpoch 8/10\n16/16 [==============================] - 0s 3ms/step - loss: 101589.6562\nEpoch 9/10\n16/16 [==============================] - 0s 2ms/step - loss: 84055.3672\nEpoch 10/10\n16/16 [==============================] - 0s 2ms/step - loss: 69379.4141\n</pre> In\u00a0[16]: Copied! <pre>import pandas as pd\n\nhistory_df = pd.DataFrame(epochs_hist.history)\n\nhistory_df['loss'].plot();\n</pre> import pandas as pd  history_df = pd.DataFrame(epochs_hist.history)  history_df['loss'].plot(); In\u00a0[27]: Copied! <pre># Previs\u00f5es com o modelo treinado\ntemp = 5\nreceita = model.predict([temp])\nprint('Previs\u00e3o de Receita Usando a ANN Treinada =', receita[0][0])\n</pre> # Previs\u00f5es com o modelo treinado temp = 5 receita = model.predict([temp]) print('Previs\u00e3o de Receita Usando a ANN Treinada =', receita[0][0]) <pre>1/1 [==============================] - 0s 39ms/step\nPrevis\u00e3o de Receita Usando a ANN Treinada = 136.9302\n</pre> In\u00a0[28]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.scatter(X_train, y_train, color = 'gray')\nplt.plot(X_train, model.predict(X_train), color = 'red')\nplt.ylabel('Receita [d\u00f3lares]')\nplt.xlabel('Temperatura [\u00b0C]')\nplt.title('Receita Gerada vs. Temperatura no Ponto de Venda de Sorvetes')\n</pre> import matplotlib.pyplot as plt  plt.scatter(X_train, y_train, color = 'gray') plt.plot(X_train, model.predict(X_train), color = 'red') plt.ylabel('Receita [d\u00f3lares]') plt.xlabel('Temperatura [\u00b0C]') plt.title('Receita Gerada vs. Temperatura no Ponto de Venda de Sorvetes') <pre>16/16 [==============================] - 0s 2ms/step\n</pre> Out[28]: <pre>Text(0.5, 1.0, 'Receita Gerada vs. Temperatura no Ponto de Venda de Sorvetes')</pre> In\u00a0[\u00a0]: Copied! <pre>### seu c\u00f3digo aqui.....\n</pre> ### seu c\u00f3digo aqui.....    In\u00a0[22]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nfrom tensorflow import keras\n\ndef carregar_e_visualizar_dados():\n    # Carregar os dados\n    #!wget https://raw.githubusercontent.com/arnaldojr/disruptivearchitectures/master/material/aulas/IA/lab07/SalesData.csv /content\n    df = pd.read_csv('SalesData.csv')\n    df.info()\n\n    # Separar os dados\n    X_train = df['Temperature']\n    y_train = df['Revenue']\n\n    # Visualizar os dados\n    sns.scatterplot(x=X_train, y=y_train)\n    plt.show()\n\n    return X_train, y_train\n\ndef criar_e_compilar_modelo():\n    # Criar o modelo\n    model = keras.Sequential([\n        layers.Dense(units=1, input_shape=[1])\n    ])\n\n    # Compilar o modelo\n    model.compile(optimizer=tf.keras.optimizers.Adam(0.1), loss='mse')\n    model.summary()\n\n    return model\n\ndef treinar_modelo(model, X_train, y_train, epochs=100):\n    historico_epochs = model.fit(X_train, y_train, epochs=epochs)\n    df_historico = pd.DataFrame(historico_epochs.history)\n    df_historico['loss'].plot()\n    plt.show()\n    return model\n\ndef avaliar_e_prever(model, X_train, y_train):\n    # Visualizar as predi\u00e7\u00f5es do modelo\n    plt.scatter(X_train, y_train, color='gray')\n    plt.plot(X_train, model.predict(X_train), color='red')\n    plt.ylabel('Receita [d\u00f3lares]')\n    plt.xlabel('Temperatura [\u00b0C]')\n    plt.title('Receita Gerada vs. Temperatura no Ponto de Venda de Sorvetes')\n    plt.show()\n\n    # Fazer uma previs\u00e3o\n    temp = 5\n    receita = model.predict([temp])\n    print('Previs\u00e3o de Receita Usando a ANN Treinada =', receita)\n\n# ---- Programa principal ----\n\n# Carregar e visualizar os dados\nX_train, y_train = carregar_e_visualizar_dados()\n\n# Criar e compilar o modelo\nmodel = criar_e_compilar_modelo()\n\n# Treinar o modelo\nmodel = treinar_modelo(model, X_train, y_train)\n\n# Avaliar e fazer predi\u00e7\u00f5es\navaliar_e_prever(model, X_train, y_train)\n</pre> import pandas as pd import numpy as np import seaborn as sns from matplotlib import pyplot as plt from tensorflow.keras import layers import tensorflow as tf from tensorflow import keras  def carregar_e_visualizar_dados():     # Carregar os dados     #!wget https://raw.githubusercontent.com/arnaldojr/disruptivearchitectures/master/material/aulas/IA/lab07/SalesData.csv /content     df = pd.read_csv('SalesData.csv')     df.info()      # Separar os dados     X_train = df['Temperature']     y_train = df['Revenue']      # Visualizar os dados     sns.scatterplot(x=X_train, y=y_train)     plt.show()      return X_train, y_train  def criar_e_compilar_modelo():     # Criar o modelo     model = keras.Sequential([         layers.Dense(units=1, input_shape=[1])     ])      # Compilar o modelo     model.compile(optimizer=tf.keras.optimizers.Adam(0.1), loss='mse')     model.summary()      return model  def treinar_modelo(model, X_train, y_train, epochs=100):     historico_epochs = model.fit(X_train, y_train, epochs=epochs)     df_historico = pd.DataFrame(historico_epochs.history)     df_historico['loss'].plot()     plt.show()     return model  def avaliar_e_prever(model, X_train, y_train):     # Visualizar as predi\u00e7\u00f5es do modelo     plt.scatter(X_train, y_train, color='gray')     plt.plot(X_train, model.predict(X_train), color='red')     plt.ylabel('Receita [d\u00f3lares]')     plt.xlabel('Temperatura [\u00b0C]')     plt.title('Receita Gerada vs. Temperatura no Ponto de Venda de Sorvetes')     plt.show()      # Fazer uma previs\u00e3o     temp = 5     receita = model.predict([temp])     print('Previs\u00e3o de Receita Usando a ANN Treinada =', receita)  # ---- Programa principal ----  # Carregar e visualizar os dados X_train, y_train = carregar_e_visualizar_dados()  # Criar e compilar o modelo model = criar_e_compilar_modelo()  # Treinar o modelo model = treinar_modelo(model, X_train, y_train)  # Avaliar e fazer predi\u00e7\u00f5es avaliar_e_prever(model, X_train, y_train)   <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 500 entries, 0 to 499\nData columns (total 2 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Temperature  500 non-null    float64\n 1   Revenue      500 non-null    float64\ndtypes: float64(2)\nmemory usage: 7.9 KB\n</pre> <pre>Model: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_4 (Dense)             (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 2 (8.00 Byte)\nTrainable params: 2 (8.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/100\n16/16 [==============================] - 0s 2ms/step - loss: 319039.4062\nEpoch 2/100\n16/16 [==============================] - 0s 2ms/step - loss: 276887.2812\nEpoch 3/100\n16/16 [==============================] - 0s 2ms/step - loss: 238628.3906\nEpoch 4/100\n16/16 [==============================] - 0s 2ms/step - loss: 204381.8906\nEpoch 5/100\n16/16 [==============================] - 0s 2ms/step - loss: 173926.2969\nEpoch 6/100\n16/16 [==============================] - 0s 2ms/step - loss: 147583.9531\nEpoch 7/100\n16/16 [==============================] - 0s 2ms/step - loss: 123943.3594\nEpoch 8/100\n16/16 [==============================] - 0s 2ms/step - loss: 103614.0625\nEpoch 9/100\n16/16 [==============================] - 0s 2ms/step - loss: 86137.4531\nEpoch 10/100\n16/16 [==============================] - 0s 2ms/step - loss: 70995.4609\nEpoch 11/100\n16/16 [==============================] - 0s 2ms/step - loss: 58197.1367\nEpoch 12/100\n16/16 [==============================] - 0s 2ms/step - loss: 47373.6836\nEpoch 13/100\n16/16 [==============================] - 0s 2ms/step - loss: 38204.8594\nEpoch 14/100\n16/16 [==============================] - 0s 2ms/step - loss: 30706.8301\nEpoch 15/100\n16/16 [==============================] - 0s 2ms/step - loss: 24447.9141\nEpoch 16/100\n16/16 [==============================] - 0s 2ms/step - loss: 19336.2246\nEpoch 17/100\n16/16 [==============================] - 0s 2ms/step - loss: 15294.8311\nEpoch 18/100\n16/16 [==============================] - 0s 2ms/step - loss: 11972.4717\nEpoch 19/100\n16/16 [==============================] - 0s 2ms/step - loss: 9316.7100\nEpoch 20/100\n16/16 [==============================] - 0s 2ms/step - loss: 7252.5850\nEpoch 21/100\n16/16 [==============================] - 0s 2ms/step - loss: 5654.0059\nEpoch 22/100\n16/16 [==============================] - 0s 2ms/step - loss: 4398.6558\nEpoch 23/100\n16/16 [==============================] - 0s 2ms/step - loss: 3426.6641\nEpoch 24/100\n16/16 [==============================] - 0s 2ms/step - loss: 2704.4099\nEpoch 25/100\n16/16 [==============================] - 0s 2ms/step - loss: 2150.8928\nEpoch 26/100\n16/16 [==============================] - 0s 2ms/step - loss: 1744.8942\nEpoch 27/100\n16/16 [==============================] - 0s 2ms/step - loss: 1441.0400\nEpoch 28/100\n16/16 [==============================] - 0s 2ms/step - loss: 1218.3285\nEpoch 29/100\n16/16 [==============================] - 0s 2ms/step - loss: 1058.0693\nEpoch 30/100\n16/16 [==============================] - 0s 3ms/step - loss: 942.4816\nEpoch 31/100\n16/16 [==============================] - 0s 2ms/step - loss: 861.4608\nEpoch 32/100\n16/16 [==============================] - 0s 2ms/step - loss: 801.4128\nEpoch 33/100\n16/16 [==============================] - 0s 2ms/step - loss: 760.8700\nEpoch 34/100\n16/16 [==============================] - 0s 2ms/step - loss: 733.4592\nEpoch 35/100\n16/16 [==============================] - 0s 2ms/step - loss: 712.1679\nEpoch 36/100\n16/16 [==============================] - 0s 2ms/step - loss: 699.5580\nEpoch 37/100\n16/16 [==============================] - 0s 2ms/step - loss: 690.8046\nEpoch 38/100\n16/16 [==============================] - 0s 2ms/step - loss: 684.1700\nEpoch 39/100\n16/16 [==============================] - 0s 2ms/step - loss: 680.7510\nEpoch 40/100\n16/16 [==============================] - 0s 2ms/step - loss: 678.0201\nEpoch 41/100\n16/16 [==============================] - 0s 2ms/step - loss: 675.8742\nEpoch 42/100\n16/16 [==============================] - 0s 2ms/step - loss: 674.7966\nEpoch 43/100\n16/16 [==============================] - 0s 2ms/step - loss: 673.9340\nEpoch 44/100\n16/16 [==============================] - 0s 2ms/step - loss: 673.5883\nEpoch 45/100\n16/16 [==============================] - 0s 2ms/step - loss: 673.2530\nEpoch 46/100\n16/16 [==============================] - 0s 2ms/step - loss: 672.8389\nEpoch 47/100\n16/16 [==============================] - 0s 2ms/step - loss: 672.6512\nEpoch 48/100\n16/16 [==============================] - 0s 2ms/step - loss: 672.6047\nEpoch 49/100\n16/16 [==============================] - 0s 2ms/step - loss: 672.3368\nEpoch 50/100\n16/16 [==============================] - 0s 2ms/step - loss: 672.2287\nEpoch 51/100\n16/16 [==============================] - 0s 2ms/step - loss: 672.0671\nEpoch 52/100\n16/16 [==============================] - 0s 2ms/step - loss: 672.0101\nEpoch 53/100\n16/16 [==============================] - 0s 2ms/step - loss: 671.8309\nEpoch 54/100\n16/16 [==============================] - 0s 2ms/step - loss: 671.6974\nEpoch 55/100\n16/16 [==============================] - 0s 3ms/step - loss: 671.6188\nEpoch 56/100\n16/16 [==============================] - 0s 2ms/step - loss: 671.5398\nEpoch 57/100\n16/16 [==============================] - 0s 2ms/step - loss: 671.3561\nEpoch 58/100\n16/16 [==============================] - 0s 2ms/step - loss: 671.3818\nEpoch 59/100\n16/16 [==============================] - 0s 2ms/step - loss: 671.1469\nEpoch 60/100\n16/16 [==============================] - 0s 2ms/step - loss: 671.0237\nEpoch 61/100\n16/16 [==============================] - 0s 2ms/step - loss: 670.8895\nEpoch 62/100\n16/16 [==============================] - 0s 2ms/step - loss: 670.7467\nEpoch 63/100\n16/16 [==============================] - 0s 2ms/step - loss: 670.6893\nEpoch 64/100\n16/16 [==============================] - 0s 2ms/step - loss: 670.5098\nEpoch 65/100\n16/16 [==============================] - 0s 2ms/step - loss: 670.4712\nEpoch 66/100\n16/16 [==============================] - 0s 3ms/step - loss: 670.2680\nEpoch 67/100\n16/16 [==============================] - 0s 3ms/step - loss: 670.3395\nEpoch 68/100\n16/16 [==============================] - 0s 2ms/step - loss: 670.0258\nEpoch 69/100\n16/16 [==============================] - 0s 2ms/step - loss: 670.1964\nEpoch 70/100\n16/16 [==============================] - 0s 2ms/step - loss: 669.7410\nEpoch 71/100\n16/16 [==============================] - 0s 2ms/step - loss: 669.5929\nEpoch 72/100\n16/16 [==============================] - 0s 2ms/step - loss: 669.4639\nEpoch 73/100\n16/16 [==============================] - 0s 3ms/step - loss: 669.3311\nEpoch 74/100\n16/16 [==============================] - 0s 2ms/step - loss: 669.4010\nEpoch 75/100\n16/16 [==============================] - 0s 2ms/step - loss: 669.0681\nEpoch 76/100\n16/16 [==============================] - 0s 2ms/step - loss: 669.0442\nEpoch 77/100\n16/16 [==============================] - 0s 2ms/step - loss: 668.7401\nEpoch 78/100\n16/16 [==============================] - 0s 2ms/step - loss: 668.9483\nEpoch 79/100\n16/16 [==============================] - 0s 2ms/step - loss: 668.5505\nEpoch 80/100\n16/16 [==============================] - 0s 2ms/step - loss: 668.4393\nEpoch 81/100\n16/16 [==============================] - 0s 2ms/step - loss: 668.2285\nEpoch 82/100\n16/16 [==============================] - 0s 2ms/step - loss: 668.0378\nEpoch 83/100\n16/16 [==============================] - 0s 2ms/step - loss: 667.9488\nEpoch 84/100\n16/16 [==============================] - 0s 2ms/step - loss: 667.7075\nEpoch 85/100\n16/16 [==============================] - 0s 2ms/step - loss: 667.6132\nEpoch 86/100\n16/16 [==============================] - 0s 2ms/step - loss: 667.4966\nEpoch 87/100\n16/16 [==============================] - 0s 2ms/step - loss: 667.4224\nEpoch 88/100\n16/16 [==============================] - 0s 2ms/step - loss: 667.1114\nEpoch 89/100\n16/16 [==============================] - 0s 2ms/step - loss: 667.0513\nEpoch 90/100\n16/16 [==============================] - 0s 2ms/step - loss: 667.2527\nEpoch 91/100\n16/16 [==============================] - 0s 2ms/step - loss: 666.6097\nEpoch 92/100\n16/16 [==============================] - 0s 2ms/step - loss: 666.5334\nEpoch 93/100\n16/16 [==============================] - 0s 2ms/step - loss: 666.3892\nEpoch 94/100\n16/16 [==============================] - 0s 2ms/step - loss: 666.1669\nEpoch 95/100\n16/16 [==============================] - 0s 2ms/step - loss: 666.0068\nEpoch 96/100\n16/16 [==============================] - 0s 2ms/step - loss: 665.8843\nEpoch 97/100\n16/16 [==============================] - 0s 2ms/step - loss: 665.7877\nEpoch 98/100\n16/16 [==============================] - 0s 2ms/step - loss: 665.5769\nEpoch 99/100\n16/16 [==============================] - 0s 2ms/step - loss: 665.4597\nEpoch 100/100\n16/16 [==============================] - 0s 2ms/step - loss: 665.3188\n</pre> <pre>16/16 [==============================] - 0s 2ms/step\n</pre> <pre>1/1 [==============================] - 0s 65ms/step\nPrevis\u00e3o de Receita Usando a ANN Treinada = [[136.9302]]\n</pre> In\u00a0[\u00a0]: Copied! <pre>### Seu c\u00f3digo aqui.....\n</pre> ### Seu c\u00f3digo aqui....."},{"location":"aulas/IA/lab07/mlp/#aprendizagem-de-maquina","title":"Aprendizagem de m\u00e1quina\u00b6","text":""},{"location":"aulas/IA/lab07/mlp/#objetivos","title":"Objetivos\u00b6","text":"<ul> <li>Conhecer e Praticar os algoritmo multilayer Perceptron (MLP)</li> <li>Conhecer uma intui\u00e7\u00e3o sobre fun\u00e7\u00e3o de ativa\u00e7\u00e3o, backpropagation</li> <li>Conhecer e praticar o framework TensorFlow</li> </ul>"},{"location":"aulas/IA/lab07/mlp/#perceptron","title":"Perceptron\u00b6","text":"<p>Relembrando o neuronio artificial:</p>"},{"location":"aulas/IA/lab07/mlp/#desafio1","title":"Desafio1\u00b6","text":"<p>Calcule a saida do perceptron abaixo:</p> <p>x0 = 2; x1 = 0; x2 = -1,24; bias = 1; w0 = 0; w1 = 2; w3 = 1; fun\u00e7\u00e3o de ativa\u00e7\u00e3o = Heaviside</p>"},{"location":"aulas/IA/lab07/mlp/#resposta","title":"Resposta:\u00b6","text":""},{"location":"aulas/IA/lab07/mlp/#implementacao-de-uma-rede-perceptron","title":"Implementa\u00e7\u00e3o de uma rede perceptron\u00b6","text":"<p>Vamos usar um framework de machine learnning chamado TensorFlow/keras para fazer esta implementa\u00e7\u00e3o.</p> <p>pip install tensorflow</p>"},{"location":"aulas/IA/lab07/mlp/#layers","title":"Layers\u00b6","text":"<p>O arranjo de neuronios define a quantidade de camadas ou <code>layers</code> que a rede neural possui na rede perceptron possui apenas uma camada. Em uma rede MLP (multlayer perceptron) possui al\u00e9m das camadas de entrada e sa\u00edda, camadas ocultas ou <code>hiden layers</code>, essas redes tambem s\u00e3o conhecidas por redes densas ou fully-connected.</p>"},{"location":"aulas/IA/lab07/mlp/#funao-de-ativacao","title":"Fun\u00e3o de ativa\u00e7\u00e3o\u00b6","text":"<p>\u00c9 basicamente uma fun\u00e7\u00e3o matematica que \u00e9 responsavel por <code>ativar</code> ou mudar o comportamento de sa\u00edda do neuronio.</p> <p>Dentre as mais comuns temos:</p> <p>Outras fun\u00e7\u00f5es de ativa\u00e7\u00e3o muito utilizadas s\u00e3o:</p> <ul> <li>softplus</li> <li>elu</li> <li>sigmoid</li> <li>tanh</li> </ul>"},{"location":"aulas/IA/lab07/mlp/#desafio-2","title":"Desafio 2\u00b6","text":"<p>Implemente a rede MLP abaixo usando TensorFlor/keras: fun\u00e7\u00e3o de ativa\u00e7\u00e3o Relu</p> <p>Dica: use o argumento <code>activation='relu'</code> em <code>layers.Dense</code></p>"},{"location":"aulas/IA/lab07/mlp/#backpropagation","title":"Backpropagation\u00b6","text":"<p>A t\u00e9cnica de backpropagation \u00e9 fundamental para o treinamento de redes neurais, pois \u00e9 atrav\u00e9s dela que os pesos s\u00e3o ajustados em fun\u00e7\u00e3o do erro calculado pela fun\u00e7\u00e3o de perda (Loss).</p>"},{"location":"aulas/IA/lab07/mlp/#funcoes-de-perda-loss-functions","title":"Fun\u00e7\u00f5es de Perda (Loss Functions)\u00b6","text":"<ul> <li><code>Mean Squared Error (MSE)</code>: Utilizado em problemas de regress\u00e3o. Calcula a m\u00e9dia dos quadrados das diferen\u00e7as entre os valores previstos e os valores reais.</li> <li><code>Mean Absolute Error (MAE)</code>: Tamb\u00e9m utilizado em problemas de regress\u00e3o. Calcula a m\u00e9dia do valor absoluto das diferen\u00e7as entre os valores previstos e os valores reais.</li> <li><code>Binary Cross-Entropy (BCE)</code>: Utilizado em problemas de classifica\u00e7\u00e3o bin\u00e1ria. Mede a diferen\u00e7a entre duas distribui\u00e7\u00f5es de probabilidade, a prevista e a real.</li> </ul>"},{"location":"aulas/IA/lab07/mlp/#otimizadores-optimizers","title":"Otimizadores (Optimizers)\u00b6","text":"<p>Os otimizadores s\u00e3o algoritmos que <code>ajustam os pesos da rede neural</code> com o objetivo de <code>minimizar a fun\u00e7\u00e3o de perda</code>. Alguns dos otimizadores mais comuns s\u00e3o:</p> <ul> <li><p><code>Stochastic Gradient Descent (SGD)</code>: Um dos otimizadores mais simples e amplamente utilizados. Atualiza os pesos em pequenos passos, na dire\u00e7\u00e3o oposta ao gradiente da fun\u00e7\u00e3o de perda.</p> </li> <li><p><code>RMSprop</code>: Adapta a taxa de aprendizado para cada par\u00e2metro, dividindo a taxa de aprendizado por uma m\u00e9dia m\u00f3vel do quadrado dos gradientes.</p> </li> <li><p><code>Adam</code>: Combina as ideias do RMSprop e do SGD com momentum. Mant\u00e9m uma m\u00e9dia m\u00f3vel tanto do gradiente quanto do quadrado do gradiente, e usa essas m\u00e9dias para adaptar a taxa de aprendizado para cada par\u00e2metro.<code>(um dos mais utilizados)</code></p> </li> <li><p><code>Adadelta</code>: Uma extens\u00e3o do Adagrad que busca reduzir seu comportamento agressivo de diminui\u00e7\u00e3o da taxa de aprendizado.</p> </li> <li><p><code>Adagrad</code>: Adapta a taxa de aprendizado para cada par\u00e2metro, escalando-os inversamente proporcionalmente \u00e0 raiz quadrada da soma de todos os gradientes quadrados passados.</p> </li> <li><p><code>Adamax</code>: Uma variante do Adam baseada na norma infinita.</p> </li> <li><p>Entre outros...</p> </li> </ul> <p></p> <p></p>"},{"location":"aulas/IA/lab07/mlp/#pausa-para-carregar-e-preparar-os-dados-para-treinamento","title":"Pausa para carregar e preparar os dados para treinamento\u00b6","text":""},{"location":"aulas/IA/lab07/mlp/#desafio-3","title":"desafio 3:\u00b6","text":"<p>O treinamento para 10 \u00e9pocas ficou bom??? se n\u00e3o, melhore o resultado.</p>"},{"location":"aulas/IA/lab07/mlp/#resumo-do-dia","title":"Resumo do dia\u00b6","text":"<p>At\u00e9 o momento fizemos o seguinte:</p> <ul> <li>Carregar e Visualizar os Dados</li> <li>Criar e Compilar o Modelo</li> <li>Treinamento</li> <li>Avalia\u00e7\u00e3o e Predi\u00e7\u00e3o</li> </ul>"},{"location":"aulas/IA/lab07/mlp/#desafio-4-implementacao-end-to-end-mlp","title":"Desafio 4: Implementa\u00e7\u00e3o end-to-end MLP\u00b6","text":"<p>Realize o treinamento de uma rede MLP para o dataset Fashion MNIST. Um guia passo a passo pode ser encontrado no link https://www.tensorflow.org/tutorials/keras/classification.</p>"},{"location":"aulas/IA/lab08/cnn%20copy/","title":"Laborat\u00f3rio: Redes Neurais Convolucionais (CNNs) - Do B\u00e1sico ao Avan\u00e7ado","text":"In\u00a0[\u00a0]: Copied! <pre># Importa\u00e7\u00f5es essenciais\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, optimizers, callbacks\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configura\u00e7\u00f5es para reprodutibilidade\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n# Configura\u00e7\u00f5es de visualiza\u00e7\u00e3o\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (12, 8)\n\n# Verificar GPU dispon\u00edvel\nprint(\"\ud83d\udd27 CONFIGURA\u00c7\u00c3O DO AMBIENTE\")\nprint(\"=\" * 50)\nprint(f\"TensorFlow vers\u00e3o: {tf.__version__}\")\nprint(f\"Keras vers\u00e3o: {keras.__version__}\")\nprint(f\"GPUs dispon\u00edveis: {len(tf.config.list_physical_devices('GPU'))}\")\n\nif tf.config.list_physical_devices('GPU'):\n    print(\"\u2705 GPU detectada e configurada!\")\n    for gpu in tf.config.list_physical_devices('GPU'):\n        print(f\"   \ud83d\udcf1 {gpu}\")\nelse:\n    print(\"\u26a0\ufe0f Executando em CPU - considere usar GPU para melhor performance\")\n\nprint(\"\\n\ud83d\ude80 Ambiente configurado com sucesso!\")\n</pre> # Importa\u00e7\u00f5es essenciais import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers, models, optimizers, callbacks from tensorflow.keras.preprocessing.image import ImageDataGenerator  import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pandas as pd from sklearn.metrics import classification_report, confusion_matrix import warnings warnings.filterwarnings('ignore')  # Configura\u00e7\u00f5es para reprodutibilidade tf.random.set_seed(42) np.random.seed(42)  # Configura\u00e7\u00f5es de visualiza\u00e7\u00e3o plt.style.use('seaborn-v0_8') sns.set_palette(\"husl\") plt.rcParams['figure.figsize'] = (12, 8)  # Verificar GPU dispon\u00edvel print(\"\ud83d\udd27 CONFIGURA\u00c7\u00c3O DO AMBIENTE\") print(\"=\" * 50) print(f\"TensorFlow vers\u00e3o: {tf.__version__}\") print(f\"Keras vers\u00e3o: {keras.__version__}\") print(f\"GPUs dispon\u00edveis: {len(tf.config.list_physical_devices('GPU'))}\")  if tf.config.list_physical_devices('GPU'):     print(\"\u2705 GPU detectada e configurada!\")     for gpu in tf.config.list_physical_devices('GPU'):         print(f\"   \ud83d\udcf1 {gpu}\") else:     print(\"\u26a0\ufe0f Executando em CPU - considere usar GPU para melhor performance\")  print(\"\\n\ud83d\ude80 Ambiente configurado com sucesso!\") In\u00a0[\u00a0]: Copied! <pre># Demonstra\u00e7\u00e3o: MLP vs CNN - N\u00famero de Par\u00e2metros\nprint(\"\ud83d\udd22 COMPARA\u00c7\u00c3O: MLP vs CNN - N\u00daMERO DE PAR\u00c2METROS\")\nprint(\"=\" * 60)\n\n# Definindo dimens\u00f5es de uma imagem t\u00edpica\naltura, largura, canais = 400, 600, 3\ntotal_pixels = altura * largura * canais\n\nprint(f\"\ud83d\udcd0 Imagem exemplo: {altura}\u00d7{largura}\u00d7{canais} = {total_pixels:,} pixels\")\n\n# MLP Tradicional\nprint(f\"\\n\ud83e\udde0 MLP TRADICIONAL:\")\nprint(f\"   Entrada: {total_pixels:,} pixels (flattened)\")\nprint(f\"   Primeira camada: 100 neur\u00f4nios\")\nparametros_mlp = total_pixels * 100 + 100  # pesos + bias\nprint(f\"   Par\u00e2metros primeira camada: {parametros_mlp:,}\")\nprint(f\"   \ud83d\udcbe Mem\u00f3ria aproximada: {parametros_mlp * 4 / 1024**2:.2f} MB\")\n\n# CNN Equivalente\nprint(f\"\\n\ud83d\udd0d CNN EQUIVALENTE:\")\nprint(f\"   Entrada: {altura}\u00d7{largura}\u00d7{canais}\")\nprint(f\"   Conv2D: 32 filtros 3\u00d73\")\nparametros_cnn = (3 * 3 * canais * 32) + 32  # kernel_size \u00d7 input_channels \u00d7 filters + bias\nprint(f\"   Par\u00e2metros primeira camada: {parametros_cnn:,}\")\nprint(f\"   \ud83d\udcbe Mem\u00f3ria aproximada: {parametros_cnn * 4 / 1024**2:.4f} MB\")\n\nprint(f\"\\n\ud83d\udcca COMPARA\u00c7\u00c3O:\")\nreducao = parametros_mlp / parametros_cnn\nprint(f\"   \ud83d\udd3b Redu\u00e7\u00e3o de par\u00e2metros: {reducao:.0f}x\")\nprint(f\"   \ud83d\udcb0 Economia de mem\u00f3ria: {(1 - parametros_cnn/parametros_mlp)*100:.1f}%\")\n</pre> # Demonstra\u00e7\u00e3o: MLP vs CNN - N\u00famero de Par\u00e2metros print(\"\ud83d\udd22 COMPARA\u00c7\u00c3O: MLP vs CNN - N\u00daMERO DE PAR\u00c2METROS\") print(\"=\" * 60)  # Definindo dimens\u00f5es de uma imagem t\u00edpica altura, largura, canais = 400, 600, 3 total_pixels = altura * largura * canais  print(f\"\ud83d\udcd0 Imagem exemplo: {altura}\u00d7{largura}\u00d7{canais} = {total_pixels:,} pixels\")  # MLP Tradicional print(f\"\\n\ud83e\udde0 MLP TRADICIONAL:\") print(f\"   Entrada: {total_pixels:,} pixels (flattened)\") print(f\"   Primeira camada: 100 neur\u00f4nios\") parametros_mlp = total_pixels * 100 + 100  # pesos + bias print(f\"   Par\u00e2metros primeira camada: {parametros_mlp:,}\") print(f\"   \ud83d\udcbe Mem\u00f3ria aproximada: {parametros_mlp * 4 / 1024**2:.2f} MB\")  # CNN Equivalente print(f\"\\n\ud83d\udd0d CNN EQUIVALENTE:\") print(f\"   Entrada: {altura}\u00d7{largura}\u00d7{canais}\") print(f\"   Conv2D: 32 filtros 3\u00d73\") parametros_cnn = (3 * 3 * canais * 32) + 32  # kernel_size \u00d7 input_channels \u00d7 filters + bias print(f\"   Par\u00e2metros primeira camada: {parametros_cnn:,}\") print(f\"   \ud83d\udcbe Mem\u00f3ria aproximada: {parametros_cnn * 4 / 1024**2:.4f} MB\")  print(f\"\\n\ud83d\udcca COMPARA\u00c7\u00c3O:\") reducao = parametros_mlp / parametros_cnn print(f\"   \ud83d\udd3b Redu\u00e7\u00e3o de par\u00e2metros: {reducao:.0f}x\") print(f\"   \ud83d\udcb0 Economia de mem\u00f3ria: {(1 - parametros_cnn/parametros_mlp)*100:.1f}%\") In\u00a0[\u00a0]: Copied! <pre># Criando os modelos para compara\u00e7\u00e3o visual\nprint(\"\ud83c\udfd7\ufe0f CONSTRUINDO MODELOS PARA COMPARA\u00c7\u00c3O\")\nprint(\"=\" * 50)\n\n# Modelo MLP (simplificado para demonstra\u00e7\u00e3o)\nmlp_model = keras.Sequential([\n    layers.Flatten(input_shape=(28, 28, 1)),  # MNIST para exemplo pr\u00e1tico\n    layers.Dense(128, activation='relu'),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')\n], name=\"MLP_Model\")\n\n# Modelo CNN equivalente\ncnn_model = keras.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')\n], name=\"CNN_Model\")\n\nprint(\"\ud83d\udccb RESUMO DOS MODELOS:\")\nprint(\"\\n\ud83e\udde0 MLP Model:\")\nmlp_model.summary()\n\nprint(\"\\n\ud83d\udd0d CNN Model:\")\ncnn_model.summary()\n\n# Compara\u00e7\u00e3o de par\u00e2metros\nmlp_params = mlp_model.count_params()\ncnn_params = cnn_model.count_params()\n\nprint(f\"\\n\ud83d\udcca COMPARA\u00c7\u00c3O FINAL:\")\nprint(f\"   MLP par\u00e2metros: {mlp_params:,}\")\nprint(f\"   CNN par\u00e2metros: {cnn_params:,}\")\nprint(f\"   CNN \u00e9 {mlp_params/cnn_params:.1f}x mais eficiente!\")\n</pre> # Criando os modelos para compara\u00e7\u00e3o visual print(\"\ud83c\udfd7\ufe0f CONSTRUINDO MODELOS PARA COMPARA\u00c7\u00c3O\") print(\"=\" * 50)  # Modelo MLP (simplificado para demonstra\u00e7\u00e3o) mlp_model = keras.Sequential([     layers.Flatten(input_shape=(28, 28, 1)),  # MNIST para exemplo pr\u00e1tico     layers.Dense(128, activation='relu'),     layers.Dense(64, activation='relu'),     layers.Dense(10, activation='softmax') ], name=\"MLP_Model\")  # Modelo CNN equivalente cnn_model = keras.Sequential([     layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),     layers.MaxPooling2D((2, 2)),     layers.Conv2D(64, (3, 3), activation='relu'),     layers.MaxPooling2D((2, 2)),     layers.Flatten(),     layers.Dense(64, activation='relu'),     layers.Dense(10, activation='softmax') ], name=\"CNN_Model\")  print(\"\ud83d\udccb RESUMO DOS MODELOS:\") print(\"\\n\ud83e\udde0 MLP Model:\") mlp_model.summary()  print(\"\\n\ud83d\udd0d CNN Model:\") cnn_model.summary()  # Compara\u00e7\u00e3o de par\u00e2metros mlp_params = mlp_model.count_params() cnn_params = cnn_model.count_params()  print(f\"\\n\ud83d\udcca COMPARA\u00c7\u00c3O FINAL:\") print(f\"   MLP par\u00e2metros: {mlp_params:,}\") print(f\"   CNN par\u00e2metros: {cnn_params:,}\") print(f\"   CNN \u00e9 {mlp_params/cnn_params:.1f}x mais eficiente!\") In\u00a0[\u00a0]: Copied! <pre># Demonstra\u00e7\u00e3o pr\u00e1tica da opera\u00e7\u00e3o de convolu\u00e7\u00e3o\nprint(\"\ud83d\udd2c DEMONSTRA\u00c7\u00c3O PR\u00c1TICA: OPERA\u00c7\u00c3O DE CONVOLU\u00c7\u00c3O\")\nprint(\"=\" * 60)\n\n# Criando uma imagem simples para demonstra\u00e7\u00e3o\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Imagem 5x5 simples\nimagem = np.array([\n    [1, 2, 3, 0, 1],\n    [0, 1, 2, 3, 1], \n    [1, 0, 1, 2, 0],\n    [2, 1, 0, 1, 2],\n    [1, 0, 2, 1, 0]\n])\n\n# Kernel detector de borda vertical\nkernel_vertical = np.array([\n    [-1, 0, 1],\n    [-1, 0, 1],\n    [-1, 0, 1]\n])\n\n# Kernel detector de borda horizontal  \nkernel_horizontal = np.array([\n    [-1, -1, -1],\n    [ 0,  0,  0],\n    [ 1,  1,  1]\n])\n\n# Kernel detector de borda (Laplaciano)\nkernel_borda = np.array([\n    [-1, -1, -1],\n    [-1,  8, -1],\n    [-1, -1, -1]\n])\n\ndef aplicar_convolucao_manual(imagem, kernel):\n    \"\"\"Aplica convolu\u00e7\u00e3o manualmente para demonstra\u00e7\u00e3o\"\"\"\n    img_h, img_w = imagem.shape\n    kernel_h, kernel_w = kernel.shape\n    \n    # Tamanho da sa\u00edda\n    out_h = img_h - kernel_h + 1\n    out_w = img_w - kernel_w + 1\n    \n    resultado = np.zeros((out_h, out_w))\n    \n    for i in range(out_h):\n        for j in range(out_w):\n            # Regi\u00e3o da imagem\n            regiao = imagem[i:i+kernel_h, j:j+kernel_w]\n            # Produto elemento a elemento + soma\n            resultado[i, j] = np.sum(regiao * kernel)\n    \n    return resultado\n\n# Aplicando diferentes kernels\nresultado_vertical = aplicar_convolucao_manual(imagem, kernel_vertical)\nresultado_horizontal = aplicar_convolucao_manual(imagem, kernel_horizontal)\nresultado_borda = aplicar_convolucao_manual(imagem, kernel_borda)\n\n# Visualiza\u00e7\u00e3o\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\n\n# Imagem original\nim1 = axes[0,0].imshow(imagem, cmap='gray')\naxes[0,0].set_title('\ud83d\uddbc\ufe0f Imagem Original')\naxes[0,0].set_xticks([])\naxes[0,0].set_yticks([])\n\n# Kernels\nim2 = axes[0,1].imshow(kernel_vertical, cmap='RdBu')\naxes[0,1].set_title('\ud83d\udd0d Kernel Vertical')\naxes[0,1].set_xticks([])\naxes[0,1].set_yticks([])\n\nim3 = axes[0,2].imshow(kernel_horizontal, cmap='RdBu')\naxes[0,2].set_title('\ud83d\udd0d Kernel Horizontal')\naxes[0,2].set_xticks([])\naxes[0,2].set_yticks([])\n\nim4 = axes[0,3].imshow(kernel_borda, cmap='RdBu')\naxes[0,3].set_title('\ud83d\udd0d Kernel Borda')\naxes[0,3].set_xticks([])\naxes[0,3].set_yticks([])\n\n# Resultados\naxes[1,0].text(0.5, 0.5, 'Resultados \u2192', ha='center', va='center', \n               transform=axes[1,0].transAxes, fontsize=12, fontweight='bold')\naxes[1,0].set_xticks([])\naxes[1,0].set_yticks([])\n\nim5 = axes[1,1].imshow(resultado_vertical, cmap='RdBu')\naxes[1,1].set_title('\ud83d\udcca Bordas Verticais')\naxes[1,1].set_xticks([])\naxes[1,1].set_yticks([])\n\nim6 = axes[1,2].imshow(resultado_horizontal, cmap='RdBu')\naxes[1,2].set_title('\ud83d\udcca Bordas Horizontais') \naxes[1,2].set_xticks([])\naxes[1,2].set_yticks([])\n\nim7 = axes[1,3].imshow(resultado_borda, cmap='RdBu')\naxes[1,3].set_title('\ud83d\udcca Todas as Bordas')\naxes[1,3].set_xticks([])\naxes[1,3].set_yticks([])\n\nplt.tight_layout()\nplt.suptitle('\ud83d\udd2c Demonstra\u00e7\u00e3o: Diferentes Kernels = Diferentes Caracter\u00edsticas', \n             fontsize=14, fontweight='bold', y=1.02)\nplt.show()\n\nprint(\"\u2705 Observe como cada kernel detecta caracter\u00edsticas diferentes!\")\nprint(\"\ud83d\udcdd Os valores nos feature maps indicam a 'for\u00e7a' da caracter\u00edstica detectada\")\n</pre> # Demonstra\u00e7\u00e3o pr\u00e1tica da opera\u00e7\u00e3o de convolu\u00e7\u00e3o print(\"\ud83d\udd2c DEMONSTRA\u00c7\u00c3O PR\u00c1TICA: OPERA\u00c7\u00c3O DE CONVOLU\u00c7\u00c3O\") print(\"=\" * 60)  # Criando uma imagem simples para demonstra\u00e7\u00e3o import numpy as np import matplotlib.pyplot as plt  # Imagem 5x5 simples imagem = np.array([     [1, 2, 3, 0, 1],     [0, 1, 2, 3, 1],      [1, 0, 1, 2, 0],     [2, 1, 0, 1, 2],     [1, 0, 2, 1, 0] ])  # Kernel detector de borda vertical kernel_vertical = np.array([     [-1, 0, 1],     [-1, 0, 1],     [-1, 0, 1] ])  # Kernel detector de borda horizontal   kernel_horizontal = np.array([     [-1, -1, -1],     [ 0,  0,  0],     [ 1,  1,  1] ])  # Kernel detector de borda (Laplaciano) kernel_borda = np.array([     [-1, -1, -1],     [-1,  8, -1],     [-1, -1, -1] ])  def aplicar_convolucao_manual(imagem, kernel):     \"\"\"Aplica convolu\u00e7\u00e3o manualmente para demonstra\u00e7\u00e3o\"\"\"     img_h, img_w = imagem.shape     kernel_h, kernel_w = kernel.shape          # Tamanho da sa\u00edda     out_h = img_h - kernel_h + 1     out_w = img_w - kernel_w + 1          resultado = np.zeros((out_h, out_w))          for i in range(out_h):         for j in range(out_w):             # Regi\u00e3o da imagem             regiao = imagem[i:i+kernel_h, j:j+kernel_w]             # Produto elemento a elemento + soma             resultado[i, j] = np.sum(regiao * kernel)          return resultado  # Aplicando diferentes kernels resultado_vertical = aplicar_convolucao_manual(imagem, kernel_vertical) resultado_horizontal = aplicar_convolucao_manual(imagem, kernel_horizontal) resultado_borda = aplicar_convolucao_manual(imagem, kernel_borda)  # Visualiza\u00e7\u00e3o fig, axes = plt.subplots(2, 4, figsize=(16, 8))  # Imagem original im1 = axes[0,0].imshow(imagem, cmap='gray') axes[0,0].set_title('\ud83d\uddbc\ufe0f Imagem Original') axes[0,0].set_xticks([]) axes[0,0].set_yticks([])  # Kernels im2 = axes[0,1].imshow(kernel_vertical, cmap='RdBu') axes[0,1].set_title('\ud83d\udd0d Kernel Vertical') axes[0,1].set_xticks([]) axes[0,1].set_yticks([])  im3 = axes[0,2].imshow(kernel_horizontal, cmap='RdBu') axes[0,2].set_title('\ud83d\udd0d Kernel Horizontal') axes[0,2].set_xticks([]) axes[0,2].set_yticks([])  im4 = axes[0,3].imshow(kernel_borda, cmap='RdBu') axes[0,3].set_title('\ud83d\udd0d Kernel Borda') axes[0,3].set_xticks([]) axes[0,3].set_yticks([])  # Resultados axes[1,0].text(0.5, 0.5, 'Resultados \u2192', ha='center', va='center',                 transform=axes[1,0].transAxes, fontsize=12, fontweight='bold') axes[1,0].set_xticks([]) axes[1,0].set_yticks([])  im5 = axes[1,1].imshow(resultado_vertical, cmap='RdBu') axes[1,1].set_title('\ud83d\udcca Bordas Verticais') axes[1,1].set_xticks([]) axes[1,1].set_yticks([])  im6 = axes[1,2].imshow(resultado_horizontal, cmap='RdBu') axes[1,2].set_title('\ud83d\udcca Bordas Horizontais')  axes[1,2].set_xticks([]) axes[1,2].set_yticks([])  im7 = axes[1,3].imshow(resultado_borda, cmap='RdBu') axes[1,3].set_title('\ud83d\udcca Todas as Bordas') axes[1,3].set_xticks([]) axes[1,3].set_yticks([])  plt.tight_layout() plt.suptitle('\ud83d\udd2c Demonstra\u00e7\u00e3o: Diferentes Kernels = Diferentes Caracter\u00edsticas',               fontsize=14, fontweight='bold', y=1.02) plt.show()  print(\"\u2705 Observe como cada kernel detecta caracter\u00edsticas diferentes!\") print(\"\ud83d\udcdd Os valores nos feature maps indicam a 'for\u00e7a' da caracter\u00edstica detectada\") In\u00a0[\u00a0]: Copied! <pre># Exemplo detalhado: calculando um pixel manualmente\nprint(\"\ud83e\uddee EXEMPLO DETALHADO: CALCULANDO UM PIXEL\")\nprint(\"=\" * 50)\n\nprint(\"\ud83d\uddbc\ufe0f Imagem (regi\u00e3o 3\u00d73):\")\nregiao = imagem[0:3, 0:3]\nprint(regiao)\n\nprint(\"\\n\ud83d\udd0d Kernel (detector de borda):\")\nprint(kernel_borda)\n\nprint(\"\\n\ud83e\uddee C\u00e1lculo passo a passo:\")\nprint(\"Produto elemento a elemento:\")\nproduto = regiao * kernel_borda\nprint(produto)\n\nprint(f\"\\n\u2795 Soma de todos os elementos: {np.sum(produto)}\")\nprint(f\"\ud83d\udcca Resultado para o pixel (0,0) do feature map: {np.sum(produto)}\")\n\nprint(\"\\n\ud83d\udca1 Interpreta\u00e7\u00e3o:\")\nif np.sum(produto) &gt; 0:\n    print(\"   \u2705 Valor positivo \u2192 Borda detectada!\")\nelse:\n    print(\"   \u274c Valor baixo \u2192 Sem borda significativa\")\n</pre> # Exemplo detalhado: calculando um pixel manualmente print(\"\ud83e\uddee EXEMPLO DETALHADO: CALCULANDO UM PIXEL\") print(\"=\" * 50)  print(\"\ud83d\uddbc\ufe0f Imagem (regi\u00e3o 3\u00d73):\") regiao = imagem[0:3, 0:3] print(regiao)  print(\"\\n\ud83d\udd0d Kernel (detector de borda):\") print(kernel_borda)  print(\"\\n\ud83e\uddee C\u00e1lculo passo a passo:\") print(\"Produto elemento a elemento:\") produto = regiao * kernel_borda print(produto)  print(f\"\\n\u2795 Soma de todos os elementos: {np.sum(produto)}\") print(f\"\ud83d\udcca Resultado para o pixel (0,0) do feature map: {np.sum(produto)}\")  print(\"\\n\ud83d\udca1 Interpreta\u00e7\u00e3o:\") if np.sum(produto) &gt; 0:     print(\"   \u2705 Valor positivo \u2192 Borda detectada!\") else:     print(\"   \u274c Valor baixo \u2192 Sem borda significativa\") In\u00a0[\u00a0]: Copied! <pre># Exemplo pr\u00e1tico: implementando camada convolucional\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nprint(\"\ud83d\udcbb IMPLEMENTA\u00c7\u00c3O: CAMADA CONVOLUCIONAL\")\nprint(\"=\" * 50)\n\n# Criando uma camada convolucional simples\nmodel_conv = keras.Sequential([\n    layers.Conv2D(\n        filters=100, \n        kernel_size=(3, 3), \n        activation='relu', \n        input_shape=(800, 600, 3),\n        name=\"conv_layer_1\"\n    ),\n])\n\nprint(\"\ud83d\udccb RESUMO DO MODELO:\")\nmodel_conv.summary()\n\nprint(f\"\\n\ud83d\udd0d AN\u00c1LISE DA CAMADA:\")\nprint(f\"   \ud83d\udcd0 Entrada: (800, 600, 3)\")\nprint(f\"   \ud83d\udd22 Filtros: 100\")\nprint(f\"   \ud83d\udccf Kernel: 3\u00d73\")\nprint(f\"   \ud83d\udcca Sa\u00edda: (798, 798, 100)\")  # 800-3+1 = 798\nprint(f\"   \ud83c\udfaf Par\u00e2metros: {(3*3*3 + 1) * 100:,}\")  # (kernel\u00d7channels + bias) \u00d7 filters\n\n# Calculando par\u00e2metros manualmente para verifica\u00e7\u00e3o\nkernel_params = 3 * 3 * 3  # kernel_size \u00d7 kernel_size \u00d7 input_channels\nbias_params = 1\ntotal_per_filter = kernel_params + bias_params\ntotal_params = total_per_filter * 100  # \u00d7 number of filters\n\nprint(f\"\\n\ud83e\uddee C\u00c1LCULO DE PAR\u00c2METROS:\")\nprint(f\"   Por filtro: {kernel_params} (pesos) + {bias_params} (bias) = {total_per_filter}\")\nprint(f\"   Total: {total_per_filter} \u00d7 {100} filtros = {total_params:,} par\u00e2metros\")\n\nprint(\"\\n\ud83d\udca1 OBSERVA\u00c7\u00d5ES:\")\nprint(\"   \u2705 Cada filtro aprende a detectar uma caracter\u00edstica espec\u00edfica\")\nprint(\"   \u2705 Mais filtros = mais caracter\u00edsticas detectadas\")\nprint(\"   \u2705 Kernel 3\u00d73 \u00e9 o mais comum (bom balance efici\u00eancia/expressividade)\")\n</pre> # Exemplo pr\u00e1tico: implementando camada convolucional import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers  print(\"\ud83d\udcbb IMPLEMENTA\u00c7\u00c3O: CAMADA CONVOLUCIONAL\") print(\"=\" * 50)  # Criando uma camada convolucional simples model_conv = keras.Sequential([     layers.Conv2D(         filters=100,          kernel_size=(3, 3),          activation='relu',          input_shape=(800, 600, 3),         name=\"conv_layer_1\"     ), ])  print(\"\ud83d\udccb RESUMO DO MODELO:\") model_conv.summary()  print(f\"\\n\ud83d\udd0d AN\u00c1LISE DA CAMADA:\") print(f\"   \ud83d\udcd0 Entrada: (800, 600, 3)\") print(f\"   \ud83d\udd22 Filtros: 100\") print(f\"   \ud83d\udccf Kernel: 3\u00d73\") print(f\"   \ud83d\udcca Sa\u00edda: (798, 798, 100)\")  # 800-3+1 = 798 print(f\"   \ud83c\udfaf Par\u00e2metros: {(3*3*3 + 1) * 100:,}\")  # (kernel\u00d7channels + bias) \u00d7 filters  # Calculando par\u00e2metros manualmente para verifica\u00e7\u00e3o kernel_params = 3 * 3 * 3  # kernel_size \u00d7 kernel_size \u00d7 input_channels bias_params = 1 total_per_filter = kernel_params + bias_params total_params = total_per_filter * 100  # \u00d7 number of filters  print(f\"\\n\ud83e\uddee C\u00c1LCULO DE PAR\u00c2METROS:\") print(f\"   Por filtro: {kernel_params} (pesos) + {bias_params} (bias) = {total_per_filter}\") print(f\"   Total: {total_per_filter} \u00d7 {100} filtros = {total_params:,} par\u00e2metros\")  print(\"\\n\ud83d\udca1 OBSERVA\u00c7\u00d5ES:\") print(\"   \u2705 Cada filtro aprende a detectar uma caracter\u00edstica espec\u00edfica\") print(\"   \u2705 Mais filtros = mais caracter\u00edsticas detectadas\") print(\"   \u2705 Kernel 3\u00d73 \u00e9 o mais comum (bom balance efici\u00eancia/expressividade)\") In\u00a0[\u00a0]: Copied! <pre># Exemplo com diferentes configura\u00e7\u00f5es\nprint(\"\ud83c\udf9b\ufe0f EXPLORANDO DIFERENTES CONFIGURA\u00c7\u00d5ES\")\nprint(\"=\" * 50)\n\n# Testando diferentes configura\u00e7\u00f5es\nconfigs = [\n    {\"filters\": 32, \"kernel_size\": (3,3), \"nome\": \"Config B\u00e1sica\"},\n    {\"filters\": 64, \"kernel_size\": (5,5), \"nome\": \"Kernel Maior\"},\n    {\"filters\": 128, \"kernel_size\": (1,1), \"nome\": \"Pointwise Conv\"},\n    {\"filters\": 32, \"kernel_size\": (3,3), \"strides\": (2,2), \"nome\": \"Stride 2\"},\n]\n\nfor i, config in enumerate(configs):\n    print(f\"\\n{i+1}\ufe0f\u20e3 {config['nome']}:\")\n    \n    # Removendo 'nome' para criar a camada\n    layer_config = {k: v for k, v in config.items() if k != 'nome'}\n    \n    model_temp = keras.Sequential([\n        layers.Conv2D(**layer_config, input_shape=(64, 64, 3))\n    ])\n    \n    # An\u00e1lise da configura\u00e7\u00e3o\n    output_shape = model_temp.layers[0].output_shape[1:]  # Remove batch dimension\n    params = model_temp.count_params()\n    \n    print(f\"   \ud83d\udcca Sa\u00edda: {output_shape}\")\n    print(f\"   \ud83d\udd22 Par\u00e2metros: {params:,}\")\n    \n    # Interpreta\u00e7\u00e3o\n    if 'strides' in config and config['strides'] == (2,2):\n        print(\"   \ud83d\udca1 Stride 2 \u2192 reduz dimens\u00e3o pela metade\")\n    if config['kernel_size'] == (1,1):\n        print(\"   \ud83d\udca1 Kernel 1\u00d71 \u2192 combina canais sem considerar vizinhan\u00e7a espacial\")\n    if config['kernel_size'] == (5,5):\n        print(\"   \ud83d\udca1 Kernel 5\u00d75 \u2192 campo receptivo maior, mais contexto\")\n\nprint(f\"\\n\ud83d\udcc8 RESUMO:\")\nprint(\"   \u2022 Mais filtros \u2192 mais caracter\u00edsticas detectadas\")\nprint(\"   \u2022 Kernel maior \u2192 mais contexto, mais par\u00e2metros\")\nprint(\"   \u2022 Stride &gt; 1 \u2192 redu\u00e7\u00e3o de dimensionalidade\")\nprint(\"   \u2022 Kernel 1\u00d71 \u2192 redu\u00e7\u00e3o/expans\u00e3o de canais eficiente\")\n</pre> # Exemplo com diferentes configura\u00e7\u00f5es print(\"\ud83c\udf9b\ufe0f EXPLORANDO DIFERENTES CONFIGURA\u00c7\u00d5ES\") print(\"=\" * 50)  # Testando diferentes configura\u00e7\u00f5es configs = [     {\"filters\": 32, \"kernel_size\": (3,3), \"nome\": \"Config B\u00e1sica\"},     {\"filters\": 64, \"kernel_size\": (5,5), \"nome\": \"Kernel Maior\"},     {\"filters\": 128, \"kernel_size\": (1,1), \"nome\": \"Pointwise Conv\"},     {\"filters\": 32, \"kernel_size\": (3,3), \"strides\": (2,2), \"nome\": \"Stride 2\"}, ]  for i, config in enumerate(configs):     print(f\"\\n{i+1}\ufe0f\u20e3 {config['nome']}:\")          # Removendo 'nome' para criar a camada     layer_config = {k: v for k, v in config.items() if k != 'nome'}          model_temp = keras.Sequential([         layers.Conv2D(**layer_config, input_shape=(64, 64, 3))     ])          # An\u00e1lise da configura\u00e7\u00e3o     output_shape = model_temp.layers[0].output_shape[1:]  # Remove batch dimension     params = model_temp.count_params()          print(f\"   \ud83d\udcca Sa\u00edda: {output_shape}\")     print(f\"   \ud83d\udd22 Par\u00e2metros: {params:,}\")          # Interpreta\u00e7\u00e3o     if 'strides' in config and config['strides'] == (2,2):         print(\"   \ud83d\udca1 Stride 2 \u2192 reduz dimens\u00e3o pela metade\")     if config['kernel_size'] == (1,1):         print(\"   \ud83d\udca1 Kernel 1\u00d71 \u2192 combina canais sem considerar vizinhan\u00e7a espacial\")     if config['kernel_size'] == (5,5):         print(\"   \ud83d\udca1 Kernel 5\u00d75 \u2192 campo receptivo maior, mais contexto\")  print(f\"\\n\ud83d\udcc8 RESUMO:\") print(\"   \u2022 Mais filtros \u2192 mais caracter\u00edsticas detectadas\") print(\"   \u2022 Kernel maior \u2192 mais contexto, mais par\u00e2metros\") print(\"   \u2022 Stride &gt; 1 \u2192 redu\u00e7\u00e3o de dimensionalidade\") print(\"   \u2022 Kernel 1\u00d71 \u2192 redu\u00e7\u00e3o/expans\u00e3o de canais eficiente\") In\u00a0[\u00a0]: Copied! <pre># \ud83c\udfaf DESAFIO 1: EXPERIMENTO PR\u00c1TICO\nprint(\"\ud83c\udfaf DESAFIO 1: AN\u00c1LISE COMPARATIVA DE PAR\u00c2METROS\")\nprint(\"=\" * 60)\n\n# Vamos comparar sistematicamente\ntamanhos_imagem = [(28, 28), (64, 64), (128, 128), (224, 224)]\n\nprint(\"\ud83d\udcca COMPARA\u00c7\u00c3O SISTEM\u00c1TICA: MLP vs CNN\")\nprint(\"=\" * 60)\n\nfor altura, largura in tamanhos_imagem:\n    print(f\"\\n\ud83d\uddbc\ufe0f IMAGEM {altura}\u00d7{largura}\u00d73:\")\n    print(\"-\" * 40)\n    \n    # MLP Model\n    mlp_temp = keras.Sequential([\n        layers.Flatten(input_shape=(altura, largura, 3)),\n        layers.Dense(128, activation='relu'),\n        layers.Dense(10, activation='softmax')\n    ])\n    \n    # CNN Model  \n    cnn_temp = keras.Sequential([\n        layers.Conv2D(32, (3,3), activation='relu', input_shape=(altura, largura, 3)),\n        layers.MaxPooling2D((2,2)),\n        layers.Conv2D(64, (3,3), activation='relu'),\n        layers.MaxPooling2D((2,2)),\n        layers.Flatten(),\n        layers.Dense(128, activation='relu'),\n        layers.Dense(10, activation='softmax')\n    ])\n    \n    mlp_params = mlp_temp.count_params()\n    cnn_params = cnn_temp.count_params()\n    razao = mlp_params / cnn_params\n    \n    print(f\"   \ud83e\udde0 MLP par\u00e2metros: {mlp_params:,}\")\n    print(f\"   \ud83d\udd0d CNN par\u00e2metros: {cnn_params:,}\")\n    print(f\"   \ud83d\udcc8 Raz\u00e3o MLP/CNN: {razao:.1f}x\")\n    \n    if razao &gt; 1:\n        print(f\"   \u2705 CNN \u00e9 {razao:.1f}x mais eficiente!\")\n    else:\n        print(f\"   \u26a0\ufe0f MLP \u00e9 mais eficiente para esta configura\u00e7\u00e3o\")\n\nprint(f\"\\n\ud83c\udfaf SUA AN\u00c1LISE:\")\nprint(\"=\" * 30)\nprint(\"Com base nos resultados acima, complete:\")\nprint(\"\\n1\ufe0f\u20e3 Em geral, CNNs t\u00eam _______ par\u00e2metros que MLPs\")\nprint(\"2\ufe0f\u20e3 Isso acontece porque CNNs usam _______\")\nprint(\"3\ufe0f\u20e3 Quando a imagem fica maior, a diferen\u00e7a _______\")\nprint(\"4\ufe0f\u20e3 Para imagens pequenas, a vantagem da CNN _______\")\n\nprint(f\"\\n\ud83d\udcad REFLEX\u00c3O:\")\nprint(\"Por que a CNN mant\u00e9m vantagem mesmo com imagens grandes?\")\nprint(\"Sua resposta: ________________________________\")\n\n# COMPLETE SUAS RESPOSTAS AQUI:\nprint(f\"\\n\ud83d\udcdd SUAS RESPOSTAS:\")\nresposta_1 = \"______\"  # menor/maior\nresposta_2 = \"______\"  # compartilhamento de pesos/mais camadas/etc\nresposta_3 = \"______\"  # aumenta/diminui/mant\u00e9m\nresposta_4 = \"______\"  # \u00e9 maior/\u00e9 menor/desaparece\n\nprint(f\"1\ufe0f\u20e3 Em geral, CNNs t\u00eam {resposta_1} par\u00e2metros que MLPs\")\nprint(f\"2\ufe0f\u20e3 Isso acontece porque CNNs usam {resposta_2}\")\nprint(f\"3\ufe0f\u20e3 Quando a imagem fica maior, a diferen\u00e7a {resposta_3}\")\nprint(f\"4\ufe0f\u20e3 Para imagens pequenas, a vantagem da CNN {resposta_4}\")\n</pre> # \ud83c\udfaf DESAFIO 1: EXPERIMENTO PR\u00c1TICO print(\"\ud83c\udfaf DESAFIO 1: AN\u00c1LISE COMPARATIVA DE PAR\u00c2METROS\") print(\"=\" * 60)  # Vamos comparar sistematicamente tamanhos_imagem = [(28, 28), (64, 64), (128, 128), (224, 224)]  print(\"\ud83d\udcca COMPARA\u00c7\u00c3O SISTEM\u00c1TICA: MLP vs CNN\") print(\"=\" * 60)  for altura, largura in tamanhos_imagem:     print(f\"\\n\ud83d\uddbc\ufe0f IMAGEM {altura}\u00d7{largura}\u00d73:\")     print(\"-\" * 40)          # MLP Model     mlp_temp = keras.Sequential([         layers.Flatten(input_shape=(altura, largura, 3)),         layers.Dense(128, activation='relu'),         layers.Dense(10, activation='softmax')     ])          # CNN Model       cnn_temp = keras.Sequential([         layers.Conv2D(32, (3,3), activation='relu', input_shape=(altura, largura, 3)),         layers.MaxPooling2D((2,2)),         layers.Conv2D(64, (3,3), activation='relu'),         layers.MaxPooling2D((2,2)),         layers.Flatten(),         layers.Dense(128, activation='relu'),         layers.Dense(10, activation='softmax')     ])          mlp_params = mlp_temp.count_params()     cnn_params = cnn_temp.count_params()     razao = mlp_params / cnn_params          print(f\"   \ud83e\udde0 MLP par\u00e2metros: {mlp_params:,}\")     print(f\"   \ud83d\udd0d CNN par\u00e2metros: {cnn_params:,}\")     print(f\"   \ud83d\udcc8 Raz\u00e3o MLP/CNN: {razao:.1f}x\")          if razao &gt; 1:         print(f\"   \u2705 CNN \u00e9 {razao:.1f}x mais eficiente!\")     else:         print(f\"   \u26a0\ufe0f MLP \u00e9 mais eficiente para esta configura\u00e7\u00e3o\")  print(f\"\\n\ud83c\udfaf SUA AN\u00c1LISE:\") print(\"=\" * 30) print(\"Com base nos resultados acima, complete:\") print(\"\\n1\ufe0f\u20e3 Em geral, CNNs t\u00eam _______ par\u00e2metros que MLPs\") print(\"2\ufe0f\u20e3 Isso acontece porque CNNs usam _______\") print(\"3\ufe0f\u20e3 Quando a imagem fica maior, a diferen\u00e7a _______\") print(\"4\ufe0f\u20e3 Para imagens pequenas, a vantagem da CNN _______\")  print(f\"\\n\ud83d\udcad REFLEX\u00c3O:\") print(\"Por que a CNN mant\u00e9m vantagem mesmo com imagens grandes?\") print(\"Sua resposta: ________________________________\")  # COMPLETE SUAS RESPOSTAS AQUI: print(f\"\\n\ud83d\udcdd SUAS RESPOSTAS:\") resposta_1 = \"______\"  # menor/maior resposta_2 = \"______\"  # compartilhamento de pesos/mais camadas/etc resposta_3 = \"______\"  # aumenta/diminui/mant\u00e9m resposta_4 = \"______\"  # \u00e9 maior/\u00e9 menor/desaparece  print(f\"1\ufe0f\u20e3 Em geral, CNNs t\u00eam {resposta_1} par\u00e2metros que MLPs\") print(f\"2\ufe0f\u20e3 Isso acontece porque CNNs usam {resposta_2}\") print(f\"3\ufe0f\u20e3 Quando a imagem fica maior, a diferen\u00e7a {resposta_3}\") print(f\"4\ufe0f\u20e3 Para imagens pequenas, a vantagem da CNN {resposta_4}\") In\u00a0[\u00a0]: Copied! <pre># Demonstra\u00e7\u00e3o pr\u00e1tica do pooling\nprint(\"\ud83c\udfca DEMONSTRA\u00c7\u00c3O PR\u00c1TICA: OPERA\u00c7\u00d5ES DE POOLING\")\nprint(\"=\" * 60)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Criando uma imagem exemplo para demonstra\u00e7\u00e3o\nnp.random.seed(42)\nimagem_exemplo = np.random.randint(0, 10, (8, 8))\n\nprint(\"\ud83d\uddbc\ufe0f IMAGEM EXEMPLO (8\u00d78):\")\nprint(imagem_exemplo)\n\ndef max_pooling_manual(imagem, pool_size=2, stride=2):\n    \"\"\"Implementa\u00e7\u00e3o manual do max pooling para demonstra\u00e7\u00e3o\"\"\"\n    h, w = imagem.shape\n    out_h = (h - pool_size) // stride + 1\n    out_w = (w - pool_size) // stride + 1\n    \n    resultado = np.zeros((out_h, out_w))\n    \n    for i in range(out_h):\n        for j in range(out_w):\n            h_start = i * stride\n            h_end = h_start + pool_size\n            w_start = j * stride  \n            w_end = w_start + pool_size\n            \n            # Max pooling\n            resultado[i, j] = np.max(imagem[h_start:h_end, w_start:w_end])\n    \n    return resultado\n\ndef avg_pooling_manual(imagem, pool_size=2, stride=2):\n    \"\"\"Implementa\u00e7\u00e3o manual do average pooling\"\"\"\n    h, w = imagem.shape\n    out_h = (h - pool_size) // stride + 1\n    out_w = (w - pool_size) // stride + 1\n    \n    resultado = np.zeros((out_h, out_w))\n    \n    for i in range(out_h):\n        for j in range(out_w):\n            h_start = i * stride\n            h_end = h_start + pool_size\n            w_start = j * stride\n            w_end = w_start + pool_size\n            \n            # Average pooling\n            resultado[i, j] = np.mean(imagem[h_start:h_end, w_start:w_end])\n    \n    return resultado\n\n# Aplicando diferentes tipos de pooling\nmax_pool_result = max_pooling_manual(imagem_exemplo)\navg_pool_result = avg_pooling_manual(imagem_exemplo)\n\n# Visualiza\u00e7\u00e3o\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Imagem original\nim1 = axes[0].imshow(imagem_exemplo, cmap='viridis', interpolation='nearest')\naxes[0].set_title('\ud83d\uddbc\ufe0f Imagem Original (8\u00d78)')\naxes[0].set_xticks(range(8))\naxes[0].set_yticks(range(8))\nplt.colorbar(im1, ax=axes[0])\n\n# Max pooling\nim2 = axes[1].imshow(max_pool_result, cmap='viridis', interpolation='nearest')\naxes[1].set_title('\ud83d\udd25 Max Pooling (4\u00d74)')\naxes[1].set_xticks(range(4))\naxes[1].set_yticks(range(4))\nplt.colorbar(im2, ax=axes[1])\n\n# Average pooling\nim3 = axes[2].imshow(avg_pool_result, cmap='viridis', interpolation='nearest')\naxes[2].set_title('\ud83d\udcca Average Pooling (4\u00d74)')\naxes[2].set_xticks(range(4))\naxes[2].set_yticks(range(4))\nplt.colorbar(im3, ax=axes[2])\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n\ud83d\udcd0 RESULTADOS:\")\nprint(f\"   Original: {imagem_exemplo.shape}\")\nprint(f\"   Max Pooling: {max_pool_result.shape}\")\nprint(f\"   Average Pooling: {avg_pool_result.shape}\")\nprint(f\"   Redu\u00e7\u00e3o: {imagem_exemplo.size // max_pool_result.size}x menos pixels\")\n\nprint(f\"\\n\ud83d\udd0d COMPARA\u00c7\u00c3O DE VALORES:\")\nprint(\"Max Pooling resultado:\")\nprint(max_pool_result.astype(int))\nprint(\"\\nAverage Pooling resultado:\")\nprint(np.round(avg_pool_result, 1))\n\nprint(\"\\n\ud83d\udca1 OBSERVA\u00c7\u00d5ES:\")\nprint(\"   \u2705 Max pooling preserva caracter\u00edsticas mais 'fortes'\")\nprint(\"   \u2705 Average pooling suaviza e reduz ru\u00eddo\")\nprint(\"   \u2705 Ambos reduzem dimensionalidade sem par\u00e2metros trein\u00e1veis\")\n</pre> # Demonstra\u00e7\u00e3o pr\u00e1tica do pooling print(\"\ud83c\udfca DEMONSTRA\u00c7\u00c3O PR\u00c1TICA: OPERA\u00c7\u00d5ES DE POOLING\") print(\"=\" * 60)  import numpy as np import matplotlib.pyplot as plt  # Criando uma imagem exemplo para demonstra\u00e7\u00e3o np.random.seed(42) imagem_exemplo = np.random.randint(0, 10, (8, 8))  print(\"\ud83d\uddbc\ufe0f IMAGEM EXEMPLO (8\u00d78):\") print(imagem_exemplo)  def max_pooling_manual(imagem, pool_size=2, stride=2):     \"\"\"Implementa\u00e7\u00e3o manual do max pooling para demonstra\u00e7\u00e3o\"\"\"     h, w = imagem.shape     out_h = (h - pool_size) // stride + 1     out_w = (w - pool_size) // stride + 1          resultado = np.zeros((out_h, out_w))          for i in range(out_h):         for j in range(out_w):             h_start = i * stride             h_end = h_start + pool_size             w_start = j * stride               w_end = w_start + pool_size                          # Max pooling             resultado[i, j] = np.max(imagem[h_start:h_end, w_start:w_end])          return resultado  def avg_pooling_manual(imagem, pool_size=2, stride=2):     \"\"\"Implementa\u00e7\u00e3o manual do average pooling\"\"\"     h, w = imagem.shape     out_h = (h - pool_size) // stride + 1     out_w = (w - pool_size) // stride + 1          resultado = np.zeros((out_h, out_w))          for i in range(out_h):         for j in range(out_w):             h_start = i * stride             h_end = h_start + pool_size             w_start = j * stride             w_end = w_start + pool_size                          # Average pooling             resultado[i, j] = np.mean(imagem[h_start:h_end, w_start:w_end])          return resultado  # Aplicando diferentes tipos de pooling max_pool_result = max_pooling_manual(imagem_exemplo) avg_pool_result = avg_pooling_manual(imagem_exemplo)  # Visualiza\u00e7\u00e3o fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # Imagem original im1 = axes[0].imshow(imagem_exemplo, cmap='viridis', interpolation='nearest') axes[0].set_title('\ud83d\uddbc\ufe0f Imagem Original (8\u00d78)') axes[0].set_xticks(range(8)) axes[0].set_yticks(range(8)) plt.colorbar(im1, ax=axes[0])  # Max pooling im2 = axes[1].imshow(max_pool_result, cmap='viridis', interpolation='nearest') axes[1].set_title('\ud83d\udd25 Max Pooling (4\u00d74)') axes[1].set_xticks(range(4)) axes[1].set_yticks(range(4)) plt.colorbar(im2, ax=axes[1])  # Average pooling im3 = axes[2].imshow(avg_pool_result, cmap='viridis', interpolation='nearest') axes[2].set_title('\ud83d\udcca Average Pooling (4\u00d74)') axes[2].set_xticks(range(4)) axes[2].set_yticks(range(4)) plt.colorbar(im3, ax=axes[2])  plt.tight_layout() plt.show()  print(f\"\\n\ud83d\udcd0 RESULTADOS:\") print(f\"   Original: {imagem_exemplo.shape}\") print(f\"   Max Pooling: {max_pool_result.shape}\") print(f\"   Average Pooling: {avg_pool_result.shape}\") print(f\"   Redu\u00e7\u00e3o: {imagem_exemplo.size // max_pool_result.size}x menos pixels\")  print(f\"\\n\ud83d\udd0d COMPARA\u00c7\u00c3O DE VALORES:\") print(\"Max Pooling resultado:\") print(max_pool_result.astype(int)) print(\"\\nAverage Pooling resultado:\") print(np.round(avg_pool_result, 1))  print(\"\\n\ud83d\udca1 OBSERVA\u00c7\u00d5ES:\") print(\"   \u2705 Max pooling preserva caracter\u00edsticas mais 'fortes'\") print(\"   \u2705 Average pooling suaviza e reduz ru\u00eddo\") print(\"   \u2705 Ambos reduzem dimensionalidade sem par\u00e2metros trein\u00e1veis\") In\u00a0[\u00a0]: Copied! <pre># Exemplo completo: Conv2D + MaxPooling2D\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nprint(\"\ud83c\udfd7\ufe0f EXEMPLO: CONVOLU\u00c7\u00c3O + POOLING\")\nprint(\"=\" * 50)\n\nmodel = keras.Sequential([\n    layers.Conv2D(filters=100, kernel_size=(3, 3), activation='relu', \n                  input_shape=(800, 600, 3), name=\"conv2d_layer\"),\n    layers.MaxPool2D(pool_size=2, strides=2, name=\"maxpool_layer\")\n])\n\nprint(\"\ud83d\udccb RESUMO DO MODELO:\")\nmodel.summary()\n\nprint(\"\\n\ud83d\udd0d AN\u00c1LISE DAS TRANSFORMA\u00c7\u00d5ES:\")\nprint(\"   \ud83d\udcd0 Entrada: (800, 600, 3)\")\nprint(\"   \ud83d\udd04 Ap\u00f3s Conv2D: (798, 798, 100)\")  # 800-3+1 = 798\nprint(\"   \ud83c\udfca Ap\u00f3s MaxPool: (399, 399, 100)\")  # 798/2 = 399\nprint(\"   \ud83d\udcc9 Redu\u00e7\u00e3o total: ~4x menos pixels\")\n\nprint(\"\\n\ud83d\udca1 OBSERVA\u00c7\u00d5ES IMPORTANTES:\")\nprint(\"   \u2705 Pooling N\u00c3O adiciona par\u00e2metros\")\nprint(\"   \u2705 Reduz dimensionalidade espacial\") \nprint(\"   \u2705 Mant\u00e9m n\u00famero de canais\")\nprint(\"   \u2705 Diminui custo computacional\")\n\n# Comparando com e sem pooling\nprint(\"\\n\u2696\ufe0f COMPARA\u00c7\u00c3O: COM vs SEM POOLING\")\nprint(\"-\" * 40)\n\n# Sem pooling\nmodel_sem_pool = keras.Sequential([\n    layers.Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),\n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(10, activation='softmax')\n])\n\n# Com pooling  \nmodel_com_pool = keras.Sequential([\n    layers.Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),\n    layers.MaxPooling2D((2,2)),\n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.MaxPooling2D((2,2)),\n    layers.Flatten(),\n    layers.Dense(10, activation='softmax')\n])\n\nprint(f\"Sem pooling: {model_sem_pool.count_params():,} par\u00e2metros\")\nprint(f\"Com pooling: {model_com_pool.count_params():,} par\u00e2metros\")\nprint(f\"Diferen\u00e7a: {abs(model_sem_pool.count_params() - model_com_pool.count_params()):,} par\u00e2metros\")\n</pre> # Exemplo completo: Conv2D + MaxPooling2D import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers  print(\"\ud83c\udfd7\ufe0f EXEMPLO: CONVOLU\u00c7\u00c3O + POOLING\") print(\"=\" * 50)  model = keras.Sequential([     layers.Conv2D(filters=100, kernel_size=(3, 3), activation='relu',                    input_shape=(800, 600, 3), name=\"conv2d_layer\"),     layers.MaxPool2D(pool_size=2, strides=2, name=\"maxpool_layer\") ])  print(\"\ud83d\udccb RESUMO DO MODELO:\") model.summary()  print(\"\\n\ud83d\udd0d AN\u00c1LISE DAS TRANSFORMA\u00c7\u00d5ES:\") print(\"   \ud83d\udcd0 Entrada: (800, 600, 3)\") print(\"   \ud83d\udd04 Ap\u00f3s Conv2D: (798, 798, 100)\")  # 800-3+1 = 798 print(\"   \ud83c\udfca Ap\u00f3s MaxPool: (399, 399, 100)\")  # 798/2 = 399 print(\"   \ud83d\udcc9 Redu\u00e7\u00e3o total: ~4x menos pixels\")  print(\"\\n\ud83d\udca1 OBSERVA\u00c7\u00d5ES IMPORTANTES:\") print(\"   \u2705 Pooling N\u00c3O adiciona par\u00e2metros\") print(\"   \u2705 Reduz dimensionalidade espacial\")  print(\"   \u2705 Mant\u00e9m n\u00famero de canais\") print(\"   \u2705 Diminui custo computacional\")  # Comparando com e sem pooling print(\"\\n\u2696\ufe0f COMPARA\u00c7\u00c3O: COM vs SEM POOLING\") print(\"-\" * 40)  # Sem pooling model_sem_pool = keras.Sequential([     layers.Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),     layers.Conv2D(64, (3,3), activation='relu'),     layers.Flatten(),     layers.Dense(10, activation='softmax') ])  # Com pooling   model_com_pool = keras.Sequential([     layers.Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),     layers.MaxPooling2D((2,2)),     layers.Conv2D(64, (3,3), activation='relu'),     layers.MaxPooling2D((2,2)),     layers.Flatten(),     layers.Dense(10, activation='softmax') ])  print(f\"Sem pooling: {model_sem_pool.count_params():,} par\u00e2metros\") print(f\"Com pooling: {model_com_pool.count_params():,} par\u00e2metros\") print(f\"Diferen\u00e7a: {abs(model_sem_pool.count_params() - model_com_pool.count_params()):,} par\u00e2metros\") In\u00a0[\u00a0]: Copied! <pre># \ud83c\udfaf DESAFIO 2: AN\u00c1LISE DO POOLING\nprint(\"\ud83c\udfaf DESAFIO 2: AN\u00c1LISE DETALHADA DO POOLING\")\nprint(\"=\" * 60)\n\n# Vamos analisar passo a passo\nmodel_analise = keras.Sequential([\n    layers.Conv2D(100, (3,3), activation='relu', input_shape=(800, 600, 3)),\n    layers.MaxPool2D(pool_size=2, strides=2)\n])\n\nprint(\"\ud83d\udcca AN\u00c1LISE DAS DIMENS\u00d5ES:\")\nprint(\"=\" * 30)\n\n# Simulando o forward pass para ver as dimens\u00f5es\nimport numpy as np\ninput_shape = (1, 800, 600, 3)  # Batch size = 1\ndummy_input = np.random.random(input_shape)\n\n# Passando pela primeira camada (Conv2D)\nconv_output = model_analise.layers[0](dummy_input)\nprint(f\"\ud83d\udd04 Ap\u00f3s Convolu\u00e7\u00e3o: {conv_output.shape}\")\nprint(f\"   Entrada: (800, 600, 3)\")\nprint(f\"   Sa\u00edda:   {conv_output.shape[1:]}\") \n\n# Passando pela segunda camada (MaxPool2D)  \npool_output = model_analise.layers[1](conv_output)\nprint(f\"\\n\ud83c\udfca Ap\u00f3s Max Pooling: {pool_output.shape}\")\nprint(f\"   Entrada: {conv_output.shape[1:]}\")\nprint(f\"   Sa\u00edda:   {pool_output.shape[1:]}\")\n\n# Calculando redu\u00e7\u00e3o\naltura_reducao = conv_output.shape[1] / pool_output.shape[1]\nlargura_reducao = conv_output.shape[2] / pool_output.shape[2]\ntotal_pixels_antes = conv_output.shape[1] * conv_output.shape[2]\ntotal_pixels_depois = pool_output.shape[1] * pool_output.shape[2]\nreducao_total = total_pixels_antes / total_pixels_depois\n\nprint(f\"\\n\ud83d\udcd0 REDU\u00c7\u00c3O DE DIMENS\u00d5ES:\")\nprint(f\"   Altura: {altura_reducao:.1f}x menor\")  \nprint(f\"   Largura: {largura_reducao:.1f}x menor\")\nprint(f\"   Total de pixels: {reducao_total:.1f}x redu\u00e7\u00e3o\")\n\nprint(f\"\\n\ud83d\udd22 AN\u00c1LISE DE PAR\u00c2METROS:\")\nprint(\"=\" * 30)\nconv_params = model_analise.layers[0].count_params()\npool_params = model_analise.layers[1].count_params()\ntotal_params = model_analise.count_params()\n\nprint(f\"   Conv2D par\u00e2metros: {conv_params:,}\")\nprint(f\"   MaxPool2D par\u00e2metros: {pool_params:,}\")\nprint(f\"   Total par\u00e2metros: {total_params:,}\")\n\nprint(f\"\\n\ud83c\udfaf SUAS RESPOSTAS:\")\nprint(\"=\" * 20)\nprint(\"1\ufe0f\u20e3 Dimens\u00e3o antes do pooling: ________________\")\nprint(\"2\ufe0f\u20e3 Dimens\u00e3o depois do pooling: _______________\")\nprint(\"3\ufe0f\u20e3 Pooling alterou total params? _____________\")\nprint(\"4\ufe0f\u20e3 Por que pooling n\u00e3o tem par\u00e2metros? _______\")\nprint(\"   _________________________________________\")\n\nprint(f\"\\n\ud83d\udca1 DICAS PARA RESPONDER:\")\nprint(\"   \u2022 Observe os valores impressos acima\")\nprint(\"   \u2022 Lembre-se: pooling \u00e9 uma opera\u00e7\u00e3o fixa (max ou m\u00e9dia)\")\nprint(\"   \u2022 Compare com a convolu\u00e7\u00e3o que TEM par\u00e2metros\")\nprint(\"   \u2022 Pense na diferen\u00e7a entre 'opera\u00e7\u00e3o' e 'aprendizado'\")\n</pre> # \ud83c\udfaf DESAFIO 2: AN\u00c1LISE DO POOLING print(\"\ud83c\udfaf DESAFIO 2: AN\u00c1LISE DETALHADA DO POOLING\") print(\"=\" * 60)  # Vamos analisar passo a passo model_analise = keras.Sequential([     layers.Conv2D(100, (3,3), activation='relu', input_shape=(800, 600, 3)),     layers.MaxPool2D(pool_size=2, strides=2) ])  print(\"\ud83d\udcca AN\u00c1LISE DAS DIMENS\u00d5ES:\") print(\"=\" * 30)  # Simulando o forward pass para ver as dimens\u00f5es import numpy as np input_shape = (1, 800, 600, 3)  # Batch size = 1 dummy_input = np.random.random(input_shape)  # Passando pela primeira camada (Conv2D) conv_output = model_analise.layers[0](dummy_input) print(f\"\ud83d\udd04 Ap\u00f3s Convolu\u00e7\u00e3o: {conv_output.shape}\") print(f\"   Entrada: (800, 600, 3)\") print(f\"   Sa\u00edda:   {conv_output.shape[1:]}\")   # Passando pela segunda camada (MaxPool2D)   pool_output = model_analise.layers[1](conv_output) print(f\"\\n\ud83c\udfca Ap\u00f3s Max Pooling: {pool_output.shape}\") print(f\"   Entrada: {conv_output.shape[1:]}\") print(f\"   Sa\u00edda:   {pool_output.shape[1:]}\")  # Calculando redu\u00e7\u00e3o altura_reducao = conv_output.shape[1] / pool_output.shape[1] largura_reducao = conv_output.shape[2] / pool_output.shape[2] total_pixels_antes = conv_output.shape[1] * conv_output.shape[2] total_pixels_depois = pool_output.shape[1] * pool_output.shape[2] reducao_total = total_pixels_antes / total_pixels_depois  print(f\"\\n\ud83d\udcd0 REDU\u00c7\u00c3O DE DIMENS\u00d5ES:\") print(f\"   Altura: {altura_reducao:.1f}x menor\")   print(f\"   Largura: {largura_reducao:.1f}x menor\") print(f\"   Total de pixels: {reducao_total:.1f}x redu\u00e7\u00e3o\")  print(f\"\\n\ud83d\udd22 AN\u00c1LISE DE PAR\u00c2METROS:\") print(\"=\" * 30) conv_params = model_analise.layers[0].count_params() pool_params = model_analise.layers[1].count_params() total_params = model_analise.count_params()  print(f\"   Conv2D par\u00e2metros: {conv_params:,}\") print(f\"   MaxPool2D par\u00e2metros: {pool_params:,}\") print(f\"   Total par\u00e2metros: {total_params:,}\")  print(f\"\\n\ud83c\udfaf SUAS RESPOSTAS:\") print(\"=\" * 20) print(\"1\ufe0f\u20e3 Dimens\u00e3o antes do pooling: ________________\") print(\"2\ufe0f\u20e3 Dimens\u00e3o depois do pooling: _______________\") print(\"3\ufe0f\u20e3 Pooling alterou total params? _____________\") print(\"4\ufe0f\u20e3 Por que pooling n\u00e3o tem par\u00e2metros? _______\") print(\"   _________________________________________\")  print(f\"\\n\ud83d\udca1 DICAS PARA RESPONDER:\") print(\"   \u2022 Observe os valores impressos acima\") print(\"   \u2022 Lembre-se: pooling \u00e9 uma opera\u00e7\u00e3o fixa (max ou m\u00e9dia)\") print(\"   \u2022 Compare com a convolu\u00e7\u00e3o que TEM par\u00e2metros\") print(\"   \u2022 Pense na diferen\u00e7a entre 'opera\u00e7\u00e3o' e 'aprendizado'\") In\u00a0[24]: Copied! <pre>import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers\n</pre> import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt from tensorflow.keras import layers In\u00a0[\u00a0]: Copied! <pre># \ud83d\udcca CARREGANDO E EXPLORANDO O FASHION MNIST\nprint(\"\ud83d\udcca CARREGANDO FASHION MNIST\")\nprint(\"=\" * 50)\n\n# Importa o dataset Fashion MNIST\nfashion_mnist = keras.datasets.fashion_mnist\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\n# Informa\u00e7\u00f5es b\u00e1sicas do dataset\nprint(f\"\ud83d\udcd0 Formato dos dados de treino: {train_images.shape}\")\nprint(f\"\ud83d\udcd0 Formato dos dados de teste: {test_images.shape}\")\nprint(f\"\ud83c\udff7\ufe0f Formato dos r\u00f3tulos de treino: {train_labels.shape}\")\nprint(f\"\ud83c\udff7\ufe0f Formato dos r\u00f3tulos de teste: {test_labels.shape}\")\n\nprint(f\"\\n\ud83d\udcca AN\u00c1LISE DOS DADOS:\")\nprint(f\"   Total de imagens de treino: {len(train_images):,}\")\nprint(f\"   Total de imagens de teste: {len(test_images):,}\")\nprint(f\"   Dimens\u00e3o de cada imagem: {train_images[0].shape}\")\nprint(f\"   Valores dos pixels: {train_images[0].min()} a {train_images[0].max()}\")\nprint(f\"   N\u00famero de classes: {len(np.unique(train_labels))}\")\n\n# Nomes das classes\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\nprint(f\"\\n\ud83d\udc57 CLASSES DO DATASET:\")\nfor i, nome in enumerate(class_names):\n    count = np.sum(train_labels == i)\n    print(f\"   {i}: {nome} ({count:,} imagens)\")\n\n# Normaliza\u00e7\u00e3o: fundamental para CNNs!\nprint(f\"\\n\ud83d\udd04 NORMALIZANDO OS DADOS...\")\nprint(\"Por que normalizar?\")\nprint(\"   \u2705 Acelera converg\u00eancia do treinamento\")\nprint(\"   \u2705 Evita domin\u00e2ncia de pixels com valores altos\")\nprint(\"   \u2705 Melhora estabilidade num\u00e9rica\")\nprint(\"   \u2705 Padr\u00e3o para redes neurais\")\n\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\n\nprint(f\"   Novos valores dos pixels: {train_images[0].min():.1f} a {train_images[0].max():.1f}\")\nprint(\"\u2705 Normaliza\u00e7\u00e3o conclu\u00edda!\")\n</pre> # \ud83d\udcca CARREGANDO E EXPLORANDO O FASHION MNIST print(\"\ud83d\udcca CARREGANDO FASHION MNIST\") print(\"=\" * 50)  # Importa o dataset Fashion MNIST fashion_mnist = keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()  # Informa\u00e7\u00f5es b\u00e1sicas do dataset print(f\"\ud83d\udcd0 Formato dos dados de treino: {train_images.shape}\") print(f\"\ud83d\udcd0 Formato dos dados de teste: {test_images.shape}\") print(f\"\ud83c\udff7\ufe0f Formato dos r\u00f3tulos de treino: {train_labels.shape}\") print(f\"\ud83c\udff7\ufe0f Formato dos r\u00f3tulos de teste: {test_labels.shape}\")  print(f\"\\n\ud83d\udcca AN\u00c1LISE DOS DADOS:\") print(f\"   Total de imagens de treino: {len(train_images):,}\") print(f\"   Total de imagens de teste: {len(test_images):,}\") print(f\"   Dimens\u00e3o de cada imagem: {train_images[0].shape}\") print(f\"   Valores dos pixels: {train_images[0].min()} a {train_images[0].max()}\") print(f\"   N\u00famero de classes: {len(np.unique(train_labels))}\")  # Nomes das classes class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']  print(f\"\\n\ud83d\udc57 CLASSES DO DATASET:\") for i, nome in enumerate(class_names):     count = np.sum(train_labels == i)     print(f\"   {i}: {nome} ({count:,} imagens)\")  # Normaliza\u00e7\u00e3o: fundamental para CNNs! print(f\"\\n\ud83d\udd04 NORMALIZANDO OS DADOS...\") print(\"Por que normalizar?\") print(\"   \u2705 Acelera converg\u00eancia do treinamento\") print(\"   \u2705 Evita domin\u00e2ncia de pixels com valores altos\") print(\"   \u2705 Melhora estabilidade num\u00e9rica\") print(\"   \u2705 Padr\u00e3o para redes neurais\")  train_images = train_images / 255.0 test_images = test_images / 255.0  print(f\"   Novos valores dos pixels: {train_images[0].min():.1f} a {train_images[0].max():.1f}\") print(\"\u2705 Normaliza\u00e7\u00e3o conclu\u00edda!\") In\u00a0[\u00a0]: Copied! <pre># \ud83d\udd04 PREPARANDO DADOS PARA CNN\nprint(\"\ud83d\udd04 PREPARANDO FORMATO PARA CNN\")\nprint(\"=\" * 50)\n\nprint(\"\ud83d\udcd0 RESHAPE NECESS\u00c1RIO:\")\nprint(f\"   Formato original: {train_images.shape}\")\nprint(\"   Formato necess\u00e1rio para CNN: (samples, height, width, channels)\")\nprint(\"   \")\nprint(\"   \ud83d\udd0d Por que precisamos de 4 dimens\u00f5es?\")\nprint(\"      \u2022 samples: n\u00famero de imagens\")\nprint(\"      \u2022 height: altura da imagem\")  \nprint(\"      \u2022 width: largura da imagem\")\nprint(\"      \u2022 channels: canais de cor (1=grayscale, 3=RGB)\")\n\n# Reshape adicionando dimens\u00e3o de canal\ntrain_images = train_images.reshape(-1, 28, 28, 1)\ntest_images = test_images.reshape(-1, 28, 28, 1)\n\nprint(f\"\\n\u2705 AP\u00d3S RESHAPE:\")\nprint(f\"   Dados de treino: {train_images.shape}\")\nprint(f\"   Dados de teste: {test_images.shape}\")\nprint(f\"   \")\nprint(f\"   \ud83d\udcca Interpreta\u00e7\u00e3o:\")\nprint(f\"      \u2022 {train_images.shape[0]:,} imagens de treino\")\nprint(f\"      \u2022 {train_images.shape[1]}\u00d7{train_images.shape[2]} pixels cada\")\nprint(f\"      \u2022 {train_images.shape[3]} canal (escala de cinza)\")\n\n# Visualizando algumas amostras\nprint(f\"\\n\ud83d\uddbc\ufe0f VISUALIZANDO AMOSTRAS:\")\nfig, axes = plt.subplots(2, 5, figsize=(12, 6))\nfig.suptitle('\ud83d\udc57 Amostras do Fashion MNIST', fontsize=16, fontweight='bold')\n\nfor i in range(10):\n    ax = axes[i//5, i%5]\n    ax.imshow(train_images[i].reshape(28, 28), cmap='gray')\n    ax.set_title(f'{class_names[train_labels[i]]}')\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\u2705 Dados preparados para treinamento da CNN!\")\n</pre> # \ud83d\udd04 PREPARANDO DADOS PARA CNN print(\"\ud83d\udd04 PREPARANDO FORMATO PARA CNN\") print(\"=\" * 50)  print(\"\ud83d\udcd0 RESHAPE NECESS\u00c1RIO:\") print(f\"   Formato original: {train_images.shape}\") print(\"   Formato necess\u00e1rio para CNN: (samples, height, width, channels)\") print(\"   \") print(\"   \ud83d\udd0d Por que precisamos de 4 dimens\u00f5es?\") print(\"      \u2022 samples: n\u00famero de imagens\") print(\"      \u2022 height: altura da imagem\")   print(\"      \u2022 width: largura da imagem\") print(\"      \u2022 channels: canais de cor (1=grayscale, 3=RGB)\")  # Reshape adicionando dimens\u00e3o de canal train_images = train_images.reshape(-1, 28, 28, 1) test_images = test_images.reshape(-1, 28, 28, 1)  print(f\"\\n\u2705 AP\u00d3S RESHAPE:\") print(f\"   Dados de treino: {train_images.shape}\") print(f\"   Dados de teste: {test_images.shape}\") print(f\"   \") print(f\"   \ud83d\udcca Interpreta\u00e7\u00e3o:\") print(f\"      \u2022 {train_images.shape[0]:,} imagens de treino\") print(f\"      \u2022 {train_images.shape[1]}\u00d7{train_images.shape[2]} pixels cada\") print(f\"      \u2022 {train_images.shape[3]} canal (escala de cinza)\")  # Visualizando algumas amostras print(f\"\\n\ud83d\uddbc\ufe0f VISUALIZANDO AMOSTRAS:\") fig, axes = plt.subplots(2, 5, figsize=(12, 6)) fig.suptitle('\ud83d\udc57 Amostras do Fashion MNIST', fontsize=16, fontweight='bold')  for i in range(10):     ax = axes[i//5, i%5]     ax.imshow(train_images[i].reshape(28, 28), cmap='gray')     ax.set_title(f'{class_names[train_labels[i]]}')     ax.axis('off')  plt.tight_layout() plt.show()  print(\"\u2705 Dados preparados para treinamento da CNN!\") In\u00a0[\u00a0]: Copied! <pre># \ud83c\udfd7\ufe0f CONSTRUINDO A CNN\nprint(\"\ud83c\udfd7\ufe0f CONSTRUINDO ARQUITETURA CNN\")\nprint(\"=\" * 50)\n\nprint(\"\ud83c\udfaf ESTRAT\u00c9GIA DE DESIGN:\")\nprint(\"   1\ufe0f\u20e3 Come\u00e7ar com CNN simples\")\nprint(\"   2\ufe0f\u20e3 Camada Conv2D para extra\u00e7\u00e3o de caracter\u00edsticas\")\nprint(\"   3\ufe0f\u20e3 MaxPooling para redu\u00e7\u00e3o dimensional\")\nprint(\"   4\ufe0f\u20e3 Flatten para converter para 1D\")\nprint(\"   5\ufe0f\u20e3 Dense layers para classifica\u00e7\u00e3o\")\n\nfrom tensorflow.keras import layers\n\n# Arquitetura inicial simples\nmodel = keras.Sequential([\n    # \ud83d\udd0d EXTRATOR DE CARACTER\u00cdSTICAS\n    layers.Conv2D(32, (3,3), activation='relu', padding='same', \n                  input_shape=(28, 28, 1), name='conv2d_1'),\n    layers.MaxPooling2D((2,2), name='maxpool_1'),\n    \n    # \ud83e\udde0 CLASSIFICADOR  \n    layers.Flatten(name='flatten'),\n    layers.Dense(128, activation='relu', name='dense_1'),\n    layers.Dense(10, activation='softmax', name='output')  # 10 classes\n])\n\nprint(\"\ud83d\udccb RESUMO DA ARQUITETURA:\")\nmodel.summary()\n\nprint(f\"\\n\ud83d\udd0d AN\u00c1LISE DETALHADA:\")\nprint(f\"   \ud83d\udcd0 Input: (28, 28, 1) - Imagem grayscale\")\nprint(f\"   \ud83d\udd04 Conv2D: 32 filtros 3\u00d73 \u2192 (28, 28, 32)\")\nprint(f\"   \ud83c\udfca MaxPool: 2\u00d72 \u2192 (14, 14, 32)\")\nprint(f\"   \ud83d\udccf Flatten: \u2192 ({14*14*32},)\")\nprint(f\"   \ud83e\udde0 Dense: 128 neur\u00f4nios \u2192 (128,)\")  \nprint(f\"   \ud83c\udfaf Output: 10 classes \u2192 (10,)\")\n\n# Calculando par\u00e2metros manualmente para compreens\u00e3o\nconv_params = (3*3*1*32) + 32  # kernel \u00d7 input_channels \u00d7 filters + bias\ndense1_params = (14*14*32*128) + 128  # input \u00d7 neurons + bias\ndense2_params = (128*10) + 10  # input \u00d7 neurons + bias\n\nprint(f\"\\n\ud83e\uddee C\u00c1LCULO MANUAL DE PAR\u00c2METROS:\")\nprint(f\"   Conv2D: {conv_params:,} par\u00e2metros\")\nprint(f\"   Dense 1: {dense1_params:,} par\u00e2metros\")\nprint(f\"   Dense 2: {dense2_params:,} par\u00e2metros\")\nprint(f\"   Total calculado: {conv_params + dense1_params + dense2_params:,}\")\nprint(f\"   Total do modelo: {model.count_params():,}\")\n\nprint(f\"\\n\ud83d\udca1 OBSERVA\u00c7\u00d5ES:\")\nprint(\"   \u2705 A maioria dos par\u00e2metros est\u00e1 nas camadas Dense\")\nprint(\"   \u2705 Conv2D tem poucos par\u00e2metros mas extrai caracter\u00edsticas importantes\")\nprint(\"   \u2705 MaxPooling n\u00e3o tem par\u00e2metros trein\u00e1veis\")\n</pre> # \ud83c\udfd7\ufe0f CONSTRUINDO A CNN print(\"\ud83c\udfd7\ufe0f CONSTRUINDO ARQUITETURA CNN\") print(\"=\" * 50)  print(\"\ud83c\udfaf ESTRAT\u00c9GIA DE DESIGN:\") print(\"   1\ufe0f\u20e3 Come\u00e7ar com CNN simples\") print(\"   2\ufe0f\u20e3 Camada Conv2D para extra\u00e7\u00e3o de caracter\u00edsticas\") print(\"   3\ufe0f\u20e3 MaxPooling para redu\u00e7\u00e3o dimensional\") print(\"   4\ufe0f\u20e3 Flatten para converter para 1D\") print(\"   5\ufe0f\u20e3 Dense layers para classifica\u00e7\u00e3o\")  from tensorflow.keras import layers  # Arquitetura inicial simples model = keras.Sequential([     # \ud83d\udd0d EXTRATOR DE CARACTER\u00cdSTICAS     layers.Conv2D(32, (3,3), activation='relu', padding='same',                    input_shape=(28, 28, 1), name='conv2d_1'),     layers.MaxPooling2D((2,2), name='maxpool_1'),          # \ud83e\udde0 CLASSIFICADOR       layers.Flatten(name='flatten'),     layers.Dense(128, activation='relu', name='dense_1'),     layers.Dense(10, activation='softmax', name='output')  # 10 classes ])  print(\"\ud83d\udccb RESUMO DA ARQUITETURA:\") model.summary()  print(f\"\\n\ud83d\udd0d AN\u00c1LISE DETALHADA:\") print(f\"   \ud83d\udcd0 Input: (28, 28, 1) - Imagem grayscale\") print(f\"   \ud83d\udd04 Conv2D: 32 filtros 3\u00d73 \u2192 (28, 28, 32)\") print(f\"   \ud83c\udfca MaxPool: 2\u00d72 \u2192 (14, 14, 32)\") print(f\"   \ud83d\udccf Flatten: \u2192 ({14*14*32},)\") print(f\"   \ud83e\udde0 Dense: 128 neur\u00f4nios \u2192 (128,)\")   print(f\"   \ud83c\udfaf Output: 10 classes \u2192 (10,)\")  # Calculando par\u00e2metros manualmente para compreens\u00e3o conv_params = (3*3*1*32) + 32  # kernel \u00d7 input_channels \u00d7 filters + bias dense1_params = (14*14*32*128) + 128  # input \u00d7 neurons + bias dense2_params = (128*10) + 10  # input \u00d7 neurons + bias  print(f\"\\n\ud83e\uddee C\u00c1LCULO MANUAL DE PAR\u00c2METROS:\") print(f\"   Conv2D: {conv_params:,} par\u00e2metros\") print(f\"   Dense 1: {dense1_params:,} par\u00e2metros\") print(f\"   Dense 2: {dense2_params:,} par\u00e2metros\") print(f\"   Total calculado: {conv_params + dense1_params + dense2_params:,}\") print(f\"   Total do modelo: {model.count_params():,}\")  print(f\"\\n\ud83d\udca1 OBSERVA\u00c7\u00d5ES:\") print(\"   \u2705 A maioria dos par\u00e2metros est\u00e1 nas camadas Dense\") print(\"   \u2705 Conv2D tem poucos par\u00e2metros mas extrai caracter\u00edsticas importantes\") print(\"   \u2705 MaxPooling n\u00e3o tem par\u00e2metros trein\u00e1veis\") In\u00a0[\u00a0]: Copied! <pre># \ud83c\udfaf COMPILA\u00c7\u00c3O E TREINAMENTO DO MODELO\nprint(\"\ud83c\udfaf COMPILA\u00c7\u00c3O DO MODELO\")\nprint(\"=\" * 50)\n\nprint(\"\u2699\ufe0f CONFIGURA\u00c7\u00d5ES DE TREINAMENTO:\")\nprint(\"   \ud83d\udd27 Optimizer: Adam (adaptativo, eficiente)\")\nprint(\"   \ud83d\udcc9 Loss: sparse_categorical_crossentropy (para m\u00faltiplas classes)\")\nprint(\"   \ud83d\udcca Metrics: accuracy (para acompanhar performance)\")\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nprint(\"\u2705 Modelo compilado com sucesso!\")\n\nprint(f\"\\n\ud83d\ude80 INICIANDO TREINAMENTO:\")\nprint(f\"   \ud83d\udcda \u00c9pocas: 5 (poucas para demonstra\u00e7\u00e3o)\")\nprint(f\"   \ud83d\udcca Validation split: 20% dos dados de treino\")\nprint(f\"   \ud83c\udfaf Objetivo: Entender o processo de aprendizado\")\n\nprint(f\"\\n\u23f1\ufe0f Treinamento em andamento...\")\n\n# Treinamento com mais \u00e9pocas e monitoramento\nepochs_hist = model.fit(\n    train_images, train_labels, \n    epochs=5,\n    validation_split=0.2,\n    verbose=1,\n    batch_size=32\n)\n\nprint(f\"\\n\ud83c\udf89 TREINAMENTO CONCLU\u00cdDO!\")\nprint(f\"\u2705 Modelo treinado por {len(epochs_hist.history['loss'])} \u00e9pocas\")\nprint(f\"\ud83d\udcc8 \u00daltima acur\u00e1cia de treino: {epochs_hist.history['accuracy'][-1]:.3f}\")\nprint(f\"\ud83d\udcca \u00daltima acur\u00e1cia de valida\u00e7\u00e3o: {epochs_hist.history['val_accuracy'][-1]:.3f}\")\n</pre> # \ud83c\udfaf COMPILA\u00c7\u00c3O E TREINAMENTO DO MODELO print(\"\ud83c\udfaf COMPILA\u00c7\u00c3O DO MODELO\") print(\"=\" * 50)  print(\"\u2699\ufe0f CONFIGURA\u00c7\u00d5ES DE TREINAMENTO:\") print(\"   \ud83d\udd27 Optimizer: Adam (adaptativo, eficiente)\") print(\"   \ud83d\udcc9 Loss: sparse_categorical_crossentropy (para m\u00faltiplas classes)\") print(\"   \ud83d\udcca Metrics: accuracy (para acompanhar performance)\")  model.compile(     optimizer='adam',     loss='sparse_categorical_crossentropy',     metrics=['accuracy'] )  print(\"\u2705 Modelo compilado com sucesso!\")  print(f\"\\n\ud83d\ude80 INICIANDO TREINAMENTO:\") print(f\"   \ud83d\udcda \u00c9pocas: 5 (poucas para demonstra\u00e7\u00e3o)\") print(f\"   \ud83d\udcca Validation split: 20% dos dados de treino\") print(f\"   \ud83c\udfaf Objetivo: Entender o processo de aprendizado\")  print(f\"\\n\u23f1\ufe0f Treinamento em andamento...\")  # Treinamento com mais \u00e9pocas e monitoramento epochs_hist = model.fit(     train_images, train_labels,      epochs=5,     validation_split=0.2,     verbose=1,     batch_size=32 )  print(f\"\\n\ud83c\udf89 TREINAMENTO CONCLU\u00cdDO!\") print(f\"\u2705 Modelo treinado por {len(epochs_hist.history['loss'])} \u00e9pocas\") print(f\"\ud83d\udcc8 \u00daltima acur\u00e1cia de treino: {epochs_hist.history['accuracy'][-1]:.3f}\") print(f\"\ud83d\udcca \u00daltima acur\u00e1cia de valida\u00e7\u00e3o: {epochs_hist.history['val_accuracy'][-1]:.3f}\") In\u00a0[\u00a0]: Copied! <pre># \ud83d\udcc8 VISUALIZA\u00c7\u00c3O DO TREINAMENTO\nprint(\"\ud83d\udcc8 AN\u00c1LISE DO TREINAMENTO\")\nprint(\"=\" * 50)\n\n# Convertendo hist\u00f3rico para DataFrame para an\u00e1lise\nimport pandas as pd\nhistory_df = pd.DataFrame(epochs_hist.history)\n\nprint(\"\ud83d\udd0d M\u00c9TRICAS POR \u00c9POCA:\")\nprint(history_df.round(4))\n\n# Visualiza\u00e7\u00e3o aprimorada dos gr\u00e1ficos\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\n# Gr\u00e1fico de Loss\nax1.plot(history_df['loss'], 'b-', label='Loss de Treino', linewidth=2)\nax1.plot(history_df['val_loss'], 'r-', label='Loss de Valida\u00e7\u00e3o', linewidth=2)\nax1.set_title('\ud83d\udcc9 Evolu\u00e7\u00e3o da Fun\u00e7\u00e3o de Loss', fontsize=14, fontweight='bold')\nax1.set_xlabel('\u00c9poca')\nax1.set_ylabel('Loss')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Gr\u00e1fico de Acur\u00e1cia\nax2.plot(history_df['accuracy'], 'b-', label='Acur\u00e1cia de Treino', linewidth=2)\nax2.plot(history_df['val_accuracy'], 'r-', label='Acur\u00e1cia de Valida\u00e7\u00e3o', linewidth=2)\nax2.set_title('\ud83d\udcca Evolu\u00e7\u00e3o da Acur\u00e1cia', fontsize=14, fontweight='bold')\nax2.set_xlabel('\u00c9poca')\nax2.set_ylabel('Acur\u00e1cia')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# An\u00e1lise dos resultados\nprint(f\"\\n\ud83d\udd0d AN\u00c1LISE DOS RESULTADOS:\")\nprint(f\"   \ud83d\udcc8 Melhoria no treino: {history_df['accuracy'].iloc[-1] - history_df['accuracy'].iloc[0]:.3f}\")\nprint(f\"   \ud83d\udcca Melhoria na valida\u00e7\u00e3o: {history_df['val_accuracy'].iloc[-1] - history_df['val_accuracy'].iloc[0]:.3f}\")\n\n# Verifica\u00e7\u00e3o de overfitting\ngap_inicial = history_df['accuracy'].iloc[0] - history_df['val_accuracy'].iloc[0]\ngap_final = history_df['accuracy'].iloc[-1] - history_df['val_accuracy'].iloc[-1]\n\nprint(f\"\\n\ud83c\udfaf DIAGN\u00d3STICO DE OVERFITTING:\")\nprint(f\"   Gap inicial (treino-val): {gap_inicial:.3f}\")\nprint(f\"   Gap final (treino-val): {gap_final:.3f}\")\n\nif abs(gap_final) &lt; 0.05:\n    print(\"   \u2705 Modelo bem balanceado!\")\nelif gap_final &gt; 0.1:\n    print(\"   \u26a0\ufe0f Poss\u00edvel overfitting - considere regulariza\u00e7\u00e3o\")\nelif gap_final &lt; -0.05:\n    print(\"   \ud83d\udd04 Modelo pode ter mais capacidade - considere treinar mais\")\nelse:\n    print(\"   \ud83d\udc4d Desempenho razo\u00e1vel\")\n</pre> # \ud83d\udcc8 VISUALIZA\u00c7\u00c3O DO TREINAMENTO print(\"\ud83d\udcc8 AN\u00c1LISE DO TREINAMENTO\") print(\"=\" * 50)  # Convertendo hist\u00f3rico para DataFrame para an\u00e1lise import pandas as pd history_df = pd.DataFrame(epochs_hist.history)  print(\"\ud83d\udd0d M\u00c9TRICAS POR \u00c9POCA:\") print(history_df.round(4))  # Visualiza\u00e7\u00e3o aprimorada dos gr\u00e1ficos fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))  # Gr\u00e1fico de Loss ax1.plot(history_df['loss'], 'b-', label='Loss de Treino', linewidth=2) ax1.plot(history_df['val_loss'], 'r-', label='Loss de Valida\u00e7\u00e3o', linewidth=2) ax1.set_title('\ud83d\udcc9 Evolu\u00e7\u00e3o da Fun\u00e7\u00e3o de Loss', fontsize=14, fontweight='bold') ax1.set_xlabel('\u00c9poca') ax1.set_ylabel('Loss') ax1.legend() ax1.grid(True, alpha=0.3)  # Gr\u00e1fico de Acur\u00e1cia ax2.plot(history_df['accuracy'], 'b-', label='Acur\u00e1cia de Treino', linewidth=2) ax2.plot(history_df['val_accuracy'], 'r-', label='Acur\u00e1cia de Valida\u00e7\u00e3o', linewidth=2) ax2.set_title('\ud83d\udcca Evolu\u00e7\u00e3o da Acur\u00e1cia', fontsize=14, fontweight='bold') ax2.set_xlabel('\u00c9poca') ax2.set_ylabel('Acur\u00e1cia') ax2.legend() ax2.grid(True, alpha=0.3)  plt.tight_layout() plt.show()  # An\u00e1lise dos resultados print(f\"\\n\ud83d\udd0d AN\u00c1LISE DOS RESULTADOS:\") print(f\"   \ud83d\udcc8 Melhoria no treino: {history_df['accuracy'].iloc[-1] - history_df['accuracy'].iloc[0]:.3f}\") print(f\"   \ud83d\udcca Melhoria na valida\u00e7\u00e3o: {history_df['val_accuracy'].iloc[-1] - history_df['val_accuracy'].iloc[0]:.3f}\")  # Verifica\u00e7\u00e3o de overfitting gap_inicial = history_df['accuracy'].iloc[0] - history_df['val_accuracy'].iloc[0] gap_final = history_df['accuracy'].iloc[-1] - history_df['val_accuracy'].iloc[-1]  print(f\"\\n\ud83c\udfaf DIAGN\u00d3STICO DE OVERFITTING:\") print(f\"   Gap inicial (treino-val): {gap_inicial:.3f}\") print(f\"   Gap final (treino-val): {gap_final:.3f}\")  if abs(gap_final) &lt; 0.05:     print(\"   \u2705 Modelo bem balanceado!\") elif gap_final &gt; 0.1:     print(\"   \u26a0\ufe0f Poss\u00edvel overfitting - considere regulariza\u00e7\u00e3o\") elif gap_final &lt; -0.05:     print(\"   \ud83d\udd04 Modelo pode ter mais capacidade - considere treinar mais\") else:     print(\"   \ud83d\udc4d Desempenho razo\u00e1vel\") In\u00a0[\u00a0]: Copied! <pre># \ud83c\udfaf AVALIA\u00c7\u00c3O COMPLETA DO MODELO\nprint(\"\ud83c\udfaf AVALIA\u00c7\u00c3O FINAL DO MODELO\")\nprint(\"=\" * 50)\n\nprint(\"\ud83d\udcca AVALIANDO PERFORMANCE...\")\n\n# Avalia\u00e7\u00e3o nos dados de treino e teste\ntrain_loss, train_acc = model.evaluate(train_images, train_labels, verbose=0)\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n\nprint(f\"\\n\ud83d\udcc8 RESULTADOS FINAIS:\")\nprint(f\"   \ud83c\udf93 Treino - Loss: {train_loss:.4f} | Acur\u00e1cia: {train_acc:.4f} ({train_acc*100:.1f}%)\")\nprint(f\"   \ud83e\uddea Teste  - Loss: {test_loss:.4f} | Acur\u00e1cia: {test_acc:.4f} ({test_acc*100:.1f}%)\")\n\n# An\u00e1lise de overfitting\noverfitting_gap = train_acc - test_acc\nprint(f\"\\n\ud83d\udd0d AN\u00c1LISE DE GENERALIZA\u00c7\u00c3O:\")\nprint(f\"   Gap treino-teste: {overfitting_gap:.4f}\")\n\nif overfitting_gap &lt; 0.02:\n    print(\"   \u2705 Excelente generaliza\u00e7\u00e3o!\")\nelif overfitting_gap &lt; 0.05:\n    print(\"   \ud83d\udc4d Boa generaliza\u00e7\u00e3o\")\nelif overfitting_gap &lt; 0.1:\n    print(\"   \u26a0\ufe0f Leve overfitting\")\nelse:\n    print(\"   \ud83d\udeab Overfitting significativo - modelo memorizou o treino\")\n\n# Compara\u00e7\u00e3o com baseline\nprint(f\"\\n\ud83c\udfaf COMPARA\u00c7\u00c3O COM BASELINES:\")\nrandom_acc = 1/10  # 10 classes, chute aleat\u00f3rio\nprint(f\"   \ud83c\udfb2 Baseline aleat\u00f3rio: {random_acc:.3f} ({random_acc*100:.1f}%)\")\nprint(f\"   \ud83d\ude80 Melhoria sobre baseline: {(test_acc/random_acc):.1f}x\")\n\n# Interpreta\u00e7\u00e3o do resultado\nif test_acc &gt; 0.85:\n    print(\"   \ud83c\udfc6 Performance excelente!\")\nelif test_acc &gt; 0.75:\n    print(\"   \u2705 Performance boa\")\nelif test_acc &gt; 0.65:\n    print(\"   \ud83d\udc4d Performance razo\u00e1vel\")\nelse:\n    print(\"   \u26a0\ufe0f Performance baixa - modelo precisa melhorar\")\n\nprint(f\"\\n\ud83d\udca1 OBSERVA\u00c7\u00d5ES:\")\nprint(\"   \u2022 Fashion MNIST \u00e9 mais desafiador que MNIST d\u00edgitos\")\nprint(\"   \u2022 CNNs simples j\u00e1 superam MLPs tradicionais\")\nprint(\"   \u2022 Margem para melhoria com arquiteturas mais complexas\")\n</pre> # \ud83c\udfaf AVALIA\u00c7\u00c3O COMPLETA DO MODELO print(\"\ud83c\udfaf AVALIA\u00c7\u00c3O FINAL DO MODELO\") print(\"=\" * 50)  print(\"\ud83d\udcca AVALIANDO PERFORMANCE...\")  # Avalia\u00e7\u00e3o nos dados de treino e teste train_loss, train_acc = model.evaluate(train_images, train_labels, verbose=0) test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)  print(f\"\\n\ud83d\udcc8 RESULTADOS FINAIS:\") print(f\"   \ud83c\udf93 Treino - Loss: {train_loss:.4f} | Acur\u00e1cia: {train_acc:.4f} ({train_acc*100:.1f}%)\") print(f\"   \ud83e\uddea Teste  - Loss: {test_loss:.4f} | Acur\u00e1cia: {test_acc:.4f} ({test_acc*100:.1f}%)\")  # An\u00e1lise de overfitting overfitting_gap = train_acc - test_acc print(f\"\\n\ud83d\udd0d AN\u00c1LISE DE GENERALIZA\u00c7\u00c3O:\") print(f\"   Gap treino-teste: {overfitting_gap:.4f}\")  if overfitting_gap &lt; 0.02:     print(\"   \u2705 Excelente generaliza\u00e7\u00e3o!\") elif overfitting_gap &lt; 0.05:     print(\"   \ud83d\udc4d Boa generaliza\u00e7\u00e3o\") elif overfitting_gap &lt; 0.1:     print(\"   \u26a0\ufe0f Leve overfitting\") else:     print(\"   \ud83d\udeab Overfitting significativo - modelo memorizou o treino\")  # Compara\u00e7\u00e3o com baseline print(f\"\\n\ud83c\udfaf COMPARA\u00c7\u00c3O COM BASELINES:\") random_acc = 1/10  # 10 classes, chute aleat\u00f3rio print(f\"   \ud83c\udfb2 Baseline aleat\u00f3rio: {random_acc:.3f} ({random_acc*100:.1f}%)\") print(f\"   \ud83d\ude80 Melhoria sobre baseline: {(test_acc/random_acc):.1f}x\")  # Interpreta\u00e7\u00e3o do resultado if test_acc &gt; 0.85:     print(\"   \ud83c\udfc6 Performance excelente!\") elif test_acc &gt; 0.75:     print(\"   \u2705 Performance boa\") elif test_acc &gt; 0.65:     print(\"   \ud83d\udc4d Performance razo\u00e1vel\") else:     print(\"   \u26a0\ufe0f Performance baixa - modelo precisa melhorar\")  print(f\"\\n\ud83d\udca1 OBSERVA\u00c7\u00d5ES:\") print(\"   \u2022 Fashion MNIST \u00e9 mais desafiador que MNIST d\u00edgitos\") print(\"   \u2022 CNNs simples j\u00e1 superam MLPs tradicionais\") print(\"   \u2022 Margem para melhoria com arquiteturas mais complexas\") In\u00a0[\u00a0]: Copied! <pre># \ud83d\udd2e FAZENDO PREDI\u00c7\u00d5ES\nprint(\"\ud83d\udd2e GERANDO PREDI\u00c7\u00d5ES DO MODELO\")\nprint(\"=\" * 50)\n\nprint(\"\ud83c\udfaf Fazendo predi\u00e7\u00f5es no conjunto de teste...\")\npredictions = model.predict(test_images, verbose=0)\n\nprint(f\"\u2705 Predi\u00e7\u00f5es conclu\u00eddas!\")\nprint(f\"\ud83d\udcca Formato das predi\u00e7\u00f5es: {predictions.shape}\")\nprint(f\"\ud83d\udca1 Cada linha cont\u00e9m probabilidades para as 10 classes\")\n\n# Analisando as predi\u00e7\u00f5es\nprint(f\"\\n\ud83d\udd0d AN\u00c1LISE DAS PREDI\u00c7\u00d5ES:\")\nmax_probs = np.max(predictions, axis=1)\npredicted_classes = np.argmax(predictions, axis=1)\n\nprint(f\"   \ud83d\udcc8 Confian\u00e7a m\u00e9dia: {np.mean(max_probs):.3f}\")\nprint(f\"   \ud83d\udcca Confian\u00e7a m\u00ednima: {np.min(max_probs):.3f}\")\nprint(f\"   \ud83d\udcca Confian\u00e7a m\u00e1xima: {np.max(max_probs):.3f}\")\n\n# Histograma de confian\u00e7a\nprint(f\"\\n\ud83d\udcca DISTRIBUI\u00c7\u00c3O DE CONFIAN\u00c7A:\")\nconfidence_ranges = [(0.0, 0.5), (0.5, 0.7), (0.7, 0.9), (0.9, 1.0)]\nfor low, high in confidence_ranges:\n    count = np.sum((max_probs &gt;= low) &amp; (max_probs &lt; high))\n    percentage = count / len(max_probs) * 100\n    print(f\"   {low:.1f}-{high:.1f}: {count:,} predi\u00e7\u00f5es ({percentage:.1f}%)\")\n\nprint(f\"\\n\ud83d\udca1 INTERPRETA\u00c7\u00c3O:\")\nhigh_confidence = np.sum(max_probs &gt; 0.9)\nlow_confidence = np.sum(max_probs &lt; 0.5)\nprint(f\"   \u2705 Alta confian\u00e7a (&gt;90%): {high_confidence} predi\u00e7\u00f5es\")\nprint(f\"   \u26a0\ufe0f Baixa confian\u00e7a (&lt;50%): {low_confidence} predi\u00e7\u00f5es\")\n</pre> # \ud83d\udd2e FAZENDO PREDI\u00c7\u00d5ES print(\"\ud83d\udd2e GERANDO PREDI\u00c7\u00d5ES DO MODELO\") print(\"=\" * 50)  print(\"\ud83c\udfaf Fazendo predi\u00e7\u00f5es no conjunto de teste...\") predictions = model.predict(test_images, verbose=0)  print(f\"\u2705 Predi\u00e7\u00f5es conclu\u00eddas!\") print(f\"\ud83d\udcca Formato das predi\u00e7\u00f5es: {predictions.shape}\") print(f\"\ud83d\udca1 Cada linha cont\u00e9m probabilidades para as 10 classes\")  # Analisando as predi\u00e7\u00f5es print(f\"\\n\ud83d\udd0d AN\u00c1LISE DAS PREDI\u00c7\u00d5ES:\") max_probs = np.max(predictions, axis=1) predicted_classes = np.argmax(predictions, axis=1)  print(f\"   \ud83d\udcc8 Confian\u00e7a m\u00e9dia: {np.mean(max_probs):.3f}\") print(f\"   \ud83d\udcca Confian\u00e7a m\u00ednima: {np.min(max_probs):.3f}\") print(f\"   \ud83d\udcca Confian\u00e7a m\u00e1xima: {np.max(max_probs):.3f}\")  # Histograma de confian\u00e7a print(f\"\\n\ud83d\udcca DISTRIBUI\u00c7\u00c3O DE CONFIAN\u00c7A:\") confidence_ranges = [(0.0, 0.5), (0.5, 0.7), (0.7, 0.9), (0.9, 1.0)] for low, high in confidence_ranges:     count = np.sum((max_probs &gt;= low) &amp; (max_probs &lt; high))     percentage = count / len(max_probs) * 100     print(f\"   {low:.1f}-{high:.1f}: {count:,} predi\u00e7\u00f5es ({percentage:.1f}%)\")  print(f\"\\n\ud83d\udca1 INTERPRETA\u00c7\u00c3O:\") high_confidence = np.sum(max_probs &gt; 0.9) low_confidence = np.sum(max_probs &lt; 0.5) print(f\"   \u2705 Alta confian\u00e7a (&gt;90%): {high_confidence} predi\u00e7\u00f5es\") print(f\"   \u26a0\ufe0f Baixa confian\u00e7a (&lt;50%): {low_confidence} predi\u00e7\u00f5es\") In\u00a0[\u00a0]: Copied! <pre># \ud83d\udd0d AN\u00c1LISE DETALHADA DE PREDI\u00c7\u00d5ES INDIVIDUAIS\nprint(\"\ud83d\udd0d AN\u00c1LISE DE PREDI\u00c7\u00d5ES INDIVIDUAIS\")\nprint(\"=\" * 50)\n\n# Escolhendo alguns exemplos para an\u00e1lise\nexemplos = [42, 1000, 2500, 4000, 7500]\n\nfor i, item in enumerate(exemplos):\n    print(f\"\\n\ud83d\udcca EXEMPLO {i+1} (\u00cdndice {item}):\")\n    print(\"-\" * 30)\n    \n    # Informa\u00e7\u00f5es da predi\u00e7\u00e3o\n    classe_predita = np.argmax(predictions[item])\n    confianca = 100 * np.max(predictions[item])\n    classe_real = test_labels[item]\n    \n    print(f\"   \ud83e\udd16 Predi\u00e7\u00e3o: {class_names[classe_predita]} (confian\u00e7a: {confianca:.1f}%)\")\n    print(f\"   \u2705 Real: {class_names[classe_real]}\")\n    \n    # Verificando se acertou\n    if classe_predita == classe_real:\n        print(f\"   \ud83c\udfaf ACERTOU! \u2713\")\n    else:\n        print(f\"   \u274c ERROU! \u2717\")\n    \n    # Top 3 predi\u00e7\u00f5es\n    top3_indices = np.argsort(predictions[item])[-3:][::-1]\n    print(f\"   \ud83c\udfc6 Top 3 predi\u00e7\u00f5es:\")\n    for j, idx in enumerate(top3_indices):\n        prob = predictions[item][idx] * 100\n        print(f\"      {j+1}\u00ba: {class_names[idx]} ({prob:.1f}%)\")\n\n# Estat\u00edsticas gerais\nacertos = np.sum(predicted_classes == test_labels)\ntotal = len(test_labels)\nacuracia_final = acertos / total\n\nprint(f\"\\n\ud83d\udcc8 ESTAT\u00cdSTICAS GERAIS:\")\nprint(f\"   \u2705 Acertos: {acertos:,} de {total:,}\")\nprint(f\"   \ud83d\udcca Acur\u00e1cia: {acuracia_final:.3f} ({acuracia_final*100:.1f}%)\")\nprint(f\"   \u274c Erros: {total - acertos:,}\")\n</pre> # \ud83d\udd0d AN\u00c1LISE DETALHADA DE PREDI\u00c7\u00d5ES INDIVIDUAIS print(\"\ud83d\udd0d AN\u00c1LISE DE PREDI\u00c7\u00d5ES INDIVIDUAIS\") print(\"=\" * 50)  # Escolhendo alguns exemplos para an\u00e1lise exemplos = [42, 1000, 2500, 4000, 7500]  for i, item in enumerate(exemplos):     print(f\"\\n\ud83d\udcca EXEMPLO {i+1} (\u00cdndice {item}):\")     print(\"-\" * 30)          # Informa\u00e7\u00f5es da predi\u00e7\u00e3o     classe_predita = np.argmax(predictions[item])     confianca = 100 * np.max(predictions[item])     classe_real = test_labels[item]          print(f\"   \ud83e\udd16 Predi\u00e7\u00e3o: {class_names[classe_predita]} (confian\u00e7a: {confianca:.1f}%)\")     print(f\"   \u2705 Real: {class_names[classe_real]}\")          # Verificando se acertou     if classe_predita == classe_real:         print(f\"   \ud83c\udfaf ACERTOU! \u2713\")     else:         print(f\"   \u274c ERROU! \u2717\")          # Top 3 predi\u00e7\u00f5es     top3_indices = np.argsort(predictions[item])[-3:][::-1]     print(f\"   \ud83c\udfc6 Top 3 predi\u00e7\u00f5es:\")     for j, idx in enumerate(top3_indices):         prob = predictions[item][idx] * 100         print(f\"      {j+1}\u00ba: {class_names[idx]} ({prob:.1f}%)\")  # Estat\u00edsticas gerais acertos = np.sum(predicted_classes == test_labels) total = len(test_labels) acuracia_final = acertos / total  print(f\"\\n\ud83d\udcc8 ESTAT\u00cdSTICAS GERAIS:\") print(f\"   \u2705 Acertos: {acertos:,} de {total:,}\") print(f\"   \ud83d\udcca Acur\u00e1cia: {acuracia_final:.3f} ({acuracia_final*100:.1f}%)\") print(f\"   \u274c Erros: {total - acertos:,}\") In\u00a0[35]: Copied! <pre>def plot_image(i, predictions_array, true_label, img):\n  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n  plt.grid(False)\n  plt.xticks([])\n  plt.yticks([])\n\n  plt.imshow(img, cmap=plt.cm.binary)\n\n  predicted_label = np.argmax(predictions_array)\n  if predicted_label == true_label:\n    color = 'blue'\n  else:\n    color = 'red'\n\n  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n                                100*np.max(predictions_array),\n                                class_names[true_label]),\n                                color=color)\n</pre> def plot_image(i, predictions_array, true_label, img):   predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]   plt.grid(False)   plt.xticks([])   plt.yticks([])    plt.imshow(img, cmap=plt.cm.binary)    predicted_label = np.argmax(predictions_array)   if predicted_label == true_label:     color = 'blue'   else:     color = 'red'    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],                                 100*np.max(predictions_array),                                 class_names[true_label]),                                 color=color) In\u00a0[36]: Copied! <pre>plt.figure(figsize=(6,3))\nplt.subplot(1,2,1)\nplot_image(item, predictions, test_labels, test_images)\nplt.show()\n</pre> plt.figure(figsize=(6,3)) plt.subplot(1,2,1) plot_image(item, predictions, test_labels, test_images) plt.show() In\u00a0[\u00a0]: Copied! <pre># \ud83c\udfaf DESAFIO 3: SUA IMPLEMENTA\u00c7\u00c3O DA LENET-5\nprint(\"\ud83c\udfaf DESAFIO 3: IMPLEMENTANDO LENET-5\")\nprint(\"=\" * 50)\n\ndef create_lenet5_fashion():\n    \"\"\"\n    Implementa a arquitetura LeNet-5 adaptada para Fashion MNIST\n    \n    Arquitetura:\n    - Conv2D: 6 filtros 5x5\n    - AvgPool2D: 2x2\n    - Conv2D: 16 filtros 5x5  \n    - AvgPool2D: 2x2\n    - Flatten\n    - Dense: 120 neur\u00f4nios\n    - Dense: 84 neur\u00f4nios\n    - Dense: 10 classes (softmax)\n    \"\"\"\n    \n    print(\"\ud83c\udfd7\ufe0f Construindo LeNet-5...\")\n    \n    # COMPLETE A IMPLEMENTA\u00c7\u00c3O AQUI:\n    # ====================================\n    \n    model = keras.Sequential([\n        # \ud83d\udd0d PRIMEIRA CAMADA CONVOLUCIONAL\n        # layers.Conv2D(?, (?,?), activation='?', input_shape=(28,28,1)),\n        \n        # \ud83c\udfca PRIMEIRA CAMADA DE POOLING  \n        # layers.AveragePooling2D((?,?)),\n        \n        # \ud83d\udd0d SEGUNDA CAMADA CONVOLUCIONAL\n        # layers.Conv2D(?, (?,?), activation='?'),\n        \n        # \ud83c\udfca SEGUNDA CAMADA DE POOLING\n        # layers.AveragePooling2D((?,?)),\n        \n        # \ud83d\udccf FLATTEN\n        # layers.Flatten(),\n        \n        # \ud83e\udde0 CAMADAS DENSAS\n        # layers.Dense(?, activation='?'),\n        # layers.Dense(?, activation='?'),\n        # layers.Dense(?, activation='?')  # Sa\u00edda\n    ])\n    \n    # ====================================\n    \n    return model\n\n# TESTE SUA IMPLEMENTA\u00c7\u00c3O:\nprint(\"\ud83e\uddea TESTANDO SUA IMPLEMENTA\u00c7\u00c3O:\")\n\n# Descomente as linhas abaixo ap\u00f3s implementar:\n# lenet5_model = create_lenet5_fashion()\n# lenet5_model.summary()\n\n# print(f\"\\n\ud83d\udcca COMPARA\u00c7\u00c3O COM CNN SIMPLES:\")\n# print(f\"   CNN Simples: {model.count_params():,} par\u00e2metros\")\n# print(f\"   LeNet-5: {lenet5_model.count_params():,} par\u00e2metros\")\n\nprint(\"\\n\ud83d\udca1 DICAS:\")\nprint(\"   \u2022 Use activation='tanh' para ser mais fiel ao original\")\nprint(\"   \u2022 Average pooling era usado na LeNet-5 original\")\nprint(\"   \u2022 Analise o n\u00famero de par\u00e2metros em cada camada\")\nprint(\"   \u2022 Compare a performance com sua CNN simples\")\n\nprint(\"\\n\ud83c\udfaf AP\u00d3S IMPLEMENTAR, TREINE E COMPARE:\")\nprint(\"   1. Compile o modelo\")\nprint(\"   2. Treine por algumas \u00e9pocas\")\nprint(\"   3. Compare m\u00e9tricas com CNN simples\")\nprint(\"   4. Analise diferen\u00e7as arquiteturais\")\n</pre> # \ud83c\udfaf DESAFIO 3: SUA IMPLEMENTA\u00c7\u00c3O DA LENET-5 print(\"\ud83c\udfaf DESAFIO 3: IMPLEMENTANDO LENET-5\") print(\"=\" * 50)  def create_lenet5_fashion():     \"\"\"     Implementa a arquitetura LeNet-5 adaptada para Fashion MNIST          Arquitetura:     - Conv2D: 6 filtros 5x5     - AvgPool2D: 2x2     - Conv2D: 16 filtros 5x5       - AvgPool2D: 2x2     - Flatten     - Dense: 120 neur\u00f4nios     - Dense: 84 neur\u00f4nios     - Dense: 10 classes (softmax)     \"\"\"          print(\"\ud83c\udfd7\ufe0f Construindo LeNet-5...\")          # COMPLETE A IMPLEMENTA\u00c7\u00c3O AQUI:     # ====================================          model = keras.Sequential([         # \ud83d\udd0d PRIMEIRA CAMADA CONVOLUCIONAL         # layers.Conv2D(?, (?,?), activation='?', input_shape=(28,28,1)),                  # \ud83c\udfca PRIMEIRA CAMADA DE POOLING           # layers.AveragePooling2D((?,?)),                  # \ud83d\udd0d SEGUNDA CAMADA CONVOLUCIONAL         # layers.Conv2D(?, (?,?), activation='?'),                  # \ud83c\udfca SEGUNDA CAMADA DE POOLING         # layers.AveragePooling2D((?,?)),                  # \ud83d\udccf FLATTEN         # layers.Flatten(),                  # \ud83e\udde0 CAMADAS DENSAS         # layers.Dense(?, activation='?'),         # layers.Dense(?, activation='?'),         # layers.Dense(?, activation='?')  # Sa\u00edda     ])          # ====================================          return model  # TESTE SUA IMPLEMENTA\u00c7\u00c3O: print(\"\ud83e\uddea TESTANDO SUA IMPLEMENTA\u00c7\u00c3O:\")  # Descomente as linhas abaixo ap\u00f3s implementar: # lenet5_model = create_lenet5_fashion() # lenet5_model.summary()  # print(f\"\\n\ud83d\udcca COMPARA\u00c7\u00c3O COM CNN SIMPLES:\") # print(f\"   CNN Simples: {model.count_params():,} par\u00e2metros\") # print(f\"   LeNet-5: {lenet5_model.count_params():,} par\u00e2metros\")  print(\"\\n\ud83d\udca1 DICAS:\") print(\"   \u2022 Use activation='tanh' para ser mais fiel ao original\") print(\"   \u2022 Average pooling era usado na LeNet-5 original\") print(\"   \u2022 Analise o n\u00famero de par\u00e2metros em cada camada\") print(\"   \u2022 Compare a performance com sua CNN simples\")  print(\"\\n\ud83c\udfaf AP\u00d3S IMPLEMENTAR, TREINE E COMPARE:\") print(\"   1. Compile o modelo\") print(\"   2. Treine por algumas \u00e9pocas\") print(\"   3. Compare m\u00e9tricas com CNN simples\") print(\"   4. Analise diferen\u00e7as arquiteturais\") In\u00a0[\u00a0]: Copied! <pre># \ud83d\udd0d AN\u00c1LISE COMPARATIVA (Complete ap\u00f3s implementar)\nprint(\"\ud83d\udd0d AN\u00c1LISE COMPARATIVA: CNN SIMPLES vs LENET-5\")\nprint(\"=\" * 60)\n\n# Esta se\u00e7\u00e3o deve ser preenchida ap\u00f3s voc\u00ea implementar a LeNet-5\n\nprint(\"\ud83d\udcca RESULTADOS COMPARATIVOS:\")\nprint(\"=\" * 30)\n\n# Exemplo de template para suas an\u00e1lises:\ncomparacao = {\n    'M\u00e9trica': ['Par\u00e2metros', 'Acur\u00e1cia Treino', 'Acur\u00e1cia Teste', 'Tempo Treino', 'Overfitting'],\n    'CNN Simples': ['?', '?', '?', '?', '?'],\n    'LeNet-5': ['?', '?', '?', '?', '?']\n}\n\nprint(\"| M\u00e9trica | CNN Simples | LeNet-5 |\")\nprint(\"|---------|-------------|---------|\")\nfor i, metrica in enumerate(comparacao['M\u00e9trica']):\n    print(f\"| {metrica} | {comparacao['CNN Simples'][i]} | {comparacao['LeNet-5'][i]} |\")\n\nprint(f\"\\n\ud83c\udfaf SUAS CONCLUS\u00d5ES:\")\nprint(\"=\" * 20)\nprint(\"1. Qual modelo teve melhor performance? Por qu\u00ea?\")\nprint(\"   Resposta: ________________________________\")\nprint(\"\")\nprint(\"2. Qual a principal diferen\u00e7a arquitetural?\")\nprint(\"   Resposta: ________________________________\")\nprint(\"\")\nprint(\"3. LeNet-5 ainda \u00e9 relevante hoje? Por qu\u00ea?\")\nprint(\"   Resposta: ________________________________\")\nprint(\"\")\nprint(\"4. O que voc\u00ea mudaria na LeNet-5 original?\")\nprint(\"   Resposta: ________________________________\")\n\nprint(f\"\\n\ud83c\udfc6 PR\u00d3XIMOS PASSOS:\")\nprint(\"   \u2022 Experimente diferentes fun\u00e7\u00f5es de ativa\u00e7\u00e3o\")\nprint(\"   \u2022 Teste com dropout para reduzir overfitting\")\nprint(\"   \u2022 Implemente data augmentation\")\nprint(\"   \u2022 Compare com arquiteturas modernas\")\n</pre> # \ud83d\udd0d AN\u00c1LISE COMPARATIVA (Complete ap\u00f3s implementar) print(\"\ud83d\udd0d AN\u00c1LISE COMPARATIVA: CNN SIMPLES vs LENET-5\") print(\"=\" * 60)  # Esta se\u00e7\u00e3o deve ser preenchida ap\u00f3s voc\u00ea implementar a LeNet-5  print(\"\ud83d\udcca RESULTADOS COMPARATIVOS:\") print(\"=\" * 30)  # Exemplo de template para suas an\u00e1lises: comparacao = {     'M\u00e9trica': ['Par\u00e2metros', 'Acur\u00e1cia Treino', 'Acur\u00e1cia Teste', 'Tempo Treino', 'Overfitting'],     'CNN Simples': ['?', '?', '?', '?', '?'],     'LeNet-5': ['?', '?', '?', '?', '?'] }  print(\"| M\u00e9trica | CNN Simples | LeNet-5 |\") print(\"|---------|-------------|---------|\") for i, metrica in enumerate(comparacao['M\u00e9trica']):     print(f\"| {metrica} | {comparacao['CNN Simples'][i]} | {comparacao['LeNet-5'][i]} |\")  print(f\"\\n\ud83c\udfaf SUAS CONCLUS\u00d5ES:\") print(\"=\" * 20) print(\"1. Qual modelo teve melhor performance? Por qu\u00ea?\") print(\"   Resposta: ________________________________\") print(\"\") print(\"2. Qual a principal diferen\u00e7a arquitetural?\") print(\"   Resposta: ________________________________\") print(\"\") print(\"3. LeNet-5 ainda \u00e9 relevante hoje? Por qu\u00ea?\") print(\"   Resposta: ________________________________\") print(\"\") print(\"4. O que voc\u00ea mudaria na LeNet-5 original?\") print(\"   Resposta: ________________________________\")  print(f\"\\n\ud83c\udfc6 PR\u00d3XIMOS PASSOS:\") print(\"   \u2022 Experimente diferentes fun\u00e7\u00f5es de ativa\u00e7\u00e3o\") print(\"   \u2022 Teste com dropout para reduzir overfitting\") print(\"   \u2022 Implemente data augmentation\") print(\"   \u2022 Compare com arquiteturas modernas\") In\u00a0[\u00a0]: Copied! <pre># \ud83d\ude80 DESAFIO EXTRA: CNN PARA CIFAR-10\nprint(\"\ud83d\ude80 DESAFIO EXTRA: CNN PARA CIFAR-10\")\nprint(\"=\" * 50)\n\n# Passo 1: Carregamento e Prepara\u00e7\u00e3o dos Dados\nprint(\"\ud83d\udcca CARREGANDO CIFAR-10...\")\n\n# COMPLETE AQUI:\n# ===============================================\n\n# (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n\n# print(f\"Formato dos dados: {x_train.shape}\")\n# print(f\"Classes: {np.unique(y_train)}\")\n\n# # Normaliza\u00e7\u00e3o\n# x_train = x_train.astype('float32') / 255.0\n# x_test = x_test.astype('float32') / 255.0\n\n# # Nomes das classes\n# cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n#                   'dog', 'frog', 'horse', 'ship', 'truck']\n\n# ===============================================\n\nprint(\"\u2705 Dados carregados e normalizados!\")\n\n# Passo 2: Visualiza\u00e7\u00e3o dos Dados\nprint(\"\\n\ud83d\uddbc\ufe0f VISUALIZANDO AMOSTRAS DO CIFAR-10:\")\n\n# COMPLETE A VISUALIZA\u00c7\u00c3O:\n# ===============================================\n\n# fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n# for i in range(10):\n#     ax = axes[i//5, i%5]\n#     ax.imshow(x_train[i])\n#     ax.set_title(f'{cifar10_classes[y_train[i][0]]}')\n#     ax.axis('off')\n# plt.show()\n\n# ===============================================\n\n# Passo 3: Constru\u00e7\u00e3o da CNN\nprint(\"\\n\ud83c\udfd7\ufe0f CONSTRUINDO CNN PARA CIFAR-10:\")\n\ndef create_cifar10_cnn():\n    \"\"\"\n    Crie uma CNN robusta para CIFAR-10\n    \n    Requisitos:\n    - Pelo menos 3 blocos convolucionais\n    - Batch Normalization\n    - Dropout para regulariza\u00e7\u00e3o\n    - Data Augmentation\n    \"\"\"\n    \n    # COMPLETE SUA IMPLEMENTA\u00c7\u00c3O:\n    # ===================================\n    \n    # model = keras.Sequential([\n    #     # Bloco 1: Conv + BatchNorm + Pool + Dropout\n    #     \n    #     # Bloco 2: Conv + BatchNorm + Pool + Dropout\n    #     \n    #     # Bloco 3: Conv + BatchNorm + Pool + Dropout\n    #     \n    #     # Classificador: Flatten + Dense + Dropout + Output\n    # ])\n    \n    # ===================================\n    \n    return None  # Substitua por seu modelo\n\n# Passo 4: Data Augmentation\nprint(\"\\n\ud83d\udd04 CONFIGURANDO DATA AUGMENTATION:\")\n\n# COMPLETE A CONFIGURA\u00c7\u00c3O:\n# ===============================================\n\n# datagen = ImageDataGenerator(\n#     rotation_range=15,\n#     width_shift_range=0.1,\n#     height_shift_range=0.1,\n#     horizontal_flip=True,\n#     zoom_range=0.1\n# )\n\n# ===============================================\n\n# Passo 5: Treinamento\nprint(\"\\n\ud83d\ude80 TREINAMENTO DO MODELO:\")\n\n# COMPLETE O TREINAMENTO:\n# ===============================================\n\n# model = create_cifar10_cnn()\n# model.compile(\n#     optimizer='adam',\n#     loss='sparse_categorical_crossentropy', \n#     metrics=['accuracy']\n# )\n\n# # Callbacks\n# callbacks = [\n#     keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n#     keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=5)\n# ]\n\n# # Treinamento\n# history = model.fit(\n#     datagen.flow(x_train, y_train, batch_size=32),\n#     steps_per_epoch=len(x_train) // 32,\n#     epochs=50,\n#     validation_data=(x_test, y_test),\n#     callbacks=callbacks\n# )\n\n# ===============================================\n\nprint(\"\u2705 Configure seu modelo e inicie o treinamento!\")\n\n# Passo 6: Avalia\u00e7\u00e3o e An\u00e1lise\nprint(\"\\n\ud83d\udcca TEMPLATE PARA AN\u00c1LISE DE RESULTADOS:\")\nprint(\"=\" * 40)\n\nresultado_template = \"\"\"\n\ud83c\udfaf RESULTADOS FINAIS:\n   Acur\u00e1cia no teste: ____%\n   Tempo de treinamento: ____\n   \u00c9pocas necess\u00e1rias: ____\n\n\ud83d\udd0d AN\u00c1LISE:\n   1. Overfitting observado? ____\n   2. Data augmentation ajudou? ____\n   3. Qual classe teve pior performance? ____\n   4. Principais desafios encontrados: ____\n\n\ud83d\ude80 MELHORIAS PROPOSTAS:\n   1. ____________________\n   2. ____________________\n   3. ____________________\n\n\ud83d\udcc8 COMPARA\u00c7\u00c3O COM FASHION MNIST:\n   Dificuldade: ____x maior\n   Tempo: ____x maior\n   Performance: ____% vs ____%\n\"\"\"\n\nprint(resultado_template)\n\nprint(\"\\n\ud83d\udca1 DICAS IMPORTANTES:\")\nprint(\"   \u2022 CIFAR-10 requer mais \u00e9pocas que Fashion MNIST\")\nprint(\"   \u2022 Use GPU se dispon\u00edvel para acelerar treinamento\")\nprint(\"   \u2022 Monitore overfitting cuidadosamente\")\nprint(\"   \u2022 Experimente diferentes learning rates\")\nprint(\"   \u2022 Transfer learning pode ser uma alternativa\")\n\nprint(\"\\n\ud83c\udfc6 META FINAL:\")\nprint(\"   Alcan\u00e7ar &gt;80% acur\u00e1cia no CIFAR-10 com sua CNN!\")\n</pre> # \ud83d\ude80 DESAFIO EXTRA: CNN PARA CIFAR-10 print(\"\ud83d\ude80 DESAFIO EXTRA: CNN PARA CIFAR-10\") print(\"=\" * 50)  # Passo 1: Carregamento e Prepara\u00e7\u00e3o dos Dados print(\"\ud83d\udcca CARREGANDO CIFAR-10...\")  # COMPLETE AQUI: # ===============================================  # (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()  # print(f\"Formato dos dados: {x_train.shape}\") # print(f\"Classes: {np.unique(y_train)}\")  # # Normaliza\u00e7\u00e3o # x_train = x_train.astype('float32') / 255.0 # x_test = x_test.astype('float32') / 255.0  # # Nomes das classes # cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', #                   'dog', 'frog', 'horse', 'ship', 'truck']  # ===============================================  print(\"\u2705 Dados carregados e normalizados!\")  # Passo 2: Visualiza\u00e7\u00e3o dos Dados print(\"\\n\ud83d\uddbc\ufe0f VISUALIZANDO AMOSTRAS DO CIFAR-10:\")  # COMPLETE A VISUALIZA\u00c7\u00c3O: # ===============================================  # fig, axes = plt.subplots(2, 5, figsize=(12, 6)) # for i in range(10): #     ax = axes[i//5, i%5] #     ax.imshow(x_train[i]) #     ax.set_title(f'{cifar10_classes[y_train[i][0]]}') #     ax.axis('off') # plt.show()  # ===============================================  # Passo 3: Constru\u00e7\u00e3o da CNN print(\"\\n\ud83c\udfd7\ufe0f CONSTRUINDO CNN PARA CIFAR-10:\")  def create_cifar10_cnn():     \"\"\"     Crie uma CNN robusta para CIFAR-10          Requisitos:     - Pelo menos 3 blocos convolucionais     - Batch Normalization     - Dropout para regulariza\u00e7\u00e3o     - Data Augmentation     \"\"\"          # COMPLETE SUA IMPLEMENTA\u00c7\u00c3O:     # ===================================          # model = keras.Sequential([     #     # Bloco 1: Conv + BatchNorm + Pool + Dropout     #          #     # Bloco 2: Conv + BatchNorm + Pool + Dropout     #          #     # Bloco 3: Conv + BatchNorm + Pool + Dropout     #          #     # Classificador: Flatten + Dense + Dropout + Output     # ])          # ===================================          return None  # Substitua por seu modelo  # Passo 4: Data Augmentation print(\"\\n\ud83d\udd04 CONFIGURANDO DATA AUGMENTATION:\")  # COMPLETE A CONFIGURA\u00c7\u00c3O: # ===============================================  # datagen = ImageDataGenerator( #     rotation_range=15, #     width_shift_range=0.1, #     height_shift_range=0.1, #     horizontal_flip=True, #     zoom_range=0.1 # )  # ===============================================  # Passo 5: Treinamento print(\"\\n\ud83d\ude80 TREINAMENTO DO MODELO:\")  # COMPLETE O TREINAMENTO: # ===============================================  # model = create_cifar10_cnn() # model.compile( #     optimizer='adam', #     loss='sparse_categorical_crossentropy',  #     metrics=['accuracy'] # )  # # Callbacks # callbacks = [ #     keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True), #     keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=5) # ]  # # Treinamento # history = model.fit( #     datagen.flow(x_train, y_train, batch_size=32), #     steps_per_epoch=len(x_train) // 32, #     epochs=50, #     validation_data=(x_test, y_test), #     callbacks=callbacks # )  # ===============================================  print(\"\u2705 Configure seu modelo e inicie o treinamento!\")  # Passo 6: Avalia\u00e7\u00e3o e An\u00e1lise print(\"\\n\ud83d\udcca TEMPLATE PARA AN\u00c1LISE DE RESULTADOS:\") print(\"=\" * 40)  resultado_template = \"\"\" \ud83c\udfaf RESULTADOS FINAIS:    Acur\u00e1cia no teste: ____%    Tempo de treinamento: ____    \u00c9pocas necess\u00e1rias: ____  \ud83d\udd0d AN\u00c1LISE:    1. Overfitting observado? ____    2. Data augmentation ajudou? ____    3. Qual classe teve pior performance? ____    4. Principais desafios encontrados: ____  \ud83d\ude80 MELHORIAS PROPOSTAS:    1. ____________________    2. ____________________    3. ____________________  \ud83d\udcc8 COMPARA\u00c7\u00c3O COM FASHION MNIST:    Dificuldade: ____x maior    Tempo: ____x maior    Performance: ____% vs ____% \"\"\"  print(resultado_template)  print(\"\\n\ud83d\udca1 DICAS IMPORTANTES:\") print(\"   \u2022 CIFAR-10 requer mais \u00e9pocas que Fashion MNIST\") print(\"   \u2022 Use GPU se dispon\u00edvel para acelerar treinamento\") print(\"   \u2022 Monitore overfitting cuidadosamente\") print(\"   \u2022 Experimente diferentes learning rates\") print(\"   \u2022 Transfer learning pode ser uma alternativa\")  print(\"\\n\ud83c\udfc6 META FINAL:\") print(\"   Alcan\u00e7ar &gt;80% acur\u00e1cia no CIFAR-10 com sua CNN!\")"},{"location":"aulas/IA/lab08/cnn%20copy/#laboratorio-redes-neurais-convolucionais-cnns-do-basico-ao-avancado","title":"Laborat\u00f3rio: Redes Neurais Convolucionais (CNNs) - Do B\u00e1sico ao Avan\u00e7ado\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#objetivos-de-aprendizagem","title":"\ud83c\udfaf Objetivos de Aprendizagem\u00b6","text":"<p>Ao final desta aula, voc\u00ea ser\u00e1 capaz de:</p> <ol> <li>Compreender os fundamentos matem\u00e1ticos das CNNs</li> <li>Implementar redes convolucionais do zero usando TensorFlow/Keras</li> <li>Aplicar t\u00e9cnicas avan\u00e7adas como data augmentation e transfer learning</li> <li>Comparar CNNs com MLPs tradicionais</li> <li>Resolver problemas reais de vis\u00e3o computacional</li> <li>Otimizar modelos para diferentes cen\u00e1rios</li> </ol>"},{"location":"aulas/IA/lab08/cnn%20copy/#material-de-apoio-obrigatorio","title":"\ud83d\udcda Material de Apoio Obrigat\u00f3rio\u00b6","text":"<p>\ud83d\udcd6 Leitura complementar: cnn_guia_completo.md - Guia te\u00f3rico completo sobre CNNs</p>"},{"location":"aulas/IA/lab08/cnn%20copy/#por-que-cnns-sao-revolucionarias","title":"\ud83e\udde0 Por que CNNs s\u00e3o Revolucion\u00e1rias?\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#problema-com-mlps-tradicionais","title":"\ud83d\udca1 Problema com MLPs Tradicionais\u00b6","text":"<p>Imagine processar uma imagem 400\u00d7600 pixels com um MLP:</p> <ul> <li>Par\u00e2metros: 400 \u00d7 600 \u00d7 100 + 100 = 24.000.100 par\u00e2metros s\u00f3 na primeira camada!</li> <li>Problemas:<ul> <li>\ud83d\udeab Ignora estrutura espacial</li> <li>\ud83d\udeab Sens\u00edvel \u00e0 posi\u00e7\u00e3o</li> <li>\ud83d\udeab Computacionalmente caro</li> <li>\ud83d\udeab Overfitting garantido</li> </ul> </li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#solucao-das-cnns","title":"\u2728 Solu\u00e7\u00e3o das CNNs\u00b6","text":"<ul> <li>\u2705 Compartilhamento de pesos: Mesmos filtros em toda imagem</li> <li>\u2705 Invari\u00e2ncia espacial: Reconhece padr\u00f5es independente da posi\u00e7\u00e3o</li> <li>\u2705 Hierarquia de features: Bordas \u2192 Formas \u2192 Objetos</li> <li>\u2705 Efici\u00eancia: Drasticamente menos par\u00e2metros</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#inspiracao-biologica","title":"\ud83d\udd2c Inspira\u00e7\u00e3o Biol\u00f3gica\u00b6","text":"<p>As CNNs s\u00e3o inspiradas no c\u00f3rtex visual dos mam\u00edferos:</p> <pre><code>C\u00e9lulas simples \u2192 C\u00e9lulas complexas \u2192 \u00c1rea V1 \u2192 V2 \u2192 V4 \u2192 IT\n      \u2193              \u2193              \u2193    \u2193    \u2193    \u2193\n   Bordas         Formas       Texturas \u2192 Partes \u2192 Objetos\n</code></pre>"},{"location":"aulas/IA/lab08/cnn%20copy/#configuracao-do-ambiente","title":"\ud83d\udd27 Configura\u00e7\u00e3o do Ambiente\u00b6","text":"<p>Vamos come\u00e7ar importando todas as bibliotecas necess\u00e1rias e configurando o ambiente de desenvolvimento.</p>"},{"location":"aulas/IA/lab08/cnn%20copy/#parte-1-cnn-vs-mlp-comparacao-pratica","title":"\ud83d\udcca Parte 1: CNN vs MLP - Compara\u00e7\u00e3o Pr\u00e1tica\u00b6","text":"<p>Vamos demonstrar por que CNNs s\u00e3o superiores para dados de imagem atrav\u00e9s de uma compara\u00e7\u00e3o direta.</p>"},{"location":"aulas/IA/lab08/cnn%20copy/#analise-de-parametros","title":"\ud83d\udd0d An\u00e1lise de Par\u00e2metros\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#visualizando-a-diferenca-estrutural","title":"\ud83c\udfa8 Visualizando a Diferen\u00e7a Estrutural\u00b6","text":"<p>Problemas do MLP para Imagens:</p> <ul> <li>\ud83d\udeab Perda de estrutura espacial: Pixels s\u00e3o tratados independentemente</li> <li>\ud83d\udeab Invari\u00e2ncia limitada: Sens\u00edvel \u00e0 posi\u00e7\u00e3o dos objetos</li> <li>\ud83d\udeab Explos\u00e3o de par\u00e2metros: Cresce exponencialmente com o tamanho da imagem</li> <li>\ud83d\udeab Overfitting: Muitos par\u00e2metros para poucos dados</li> </ul> <p>Vantagens da CNN:</p> <ul> <li>\u2705 Preserva estrutura espacial: Convolu\u00e7\u00f5es mant\u00eam rela\u00e7\u00f5es espaciais</li> <li>\u2705 Compartilhamento de pesos: Mesmo filtro detecta padr\u00e3o em qualquer posi\u00e7\u00e3o</li> <li>\u2705 Hierarquia de features: Aprende caracter\u00edsticas progressivamente</li> <li>\u2705 Efici\u00eancia computacional: Muito menos par\u00e2metros</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#parte-2-operacao-de-convolucao-o-coracao-das-cnns","title":"\ud83d\udd2c Parte 2: Opera\u00e7\u00e3o de Convolu\u00e7\u00e3o - O Cora\u00e7\u00e3o das CNNs\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#fundamentos-matematicos","title":"\ud83e\uddee Fundamentos Matem\u00e1ticos\u00b6","text":"<p>A convolu\u00e7\u00e3o \u00e9 uma opera\u00e7\u00e3o matem\u00e1tica fundamental que permite filtragem no dom\u00ednio espacial. \u00c9 aplicada atrav\u00e9s de filtros/kernels que \"varrem\" a imagem para detectar padr\u00f5es espec\u00edficos.</p>"},{"location":"aulas/IA/lab08/cnn%20copy/#formula-da-convolucao-2d","title":"\ud83d\udcd0 F\u00f3rmula da Convolu\u00e7\u00e3o 2D:\u00b6","text":"<pre><code>S(i,j) = (I * K)(i,j) = \u03a3\u03a3 I(i+m, j+n) \u00d7 K(m,n)\n                        m n\n</code></pre> <p>Onde:</p> <ul> <li><code>I</code>: Imagem de entrada</li> <li><code>K</code>: Kernel/filtro</li> <li><code>S</code>: Feature map (resultado)</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#como-funciona-a-convolucao","title":"\ud83c\udfaf Como Funciona a Convolu\u00e7\u00e3o\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#1-kernel-percorre-a-imagem","title":"1\ufe0f\u20e3 Kernel percorre a imagem\u00b6","text":"<p>O kernel (cinza) varre a imagem (azul) produzindo o feature map (verde)</p>"},{"location":"aulas/IA/lab08/cnn%20copy/#2-operacao-em-cada-posicao","title":"2\ufe0f\u20e3 Opera\u00e7\u00e3o em cada posi\u00e7\u00e3o\u00b6","text":"<p>Produto elemento a elemento + soma = valor do pixel no feature map</p>"},{"location":"aulas/IA/lab08/cnn%20copy/#3-resultado-para-cada-pixel","title":"3\ufe0f\u20e3 Resultado para cada pixel\u00b6","text":"<p>Visualiza\u00e7\u00e3o 3D da opera\u00e7\u00e3o de convolu\u00e7\u00e3o</p>"},{"location":"aulas/IA/lab08/cnn%20copy/#4-resultado-final-na-imagem","title":"4\ufe0f\u20e3 Resultado final na imagem\u00b6","text":"<p>Diferentes kernels detectam diferentes caracter\u00edsticas</p>"},{"location":"aulas/IA/lab08/cnn%20copy/#implementacao-em-codigo-tensorflowkeras","title":"\ud83d\udcbb Implementa\u00e7\u00e3o em C\u00f3digo - TensorFlow/Keras\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#camada-convolucional","title":"\ud83d\udd27 Camada Convolucional\u00b6","text":"<p>A implementa\u00e7\u00e3o de uma camada convolucional \u00e9 surpreendentemente simples:</p> <pre>layers.Conv2D(filters=100, kernel_size=(3, 3), activation='relu', input_shape=(height, width, channels))\n</pre> <p>Par\u00e2metros principais:</p> <ul> <li>\ud83d\udd22 filters: N\u00famero de filtros (kernels) - define quantos feature maps s\u00e3o gerados</li> <li>\ud83d\udcd0 kernel_size: Tamanho do filtro - (3,3) \u00e9 mais comum</li> <li>\u26a1 activation: Fun\u00e7\u00e3o de ativa\u00e7\u00e3o aplicada ap\u00f3s convolu\u00e7\u00e3o</li> <li>\ud83d\udcca input_shape: Formato da entrada (apenas na primeira camada)</li> </ul> <p>Par\u00e2metros avan\u00e7ados:</p> <ul> <li>\ud83d\udc63 strides: Passo do filtro (default: (1,1))</li> <li>\ud83c\udfaf padding: 'valid' (sem padding) ou 'same' (mant\u00e9m dimens\u00e3o)</li> <li>\ud83d\udd04 dilation_rate: Convolu\u00e7\u00f5es dilatadas para campo receptivo maior</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#desafio-1-analise-de-parametros","title":"\ud83c\udfaf Desafio 1: An\u00e1lise de Par\u00e2metros\u00b6","text":"<p>Pergunta: Compare a quantidade de <code>Total params</code> - em uma rede CNN esse valor \u00e9 menor ou maior comparado com uma rede MLP?</p>"},{"location":"aulas/IA/lab08/cnn%20copy/#para-responder","title":"\ud83d\udca1 Para Responder:\u00b6","text":"<ol> <li>Observe os modelos criados acima</li> <li>Compare CNN (100 filtros 3\u00d73) vs MLP (entrada flattened)</li> <li>Analise como o compartilhamento de pesos afeta o total</li> <li>Considere o que acontece com imagens maiores</li> </ol>"},{"location":"aulas/IA/lab08/cnn%20copy/#experimento-guiado","title":"\ud83d\udd0d Experimento Guiado:\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#parte-3-pooling-reduzindo-dimensionalidade-com-inteligencia","title":"\ud83c\udfca Parte 3: Pooling - Reduzindo Dimensionalidade com Intelig\u00eancia\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#o-que-e-pooling","title":"\ud83c\udfaf O que \u00e9 Pooling?\u00b6","text":"<p>O pooling \u00e9 uma opera\u00e7\u00e3o de subsampling que:</p> <ul> <li>\ud83d\udcc9 Reduz dimensionalidade dos feature maps</li> <li>\ud83c\udfaf Mant\u00e9m caracter\u00edsticas importantes</li> <li>\u26a1 Diminui custo computacional</li> <li>\ud83d\udee1\ufe0f Adiciona invari\u00e2ncia a pequenas transla\u00e7\u00f5es</li> <li>\ud83d\udeab Reduz overfitting</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#tipos-de-pooling","title":"\ud83d\udd0d Tipos de Pooling\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#1-max-pooling-mais-comum","title":"1\ufe0f\u20e3 Max Pooling (Mais Comum)\u00b6","text":"<ul> <li>Opera\u00e7\u00e3o: Seleciona o valor m\u00e1ximo na janela</li> <li>Intui\u00e7\u00e3o: Preserva as caracter\u00edsticas mais ativas</li> <li>Uso: Detec\u00e7\u00e3o de bordas, texturas</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#2-average-pooling","title":"2\ufe0f\u20e3 Average Pooling\u00b6","text":"<ul> <li>Opera\u00e7\u00e3o: Calcula a m\u00e9dia dos valores na janela</li> <li>Intui\u00e7\u00e3o: Suaviza e reduz ru\u00eddo</li> <li>Uso: Menos comum, algumas arquiteturas espec\u00edficas</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#3-global-average-pooling","title":"3\ufe0f\u20e3 Global Average Pooling\u00b6","text":"<ul> <li>Opera\u00e7\u00e3o: M\u00e9dia de todo o feature map \u2192 1 valor</li> <li>Vantagem: Substitui camadas Dense finais</li> <li>Benef\u00edcio: Reduz drasticamente overfitting</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#matematica-do-pooling","title":"\ud83d\udcd0 Matem\u00e1tica do Pooling\u00b6","text":"<p>Max Pooling 2\u00d72:</p> <pre><code>Entrada (4\u00d74):           Sa\u00edda (2\u00d72):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u25021  3  2  4\u2502           \u2502max(1,3,0,1) max(2,4,1,2)\u2502\n\u25020  1  1  2\u2502    \u2192      \u2502    = 3         = 4     \u2502\n\u25022  2  0  1\u2502           \u2502max(2,2,3,1) max(0,1,3,5)\u2502\n\u25023  1  3  5\u2502           \u2502    = 3         = 5     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nResultado: [3, 4]\n           [3, 5]\n</code></pre>"},{"location":"aulas/IA/lab08/cnn%20copy/#visualizacao-do-pooling","title":"\ud83d\uddbc\ufe0f Visualiza\u00e7\u00e3o do Pooling\u00b6","text":"<p>O pooling 2\u00d72 reduz a dimensionalidade pela metade</p>"},{"location":"aulas/IA/lab08/cnn%20copy/#resultado-visual","title":"\ud83d\udcca Resultado Visual\u00b6","text":"<p>Compara\u00e7\u00e3o: imagem original vs ap\u00f3s max pooling</p>"},{"location":"aulas/IA/lab08/cnn%20copy/#parametros-do-pooling","title":"\u26a1 Par\u00e2metros do Pooling\u00b6","text":"<p>Caracter\u00edsticas importantes:</p> <ul> <li>\ud83d\udd22 Zero par\u00e2metros: N\u00e3o h\u00e1 pesos para aprender</li> <li>\ud83c\udf9b\ufe0f Pool size: Tamanho da janela (geralmente 2\u00d72)</li> <li>\ud83d\udc63 Stride: Passo do deslocamento (geralmente = pool_size)</li> <li>\ud83d\udcd0 Padding: Raramente usado em pooling</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#implementacao-em-codigo","title":"Implementa\u00e7\u00e3o em c\u00f3digo\u00b6","text":"<p>Para implementar pooling em Keras \u00e9 muito simples:</p> <pre>layers.MaxPool2D(pool_size=2, strides=2)\n</pre> <p>Par\u00e2metros principais:</p> <ul> <li>\ud83c\udf9b\ufe0f pool_size: Tamanho da janela de pooling (2\u00d72 \u00e9 padr\u00e3o)</li> <li>\ud83d\udc63 strides: Passo do deslocamento (geralmente = pool_size)</li> <li>\ud83c\udfaf padding: 'valid' (padr\u00e3o) ou 'same'</li> </ul> <p>Outros tipos:</p> <ul> <li><code>layers.AveragePooling2D()</code>: Average pooling</li> <li><code>layers.GlobalMaxPooling2D()</code>: Max pooling global</li> <li><code>layers.GlobalAveragePooling2D()</code>: Average pooling global</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#desafio-2-analise-do-pooling","title":"\ud83c\udfaf Desafio 2: An\u00e1lise do Pooling\u00b6","text":"<p>Perguntas:</p> <ol> <li>Qual a dimens\u00e3o da imagem antes e depois do pooling?</li> <li>A camada de pooling alterou o <code>total params</code>?</li> <li>Por que o pooling n\u00e3o tem par\u00e2metros trein\u00e1veis?</li> </ol>"},{"location":"aulas/IA/lab08/cnn%20copy/#analise-guiada","title":"\ud83d\udd0d An\u00e1lise Guiada:\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#parte-4-arquitetura-completa-da-cnn-extrator-classificador","title":"\ud83d\udd0d Parte 4: Arquitetura Completa da CNN - Extrator + Classificador\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#estrutura-geral","title":"\ud83c\udfd7\ufe0f Estrutura Geral\u00b6","text":"<p>Uma CNN completa \u00e9 composta por duas partes principais:</p>"},{"location":"aulas/IA/lab08/cnn%20copy/#1-extrator-de-caracteristicas-feature-extractor","title":"1\ufe0f\u20e3 Extrator de Caracter\u00edsticas (Feature Extractor)\u00b6","text":"<ul> <li>Fun\u00e7\u00e3o: Detectar padr\u00f5es visuais hier\u00e1rquicos</li> <li>Componentes: Conv2D + Pooling + Ativa\u00e7\u00e3o</li> <li>Processo: Bordas \u2192 Texturas \u2192 Formas \u2192 Objetos</li> <li>Sa\u00edda: Feature maps com caracter\u00edsticas extra\u00eddas</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#2-classificador-classifier","title":"2\ufe0f\u20e3 Classificador (Classifier)\u00b6","text":"<ul> <li>Fun\u00e7\u00e3o: Tomar decis\u00e3o baseada nas caracter\u00edsticas</li> <li>Componentes: Flatten + Dense layers (MLP)</li> <li>Processo: Features \u2192 Combina\u00e7\u00f5es \u2192 Probabilidades</li> <li>Sa\u00edda: Classifica\u00e7\u00e3o final</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#arquitetura-visual","title":"\ud83c\udfaf Arquitetura Visual\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#fluxo-de-processamento","title":"\ud83d\udd04 Fluxo de Processamento\u00b6","text":"<pre><code>Imagem \u2192 [Conv\u2192ReLU\u2192Pool]\u00d7N \u2192 Flatten \u2192 [Dense\u2192ReLU]\u00d7M \u2192 Softmax \u2192 Classes\n  \u2193             \u2193                \u2193            \u2193              \u2193\n Raw         Features         Vector      Hidden        Probabilities\nPixels      Extraction      Formato 1D   Layers        por Classe\n</code></pre>"},{"location":"aulas/IA/lab08/cnn%20copy/#hierarquia-de-caracteristicas","title":"\ud83e\uddee Hierarquia de Caracter\u00edsticas\u00b6","text":"<p>| Camada | Detecta | Exemplo | |--------|---------|---------| | Conv1 | \ud83d\udd0d Bordas, linhas | <code>/</code>, <code>\\</code>, <code>\u2014</code>, <code>|</code> | | Conv2 | \ud83d\udd3a Formas simples | Cantos, curvas | | Conv3 | \ud83c\udfaf Partes de objetos | Olhos, rodas, janelas | | Conv4+ | \ud83d\uddbc\ufe0f Objetos completos | Faces, carros, casas |</p>"},{"location":"aulas/IA/lab08/cnn%20copy/#por-que-essa-divisao","title":"\u26a1 Por que essa Divis\u00e3o?\u00b6","text":"<p>Extrator (Convolucional):</p> <ul> <li>\u2705 Preserva informa\u00e7\u00e3o espacial</li> <li>\u2705 Detecta padr\u00f5es locais</li> <li>\u2705 Invariante \u00e0 posi\u00e7\u00e3o</li> <li>\u2705 Compartilha pesos</li> </ul> <p>Classificador (Dense):</p> <ul> <li>\u2705 Combina informa\u00e7\u00e3o global</li> <li>\u2705 Aprende rela\u00e7\u00f5es complexas</li> <li>\u2705 Produz probabilidades</li> <li>\u2705 Decis\u00e3o final</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#parte-5-exemplo-pratico-fashion-mnist-com-cnn","title":"\ud83d\udc57 Parte 5: Exemplo Pr\u00e1tico - Fashion MNIST com CNN\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#por-que-fashion-mnist","title":"\ud83c\udfaf Por que Fashion MNIST?\u00b6","text":"<p>O Fashion MNIST \u00e9 um dataset ideal para aprender CNNs:</p> <ul> <li>\ud83d\udcca Estrutura: 70.000 imagens 28\u00d728 em escala de cinza</li> <li>\ud83d\udc55 Classes: 10 tipos de roupas e acess\u00f3rios</li> <li>\ud83c\udf93 Complexidade: Mais desafiador que d\u00edgitos do MNIST tradicional</li> <li>\ud83d\udcc8 Benchmark: Padr\u00e3o da ind\u00fastria para testes iniciais</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#classes-do-dataset","title":"\ud83c\udff7\ufe0f Classes do Dataset:\u00b6","text":"C\u00f3digo Classe Emoji Exemplo 0 T-shirt/top \ud83d\udc55 Camisetas 1 Trouser \ud83d\udc56 Cal\u00e7as 2 Pullover \ud83e\udde5 Su\u00e9ter 3 Dress \ud83d\udc57 Vestidos 4 Coat \ud83e\udde5 Casacos 5 Sandal \ud83d\udc61 Sand\u00e1lias 6 Shirt \ud83d\udc54 Camisas 7 Sneaker \ud83d\udc5f T\u00eanis 8 Bag \ud83d\udc5c Bolsas 9 Ankle boot \ud83d\udc62 Botas"},{"location":"aulas/IA/lab08/cnn%20copy/#nossa-estrategia","title":"\ud83d\udd04 Nossa Estrat\u00e9gia:\u00b6","text":"<ol> <li>Carregar e explorar os dados</li> <li>Pr\u00e9-processar as imagens</li> <li>Construir CNN progressivamente</li> <li>Treinar e avaliar o modelo</li> <li>Visualizar resultados e interpretar erros</li> <li>Comparar com MLP tradicional</li> </ol>"},{"location":"aulas/IA/lab08/cnn%20copy/#o-que-esperamos-aprender","title":"\ud83d\udca1 O que Esperamos Aprender:\u00b6","text":"<ul> <li>Como CNNs extraem caracter\u00edsticas hier\u00e1rquicas</li> <li>Diferen\u00e7a de performance CNN vs MLP</li> <li>Interpreta\u00e7\u00e3o dos filtros aprendidos</li> <li>An\u00e1lise de erros e limita\u00e7\u00f5es</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#parte-5-exemplo-pratico-fashion-mnist-com-cnn","title":"\ud83d\udc57 Parte 5: Exemplo Pr\u00e1tico - Fashion MNIST com CNN\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#por-que-fashion-mnist","title":"\ud83c\udfaf Por que Fashion MNIST?\u00b6","text":"<p>O Fashion MNIST \u00e9 um dataset ideal para aprender CNNs:</p> <ul> <li>\ud83d\udcca Estrutura: 70.000 imagens 28\u00d728 em escala de cinza</li> <li>\ud83d\udc55 Classes: 10 tipos de roupas e acess\u00f3rios</li> <li>\ud83c\udf93 Complexidade: Mais desafiador que d\u00edgitos do MNIST tradicional</li> <li>\ud83d\udcc8 Benchmark: Padr\u00e3o da ind\u00fastria para testes iniciais</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#classes-do-dataset","title":"\ud83c\udff7\ufe0f Classes do Dataset:\u00b6","text":"C\u00f3digo Classe Emoji Exemplo 0 T-shirt/top \ud83d\udc55 Camisetas 1 Trouser \ud83d\udc56 Cal\u00e7as 2 Pullover \ud83e\udde5 Su\u00e9ter 3 Dress \ud83d\udc57 Vestidos 4 Coat \ud83e\udde5 Casacos 5 Sandal \ud83d\udc61 Sand\u00e1lias 6 Shirt \ud83d\udc54 Camisas 7 Sneaker \ud83d\udc5f T\u00eanis 8 Bag \ud83d\udc5c Bolsas 9 Ankle boot \ud83d\udc62 Botas"},{"location":"aulas/IA/lab08/cnn%20copy/#nossa-estrategia","title":"\ud83d\udd04 Nossa Estrat\u00e9gia:\u00b6","text":"<ol> <li>Carregar e explorar os dados</li> <li>Pr\u00e9-processar as imagens</li> <li>Construir CNN progressivamente</li> <li>Treinar e avaliar o modelo</li> <li>Visualizar resultados e interpretar erros</li> <li>Comparar com MLP tradicional</li> </ol>"},{"location":"aulas/IA/lab08/cnn%20copy/#o-que-esperamos-aprender","title":"\ud83d\udca1 O que Esperamos Aprender:\u00b6","text":"<ul> <li>Como CNNs extraem caracter\u00edsticas hier\u00e1rquicas</li> <li>Diferen\u00e7a de performance CNN vs MLP</li> <li>Interpreta\u00e7\u00e3o dos filtros aprendidos</li> <li>An\u00e1lise de erros e limita\u00e7\u00f5es</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#desafio-3-implementando-a-lenet-5-a-cnn-pioneira","title":"\ud83c\udfaf Desafio 3: Implementando a LeNet-5 - A CNN Pioneira\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#contexto-historico","title":"\ud83d\udcda Contexto Hist\u00f3rico\u00b6","text":"<p>A LeNet-5, desenvolvida por Yann LeCun em 1998, foi uma das primeiras CNNs bem-sucedidas e estabeleceu muitos dos princ\u00edpios fundamentais ainda usados hoje.</p>"},{"location":"aulas/IA/lab08/cnn%20copy/#arquitetura-da-lenet-5","title":"\ud83c\udfd7\ufe0f Arquitetura da LeNet-5\u00b6","text":"<p>Especifica\u00e7\u00f5es originais:</p> <pre><code>INPUT(32\u00d732\u00d71) \u2192 CONV1(28\u00d728\u00d76) \u2192 POOL1(14\u00d714\u00d76) \u2192 \nCONV2(10\u00d710\u00d716) \u2192 POOL2(5\u00d75\u00d716) \u2192 FC1(120) \u2192 FC2(84) \u2192 OUTPUT(10)\n</code></pre>"},{"location":"aulas/IA/lab08/cnn%20copy/#caracteristicas-da-lenet-5","title":"\ud83c\udfaf Caracter\u00edsticas da LeNet-5:\u00b6","text":"<ol> <li>Entrada: 32\u00d732 pixels (grayscale)</li> <li>C1: 6 filtros 5\u00d75, sem padding</li> <li>S2: Subsampling (average pooling) 2\u00d72</li> <li>C3: 16 filtros 5\u00d75</li> <li>S4: Subsampling 2\u00d72</li> <li>FC5: 120 neur\u00f4nios</li> <li>FC6: 84 neur\u00f4nios</li> <li>OUTPUT: 10 classes</li> </ol>"},{"location":"aulas/IA/lab08/cnn%20copy/#desafio-para-voce","title":"\ud83d\udca1 Desafio para Voc\u00ea:\u00b6","text":"<p>Implemente a LeNet-5 adaptada para Fashion MNIST (28\u00d728) seguindo estas especifica\u00e7\u00f5es:</p> <pre># Sua implementa\u00e7\u00e3o deve seguir esta estrutura:\ndef create_lenet5_fashion():\n    model = keras.Sequential([\n        # C1: Convolutional Layer\n        # S2: Subsampling Layer (Pooling)\n        # C3: Convolutional Layer\n        # S4: Subsampling Layer\n        # Flatten\n        # FC5: Dense Layer (120 neurons)\n        # FC6: Dense Layer (84 neurons)\n        # Output Layer (10 classes)\n    ])\n    return model\n</pre>"},{"location":"aulas/IA/lab08/cnn%20copy/#requisitos-do-desafio","title":"\ud83d\udccb Requisitos do Desafio:\u00b6","text":"<ol> <li>\u2705 Implementar LeNet-5 seguindo a arquitetura original</li> <li>\u2705 Comparar com sua CNN simples anterior</li> <li>\u2705 Analisar diferen\u00e7as de performance</li> <li>\u2705 Documentar observa\u00e7\u00f5es sobre cada camada</li> <li>\u2705 Visualizar resultados e m\u00e9tricas</li> </ol>"},{"location":"aulas/IA/lab08/cnn%20copy/#metricas-de-sucesso","title":"\ud83c\udfaf M\u00e9tricas de Sucesso:\u00b6","text":"<ul> <li>Acur\u00e1cia &gt; 85% no Fashion MNIST</li> <li>Menos overfitting que a CNN simples</li> <li>An\u00e1lise comparativa detalhada</li> <li>C\u00f3digo bem documentado</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#perguntas-para-reflexao","title":"\ud83d\udcad Perguntas para Reflex\u00e3o:\u00b6","text":"<ol> <li>Por que LeNet-5 tem 2 camadas convolucionais?</li> <li>Qual a vantagem de ter camadas FC decrescentes (120\u219284\u219210)?</li> <li>Como a LeNet-5 se compara com CNNs modernas?</li> <li>O que voc\u00ea mudaria na arquitetura original?</li> </ol>"},{"location":"aulas/IA/lab08/cnn%20copy/#desafio-extra-cnn-para-cifar-10","title":"\ud83d\ude80 Desafio Extra: CNN para CIFAR-10\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#por-que-cifar-10-e-mais-desafiador","title":"\ud83c\udfaf Por que CIFAR-10 \u00e9 Mais Desafiador?\u00b6","text":"<p>O CIFAR-10 \u00e9 significativamente mais complexo que Fashion MNIST:</p> Aspecto Fashion MNIST CIFAR-10 Resolu\u00e7\u00e3o 28\u00d728 32\u00d732 Canais 1 (grayscale) 3 (RGB) Classes 10 roupas 10 objetos Complexidade Texturas simples Objetos naturais Variabilidade Baixa Alta"},{"location":"aulas/IA/lab08/cnn%20copy/#classes-do-cifar-10","title":"\ud83c\udff7\ufe0f Classes do CIFAR-10:\u00b6","text":"<ul> <li>\u2708\ufe0f airplane | \ud83d\ude97 automobile | \ud83d\udc26 bird | \ud83d\udc31 cat | \ud83e\udd8c deer</li> <li>\ud83d\udc36 dog | \ud83d\udc38 frog | \ud83d\udc34 horse | \ud83d\udea2 ship | \ud83d\ude9b truck</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#estrategias-recomendadas","title":"\ud83d\udca1 Estrat\u00e9gias Recomendadas:\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#1-arquitetura-mais-profunda","title":"1\ufe0f\u20e3 Arquitetura Mais Profunda\u00b6","text":"<pre># CNN mais robusta para CIFAR-10\nmodel = Sequential([\n    # Bloco 1\n    Conv2D(32, (3,3), activation='relu', padding='same'),\n    BatchNormalization(),\n    Conv2D(32, (3,3), activation='relu', padding='same'),\n    MaxPooling2D((2,2)),\n    Dropout(0.25),\n    \n    # Bloco 2  \n    Conv2D(64, (3,3), activation='relu', padding='same'),\n    BatchNormalization(),\n    Conv2D(64, (3,3), activation='relu', padding='same'),\n    MaxPooling2D((2,2)),\n    Dropout(0.25),\n    \n    # Classificador\n    Flatten(),\n    Dense(512, activation='relu'),\n    Dropout(0.5),\n    Dense(10, activation='softmax')\n])\n</pre>"},{"location":"aulas/IA/lab08/cnn%20copy/#2-tecnicas-essenciais","title":"2\ufe0f\u20e3 T\u00e9cnicas Essenciais\u00b6","text":"<ul> <li>Data Augmentation: Rota\u00e7\u00e3o, flip, zoom</li> <li>Batch Normalization: Estabiliza treinamento</li> <li>Dropout: Reduz overfitting</li> <li>Learning Rate Scheduling: Melhora converg\u00eancia</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#3-metricas-de-sucesso","title":"3\ufe0f\u20e3 M\u00e9tricas de Sucesso\u00b6","text":"<ul> <li>\ud83c\udfaf Baseline: &gt;70% acur\u00e1cia</li> <li>\ud83d\ude80 Bom: &gt;80% acur\u00e1cia</li> <li>\ud83c\udfc6 Excelente: &gt;85% acur\u00e1cia</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#seu-desafio","title":"\ud83c\udfaf Seu Desafio:\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#conclusoes-e-proximos-passos","title":"\ud83c\udf93 Conclus\u00f5es e Pr\u00f3ximos Passos\u00b6","text":""},{"location":"aulas/IA/lab08/cnn%20copy/#o-que-aprendemos-hoje","title":"\u2705 O que aprendemos hoje:\u00b6","text":"<ol> <li>Fundamentos: Opera\u00e7\u00e3o de convolu\u00e7\u00e3o, pooling e arquitetura CNN</li> <li>Implementa\u00e7\u00e3o: TensorFlow/Keras para construir CNNs</li> <li>Compara\u00e7\u00e3o: CNNs vs MLPs tradicionais em termos de efici\u00eancia</li> <li>Aplica\u00e7\u00e3o: Classifica\u00e7\u00e3o de imagens com Fashion MNIST</li> <li>Arquiteturas hist\u00f3ricas: LeNet-5 e sua import\u00e2ncia</li> <li>Desafios pr\u00e1ticos: CIFAR-10 como pr\u00f3ximo passo</li> </ol>"},{"location":"aulas/IA/lab08/cnn%20copy/#proximos-passos-recomendados","title":"\ud83d\ude80 Pr\u00f3ximos passos recomendados:\u00b6","text":"<ol> <li>Arquiteturas modernas: ResNet, VGG, EfficientNet</li> <li>Transfer Learning: Usar modelos pr\u00e9-treinados</li> <li>T\u00e9cnicas avan\u00e7adas: Data augmentation, batch normalization</li> <li>Aplica\u00e7\u00f5es: Detec\u00e7\u00e3o de objetos, segmenta\u00e7\u00e3o sem\u00e2ntica</li> <li>Vision Transformers: Estado da arte atual</li> </ol>"},{"location":"aulas/IA/lab08/cnn%20copy/#recursos-para-continuar-aprendendo","title":"\ud83d\udcda Recursos para continuar aprendendo:\u00b6","text":"<ul> <li>\ud83d\udcd6 Livros: \"Deep Learning\" (Goodfellow), \"Hands-On ML\" (G\u00e9ron)</li> <li>\ud83c\udf93 Cursos: CS231n (Stanford), Fast.AI</li> <li>\ud83d\udcbb Pr\u00e1tica: Kaggle competitions, Papers With Code</li> <li>\ud83c\udfc6 Projetos: Crie seu pr\u00f3prio classificador de imagens</li> </ul>"},{"location":"aulas/IA/lab08/cnn%20copy/#dicas-finais","title":"\ud83d\udca1 Dicas finais:\u00b6","text":"<ol> <li>Comece simples: LeNet-5 \u2192 VGG \u2192 ResNet \u2192 Modernas</li> <li>Entenda os dados: EDA \u00e9 fundamental</li> <li>Experimente: Diferentes arquiteturas e hiperpar\u00e2metros</li> <li>Monitore: Overfitting vs underfitting</li> <li>Pratique: Projetos reais consolidam o aprendizado</li> </ol> <p>\ud83c\udf89 Parab\u00e9ns por completar este laborat\u00f3rio!</p> <p>Voc\u00ea agora tem uma base s\u00f3lida em CNNs. Continue explorando e construindo - o mundo da Vis\u00e3o Computacional est\u00e1 cheio de oportunidades fascinantes! \ud83c\udf1f</p> <p>Bons estudos e que a for\u00e7a convolucional esteja com voc\u00ea! \ud83e\udd16\u2728</p>"},{"location":"aulas/IA/lab08/cnn/","title":"Cnn","text":"In\u00a0[2]: Copied! <pre>import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\nmodel = keras.Sequential([\n    layers.Flatten(input_shape=(800,600)),\n    layers.Dense(units=100)\n])\n\nmodel.summary()\n</pre> import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers   model = keras.Sequential([     layers.Flatten(input_shape=(800,600)),     layers.Dense(units=100) ])  model.summary()  <pre>Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten_1 (Flatten)         (None, 480000)            0         \n                                                                 \n dense_1 (Dense)             (None, 100)               48000100  \n                                                                 \n=================================================================\nTotal params: 48,000,100\nTrainable params: 48,000,100\nNon-trainable params: 0\n_________________________________________________________________\n</pre> In\u00a0[21]: Copied! <pre>import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\nmodel = keras.Sequential([\n    layers.Conv2D(filters = 100, kernel_size = (3, 3), activation='relu', input_shape=(800,600, 3)),\n])\n\nmodel.summary()\n</pre> import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers   model = keras.Sequential([     layers.Conv2D(filters = 100, kernel_size = (3, 3), activation='relu', input_shape=(800,600, 3)), ])  model.summary()  <pre>Model: \"sequential_9\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_10 (Conv2D)          (None, 798, 598, 100)     2800      \n                                                                 \n=================================================================\nTotal params: 2,800\nTrainable params: 2,800\nNon-trainable params: 0\n_________________________________________________________________\n</pre> In\u00a0[18]: Copied! <pre>### Sua resposta aqui...\n</pre> ### Sua resposta aqui...    In\u00a0[20]: Copied! <pre>import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\nmodel = keras.Sequential([\n    layers.Conv2D(filters = 100, kernel_size = (3, 3), activation='relu', input_shape=(800,600, 3)), # camada de convolu\u00e7\u00e3o\n    layers.MaxPool2D(pool_size=2, strides=2) # camada de pooling\n\n])\n\nmodel.summary()\n</pre> import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers   model = keras.Sequential([     layers.Conv2D(filters = 100, kernel_size = (3, 3), activation='relu', input_shape=(800,600, 3)), # camada de convolu\u00e7\u00e3o     layers.MaxPool2D(pool_size=2, strides=2) # camada de pooling  ])  model.summary() <pre>Model: \"sequential_8\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_9 (Conv2D)           (None, 798, 598, 100)     2800      \n                                                                 \n max_pooling2d_5 (MaxPooling  (None, 399, 299, 100)    0         \n 2D)                                                             \n                                                                 \n=================================================================\nTotal params: 2,800\nTrainable params: 2,800\nNon-trainable params: 0\n_________________________________________________________________\n</pre> In\u00a0[23]: Copied! <pre>'''\n### suas respostas.....\n\n1.\n\n\n2.\n\n\n'''\n</pre> ''' ### suas respostas.....  1.   2.   ''' Out[23]: <pre>'\\n### suas respostas.....\\n\\n1.\\n\\n\\n2.\\n\\n\\n'</pre> In\u00a0[24]: Copied! <pre>import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers\n</pre> import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt from tensorflow.keras import layers In\u00a0[25]: Copied! <pre># Importa o dataset Fashion Mnist\nfashion_mnist = keras.datasets.fashion_mnist\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\n#normaliza os dados para o pixel ficar com valores entre 0 e 1\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n</pre> # Importa o dataset Fashion Mnist fashion_mnist = keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()  #normaliza os dados para o pixel ficar com valores entre 0 e 1 train_images = train_images / 255.0 test_images = test_images / 255.0 class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] In\u00a0[26]: Copied! <pre>train_images = train_images.reshape(-1,28,28,1)\nprint(train_images.shape)\ntest_images = test_images.reshape(-1,28,28,1)\ntest_images.shape\n</pre> train_images = train_images.reshape(-1,28,28,1) print(train_images.shape) test_images = test_images.reshape(-1,28,28,1) test_images.shape <pre>(60000, 28, 28, 1)\n</pre> Out[26]: <pre>(10000, 28, 28, 1)</pre> In\u00a0[37]: Copied! <pre>###### montar a arquitetura da rede neural \n\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    \n#####-------CNN-------#####\n\n    layers.Conv2D(5, (3,3), activation='relu', padding=\"same\", input_shape=(28, 28,1)),\n    layers.MaxPooling2D((2,2)),\n \n\n #######------ MLP-----####\n    layers.Flatten(),\n    layers.Dense(120, activation='relu'),\n    layers.Dense(10, activation='softmax')  ###### neuroniios especialistasss \n])\n\n\nmodel.summary()\n</pre> ###### montar a arquitetura da rede neural   from tensorflow.keras import layers  model = keras.Sequential([      #####-------CNN-------#####      layers.Conv2D(5, (3,3), activation='relu', padding=\"same\", input_shape=(28, 28,1)),     layers.MaxPooling2D((2,2)),     #######------ MLP-----####     layers.Flatten(),     layers.Dense(120, activation='relu'),     layers.Dense(10, activation='softmax')  ###### neuroniios especialistasss  ])   model.summary()  <pre>Model: \"sequential_11\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_12 (Conv2D)          (None, 28, 28, 5)         50        \n                                                                 \n max_pooling2d_7 (MaxPooling  (None, 14, 14, 5)        0         \n 2D)                                                             \n                                                                 \n flatten_5 (Flatten)         (None, 980)               0         \n                                                                 \n dense_9 (Dense)             (None, 120)               117720    \n                                                                 \n dense_10 (Dense)            (None, 10)                1210      \n                                                                 \n=================================================================\nTotal params: 118,980\nTrainable params: 118,980\nNon-trainable params: 0\n_________________________________________________________________\n</pre> In\u00a0[30]: Copied! <pre>model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nepochs_hist = model.fit(train_images, train_labels, epochs=3, validation_split=0.2)\n</pre> model.compile(optimizer='adam',               loss='sparse_categorical_crossentropy',               metrics=['accuracy'])  epochs_hist = model.fit(train_images, train_labels, epochs=3, validation_split=0.2)  <pre>Epoch 1/3\n1500/1500 [==============================] - 23s 15ms/step - loss: 0.2374 - accuracy: 0.9128 - val_loss: 0.2589 - val_accuracy: 0.9027\nEpoch 2/3\n1500/1500 [==============================] - 24s 16ms/step - loss: 0.2136 - accuracy: 0.9217 - val_loss: 0.2328 - val_accuracy: 0.9143\nEpoch 3/3\n1500/1500 [==============================] - 21s 14ms/step - loss: 0.1950 - accuracy: 0.9287 - val_loss: 0.2393 - val_accuracy: 0.9117\n</pre> In\u00a0[\u00a0]: Copied! <pre>## exibe os graficos da fun\u00e7\u00e3o loss e acuracia\nimport pandas as pd\nhistory_df = pd.DataFrame(epochs_hist.history)\n\nhistory_df[['loss','val_loss']].plot();\nhistory_df[['accuracy','val_accuracy']].plot();\n</pre> ## exibe os graficos da fun\u00e7\u00e3o loss e acuracia import pandas as pd history_df = pd.DataFrame(epochs_hist.history)  history_df[['loss','val_loss']].plot(); history_df[['accuracy','val_accuracy']].plot();  In\u00a0[32]: Copied! <pre>#Validad\u00e7\u00e3o\ntrain_loss, train_acc = model.evaluate(train_images,  train_labels, verbose=2)\ntest_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n</pre> #Validad\u00e7\u00e3o train_loss, train_acc = model.evaluate(train_images,  train_labels, verbose=2) test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)  <pre>1875/1875 - 10s - loss: 0.1883 - accuracy: 0.9318 - 10s/epoch - 5ms/step\n313/313 - 4s - loss: 0.2789 - accuracy: 0.8977 - 4s/epoch - 12ms/step\n</pre> In\u00a0[33]: Copied! <pre># Previs\u00f5es com o modelo treinado\n\npredictions = model.predict(test_images)\n</pre> # Previs\u00f5es com o modelo treinado  predictions = model.predict(test_images) <pre>313/313 [==============================] - 2s 6ms/step\n</pre> In\u00a0[34]: Copied! <pre>#Verica\u00e7\u00e3o dos itens preditos\n\nitem = 4000\n\nprint(\"\\nClasse predita foi {} com {:2.0f}%. Classe correta \u00e9 {}, {}.\".format(np.argmax(predictions[item]), \n                                                                 100*np.max(predictions[item]),\n                                                                 test_labels[item], \n                                                                 class_names[test_labels[item]]))\n\na=100*np.max(predictions[item])\n</pre> #Verica\u00e7\u00e3o dos itens preditos  item = 4000  print(\"\\nClasse predita foi {} com {:2.0f}%. Classe correta \u00e9 {}, {}.\".format(np.argmax(predictions[item]),                                                                   100*np.max(predictions[item]),                                                                  test_labels[item],                                                                   class_names[test_labels[item]]))  a=100*np.max(predictions[item]) <pre>\nClasse predita foi 0 com 100%. Classe correta \u00e9 0, T-shirt/top.\n</pre> In\u00a0[35]: Copied! <pre>def plot_image(i, predictions_array, true_label, img):\n  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n  plt.grid(False)\n  plt.xticks([])\n  plt.yticks([])\n\n  plt.imshow(img, cmap=plt.cm.binary)\n\n  predicted_label = np.argmax(predictions_array)\n  if predicted_label == true_label:\n    color = 'blue'\n  else:\n    color = 'red'\n\n  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n                                100*np.max(predictions_array),\n                                class_names[true_label]),\n                                color=color)\n</pre> def plot_image(i, predictions_array, true_label, img):   predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]   plt.grid(False)   plt.xticks([])   plt.yticks([])    plt.imshow(img, cmap=plt.cm.binary)    predicted_label = np.argmax(predictions_array)   if predicted_label == true_label:     color = 'blue'   else:     color = 'red'    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],                                 100*np.max(predictions_array),                                 class_names[true_label]),                                 color=color) In\u00a0[36]: Copied! <pre>plt.figure(figsize=(6,3))\nplt.subplot(1,2,1)\nplot_image(item, predictions, test_labels, test_images)\nplt.show()\n</pre> plt.figure(figsize=(6,3)) plt.subplot(1,2,1) plot_image(item, predictions, test_labels, test_images) plt.show() In\u00a0[40]: Copied! <pre>###### Seu c\u00f3digo aqui......\n</pre> ###### Seu c\u00f3digo aqui......      In\u00a0[39]: Copied! <pre>#### seu c\u00f3digo aqui......\n</pre> #### seu c\u00f3digo aqui......"},{"location":"aulas/IA/lab08/cnn/#redes-neurais","title":"Redes Neurais\u00b6","text":""},{"location":"aulas/IA/lab08/cnn/#objetivos","title":"Objetivos\u00b6","text":"<ul> <li>Conhecer e praticar Redes Neurais Convolucionais</li> <li>Conhecer uma intui\u00e7\u00e3o sobre Convolu\u00e7\u00e3o, Pooling</li> <li>Praticar a classifica\u00e7\u00e3o de objeto usando Keras/TensorFlow</li> </ul>"},{"location":"aulas/IA/lab08/cnn/#redes-neurais-convolucionais","title":"Redes Neurais Convolucionais\u00b6","text":"<p>A Redes Neurais Convolucionais ou CNN (Convolutional Neural Network) ou at\u00e9 mesmo ConvNet, s\u00e3o redes neurais de aprendizado profundo, <code>Deep Learning</code> muito utilizadas na \u00e1rea de Vis\u00e3o Computacional <code>classifica\u00e7\u00e3o</code>,<code>detec\u00e7\u00e3o de objetos</code> ou <code>segmenta\u00e7\u00e3o sem\u00e2ntica</code>.</p> <p>Para compreender uma CNN, precisamos compreender o funcionamento de alguns blocos novos fundamentais.</p> <ul> <li>Extra\u00e7\u00e3o de caracteristicas</li> <li>Convolu\u00e7\u00e3o</li> <li>Pooling</li> </ul>"},{"location":"aulas/IA/lab08/cnn/#diferenca-de-mlp-para-cnn","title":"Diferen\u00e7a de MLP para CNN\u00b6","text":"<ul> <li><p>Em uma rede MLP, cada pixel \u00e9 tratado de forma isolada, sem considerar os demais pixels, dificultando a caracteriza\u00e7\u00e3o de features mais complexas. N\u00e3o \u00e9 levado em considera\u00e7\u00e3o se o pixel est\u00e1 na borda ou centro da imagem. Em um CNN o processo de convolu\u00e7\u00e3o leva em considera\u00e7\u00e3o esta condi\u00e7\u00e3o.</p> </li> <li><p>Outro ponto importante est\u00e1 relacionado a quantidade de par\u00e2metros para treinamento para uma imagem. Exemplo: uma imagem de 400x600 na escala de cinza e 100 neur\u00f4nios na primeira camada. Par\u00e2metros = (400x600*100 +100) = 24.000.100 de par\u00e2metros para treinamento, apenas na primeira camada.</p> </li> </ul>"},{"location":"aulas/IA/lab08/cnn/#convolucao","title":"Convolu\u00e7\u00e3o\u00b6","text":"<p>A convolu\u00e7\u00e3o  permite uma filtragem no dom\u00ednio espacial. Esse processo ocorre com a aplica\u00e7\u00e3o de filtros (pequenas matrizes), posicionadas sob cada pixel da imagem. Estes filtros, normalmente, s\u00e3o chamados de kernels (ou n\u00facleos). O resultado final do valor do pixel \u00e9 calculado atrav\u00e9s de um produto de convolu\u00e7\u00e3o.</p> <p>Normalmente os kernels s\u00e3o matrizes 3x3. E os pesos s\u00e3o ajustados a cada itera\u00e7\u00e3o pelo backpropagation</p> <p>Nesta imagem temos a imagem original em azul, o kernel em cinza varrendo a imagem e o resultado da convolu\u00e7\u00e3o em verde.</p> <p>Vamos analizar o que acontece em apenas um pixel da imagem:</p> <p>O resultado para cada pixel \u00e9 esse:</p> <p>O resultado em uma imagem \u00e9 o seguinte:</p>"},{"location":"aulas/IA/lab08/cnn/#implementacao-em-codigo","title":"Implementa\u00e7\u00e3o em c\u00f3digo\u00b6","text":"<p>Para implementar \u00e9 simples.</p> <p><code>layers.Conv2D(100, (3, 3))</code></p> <p>-&gt; 100 \u00e9 a quantidade de filtros (kernels) que ser\u00e3o criados</p> <p>-&gt; (3,3) \u00e9 o tamanho do filtro que ser\u00e1 usado.</p> <p>-&gt; outros argumentos como activation, input_shape tambem podem ser utlilizados.</p>"},{"location":"aulas/IA/lab08/cnn/#desafio-1","title":"Desafio 1\u00b6","text":"<p>Compare a quantidade de <code>Total params</code>, em uma rede CNN esse valor \u00e9 menor ou maior comparado com uma rede MLP?</p>"},{"location":"aulas/IA/lab08/cnn/#pooling","title":"Pooling\u00b6","text":"<p>De forma geral a camada de <code>pooling</code> realiza uma opera\u00e7\u00e3o de redu\u00e7\u00e3o da imagem de entrada tentando manter as caracteristicas mais relevantes. Por consequ\u00eancia, o custo computacional diminui, al\u00e9m disso, \u00e9 nesta etapa que s\u00e3o extra\u00eddas as caracter\u00edstica <code>features</code> mais importantes da imagem.</p> <p>O pooling mais comum \u00e9 utilizando um kernel 2x2, e um passo <code>stride</code> de 2, por consequ\u00eancia a imagem de sa\u00edda ter\u00e1 a metade da imagem de entrada. A opera\u00e7\u00e3o de pooling ir\u00e1 selecionar dentro da janela do kernel o valor que ser\u00e1 aplicado na pr\u00f3xima camada, pode ser o maior valor <code>Maxpooling()</code> ou a m\u00e9dia <code>AveragePooling()</code></p> <p>O resultado visual \u00e9 o seguinte:</p>"},{"location":"aulas/IA/lab08/cnn/#implementacao-em-codigo","title":"Implementa\u00e7\u00e3o em c\u00f3digo\u00b6","text":"<p>Para implementar \u00e9 simples.</p> <p><code>layers.MaxPool2D(pool_size=2, strides=2)</code></p> <p>-&gt; pool_size \u00e9 o tamanho do filtro (2,2) que ser\u00e1 usado.</p> <p>-&gt; strides \u00e9 o passo 2 para percorrer a imagem</p> <p>-&gt; Existem outros argumentos que podem ser utlilizados (da uma olhada na documenta\u00e7\u00e3o).</p>"},{"location":"aulas/IA/lab08/cnn/#desafio-2","title":"Desafio 2\u00b6","text":"<ol> <li><p>Qual o dimensional da imagem antes e depois do pooling ?</p> </li> <li><p>Com a camada de pooling teve altera\u00e7\u00e3o <code>total params</code>?</p> </li> </ol>"},{"location":"aulas/IA/lab08/cnn/#extrator-de-caracteristicas","title":"Extrator de caracteristicas\u00b6","text":"<p>A extra\u00e7\u00e3o de caracter\u00edsticas \u00e9 o processo pelo qual a CNN identifica padr\u00f5es e caracter\u00edsticas relevantes em uma imagem. As caracter\u00edsticas s\u00e3o extra\u00eddas usando camadas convolucionais seguidas de camadas de pooling.</p> <p>Ap\u00f3s a extra\u00e7\u00e3o de caracter\u00edsticas \u00e9 aplicado uma rede MLP para realizar a etapa de classica\u00e7\u00e3o da imagem.</p>"},{"location":"aulas/IA/lab08/cnn/#exemplo-pratico-fashion-mnist","title":"Exemplo pr\u00e1tico - Fashion MNIST\u00b6","text":"<p>Vamos utilizar novamente o dataset do Fashion MNIST para classifica\u00e7\u00e3o de imagens, mas desta vez vamos utilizar uma CNN para realizar a extra\u00e7\u00e3o de caracteristicas da imagem seguida de um classificador MLP.</p>"},{"location":"aulas/IA/lab08/cnn/#parte-5-exemplo-pratico-fashion-mnist-com-cnn","title":"\ud83d\udc57 Parte 5: Exemplo Pr\u00e1tico - Fashion MNIST com CNN\u00b6","text":""},{"location":"aulas/IA/lab08/cnn/#por-que-fashion-mnist","title":"\ud83c\udfaf Por que Fashion MNIST?\u00b6","text":"<p>O Fashion MNIST \u00e9 um dataset ideal para aprender CNNs:</p> <ul> <li>\ud83d\udcca Estrutura: 70.000 imagens 28\u00d728 em escala de cinza</li> <li>\ud83d\udc55 Classes: 10 tipos de roupas e acess\u00f3rios</li> <li>\ud83c\udf93 Complexidade: Mais desafiador que d\u00edgitos do MNIST tradicional</li> <li>\ud83d\udcc8 Benchmark: Padr\u00e3o da ind\u00fastria para testes iniciais</li> </ul>"},{"location":"aulas/IA/lab08/cnn/#classes-do-dataset","title":"\ud83c\udff7\ufe0f Classes do Dataset:\u00b6","text":"C\u00f3digo Classe Emoji Exemplo 0 T-shirt/top \ud83d\udc55 Camisetas 1 Trouser \ud83d\udc56 Cal\u00e7as 2 Pullover \ud83e\udde5 Su\u00e9ter 3 Dress \ud83d\udc57 Vestidos 4 Coat \ud83e\udde5 Casacos 5 Sandal \ud83d\udc61 Sand\u00e1lias 6 Shirt \ud83d\udc54 Camisas 7 Sneaker \ud83d\udc5f T\u00eanis 8 Bag \ud83d\udc5c Bolsas 9 Ankle boot \ud83d\udc62 Botas"},{"location":"aulas/IA/lab08/cnn/#nossa-estrategia","title":"\ud83d\udd04 Nossa Estrat\u00e9gia:\u00b6","text":"<ol> <li>Carregar e explorar os dados</li> <li>Pr\u00e9-processar as imagens</li> <li>Construir CNN progressivamente</li> <li>Treinar e avaliar o modelo</li> <li>Visualizar resultados e interpretar erros</li> <li>Comparar com MLP tradicional</li> </ol>"},{"location":"aulas/IA/lab08/cnn/#o-que-esperamos-aprender","title":"\ud83d\udca1 O que Esperamos Aprender:\u00b6","text":"<ul> <li>Como CNNs extraem caracter\u00edsticas hier\u00e1rquicas</li> <li>Diferen\u00e7a de performance CNN vs MLP</li> <li>Interpreta\u00e7\u00e3o dos filtros aprendidos</li> <li>An\u00e1lise de erros e limita\u00e7\u00f5es</li> </ul>"},{"location":"aulas/IA/lab08/cnn/#desafio-3","title":"Desafio 3\u00b6","text":"<p>O resultado n\u00e3o ficou muito bom, mas podemos melhorar!</p> <p>Implemente a arquitetura da rede LeNet-5. Para treinar o Fashion MNIST.</p> <p>A leNet-5 foi publicada por leCun em 1998. E \u00e9 composta basicamente por:</p> <ul> <li>Convolutional Layers (CONV);</li> <li>Pooling Layers (POOL);</li> <li>Fully-Connected Layers (FC).</li> </ul> <p>Um exemplo de aplica\u00e7\u00e3o: https://github.com/gary30404/convolutional-neural-network-from-scratch-python</p>"},{"location":"aulas/IA/lab08/cnn/#desafio-extra","title":"Desafio Extra\u00b6","text":"<p>Use uma rede CNN para treinar o cifar-10 usado na rodada3 da batalha das redes.</p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/","title":"Cnn guia completo","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#introducao-as-cnns","title":"Introdu\u00e7\u00e3o \u00e0s CNNs","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#o-que-sao-redes-neurais-convolucionais","title":"O que s\u00e3o Redes Neurais Convolucionais?","text":"<p>As Redes Neurais Convolucionais (CNNs) s\u00e3o um tipo de rede neural artificial, projetada para processar dados que possuem uma estrutura topol\u00f3gica similar a uma grade, como:</p> <ul> <li>Imagens (grade 2D de pixels)</li> <li>Sinais de \u00e1udio (grade 1D temporal)</li> <li>V\u00eddeos (grade 3D: altura \u00d7 largura \u00d7 tempo)</li> <li>Sequ\u00eancias de DNA (grade 1D de nucleot\u00eddeos)</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#vantagens-sobre-mlps-tradicionais","title":"Vantagens sobre MLPs Tradicionais","text":"Aspecto MLP Tradicional CNN Par\u00e2metros 24M+ para imagem 400\u00d7600 ~100K para mesma imagem Estrutura espacial Ignorada Preservada Invari\u00e2ncia Sens\u00edvel \u00e0 posi\u00e7\u00e3o Invariante \u00e0 transla\u00e7\u00e3o Compartilhamento Sem reutiliza\u00e7\u00e3o Compartilha pesos Efici\u00eancia Computacionalmente caro Eficiente"},{"location":"aulas/IA/lab08/cnn_guia_completo/#arquitetura-geral-de-uma-cnn","title":"Arquitetura Geral de uma CNN","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#fundamentos-matematicos","title":"Fundamentos Matem\u00e1ticos","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#operacao-de-convolucao-matematica","title":"Opera\u00e7\u00e3o de Convolu\u00e7\u00e3o Matem\u00e1tica","text":"<p>A convolu\u00e7\u00e3o \u00e9 uma opera\u00e7\u00e3o matem\u00e1tica fundamental definida como:</p> <p>Convolu\u00e7\u00e3o Cont\u00ednua:</p> <pre><code>(f * g)(t) = \u222b_{-\u221e}^{\u221e} f(\u03c4)g(t-\u03c4)d\u03c4\n</code></pre> <p>Convolu\u00e7\u00e3o Discreta (usada em CNNs):</p> <pre><code>(f * g)[n] = \u03a3_{m=-\u221e}^{\u221e} f[m]g[n-m]\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#convolucao-2d-para-imagens","title":"Convolu\u00e7\u00e3o 2D para Imagens","text":"<p>Para imagens, usamos correla\u00e7\u00e3o cruzada (tecnicamente, n\u00e3o convolu\u00e7\u00e3o pura):</p> <p></p> <pre><code>S(i,j) = (I * K)(i,j) = \u03a3\u03a3 I(i+m, j+n) \u00d7 K(m,n)\n                        m n\n</code></pre> <p>Onde: - <code>I</code>: Imagem de entrada - <code>K</code>: Kernel (filtro) - <code>S</code>: Feature map (mapa de caracter\u00edsticas)</p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#exemplo-pratico-de-convolucao","title":"Exemplo Pr\u00e1tico de Convolu\u00e7\u00e3o","text":"<p>Imagem 5\u00d75: <pre><code>1  2  3  0  1\n0  1  2  3  1\n1  0  1  2  0\n2  1  0  1  2\n1  0  2  1  0\n</code></pre></p> <p>Kernel 3\u00d73 (Detector de Borda): <pre><code>-1 -1 -1\n-1  8 -1\n-1 -1 -1\n</code></pre></p> <p>Resultado (Feature Map): <pre><code>Posi\u00e7\u00e3o (1,1): (-1\u00d71) + (-1\u00d72) + (-1\u00d73) + (-1\u00d70) + (8\u00d71) + (-1\u00d72) + (-1\u00d71) + (-1\u00d70) + (-1\u00d71) = -5\n</code></pre></p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#operacao-de-convolucao","title":"Opera\u00e7\u00e3o de Convolu\u00e7\u00e3o","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#componentes-da-camada-convolucional","title":"Componentes da Camada Convolucional","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#1-kernelsfiltros","title":"1. Kernels/Filtros","text":"<ul> <li>Tamanho: Normalmente 3\u00d73, 5\u00d75, 7\u00d77</li> <li>Profundidade: Igual \u00e0 profundidade da entrada</li> <li>Quantidade: Hyperpar\u00e2metro (32, 64, 128, 256...)</li> <li>Pesos: Aprendidos durante treinamento</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#2-stride-passo","title":"2. Stride (Passo)","text":"<ul> <li>Defini\u00e7\u00e3o: Quantos pixels o kernel \"pula\" a cada opera\u00e7\u00e3o</li> <li>Stride = 1: Sobreposi\u00e7\u00e3o m\u00e1xima</li> <li>Stride = 2: Reduz dimens\u00e3o pela metade</li> <li>F\u00f3rmula de sa\u00edda: <code>(W - F + 2P) / S + 1</code></li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#3-padding-preenchimento","title":"3. Padding (Preenchimento)","text":"<ul> <li>Valid: Sem padding (sa\u00edda menor)</li> <li>Same: Padding para manter dimens\u00e3o</li> <li>Causal: Para dados sequenciais</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#tipos-de-convolucoes","title":"Tipos de Convolu\u00e7\u00f5es","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#convolucao-standard","title":"Convolu\u00e7\u00e3o Standard","text":"<pre><code># Exemplo com TensorFlow/Keras\nlayers.Conv2D(filters=32, kernel_size=(3,3), stride=(1,1), padding='same')\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#convolucao-depthwise-separable","title":"Convolu\u00e7\u00e3o Depthwise Separable","text":"<p><pre><code>layers.SeparableConv2D(filters=32, kernel_size=(3,3))\n</code></pre> - Vantagem: Menos par\u00e2metros (~9x redu\u00e7\u00e3o) - Uso: MobileNets, Xception</p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#convolucao-dilatada-atrous","title":"Convolu\u00e7\u00e3o Dilatada (Atrous)","text":"<p><pre><code>layers.Conv2D(filters=32, kernel_size=(3,3), dilation_rate=(2,2))\n</code></pre> - Vantagem: Campo receptivo maior sem perder resolu\u00e7\u00e3o - Uso: Segmenta\u00e7\u00e3o sem\u00e2ntica</p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#convolucao-transposta-deconvolucao","title":"Convolu\u00e7\u00e3o Transposta (Deconvolu\u00e7\u00e3o)","text":"<p><pre><code>layers.Conv2DTranspose(filters=32, kernel_size=(3,3), strides=(2,2))\n</code></pre> - Uso: Upsampling, GANs, Autoencoders</p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#visualizacao-da-convolucao","title":"Visualiza\u00e7\u00e3o da Convolu\u00e7\u00e3o","text":"<pre><code>Entrada (6\u00d76):          Kernel (3\u00d73):        Sa\u00edda (4\u00d74):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u25021 2 3 0 1 2\u2502        \u25021 0 1\u2502            \u2502? ? ? ?\u2502\n\u25020 1 2 3 1 0\u2502   *    \u25020 1 0\u2502     =      \u2502? ? ? ?\u2502\n\u25021 0 1 2 0 1\u2502        \u25021 0 1\u2502            \u2502? ? ? ?\u2502\n\u25022 1 0 1 2 0\u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502? ? ? ?\u2502\n\u25021 0 2 1 0 2\u2502                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u25020 1 0 2 1 0\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#pooling-e-subsampling","title":"Pooling e Subsampling","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#objetivos-do-pooling","title":"Objetivos do Pooling","text":"<ol> <li>Redu\u00e7\u00e3o dimensional: Diminui tamanho dos feature maps</li> <li>Invari\u00e2ncia: Pequenas transla\u00e7\u00f5es n\u00e3o afetam resultado</li> <li>Redu\u00e7\u00e3o de overfitting: Menos par\u00e2metros</li> <li>Efici\u00eancia computacional: Opera\u00e7\u00e3o mais r\u00e1pida</li> </ol>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#tipos-de-pooling","title":"Tipos de Pooling","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#max-pooling","title":"Max Pooling","text":"<pre><code>layers.MaxPool2D(pool_size=(2,2), strides=(2,2))\n</code></pre> <p>Exemplo: <pre><code>Entrada (4\u00d74):           Max Pool 2\u00d72:        Sa\u00edda (2\u00d72):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u25021 3 2 4\u2502               \u2502max(1,3,0,1)\u2502      \u25023 4\u2502\n\u25020 1 1 2\u2502        \u2192      \u2502max(2,4,1,2)\u2502  =   \u25022 5\u2502\n\u25022 2 0 1\u2502               \u2502max(2,2,3,1)\u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u25023 1 3 5\u2502               \u2502max(0,1,3,5)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#average-pooling","title":"Average Pooling","text":"<pre><code>layers.AveragePooling2D(pool_size=(2,2))\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#global-average-pooling","title":"Global Average Pooling","text":"<p><pre><code>layers.GlobalAveragePooling2D()\n</code></pre> - Uso: Substituir camadas FC finais - Vantagem: Reduz overfitting, menos par\u00e2metros</p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#adaptive-pooling","title":"Adaptive Pooling","text":"<ul> <li>Objetivo: Sa\u00edda com tamanho fixo independente da entrada</li> <li>Uso: Redes com entradas de tamanhos variados</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#pooling-vs-stride-convolution","title":"Pooling vs Stride Convolution","text":"Aspecto Pooling Strided Convolution Par\u00e2metros 0 Sim Aprendizado N\u00e3o Sim Flexibilidade Fixa Adapt\u00e1vel Tend\u00eancia atual \u2193 Diminuindo \u2191 Aumentando"},{"location":"aulas/IA/lab08/cnn_guia_completo/#arquiteturas-classicas","title":"Arquiteturas Cl\u00e1ssicas","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#lenet-5-1998-yann-lecun","title":"LeNet-5 (1998) - Yann LeCun","text":"<p>Arquitetura: <pre><code>INPUT(32\u00d732\u00d71) \u2192 CONV1(28\u00d728\u00d76) \u2192 POOL1(14\u00d714\u00d76) \u2192 \nCONV2(10\u00d710\u00d716) \u2192 POOL2(5\u00d75\u00d716) \u2192 FC1(120) \u2192 FC2(84) \u2192 OUTPUT(10)\n</code></pre></p> <p>Caracter\u00edsticas: - \u2705 Primeira CNN bem-sucedida - \u2705 Reconhecimento de d\u00edgitos - \u2705 Base para arquiteturas modernas</p> <p>Implementa\u00e7\u00e3o: <pre><code>model = Sequential([\n    Conv2D(6, (5,5), activation='tanh', input_shape=(32,32,1)),\n    AveragePooling2D((2,2)),\n    Conv2D(16, (5,5), activation='tanh'),\n    AveragePooling2D((2,2)),\n    Flatten(),\n    Dense(120, activation='tanh'),\n    Dense(84, activation='tanh'),\n    Dense(10, activation='softmax')\n])\n</code></pre></p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#alexnet-2012-alex-krizhevsky","title":"AlexNet (2012) - Alex Krizhevsky","text":"<p>Inova\u00e7\u00f5es: - \ud83d\ude80 ReLU: Primeira CNN com ReLU em larga escala - \ud83d\udd04 Dropout: Regulariza\u00e7\u00e3o efetiva - \ud83d\udcca Data Augmentation: Aumento artificial do dataset - \u26a1 GPU: Treinamento paralelo</p> <p>Arquitetura: <pre><code>INPUT(224\u00d7224\u00d73) \u2192 CONV1(55\u00d755\u00d796) \u2192 POOL1 \u2192 CONV2(27\u00d727\u00d7256) \u2192 POOL2 \u2192\nCONV3(13\u00d713\u00d7384) \u2192 CONV4(13\u00d713\u00d7384) \u2192 CONV5(13\u00d713\u00d7256) \u2192 POOL3 \u2192\nFC1(4096) \u2192 FC2(4096) \u2192 FC3(1000)\n</code></pre></p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#vggnet-2014-oxford","title":"VGGNet (2014) - Oxford","text":"<p>Filosofia: \"Convolu\u00e7\u00f5es pequenas e profundas\"</p> <p>Princ\u00edpios: - \ud83d\udd39 Kernels 3\u00d73: Exclusivamente - \ud83d\udcda Profundidade: 16-19 camadas - \ud83d\udd04 Repeti\u00e7\u00e3o: Padr\u00f5es consistentes</p> <p>VGG-16 Arquitetura: <pre><code># Bloco 1\nConv2D(64, (3,3), activation='relu', padding='same')\nConv2D(64, (3,3), activation='relu', padding='same')\nMaxPooling2D((2,2), strides=(2,2))\n\n# Bloco 2\nConv2D(128, (3,3), activation='relu', padding='same')\nConv2D(128, (3,3), activation='relu', padding='same')\nMaxPooling2D((2,2), strides=(2,2))\n\n# ... continua com blocos similares\n</code></pre></p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#resnet-2015-microsoft-research","title":"ResNet (2015) - Microsoft Research","text":"<p>Problema Resolvido: Degrada\u00e7\u00e3o em redes muito profundas</p> <p>Inova\u00e7\u00e3o: Conex\u00f5es Residuais (Skip Connections)</p> <pre><code>x \u2192 [CONV\u2192BN\u2192ReLU\u2192CONV\u2192BN] \u2192 + \u2192 ReLU\n\u2193                              \u2191\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        (skip connection)\n</code></pre> <p>Bloco Residual: <pre><code>def residual_block(x, filters):\n    shortcut = x\n\n    x = Conv2D(filters, (3,3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters, (3,3), padding='same')(x)\n    x = BatchNormalization()(x)\n\n    x = Add()([x, shortcut])\n    x = Activation('relu')(x)\n\n    return x\n</code></pre></p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#arquiteturas-modernas","title":"Arquiteturas Modernas","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#efficientnet-2019","title":"EfficientNet (2019)","text":"<ul> <li>Compound Scaling: Balanceia largura, profundidade e resolu\u00e7\u00e3o</li> <li>Neural Architecture Search: Arquitetura otimizada automaticamente</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#vision-transformer-vit-2020","title":"Vision Transformer (ViT) (2020)","text":"<ul> <li>Attention Mechanism: Substitui convolu\u00e7\u00f5es por aten\u00e7\u00e3o</li> <li>Patches: Divide imagem em patches como tokens</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#convnext-2022","title":"ConvNeXt (2022)","text":"<ul> <li>CNN Modernizada: Incorpora ideias dos Transformers</li> <li>Performance: Competitiva com ViTs</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#implementacao-pratica","title":"Implementa\u00e7\u00e3o Pr\u00e1tica","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#preparacao-dos-dados","title":"Prepara\u00e7\u00e3o dos Dados","text":"<pre><code>import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Carregamento e prepara\u00e7\u00e3o\n(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n\n# Normaliza\u00e7\u00e3o\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\n\n# One-hot encoding\ny_train = keras.utils.to_categorical(y_train, 10)\ny_test = keras.utils.to_categorical(y_test, 10)\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#cnn-basica-para-cifar-10","title":"CNN B\u00e1sica para CIFAR-10","text":"<pre><code>def create_basic_cnn():\n    model = keras.Sequential([\n        # Bloco 1\n        layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n        layers.BatchNormalization(),\n        layers.Conv2D(32, (3,3), activation='relu'),\n        layers.MaxPooling2D((2,2)),\n        layers.Dropout(0.25),\n\n        # Bloco 2\n        layers.Conv2D(64, (3,3), activation='relu'),\n        layers.BatchNormalization(),\n        layers.Conv2D(64, (3,3), activation='relu'),\n        layers.MaxPooling2D((2,2)),\n        layers.Dropout(0.25),\n\n        # Bloco 3\n        layers.Conv2D(128, (3,3), activation='relu'),\n        layers.BatchNormalization(),\n        layers.Conv2D(128, (3,3), activation='relu'),\n        layers.MaxPooling2D((2,2)),\n        layers.Dropout(0.25),\n\n        # Classificador\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(10, activation='softmax')\n    ])\n\n    return model\n\nmodel = create_basic_cnn()\nmodel.summary()\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#tecnicas-de-treinamento","title":"T\u00e9cnicas de Treinamento","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#data-augmentation","title":"Data Augmentation","text":"<pre><code>datagen = keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    zoom_range=0.1\n)\n\ndatagen.fit(x_train)\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#callbacks","title":"Callbacks","text":"<pre><code>callbacks = [\n    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n    keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=5),\n    keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True)\n]\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#compilacao-e-treinamento","title":"Compila\u00e7\u00e3o e Treinamento","text":"<pre><code>model.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    datagen.flow(x_train, y_train, batch_size=32),\n    steps_per_epoch=len(x_train) // 32,\n    epochs=100,\n    validation_data=(x_test, y_test),\n    callbacks=callbacks\n)\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#tecnicas-avancadas","title":"T\u00e9cnicas Avan\u00e7adas","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#transfer-learning","title":"Transfer Learning","text":"<p>Conceito: Usar modelos pr\u00e9-treinados como ponto de partida</p> <pre><code># Carregar modelo pr\u00e9-treinado\nbase_model = keras.applications.VGG16(\n    weights='imagenet',\n    include_top=False,\n    input_shape=(224, 224, 3)\n)\n\n# Congelar camadas base\nbase_model.trainable = False\n\n# Adicionar cabe\u00e7alho personalizado\nmodel = keras.Sequential([\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(num_classes, activation='softmax')\n])\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#fine-tuning","title":"Fine-tuning","text":"<pre><code># Ap\u00f3s treinamento inicial, descongelar e treinar com LR baixa\nbase_model.trainable = True\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-5),  # LR muito baixa\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Treinar mais algumas \u00e9pocas\nhistory_fine = model.fit(...)\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#interpretabilidade","title":"Interpretabilidade","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#grad-cam-gradient-weighted-class-activation-mapping","title":"Grad-CAM (Gradient-weighted Class Activation Mapping)","text":"<pre><code>def generate_gradcam(model, img_array, layer_name, class_index):\n    grad_model = keras.Model(\n        inputs=model.inputs,\n        outputs=[model.get_layer(layer_name).output, model.output]\n    )\n\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        loss = predictions[:, class_index]\n\n    grads = tape.gradient(loss, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    conv_outputs = conv_outputs[0]\n    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n\n    return heatmap.numpy()\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#otimizacoes-de-performance","title":"Otimiza\u00e7\u00f5es de Performance","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#mixed-precision-training","title":"Mixed Precision Training","text":"<pre><code>policy = keras.mixed_precision.Policy('mixed_float16')\nkeras.mixed_precision.set_global_policy(policy)\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#quantizacao","title":"Quantiza\u00e7\u00e3o","text":"<pre><code># Post-training quantization\nconverter = tf.lite.TFLiteConverter.from_saved_model('model_path')\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nquantized_model = converter.convert()\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#aplicacoes-e-casos-de-uso","title":"Aplica\u00e7\u00f5es e Casos de Uso","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#1-classificacao-de-imagens","title":"1. Classifica\u00e7\u00e3o de Imagens","text":"<p>Datasets Cl\u00e1ssicos: - MNIST: D\u00edgitos manuscritos (28\u00d728) - CIFAR-10/100: Objetos naturais (32\u00d732) - ImageNet: 1000 classes, milh\u00f5es de imagens - Places365: Reconhecimento de cenas</p> <p>Aplica\u00e7\u00f5es Reais: - \ud83c\udfe5 Diagn\u00f3stico m\u00e9dico: Raio-X, resson\u00e2ncia, dermatologia - \ud83d\ude97 Ve\u00edculos aut\u00f4nomos: Detec\u00e7\u00e3o de placas, pedestres - \ud83d\udee1\ufe0f Seguran\u00e7a: Reconhecimento facial, videomonitoramento - \ud83d\udcf1 Mobile: Filtros, busca por imagem</p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#2-deteccao-de-objetos","title":"2. Detec\u00e7\u00e3o de Objetos","text":"<p>Arquiteturas: - R-CNN Family: R-CNN, Fast R-CNN, Faster R-CNN - YOLO: You Only Look Once (v1-v8) - SSD: Single Shot MultiBox Detector - EfficientDet: Detec\u00e7\u00e3o eficiente</p> <p>Aplica\u00e7\u00f5es: - \ud83d\udea6 Tr\u00e2nsito inteligente: Contagem de ve\u00edculos - \ud83c\udfed Ind\u00fastria: Controle de qualidade, automa\u00e7\u00e3o - \ud83c\udfea Retail: Checkout autom\u00e1tico, invent\u00e1rio - \ud83c\udf3e Agricultura: Monitoramento de culturas</p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#3-segmentacao-semantica","title":"3. Segmenta\u00e7\u00e3o Sem\u00e2ntica","text":"<p>Arquiteturas: - U-Net: Segmenta\u00e7\u00e3o m\u00e9dica - DeepLab: Convolu\u00e7\u00e3o atrous - PSPNet: Pyramid Scene Parsing - Mask R-CNN: Segmenta\u00e7\u00e3o de inst\u00e2ncias</p> <p>Aplica\u00e7\u00f5es: - \ud83c\udfe5 Medicina: Segmenta\u00e7\u00e3o de \u00f3rg\u00e3os, tumores - \ud83d\udef0\ufe0f Sensoriamento remoto: An\u00e1lise de sat\u00e9lites - \ud83c\udfac Entretenimento: Chroma key, efeitos especiais - \ud83c\udfd7\ufe0f Arquitetura: An\u00e1lise urbana, planejamento</p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#4-processamento-de-video","title":"4. Processamento de V\u00eddeo","text":"<p>T\u00e9cnicas: - 3D CNNs: Convolu\u00e7\u00e3o espa\u00e7o-temporal - Two-Stream Networks: RGB + Optical Flow - LSTM + CNN: Sequ\u00eancias temporais</p> <p>Aplica\u00e7\u00f5es: - \ud83c\udfaf Reconhecimento de a\u00e7\u00f5es: Esportes, vigil\u00e2ncia - \ud83c\udf9e\ufe0f An\u00e1lise de v\u00eddeo: Sumariza\u00e7\u00e3o, indexa\u00e7\u00e3o - \ud83c\udfc3 An\u00e1lise de movimento: Biomec\u00e2nica, reabilita\u00e7\u00e3o</p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#exercicios-e-projetos","title":"Exerc\u00edcios e Projetos","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#nivel-iniciante","title":"N\u00edvel Iniciante","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#projeto-1-classificador-de-digitos-mnist","title":"Projeto 1: Classificador de D\u00edgitos MNIST","text":"<pre><code># Implemente uma CNN simples para MNIST\n# Objetivo: &gt;98% de acur\u00e1cia\n# T\u00e9cnicas: Conv2D, MaxPooling, Dropout\n\ndef create_mnist_cnn():\n    # Seu c\u00f3digo aqui\n    pass\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#projeto-2-fashion-mnist","title":"Projeto 2: Fashion-MNIST","text":"<pre><code># Classifique itens de vestu\u00e1rio\n# Objetivo: &gt;90% de acur\u00e1cia\n# Desafio: Mais complexo que d\u00edgitos\n\ndef create_fashion_cnn():\n    # Seu c\u00f3digo aqui\n    pass\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#nivel-intermediario","title":"N\u00edvel Intermedi\u00e1rio","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#projeto-3-cifar-10-com-data-augmentation","title":"Projeto 3: CIFAR-10 com Data Augmentation","text":"<pre><code># Objetivo: &gt;85% de acur\u00e1cia\n# T\u00e9cnicas: Data augmentation, batch normalization\n# Tempo limite: 2 horas de treinamento\n\ndef create_cifar10_cnn():\n    # Seu c\u00f3digo aqui\n    pass\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#projeto-4-transfer-learning","title":"Projeto 4: Transfer Learning","text":"<pre><code># Use um modelo pr\u00e9-treinado para novo dataset\n# Compare com treinamento do zero\n# Analise o tempo de converg\u00eancia\n\ndef transfer_learning_project():\n    # Seu c\u00f3digo aqui\n    pass\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#nivel-avancado","title":"N\u00edvel Avan\u00e7ado","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#projeto-5-implementar-resnet-do-zero","title":"Projeto 5: Implementar ResNet do Zero","text":"<pre><code># Implemente blocos residuais\n# Compare com CNN convencional\n# Analise o gradiente em redes profundas\n\nclass ResNetBlock(layers.Layer):\n    def __init__(self, filters, downsample=False):\n        # Seu c\u00f3digo aqui\n        pass\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#projeto-6-deteccao-de-objetos-simples","title":"Projeto 6: Detec\u00e7\u00e3o de Objetos Simples","text":"<pre><code># Implemente um detector simples\n# Use t\u00e9cnicas de sliding window\n# Avalie com m\u00e9tricas de detec\u00e7\u00e3o (mAP)\n\ndef simple_object_detector():\n    # Seu c\u00f3digo aqui\n    pass\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#projetos-aplicados","title":"Projetos Aplicados","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#projeto-7-diagnostico-medico","title":"Projeto 7: Diagn\u00f3stico M\u00e9dico","text":"<ul> <li>Dataset: Chest X-Ray pneumonia</li> <li>Objetivo: Classificar pneumonia vs normal</li> <li>M\u00e9tricas: Sensibilidade, especificidade, F1-score</li> <li>Considera\u00e7\u00f5es \u00e9ticas: Falsos negativos</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#projeto-8-classificacao-de-plantas","title":"Projeto 8: Classifica\u00e7\u00e3o de Plantas","text":"<ul> <li>Dataset: PlantNet ou similar</li> <li>T\u00e9cnicas: Transfer learning, data augmentation</li> <li>Aplica\u00e7\u00e3o: App m\u00f3vel de identifica\u00e7\u00e3o</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#projeto-9-analise-de-sentimentos-visual","title":"Projeto 9: An\u00e1lise de Sentimentos Visual","text":"<ul> <li>Dataset: Imagens de redes sociais</li> <li>Objetivo: Predizer sentimento pela imagem</li> <li>Desafio: Multimodalidade (imagem + texto)</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#debugging-e-troubleshooting","title":"Debugging e Troubleshooting","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#problemas-comuns","title":"Problemas Comuns","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#1-overfitting","title":"1. Overfitting","text":"<p>Sintomas: - Alta acur\u00e1cia no treino, baixa no teste - Gap crescente entre curvas de treino e valida\u00e7\u00e3o</p> <p>Solu\u00e7\u00f5es: <pre><code># Mais dados\ndatagen = ImageDataGenerator(...)\n\n# Dropout\nlayers.Dropout(0.5)\n\n# Regulariza\u00e7\u00e3o L2\nlayers.Conv2D(64, (3,3), kernel_regularizer=l2(0.01))\n\n# Early stopping\ncallbacks = [EarlyStopping(patience=10)]\n</code></pre></p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#2-underfitting","title":"2. Underfitting","text":"<p>Sintomas: - Baixa acur\u00e1cia tanto no treino quanto no teste - Curvas de loss n\u00e3o convergem</p> <p>Solu\u00e7\u00f5es: <pre><code># Modelo mais complexo\n# Mais camadas ou mais filtros\n\n# Learning rate adequada\noptimizer = Adam(learning_rate=0.001)\n\n# Mais \u00e9pocas de treinamento\nepochs = 200\n</code></pre></p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#3-vanishing-gradients","title":"3. Vanishing Gradients","text":"<p>Sintomas: - Camadas iniciais n\u00e3o aprendem - Gradientes muito pequenos</p> <p>Solu\u00e7\u00f5es: <pre><code># Batch Normalization\nlayers.BatchNormalization()\n\n# Residual connections\n# Skip connections\n\n# Ativa\u00e7\u00f5es adequadas (ReLU, n\u00e3o sigmoid)\nactivation='relu'\n</code></pre></p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#4-exploding-gradients","title":"4. Exploding Gradients","text":"<p>Sintomas: - Loss explode para infinito - Pesos ficam NaN</p> <p>Solu\u00e7\u00f5es: <pre><code># Gradient clipping\noptimizer = Adam(clipnorm=1.0)\n\n# Learning rate menor\nlearning_rate = 0.0001\n\n# Batch normalization\nlayers.BatchNormalization()\n</code></pre></p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#monitoramento-de-treinamento","title":"Monitoramento de Treinamento","text":"<pre><code># Visualiza\u00e7\u00e3o em tempo real\ndef plot_training_history(history):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n    # Loss\n    ax1.plot(history.history['loss'], label='Train Loss')\n    ax1.plot(history.history['val_loss'], label='Val Loss')\n    ax1.set_title('Model Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n\n    # Accuracy\n    ax2.plot(history.history['accuracy'], label='Train Acc')\n    ax2.plot(history.history['val_accuracy'], label='Val Acc')\n    ax2.set_title('Model Accuracy')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.legend()\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#metricas-de-avaliacao","title":"M\u00e9tricas de Avalia\u00e7\u00e3o","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#classificacao","title":"Classifica\u00e7\u00e3o","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#metricas-basicas","title":"M\u00e9tricas B\u00e1sicas","text":"<pre><code>from sklearn.metrics import classification_report, confusion_matrix\n\n# Predi\u00e7\u00f5es\ny_pred = model.predict(x_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n# Relat\u00f3rio completo\nprint(classification_report(y_true, y_pred_classes))\n\n# Matriz de confus\u00e3o\ncm = confusion_matrix(y_true, y_pred_classes)\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#metricas-avancadas","title":"M\u00e9tricas Avan\u00e7adas","text":"<pre><code># Top-k accuracy\ntop_k_acc = keras.metrics.top_k_categorical_accuracy(y_test, y_pred, k=5)\n\n# Curva ROC (para classifica\u00e7\u00e3o bin\u00e1ria)\nfrom sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(y_true, y_pred_proba)\nroc_auc = auc(fpr, tpr)\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#deteccao-de-objetos","title":"Detec\u00e7\u00e3o de Objetos","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#mean-average-precision-map","title":"Mean Average Precision (mAP)","text":"<pre><code>def calculate_map(true_boxes, pred_boxes, iou_threshold=0.5):\n    \"\"\"\n    Calcula mAP para detec\u00e7\u00e3o de objetos\n    \"\"\"\n    # Implementa\u00e7\u00e3o simplificada\n    pass\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#segmentacao","title":"Segmenta\u00e7\u00e3o","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#intersection-over-union-iou","title":"Intersection over Union (IoU)","text":"<pre><code>def calculate_iou(y_true, y_pred):\n    intersection = np.logical_and(y_true, y_pred)\n    union = np.logical_or(y_true, y_pred)\n    iou = np.sum(intersection) / np.sum(union)\n    return iou\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#ferramentas-e-frameworks","title":"Ferramentas e Frameworks","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#tensorflowkeras","title":"TensorFlow/Keras","text":"<pre><code># Instala\u00e7\u00e3o\npip install tensorflow tensorflow-gpu\n\n# Uso b\u00e1sico\nimport tensorflow as tf\nfrom tensorflow import keras\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#pytorch","title":"PyTorch","text":"<pre><code># Instala\u00e7\u00e3o\npip install torch torchvision\n\n# Uso b\u00e1sico\nimport torch\nimport torch.nn as nn\nimport torchvision\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#outras-ferramentas","title":"Outras Ferramentas","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#visualizacao","title":"Visualiza\u00e7\u00e3o","text":"<pre><code># TensorBoard\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir='./logs')\n\n# Weights &amp; Biases\nimport wandb\nwandb.init(project=\"my-cnn-project\")\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#datasets","title":"Datasets","text":"<pre><code># TensorFlow Datasets\nimport tensorflow_datasets as tfds\ndataset = tfds.load('cifar10', split='train')\n\n# Torchvision datasets\nfrom torchvision import datasets\ndataset = datasets.CIFAR10(root='./data', download=True)\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#augmentation","title":"Augmentation","text":"<pre><code># Albumentations\nimport albumentations as A\ntransform = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.Rotate(limit=15, p=0.5),\n    A.RandomBrightnessContrast(p=0.2)\n])\n</code></pre>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#recursos-adicionais","title":"Recursos Adicionais","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#cursos-online","title":"Cursos Online","text":"<ul> <li>\ud83c\udf93 CS231n: Stanford - Convolutional Neural Networks</li> <li>\ud83c\udf93 Fast.ai: Practical Deep Learning for Coders</li> <li>\ud83c\udf93 Deep Learning Specialization: Coursera (Andrew Ng)</li> <li>\ud83c\udf93 TensorFlow Developer Certificate: Google</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#livros-recomendados","title":"Livros Recomendados","text":"<ul> <li>\ud83d\udcda \"Deep Learning\" - Ian Goodfellow, Yoshua Bengio, Aaron Courville</li> <li>\ud83d\udcda \"Hands-On Machine Learning\" - Aur\u00e9lien G\u00e9ron</li> <li>\ud83d\udcda \"Deep Learning with Python\" - Fran\u00e7ois Chollet</li> <li>\ud83d\udcda \"Computer Vision: Algorithms and Applications\" - Richard Szeliski</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#papers-fundamentais","title":"Papers Fundamentais","text":"<ul> <li>\ud83d\udcc4 LeNet-5 (1998): \"Gradient-based learning applied to document recognition\"</li> <li>\ud83d\udcc4 AlexNet (2012): \"ImageNet Classification with Deep Convolutional Neural Networks\"</li> <li>\ud83d\udcc4 VGG (2014): \"Very Deep Convolutional Networks for Large-Scale Image Recognition\"</li> <li>\ud83d\udcc4 ResNet (2015): \"Deep Residual Learning for Image Recognition\"</li> <li>\ud83d\udcc4 Attention (2017): \"Attention Is All You Need\"</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#datasets-populares","title":"Datasets Populares","text":"<ul> <li>\ud83d\uddc2\ufe0f ImageNet: 14M imagens, 1000 classes</li> <li>\ud83d\uddc2\ufe0f COCO: Detec\u00e7\u00e3o e segmenta\u00e7\u00e3o</li> <li>\ud83d\uddc2\ufe0f Open Images: 9M imagens anotadas</li> <li>\ud83d\uddc2\ufe0f Places365: Reconhecimento de cenas</li> <li>\ud83d\uddc2\ufe0f CelebA: Atributos faciais</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#competicoes-e-desafios","title":"Competi\u00e7\u00f5es e Desafios","text":"<ul> <li>\ud83c\udfc6 ImageNet Large Scale Visual Recognition Challenge (ILSVRC)</li> <li>\ud83c\udfc6 Kaggle Computer Vision Competitions</li> <li>\ud83c\udfc6 COCO Detection Challenge</li> <li>\ud83c\udfc6 Pascal VOC Challenge</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#comunidades","title":"Comunidades","text":"<ul> <li>\ud83d\udcac Reddit: r/MachineLearning, r/ComputerVision</li> <li>\ud83d\udcac Discord: TensorFlow Community, PyTorch Community</li> <li>\ud83d\udcac Stack Overflow: Tags [tensorflow], [computer-vision]</li> <li>\ud83d\udcac Papers with Code: Estado da arte em CV</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#tendencias-futuras","title":"Tend\u00eancias Futuras","text":""},{"location":"aulas/IA/lab08/cnn_guia_completo/#vision-transformers-vits","title":"Vision Transformers (ViTs)","text":"<ul> <li>Substitui\u00e7\u00e3o gradual de CNNs em alguns dom\u00ednios</li> <li>Melhor performance em datasets grandes</li> <li>Aten\u00e7\u00e3o global vs. campos receptivos locais</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#neural-architecture-search-nas","title":"Neural Architecture Search (NAS)","text":"<ul> <li>Automa\u00e7\u00e3o do design de arquiteturas</li> <li>EfficientNet, RegNet como exemplos</li> <li>Otimiza\u00e7\u00e3o para dispositivos espec\u00edficos</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#self-supervised-learning","title":"Self-Supervised Learning","text":"<ul> <li>Aprendizado sem r\u00f3tulos</li> <li>Contrastive learning, MAE (Masked Autoencoders)</li> <li>Redu\u00e7\u00e3o da depend\u00eancia de dados anotados</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#edge-computing","title":"Edge Computing","text":"<ul> <li>CNNs otimizadas para dispositivos m\u00f3veis</li> <li>Quantiza\u00e7\u00e3o, pruning, knowledge distillation</li> <li>MobileNets, EfficientNets como precursores</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#multimodalidade","title":"Multimodalidade","text":"<ul> <li>Integra\u00e7\u00e3o de vis\u00e3o com linguagem</li> <li>CLIP, DALL-E como exemplos</li> <li>Aplica\u00e7\u00f5es em rob\u00f3tica e IA geral</li> </ul>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#conclusao","title":"Conclus\u00e3o","text":"<p>As Redes Neurais Convolucionais revolucionaram o campo da Vis\u00e3o Computacional e continuam sendo uma ferramenta fundamental para processamento de dados visuais. Desde a simples LeNet-5 at\u00e9 as arquiteturas modernas como EfficientNet e Vision Transformers, as CNNs demonstraram capacidade excepcional de:</p> <p>\u2705 Aprender representa\u00e7\u00f5es hier\u00e1rquicas de caracter\u00edsticas visuais \u2705 Generalizar para novos dados com performance superior \u2705 Escalar para problemas complexos do mundo real \u2705 Adaptar-se a diferentes dom\u00ednios atrav\u00e9s de transfer learning</p>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#pontos-chave-para-lembrar","title":"Pontos-chave para lembrar:","text":"<ol> <li>Fundamentos s\u00f3lidos: Entenda convolu\u00e7\u00e3o, pooling e backpropagation</li> <li>Pr\u00e1tica constante: Implemente desde CNNs b\u00e1sicas at\u00e9 arquiteturas avan\u00e7adas  </li> <li>Experimenta\u00e7\u00e3o: Teste diferentes arquiteturas e hiperpar\u00e2metros</li> <li>Dados de qualidade: Invista tempo em prepara\u00e7\u00e3o e augmentation</li> <li>Avalia\u00e7\u00e3o rigorosa: Use m\u00e9tricas apropriadas e valida\u00e7\u00e3o cruzada</li> <li>Acompanhe tend\u00eancias: Campo em r\u00e1pida evolu\u00e7\u00e3o</li> </ol>"},{"location":"aulas/IA/lab08/cnn_guia_completo/#proximos-passos-recomendados","title":"Pr\u00f3ximos passos recomendados:","text":"<p>\ud83d\ude80 Imediatos: Complete os exerc\u00edcios pr\u00e1ticos deste guia \ud83d\ude80 Curto prazo: Participe de competi\u00e7\u00f5es Kaggle \ud83d\ude80 M\u00e9dio prazo: Estude Vision Transformers e t\u00e9cnicas modernas \ud83d\ude80 Longo prazo: Contribua para projetos open source e pesquisa</p> <p>O dom\u00ednio das CNNs abre portas para \u00e1reas fascinantes como rob\u00f3tica, realidade aumentada, medicina digital e muito mais. Continue praticando, experimentando e explorando - o futuro da vis\u00e3o computacional est\u00e1 em suas m\u00e3os! \ud83c\udf1f</p> <p>\"The best way to learn deep learning is by doing deep learning.\" - Andrew Ng</p> <p>Bons estudos e que a for\u00e7a (convolucional) esteja com voc\u00ea! \ud83e\udd16\u2728</p>"},{"location":"aulas/IA/lab09/transferlearning/","title":"Transferlearning","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nfrom tensorflow.keras.preprocessing import image\n\n\n## importa o modelo da VGG16 pr\u00e9-treinado\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n</pre> import numpy as np from tensorflow.keras.preprocessing import image   ## importa o modelo da VGG16 pr\u00e9-treinado from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions   In\u00a0[2]: Copied! <pre># Carrega o modelo VGG16 pr\u00e9-treinado com ImageNet:\nmodel = VGG16()\n</pre> # Carrega o modelo VGG16 pr\u00e9-treinado com ImageNet: model = VGG16()  <pre>Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n553467096/553467096 [==============================] - 24s 0us/step\n</pre> In\u00a0[18]: Copied! <pre># Carrega uma imagem e prepara para ser predita pela VGG16\n\n## teste 1\n!wget https://images.tcdn.com.br/img/img_prod/777105/bicicleta_29_hope_21_velocidades_shimano_freios_disco_tamanho_17_12475_1_ac0b7c63eee851b87bcc9832033c9826.jpg -O /content/bike.jpg\nimg_path = 'bike.jpg'\n\n# teste 2\n#!wget https://liberal.com.br/wp-content/uploads/2019/11/buraco-rua-dos-anturios.jpg -O /content/buraco.jpg\n#img_path = 'buraco.jpg'\n\n\n# teste 3\n#img_path = 'COLOQUE_UMA_IMAGEM.jpg'\n\n\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\nprint(\"A imagem \u00e9 carregada e transformada de {}, para {}\".format(img.size,x.shape))\n</pre> # Carrega uma imagem e prepara para ser predita pela VGG16  ## teste 1 !wget https://images.tcdn.com.br/img/img_prod/777105/bicicleta_29_hope_21_velocidades_shimano_freios_disco_tamanho_17_12475_1_ac0b7c63eee851b87bcc9832033c9826.jpg -O /content/bike.jpg img_path = 'bike.jpg'  # teste 2 #!wget https://liberal.com.br/wp-content/uploads/2019/11/buraco-rua-dos-anturios.jpg -O /content/buraco.jpg #img_path = 'buraco.jpg'   # teste 3 #img_path = 'COLOQUE_UMA_IMAGEM.jpg'   img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) print(\"A imagem \u00e9 carregada e transformada de {}, para {}\".format(img.size,x.shape)) <pre>--2023-05-02 11:35:17--  https://images.tcdn.com.br/img/img_prod/777105/bicicleta_29_hope_21_velocidades_shimano_freios_disco_tamanho_17_12475_1_ac0b7c63eee851b87bcc9832033c9826.jpg\nResolving images.tcdn.com.br (images.tcdn.com.br)... 152.199.40.152\nConnecting to images.tcdn.com.br (images.tcdn.com.br)|152.199.40.152|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 286478 (280K) [image/jpeg]\nSaving to: \u2018/content/bike.jpg\u2019\n\n/content/bike.jpg   100%[===================&gt;] 279.76K  --.-KB/s    in 0.005s  \n\n2023-05-02 11:35:18 (57.9 MB/s) - \u2018/content/bike.jpg\u2019 saved [286478/286478]\n\nA imagem \u00e9 carregada e transformada de (224, 224), para (1, 224, 224, 3)\n</pre> In\u00a0[19]: Copied! <pre>## faz a predi\u00e7\u00e3o da imagem\n\npreds = model.predict(x)\n</pre> ## faz a predi\u00e7\u00e3o da imagem  preds = model.predict(x)  <pre>1/1 [==============================] - 0s 21ms/step\n</pre> In\u00a0[20]: Copied! <pre>decoded_preds = decode_predictions(preds)[0]\n\nfor i, (imagenet_id, label, score) in enumerate(decoded_preds):\n    print(f\"{i+1}. {label}: {score * 100:.2f}%\")\n</pre> decoded_preds = decode_predictions(preds)[0]  for i, (imagenet_id, label, score) in enumerate(decoded_preds):     print(f\"{i+1}. {label}: {score * 100:.2f}%\")  <pre>1. mountain_bike: 81.59%\n2. alp: 3.85%\n3. crash_helmet: 1.89%\n4. bicycle-built-for-two: 1.80%\n5. tricycle: 1.75%\n</pre> In\u00a0[21]: Copied! <pre>from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n\nmodel = ResNet50(weights='imagenet')\n\n\n#### seu c\u00f3digo aqui....\n</pre> from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions  model = ResNet50(weights='imagenet')   #### seu c\u00f3digo aqui....    <pre>Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n102967424/102967424 [==============================] - 5s 0us/step\n</pre> In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Configura\u00e7\u00e3o dos diret\u00f3rios e par\u00e2metros do conjunto de dados\n_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\npath_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\nPATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_extract/cats_and_dogs_filtered')\n\ntrain_dir = os.path.join(PATH, 'train')\nvalidation_dir = os.path.join(PATH, 'validation')\n\nbatch_size = 32\nimage_size = (224, 224)\n</pre> import os import numpy as np import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras import layers, models from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Configura\u00e7\u00e3o dos diret\u00f3rios e par\u00e2metros do conjunto de dados _URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip' path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True) PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_extract/cats_and_dogs_filtered')  train_dir = os.path.join(PATH, 'train') validation_dir = os.path.join(PATH, 'validation')  batch_size = 32 image_size = (224, 224)  In\u00a0[43]: Copied! <pre># Fun\u00e7\u00e3o para exibir algumas imagens do conjunto de dados\ndef plot_images(images, labels, class_names):\n    plt.figure(figsize=(10, 10))\n    for i, (img, label) in enumerate(zip(images, labels)):\n        plt.subplot(3, 3, i + 1)\n        plt.imshow(img)\n        plt.title(class_names[label])\n        plt.axis(\"off\")\n    plt.show()\n\n# Carregar imagens e r\u00f3tulos do conjunto de dados de treinamento\ntrain_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\nvalidation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\n# Carregar imagens e r\u00f3tulos do conjunto de dados de valida\u00e7\u00e3o\nvalidation_generator = validation_datagen.flow_from_directory(\n    validation_dir,\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\n# Carregar algumas imagens e r\u00f3tulos do conjunto de dados de treinamento\nsample_datagen = ImageDataGenerator(rescale=1./255)\nsample_generator = sample_datagen.flow_from_directory(\n    train_dir,\n    target_size=image_size,\n    batch_size=9,\n    class_mode='binary'\n)\n\nsample_images, sample_labels = next(sample_generator)\nclass_names = {v: k for k, v in sample_generator.class_indices.items()}\nplot_images(sample_images, sample_labels, class_names)\n</pre> # Fun\u00e7\u00e3o para exibir algumas imagens do conjunto de dados def plot_images(images, labels, class_names):     plt.figure(figsize=(10, 10))     for i, (img, label) in enumerate(zip(images, labels)):         plt.subplot(3, 3, i + 1)         plt.imshow(img)         plt.title(class_names[label])         plt.axis(\"off\")     plt.show()  # Carregar imagens e r\u00f3tulos do conjunto de dados de treinamento train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input) validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)  train_generator = train_datagen.flow_from_directory(     train_dir,     target_size=image_size,     batch_size=batch_size,     class_mode='binary' )  # Carregar imagens e r\u00f3tulos do conjunto de dados de valida\u00e7\u00e3o validation_generator = validation_datagen.flow_from_directory(     validation_dir,     target_size=image_size,     batch_size=batch_size,     class_mode='binary' )  # Carregar algumas imagens e r\u00f3tulos do conjunto de dados de treinamento sample_datagen = ImageDataGenerator(rescale=1./255) sample_generator = sample_datagen.flow_from_directory(     train_dir,     target_size=image_size,     batch_size=9,     class_mode='binary' )  sample_images, sample_labels = next(sample_generator) class_names = {v: k for k, v in sample_generator.class_indices.items()} plot_images(sample_images, sample_labels, class_names) <pre>Found 2000 images belonging to 2 classes.\nFound 1000 images belonging to 2 classes.\nFound 2000 images belonging to 2 classes.\n</pre> In\u00a0[87]: Copied! <pre>from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n\n# Cria o base_model referente a MobileNet V2, sem a camada de classifica\u00e7\u00e3o\nbase_model = MobileNetV2(input_shape=(224, 224, 3),\n                        include_top=False,\n                        weights='imagenet')\n</pre> from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input  # Cria o base_model referente a MobileNet V2, sem a camada de classifica\u00e7\u00e3o base_model = MobileNetV2(input_shape=(224, 224, 3),                         include_top=False,                         weights='imagenet') In\u00a0[\u00a0]: Copied! <pre>base_model.summary()\n</pre> base_model.summary() In\u00a0[90]: Copied! <pre>#Congela a base_model para n\u00e3o atuaizar os pesos quando treinar.\n\nbase_model.trainable = False\n</pre> #Congela a base_model para n\u00e3o atuaizar os pesos quando treinar.  base_model.trainable = False In\u00a0[\u00a0]: Copied! <pre>base_model.summary()\n</pre> base_model.summary() In\u00a0[92]: Copied! <pre>#Camada  para gerar um vetor de 1280 elementos \nglobal_average_layer = layers.GlobalAveragePooling2D()\n\n# O Classificador para gato cachorro com 1 neuronio \nsaida_layer = layers.Dense(1, activation='sigmoid')\n</pre> #Camada  para gerar um vetor de 1280 elementos  global_average_layer = layers.GlobalAveragePooling2D()  # O Classificador para gato cachorro com 1 neuronio  saida_layer = layers.Dense(1, activation='sigmoid') In\u00a0[93]: Copied! <pre>model = tf.keras.Sequential([\n  base_model,   #### cnn mobilenet\n  global_average_layer, ###flatten\n  saida_layer ### especiallista\n])\n\nmodel.summary()\n</pre> model = tf.keras.Sequential([   base_model,   #### cnn mobilenet   global_average_layer, ###flatten   saida_layer ### especiallista ])  model.summary() <pre>Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n ional)                                                          \n                                                                 \n global_average_pooling2d_5   (None, 1280)             0         \n (GlobalAveragePooling2D)                                        \n                                                                 \n dense_4 (Dense)             (None, 1)                 1281      \n                                                                 \n=================================================================\nTotal params: 2,259,265\nTrainable params: 1,281\nNon-trainable params: 2,257,984\n_________________________________________________________________\n</pre> <p>Pronto! J\u00e1 criamos a nossa rede para classifica\u00e7\u00e3o. Agora podemos treinar nossa rede e testar.</p> In\u00a0[94]: Copied! <pre>model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</pre>  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) In\u00a0[95]: Copied! <pre>#Avalia\u00e7\u00e3o do modelo antes de trein\u00e1-lo com novas imagens\nvalidation_steps=20\n\nloss0,accuracy0 = model.evaluate(train_generator, steps = validation_steps)\n</pre> #Avalia\u00e7\u00e3o do modelo antes de trein\u00e1-lo com novas imagens validation_steps=20  loss0,accuracy0 = model.evaluate(train_generator, steps = validation_steps) <pre>20/20 [==============================] - 5s 178ms/step - loss: 0.8159 - accuracy: 0.5125\n</pre> In\u00a0[62]: Copied! <pre># Treinamento da nova CNN\n\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n</pre> # Treinamento da nova CNN  history = model.fit(train_generator, epochs=5, validation_data=validation_generator)  <pre>Epoch 1/5\n63/63 [==============================] - 19s 232ms/step - loss: 0.6893 - accuracy: 0.6200 - val_loss: 0.5397 - val_accuracy: 0.7490\nEpoch 2/5\n63/63 [==============================] - 13s 205ms/step - loss: 0.4975 - accuracy: 0.7575 - val_loss: 0.4721 - val_accuracy: 0.7780\nEpoch 3/5\n63/63 [==============================] - 13s 206ms/step - loss: 0.4484 - accuracy: 0.7880 - val_loss: 0.4458 - val_accuracy: 0.7980\nEpoch 4/5\n63/63 [==============================] - 13s 202ms/step - loss: 0.4176 - accuracy: 0.8005 - val_loss: 0.4327 - val_accuracy: 0.8020\nEpoch 5/5\n63/63 [==============================] - 13s 204ms/step - loss: 0.3953 - accuracy: 0.8235 - val_loss: 0.4159 - val_accuracy: 0.8180\n</pre> In\u00a0[64]: Copied! <pre>import pandas as pd\n\nmetrics_df = pd.DataFrame(history.history)\nmetrics_df[[\"loss\",\"val_loss\"]].plot();\nmetrics_df[[\"accuracy\", \"val_accuracy\"]].plot();\n</pre> import pandas as pd  metrics_df = pd.DataFrame(history.history) metrics_df[[\"loss\",\"val_loss\"]].plot(); metrics_df[[\"accuracy\", \"val_accuracy\"]].plot(); In\u00a0[67]: Copied! <pre>import numpy as np\nfrom tensorflow.keras.preprocessing import image\n\ndef predict_cat_or_dog(img_path):\n    img = image.load_img(img_path, target_size=image_size)\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = preprocess_input(img_array)\n\n    prediction = model.predict(img_array)\n    \n    if prediction[0][0] &lt; 0.5:\n        return \"gatinhooooo\"\n    else:\n        return \"cachorrinho\"\n\n# Teste a fun\u00e7\u00e3o de previs\u00e3o com uma imagem\n\n!wget https://uploads.metropoles.com/wp-content/uploads/2022/07/21154234/como-identificar-que-um-cachorro-esta-sendo-vitima-de-maus-tratos-1.jpg -O /content/cachorro.jpg\nimg_path = \"cachorro.jpg\"\nresult = predict_cat_or_dog(img_path)\nprint(\"Essa foto \u00e9 de um \", result)\n</pre> import numpy as np from tensorflow.keras.preprocessing import image  def predict_cat_or_dog(img_path):     img = image.load_img(img_path, target_size=image_size)     img_array = image.img_to_array(img)     img_array = np.expand_dims(img_array, axis=0)     img_array = preprocess_input(img_array)      prediction = model.predict(img_array)          if prediction[0][0] &lt; 0.5:         return \"gatinhooooo\"     else:         return \"cachorrinho\"  # Teste a fun\u00e7\u00e3o de previs\u00e3o com uma imagem  !wget https://uploads.metropoles.com/wp-content/uploads/2022/07/21154234/como-identificar-que-um-cachorro-esta-sendo-vitima-de-maus-tratos-1.jpg -O /content/cachorro.jpg img_path = \"cachorro.jpg\" result = predict_cat_or_dog(img_path) print(\"Essa foto \u00e9 de um \", result) <pre>--2023-05-02 12:32:46--  https://uploads.metropoles.com/wp-content/uploads/2022/07/21154234/como-identificar-que-um-cachorro-esta-sendo-vitima-de-maus-tratos-1.jpg\nResolving uploads.metropoles.com (uploads.metropoles.com)... 179.191.175.68, 179.191.175.67, 179.191.177.66, ...\nConnecting to uploads.metropoles.com (uploads.metropoles.com)|179.191.175.68|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 47409 (46K) [image/jpeg]\nSaving to: \u2018/content/cachorro.jpg\u2019\n\n/content/cachorro.j 100%[===================&gt;]  46.30K   200KB/s    in 0.2s    \n\n2023-05-02 12:32:47 (200 KB/s) - \u2018/content/cachorro.jpg\u2019 saved [47409/47409]\n\n1/1 [==============================] - 0s 23ms/step\nEssa foto \u00e9 de um  cachorrinho\n</pre> In\u00a0[69]: Copied! <pre># Salvando a rede \nmodel.save(\"dogs_vs_cats.h5\")\n\n#Carregando uma rede .h5\nnew_model = models.load_model('dogs_vs_cats.h5')\n</pre> # Salvando a rede  model.save(\"dogs_vs_cats.h5\")  #Carregando uma rede .h5 new_model = models.load_model('dogs_vs_cats.h5') In\u00a0[\u00a0]: Copied! <pre>### Seu c\u00f3digo aqui....\n</pre> ### Seu c\u00f3digo aqui...."},{"location":"aulas/IA/lab09/transferlearning/#2-redes-neurais","title":"2. Redes Neurais\u00b6","text":""},{"location":"aulas/IA/lab09/transferlearning/#objetivos","title":"Objetivos\u00b6","text":"<ul> <li>Conhecer e praticar Arquiteturas complexas de Redes Neurais Convolucionais</li> <li>Aprendizagem por transfer\u00eancia</li> <li>Praticar a classifica\u00e7\u00e3o de objeto usando framework TensorFlow</li> </ul>"},{"location":"aulas/IA/lab09/transferlearning/#arquitetura-de-redes-neurais-convolucionais","title":"Arquitetura de Redes Neurais Convolucionais\u00b6","text":"<p>Existem diversas arquitetura de CNN, cada rede com suas pr\u00f3prias caracter\u00edsticas, principalmente para vis\u00e3o computacional. Mas todas ter\u00e3o em comum camadas de convolu\u00e7\u00e3o e maxpooling, dropout e algumas coisas a mais...</p>"},{"location":"aulas/IA/lab09/transferlearning/#por-que-utilizar-uma-arquitetura-cnn","title":"Por que utilizar uma arquitetura CNN\u00b6","text":"<p>Utilizar uma arquitetura de CNN possibilita reduzir o tempo de pesquisa com o desenvolvimento de novas arquiteturas uma vez que essas arquiteturas j\u00e1 foram sistematicamente revisadas.</p>"},{"location":"aulas/IA/lab09/transferlearning/#exemplos-de-arquiteturas","title":"Exemplos de arquiteturas:\u00b6","text":"<p><code>LeNET</code>: Desenvolvida em 1998 por Yann LeCun, a LeNet foi pioneira no uso de camadas de convolu\u00e7\u00e3o com filtros 5x5 e passo 1, al\u00e9m de camadas de agrupamento com filtros 2x2 e passo 2, intercaladas por camadas totalmente conectadas (FC). A ordem das camadas \u00e9: CONV-POOL-CONV-POOL-FC-FC. Essa arquitetura teve um papel fundamental no reconhecimento de d\u00edgitos manuscritos.</p> <p></p> <p><code>AlexNET</code>: Criada em 2012 por Alex Krizhevsky, Ilya Sutskever e Geoffrey Hinton, a AlexNet \u00e9 uma arquitetura mais avan\u00e7ada que a LeNet. Possui cinco camadas convolucionais seguidas de tr\u00eas camadas FC, e emprega a fun\u00e7\u00e3o de ativa\u00e7\u00e3o ReLU. Vencedora da competi\u00e7\u00e3o ImageNet de 2012, marcou o in\u00edcio da populariza\u00e7\u00e3o das redes neurais convolucionais profundas.</p> <p></p> <p><code>VGG</code>: A arquitetura VGG, concebida em 2014 pelo Visual Geometry Group da Universidade de Oxford, prop\u00f4s o uso de filtros menores (3x3) em redes mais profundas, com no m\u00ednimo 16 camadas convolucionais e maxpooling com filtros 2x2. Apesar de os filtros menores gerarem menos par\u00e2metros, as camadas FC e as convolu\u00e7\u00f5es iniciais demandavam grande quantidade de mem\u00f3ria RAM, resultando em uma rede pesada.</p> <p></p> <p><code>GoogleNET</code>: Paralelamente \u00e0 VGG, em 2014, pesquisadores do Google desenvolveram a GoogleNet, que introduziu o m\u00f3dulo Inception como elemento fundamental. Com nove m\u00f3dulos Inception em sequ\u00eancia, a arquitetura utiliza convolu\u00e7\u00f5es 3x3 e 5x5 precedidas por convolu\u00e7\u00f5es 1x1 para diminuir o custo computacional. A GoogleNet foi projetada para ser eficiente em termos de recursos e venceu a competi\u00e7\u00e3o ImageNet de 2014.</p> <p></p> <p><code>ResNET</code>: A rede residual, proposta em 2015 por Kaiming He e colaboradores, tem como caracter\u00edstica principal a inclus\u00e3o de conex\u00f5es residuais (curto-circuitos) a cada duas convolu\u00e7\u00f5es, adicionando um resultado anterior ao resultado futuro. Isso permite treinar redes mais profundas sem problemas de degrada\u00e7\u00e3o do desempenho. ResNets com 50, 101 e 152 camadas utilizam blocos residuais com \"bottleneck\", que consistem em duas convolu\u00e7\u00f5es 3x3 intercaladas por convolu\u00e7\u00f5es 1x1, diminuindo o custo computacional.</p> <p></p> <p><code>MobileNet</code>: Proposta em 2017, \u00e9 uma arquitetura otimizada para dispositivos m\u00f3veis e aplicativos com limita\u00e7\u00f5es de recursos computacionais. Utiliza convolu\u00e7\u00f5es separ\u00e1veis por profundidade para reduzir o n\u00famero de par\u00e2metros e o consumo de mem\u00f3ria.</p> <p><code>EfficientNet</code>: Proposta em 2019, \u00e9 uma fam\u00edlia de redes neurais convolucionais que busca melhorar a efici\u00eancia em termos de recursos computacionais e desempenho, atrav\u00e9s do ajuste coordenado da largura, profundidade e resolu\u00e7\u00e3o das redes.</p> <p><code>InceptionV3</code>: Uma evolu\u00e7\u00e3o do GoogleNet, a InceptionV3 \u00e9 uma arquitetura desenvolvida em 2015 que aprimora o m\u00f3dulo Inception e implementa t\u00e9cnicas de normaliza\u00e7\u00e3o em lotes. Essa arquitetura alcan\u00e7a um desempenho superior com menos par\u00e2metros e menor custo computacional.</p> <p><code>DenseNet</code>: Proposta em 2016, a DenseNet \u00e9 uma arquitetura que introduz conex\u00f5es densas entre as camadas. Cada camada recebe as caracter\u00edsticas de todas as camadas anteriores, o que melhora o fluxo de informa\u00e7\u00f5es e gradientes durante o treinamento. Isso permite a constru\u00e7\u00e3o de redes mais profundas e eficientes.</p> <p><code>YOLO</code> (You Only Look Once): \u00c9 uma arquitetura de rede neural focada em detec\u00e7\u00e3o de objetos em tempo real. Proposta em 2016, a YOLO divide a imagem em regi\u00f5es e prev\u00ea, de uma s\u00f3 vez, as probabilidades de classes e as coordenadas das caixas delimitadoras. A YOLO \u00e9 conhecida por sua velocidade e capacidade de detectar objetos em tempo real.</p> <p><code>Transformer</code>: Embora n\u00e3o seja uma arquitetura de rede neural convolucional, o Transformer, proposto em 2017, \u00e9 uma arquitetura de rede neural not\u00e1vel para processamento de linguagem natural e outras tarefas sequenciais. O Transformer introduziu o conceito de aten\u00e7\u00e3o auto-regressiva, que permite que a rede aprenda relacionamentos complexos entre as entradas, e tem sido a base para modelos de linguagem de \u00faltima gera\u00e7\u00e3o, como BERT e GPT.</p> <p>Parace que s\u00e3o muitas, mas essas s\u00e3o apenas algumas arquiteturas de redes neurais desenvolvidas nos \u00faltimos anos. Dependendo da aplica\u00e7\u00e3o e das restri\u00e7\u00f5es de recursos, voc\u00ea pode encontrar uma arquitetura adequada \u00e0s suas necessidades espec\u00edficas.</p>"},{"location":"aulas/IA/lab09/transferlearning/#modelos-de-cnn-pre-treinados","title":"Modelos de CNN pr\u00e9-treinados\u00b6","text":"<p>O treinamento de uma boa CNN n\u00e3o \u00e9 simples, al\u00e9m de muitos dados (milhares de imagens) e muito tempo de processamento.</p> <p>Mas usar essas redes \u00e9 super super facil!!</p> <p>Vamos usar o VGG16 para fazer a classifica\u00e7\u00e3o de uma imagem.</p> <p>Recomendo dar uma olhada na documenta\u00e7\u00e3o oficial do Keras:  https://keras.io/api/applications/</p>"},{"location":"aulas/IA/lab09/transferlearning/#desafio-1","title":"Desafio 1\u00b6","text":"<p>Agora avalie outras arquiteturas de redes neurais dispon\u00edveis no Keras, como ResNet50, InceptionV3, MobileNet e EfficientNet.</p> <p>Basta substituir a importa\u00e7\u00e3o e a fun\u00e7\u00e3o de carregamento do modelo conforme necess\u00e1rio. Por exemplo, para usar a ResNet50:</p>"},{"location":"aulas/IA/lab09/transferlearning/#introducao-ao-transfer-learning-com-redes-pre-treinadas","title":"Introdu\u00e7\u00e3o ao Transfer Learning com redes pr\u00e9-treinadas\u00b6","text":"<p>Excelente! Agora que j\u00e1 sabemos como utilizar uma rede pr\u00e9-treinada, vamos explorar uma t\u00e9cnica poderosa chamada Transfer Learning (Aprendizagem por Transfer\u00eancia). Essa abordagem nos permite tirar proveito das arquiteturas de redes neurais existentes e trein\u00e1-las para classificar objetos personalizados ou novas categorias de imagens.</p> <p>O Transfer Learning \u00e9 uma t\u00e9cnica em que um modelo de aprendizado profundo, treinado previamente em um conjunto de dados maior e mais diversificado, \u00e9 adaptado para ser aplicado a um novo problema. O conhecimento adquirido pelo modelo original \u00e9 transferido para o novo problema, permitindo um treinamento mais r\u00e1pido e, muitas vezes, um desempenho melhor do que treinar uma rede neural do zero.</p> <p>A ideia por tr\u00e1s do Transfer Learning \u00e9 que as redes neurais pr\u00e9-treinadas, como VGG, ResNet e Inception, j\u00e1 aprenderam a <code>extrair caracter\u00edsticas</code> importantes das imagens em seus primeiros est\u00e1gios. Essas caracter\u00edsticas podem ser comuns a muitos problemas de classifica\u00e7\u00e3o de imagens, como detec\u00e7\u00e3o de bordas, texturas e padr\u00f5es. Ao aproveitar esse conhecimento pr\u00e9vio, podemos nos concentrar no treinamento das \u00faltimas camadas do modelo, que s\u00e3o respons\u00e1veis por aprender caracter\u00edsticas espec\u00edficas do novo problema.</p> <p>Ao utilizar o Transfer Learning, podemos economizar tempo e recursos computacionais, al\u00e9m de obter melhores resultados do que treinar uma rede do zero para um conjunto de dados menor e espec\u00edfico. Portanto, \u00e9 uma t\u00e9cnica amplamente utilizada em aplica\u00e7\u00f5es pr\u00e1ticas de aprendizado profundo e processamento de imagens.</p>"},{"location":"aulas/IA/lab09/transferlearning/#combinando-a-rede-pre-treinada-com-um-classificador-mlp","title":"Combinando a rede pr\u00e9-treinada com um classificador MLP\u00b6","text":"<p>Ao aplicar o Transfer Learning, nossa rede convolucional ser\u00e1 composta por duas partes principais: o extrator de caracter\u00edsticas e o classificador. O extrator de caracter\u00edsticas ser\u00e1 baseado em uma rede pr\u00e9-treinada, como VGG16, ResNet50 ou InceptionV3. Essa parte da rede j\u00e1 aprendeu a extrair caracter\u00edsticas relevantes de imagens, como bordas, texturas e padr\u00f5es, durante o treinamento em um grande conjunto de dados, como o ImageNet.</p> <p>Em seguida, adicionaremos um classificador MLP (Multilayer Perceptron) personalizado para resolver o nosso problema espec\u00edfico de classifica\u00e7\u00e3o de imagens. Esse classificador ser\u00e1 respons\u00e1vel por aprender as caracter\u00edsticas espec\u00edficas do novo conjunto de dados e classificar as imagens nas categorias desejadas.</p> <p>Dessa forma, a rede ajustada combina o poder das redes pr\u00e9-treinadas, que j\u00e1 aprenderam a extrair caracter\u00edsticas gerais de imagens, com um classificador personalizado que aprender\u00e1 a distinguir as categorias espec\u00edficas do nosso problema. Como mostra a figura abaixo:</p> <p></p> <p>Agora que entendemos os conceitos b\u00e1sicos de Transfer Learning, podemos prosseguir com os passos para aplicar o Transfer Learning e adaptar a rede pr\u00e9-treinada ao nosso problema de classifica\u00e7\u00e3o de imagens.</p>"},{"location":"aulas/IA/lab09/transferlearning/#passo-a-passo-para-aplicar-transfer-learning","title":"Passo a passo para aplicar Transfer Learning\u00b6","text":"<ol> <li><p>Escolha uma rede pr\u00e9-treinada: Selecione uma rede neural pr\u00e9-treinada dispon\u00edvel no Keras (por exemplo, VGG16, ResNet50, InceptionV3) com base nas caracter\u00edsticas e requisitos do seu problema. Cada arquitetura tem suas pr\u00f3prias vantagens e desvantagens, portanto, escolha aquela que melhor se adapta \u00e0s suas necessidades.</p> </li> <li><p>Remova a camada de classifica\u00e7\u00e3o: Carregue a rede neural pr\u00e9-treinada sem a camada de classifica\u00e7\u00e3o final. Isso pode ser feito usando o argumento include_top=False ao carregar o modelo no Keras. Isso permitir\u00e1 que voc\u00ea adicione suas pr\u00f3prias camadas personalizadas para classificar as novas categorias.</p> </li> <li><p>Adicione camadas personalizadas: Adicione camadas espec\u00edficas para o seu problema de classifica\u00e7\u00e3o. Normalmente, isso inclui uma camada de GlobalAveragePooling2D, seguida por uma camada densa com uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o softmax e o n\u00famero de neuronios igual ao n\u00famero de classes do novo problema.</p> </li> <li><p>Congele as camadas pr\u00e9-treinadas: \u00c9 uma boa pr\u00e1tica congelar as camadas pr\u00e9-treinadas da rede neural, especialmente durante as primeiras \u00e9pocas do treinamento. Isso evitar\u00e1 que os pesos dessas camadas sejam atualizados e preservar\u00e1 o conhecimento pr\u00e9vio que elas possuem. No Keras, voc\u00ea pode fazer isso com o modelxxx.trainable = False</p> </li> <li><p>Pr\u00e9-processamento dos dados: Prepare os dados de acordo com a rede pr\u00e9-treinada escolhida. Isso inclui redimensionar as imagens, normalizar os valores dos pixels e codificar as etiquetas das categorias. Lembre-se de aplicar as mesmas transforma\u00e7\u00f5es usadas no conjunto de dados original da rede pr\u00e9-treinada.</p> </li> <li><p>Treine o modelo: Treine o modelo ajustado no seu conjunto de dados. Durante as primeiras \u00e9pocas, com as camadas pr\u00e9-treinadas congeladas, o modelo aprender\u00e1 as caracter\u00edsticas espec\u00edficas do novo problema.</p> </li> <li><p>Avalie e otimize: Avalie o desempenho do modelo ajustado em um conjunto de teste e otimize os hiperpar\u00e2metros conforme necess\u00e1rio. Voc\u00ea pode experimentar diferentes arquiteturas de redes neurais, taxas de aprendizado, otimizadores e outros hiperpar\u00e2metros para encontrar a melhor configura\u00e7\u00e3o para o seu problema.</p> </li> </ol>"},{"location":"aulas/IA/lab09/transferlearning/#aplicando-transfer-learning-em-um-dataset-ja-preparado-pelo-tensorflow","title":"Aplicando transfer learning em um dataset j\u00e1 preparado pelo tensorflow\u00b6","text":"<p>Vamos usar o dataset <code>cats_vs_dogs</code> que \u00e9 disponibilizado pelo proprio tensorflow, desta forma focamos apenas no entendimento da tecnica de transfer learning e menos em preprocessamento e cria\u00e7\u00e3o de dados. nas proximas aulas vamos criar nosso proprio dataset...</p>"},{"location":"aulas/IA/lab09/transferlearning/#escolhendo-um-modelo-pre-treinado","title":"Escolhendo um modelo pr\u00e9-treinado\u00b6","text":"<p>A <code>MobileNet V2</code> desenvolvido no Google e foi treinado com <code>1,4 milh\u00e3o de imagens</code> e possui <code>1000 classes diferentes</code> com pesos predeterminados do imagenet (Googles dataset).</p> <p>Carregue a rede neural pr\u00e9-treinada sem a camada de classifica\u00e7\u00e3o final. Isso pode ser feito usando o argumento <code>include_top=False</code></p>"},{"location":"aulas/IA/lab09/transferlearning/#adicionando-um-classificador","title":"Adicionando um Classificador\u00b6","text":""},{"location":"aulas/IA/lab09/transferlearning/#desafio-2","title":"Desafio 2\u00b6","text":"<p>Vamos entender o que acabamos de fazer. Avalie a quantidade de parametros total, treinaveis e n\u00e3o treinaveis. O que foi identificado?</p>"},{"location":"aulas/IA/lab09/transferlearning/#sua-resposta-aqui","title":"sua resposta aqui.....\u00b6","text":"<p>.</p>"},{"location":"aulas/IA/lab09/transferlearning/#treinamento-do-modelo","title":"Treinamento do modelo\u00b6","text":""},{"location":"aulas/IA/lab09/transferlearning/#fazendo-predicoes","title":"Fazendo predi\u00e7\u00f5es\u00b6","text":""},{"location":"aulas/IA/lab09/transferlearning/#salvando-o-modelo-da-rede-treinada","title":"Salvando o modelo da rede treinada\u00b6","text":"<p>Agora que j\u00e1 temos um modelo treinado e ajustado para resolver o problema especifico que temos, podemos salver a arquitetura e os pesos em um arquivo com extens\u00e3o .h5</p> <p>para usar esta rede, basta carregar o arquivo.h5</p>"},{"location":"aulas/IA/lab09/transferlearning/#desafio-3","title":"Desafio 3\u00b6","text":"<p>Aplicar o Transfer Learning usando a rede pr\u00e9-treinada ResNet50 e o conjunto de dados CIFAR-10, que possui 10 classes de objetos.</p>"},{"location":"aulas/IA/lab11/teoria_yolo/","title":"Teoria yolo","text":""},{"location":"aulas/IA/lab11/teoria_yolo/#yolo-you-only-look-once","title":"YOLO - You Only Look Once","text":"<ul> <li>Notebook 1</li> <li>Notebook 2</li> <li>Notebook 3</li> </ul> <p>Modelo popular de detec\u00e7\u00e3o de objetos e segmenta\u00e7\u00e3o de imagens, foi desenvolvido por Joseph Redmon e Ali Farhadi na Universidade de Washington. Lan\u00e7ado em 2015, o YOLO ganhou popularidade por sua alta velocidade e precis\u00e3o.</p> <p>Voc\u00ea n\u00e3o precisa implementar do zero desde vers\u00e3o V1.  Voc\u00ea precisa usar YOLO efetivamente. O contexto hist\u00f3rico serve apenas para entender por que YOLO \u00e9 dominante hoje.</p> <pre><code>2015: YOLOv1 - \"Eureka moment\"\n\u251c\u2500\u2500 Primeira detec\u00e7\u00e3o em tempo real real\n\u2514\u2500\u2500 Mudou paradigma da \u00e1rea forever\n\n2018: YOLOv3 - \"Consolida\u00e7\u00e3o\"\n\u251c\u2500\u2500 Multi-scale detection\n\u2514\u2500\u2500 Tornou-se padr\u00e3o industrial\n\n2020: YOLOv5 - \"Democratiza\u00e7\u00e3o\"  \n\u251c\u2500\u2500 PyTorch implementation\n\u251c\u2500\u2500 API amig\u00e1vel (Ultralytics)\n\u2514\u2500\u2500 Ado\u00e7\u00e3o massiva\n\n2023: YOLOv8 - \"Unifica\u00e7\u00e3o\"\n\u251c\u2500\u2500 Multi-task (detection + segmentation + pose)\n\u251c\u2500\u2500 Anchor-free simplification\n\u2514\u2500\u2500 Estado atual da arte\n\n2025: YOLOv12 - \"Estado da arte\"\n\u2514\u2500\u2500 Estado atual da arte\n</code></pre>"},{"location":"aulas/IA/lab11/teoria_yolo/#documentacao-oficial","title":"Documenta\u00e7\u00e3o Oficial:","text":"<ul> <li>Ultralytics Docs: https://docs.ultralytics.com/</li> <li>GitHub: https://github.com/ultralytics/ultralytics</li> <li>Google Colab Examples: https://colab.research.google.com/github/ultralytics/ultralytics/</li> </ul>"},{"location":"aulas/IA/lab11/teoria_yolo/#filosofia-central-imutavel-desde-2015","title":"Filosofia Central (Imut\u00e1vel desde 2015)","text":"<p>O YOLO resolve o problema fundamental da detec\u00e7\u00e3o de objetos com uma abordagem elegante:</p> <p>\"Analise a imagem inteira uma \u00fanica vez e preveja simultaneamente onde est\u00e3o TODOS os objetos\"</p>"},{"location":"aulas/IA/lab11/teoria_yolo/#familia-yolov8","title":"Fam\u00edlia YOLOv8","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Modelo    \u2502  mAP    \u2502   FPS   \u2502  Params  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  YOLOv8`n`    \u2502  37.3%  \u2502  165+   \u2502   3.2M   \u2502\n\u2502  YOLOv8`s`    \u2502  44.9%  \u2502  120    \u2502  11.2M   \u2502\n\u2502  YOLOv8`m`    \u2502  50.2%  \u2502   90    \u2502  25.9M   \u2502\n\u2502  YOLOv8`l`    \u2502  52.9%  \u2502   65    \u2502  43.7M   \u2502\n\u2502  YOLOv8`x`    \u2502  53.9%  \u2502   45    \u2502  68.2M   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aulas/IA/lab11/teoria_yolo/#arquitetura-atual-simplificada","title":"Arquitetura Atual Simplificada","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   INPUT     \u2502 \u2190 Imagem 640\u00d7640 (padr\u00e3o)\n\u2502   IMAGE     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  BACKBONE   \u2502 \u2190 Extra\u00e7\u00e3o de features hier\u00e1rquicas\n\u2502 (CSPDarknet)\u2502   \u2022 P3: 80\u00d780 (objetos pequenos)\n\u2502             \u2502   \u2022 P4: 40\u00d740 (objetos m\u00e9dios)  \n\u2502             \u2502   \u2022 P5: 20\u00d720 (objetos grandes)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    NECK     \u2502 \u2190 Fus\u00e3o multi-escala (PANet)\n\u2502  (PANet)    \u2502   Combina informa\u00e7\u00f5es de todas as escalas\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    HEAD     \u2502 \u2190 Predi\u00e7\u00f5es finais (Anchor-free)\n\u2502 (Decoupled) \u2502   \u2022 Classifica\u00e7\u00e3o\n\u2502             \u2502   \u2022 Regress\u00e3o (coordenadas)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   OUTPUT    \u2502 \u2190 [x, y, w, h, conf, class_probs]\n\u2502 DETECTIONS  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aulas/IA/lab11/teoria_yolo/#setup","title":"Setup","text":"<pre><code># Instalar Ultralytics (vers\u00e3o atual)\npip install ultralytics\n\n# Verificar instala\u00e7\u00e3o\nyolo version\n</code></pre>"},{"location":"aulas/IA/lab11/teoria_yolo/#deteccao-em-3-linhas-de-codigo","title":"Detec\u00e7\u00e3o em 3 Linhas de C\u00f3digo","text":"<pre><code>from ultralytics import YOLO\n\n# Carregar modelo pr\u00e9-treinado\nmodel = YOLO('yolov8n.pt')  # Download autom\u00e1tico na primeira vez\n\n# Detectar objetos\nresults = model('sua_imagem.jpg')\n\n# Visualizar resultados\nresults[0].show()  # Mostra imagem com detec\u00e7\u00f5es\n</code></pre>"},{"location":"aulas/IA/lab11/teoria_yolo/#deteccao-em-webcam-tempo-real","title":"Detec\u00e7\u00e3o em Webcam (Tempo Real)","text":"<pre><code>import cv2\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Detec\u00e7\u00e3o em tempo real\n    results = model(frame, stream=True, verbose=False)\n\n    for result in results:\n        # Desenhar detec\u00e7\u00f5es\n        annotated_frame = result.plot()\n        cv2.imshow('YOLO Webcam', annotated_frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"aulas/IA/lab11/teoria_yolo/#treinamento-personalizado","title":"Treinamento Personalizado","text":"<p>Podemos realizar um treinamento customizado para uma determinada classe de objetos que queremos detectar, por meio de \"transfer learning\", para isso precisamos basicamente que o dataset seja montado da seguinte forma:</p>"},{"location":"aulas/IA/lab11/teoria_yolo/#estrutura-do-dataset-yolo","title":"Estrutura do Dataset YOLO","text":"<pre><code>meu_dataset/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 img001.jpg\n\u2502   \u2502   \u2514\u2500\u2500 img002.jpg\n\u2502   \u2514\u2500\u2500 val/\n\u2502       \u251c\u2500\u2500 img003.jpg\n\u2502       \u2514\u2500\u2500 img004.jpg\n\u251c\u2500\u2500 labels/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 img001.txt  # Anota\u00e7\u00f5es YOLO format\n\u2502   \u2502   \u2514\u2500\u2500 img002.txt\n\u2502   \u2514\u2500\u2500 val/\n\u2502       \u251c\u2500\u2500 img003.txt\n\u2502       \u2514\u2500\u2500 img004.txt\n\u2514\u2500\u2500 data.yaml  # Configura\u00e7\u00e3o do dataset\n</code></pre>"},{"location":"aulas/IA/lab11/teoria_yolo/#formato-de-anotacao-yolo","title":"Formato de Anota\u00e7\u00e3o YOLO","text":"<pre><code># Cada linha no arquivo .txt representa um objeto:\n# class_id x_center y_center width height\n# (todas as coordenadas normalizadas 0-1)\n\n0 0.5 0.3 0.2 0.4    # Pessoa no centro-superior\n1 0.8 0.7 0.15 0.25  # Carro no canto inferior direito\n</code></pre>"},{"location":"aulas/IA/lab11/teoria_yolo/#arquivo-de-configuracao-datayaml","title":"Arquivo de Configura\u00e7\u00e3o (data.yaml)","text":"<p>O arquivo <code>.yaml</code> possui as diretivas de configura\u00e7\u00e3o para o treinamento do modelo, deve ser passado o caminho para os dados e as classes</p> <pre><code># data.yaml\npath: /caminho/para/meu_dataset\ntrain: images/train\nval: images/val\n\nnames:\n  0: pessoa\n  1: carro\n  2: bicicleta\n\nnc: 3  # n\u00famero de classes\n</code></pre>"},{"location":"aulas/IA/lab11/teoria_yolo/#treinamento","title":"Treinamento","text":"<pre><code>from ultralytics import YOLO\n\n# Carregar modelo base (transfer learning)\nmodel = YOLO('yolov8n.pt')\n\n# Treinar no seu dataset\nresults = model.train(\n    data='data.yaml',\n    epochs=100,\n    imgsz=640,\n    batch=16,\n    name='meu_modelo',\n    patience=20,      # Early stopping\n    save_period=10,   # Salvar checkpoint a cada 10 \u00e9pocas\n    device='0'        # GPU 0, ou 'cpu' para CPU\n)\n\n# Avaliar modelo treinado\nmetrics = model.val()\nprint(f\"mAP@0.5: {metrics.box.map50}\")\nprint(f\"mAP@0.5:0.95: {metrics.box.map}\")\n</code></pre>"},{"location":"aulas/IA/lab11/teoria_yolo/#deployment-e-otimizacao","title":"Deployment e Otimiza\u00e7\u00e3o","text":""},{"location":"aulas/IA/lab11/teoria_yolo/#exportacao-para-producao","title":"Exporta\u00e7\u00e3o para Produ\u00e7\u00e3o","text":"<pre><code>model = YOLO('meu_modelo.pt')\n\n# Exportar para diferentes formatos\nmodel.export(format='onnx')        # ONNX (recomendado)\nmodel.export(format='engine')      # TensorRT (NVIDIA GPUs)\nmodel.export(format='coreml')      # Apple devices\nmodel.export(format='tflite')      # Mobile (Android/iOS)\nmodel.export(format='openvino')    # Intel hardware\n</code></pre>"},{"location":"aulas/IA/lab11/yolo/","title":"Yolo","text":"In\u00a0[\u00a0]: Copied! <pre># Instala\u00e7\u00e3o da biblioteca Ultralytics\n# !pip install ultralytics\n# !pip install opencv-python matplotlib\n</pre> # Instala\u00e7\u00e3o da biblioteca Ultralytics # !pip install ultralytics # !pip install opencv-python matplotlib In\u00a0[2]: Copied! <pre># Importa\u00e7\u00e3o das bibliotecas necess\u00e1rias\nfrom ultralytics import YOLO\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n</pre> # Importa\u00e7\u00e3o das bibliotecas necess\u00e1rias from ultralytics import YOLO import cv2 import matplotlib.pyplot as plt import numpy as np In\u00a0[3]: Copied! <pre># Carregando o modelo YOLOv8n (nano - vers\u00e3o mais leve)\nmodel = YOLO('model/yolov8n.pt')\n\n# Tamb\u00e9m podemos carregar diferentes tamanhos de modelo:\n# model = YOLO('model/yolov8s.pt')  # small\n# model = YOLO('model/yolov8m.pt')  # medium\n# model = YOLO('model/yolov8l.pt')  # large\n# model = YOLO('model/yolov8x.pt')  # extra large\n</pre> # Carregando o modelo YOLOv8n (nano - vers\u00e3o mais leve) model = YOLO('model/yolov8n.pt')  # Tamb\u00e9m podemos carregar diferentes tamanhos de modelo: # model = YOLO('model/yolov8s.pt')  # small # model = YOLO('model/yolov8m.pt')  # medium # model = YOLO('model/yolov8l.pt')  # large # model = YOLO('model/yolov8x.pt')  # extra large In\u00a0[4]: Copied! <pre>def visualize_detection(image, results):\n    \"\"\"\n    Visualiza a imagem com as detec\u00e7\u00f5es\n    \"\"\"\n    # Se a imagem for uma string (caminho), carregamos ela\n    if isinstance(image, str):\n        image = cv2.imread(image)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    # Plotamos a imagem com as detec\u00e7\u00f5es\n    plt.figure(figsize=(10, 10))\n    plt.imshow(cv2.cvtColor(results[0].plot(), cv2.COLOR_BGR2RGB))\n    plt.axis('off')\n    plt.show()\n    \n    # Mostramos informa\u00e7\u00f5es sobre as detec\u00e7\u00f5es\n    boxes = results[0].boxes\n    print(f\"Foram detectados {len(boxes)} objetos\")\n    \n    # Exibimos cada detec\u00e7\u00e3o\n    for i, box in enumerate(boxes):\n        cls = int(box.cls[0])\n        name = model.names[cls]\n        conf = float(box.conf[0])\n        print(f\"Detec\u00e7\u00e3o {i+1}: {name} com confian\u00e7a {conf:.2f}\")\n</pre> def visualize_detection(image, results):     \"\"\"     Visualiza a imagem com as detec\u00e7\u00f5es     \"\"\"     # Se a imagem for uma string (caminho), carregamos ela     if isinstance(image, str):         image = cv2.imread(image)         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)          # Plotamos a imagem com as detec\u00e7\u00f5es     plt.figure(figsize=(10, 10))     plt.imshow(cv2.cvtColor(results[0].plot(), cv2.COLOR_BGR2RGB))     plt.axis('off')     plt.show()          # Mostramos informa\u00e7\u00f5es sobre as detec\u00e7\u00f5es     boxes = results[0].boxes     print(f\"Foram detectados {len(boxes)} objetos\")          # Exibimos cada detec\u00e7\u00e3o     for i, box in enumerate(boxes):         cls = int(box.cls[0])         name = model.names[cls]         conf = float(box.conf[0])         print(f\"Detec\u00e7\u00e3o {i+1}: {name} com confian\u00e7a {conf:.2f}\") In\u00a0[\u00a0]: Copied! <pre># Baixar uma imagem de exemplo\n\n# no linux ou Google Colab\n# !wget -O sample_image.jpg https://ultralytics.com/images/bus.jpg\n# no macOS\n# !curl -o sample_image.jpg https://ultralytics.com/images/bus.jpg\n\n# pode usar requests para baixar a imagem independente do sistema operacional\n\nimport requests\nimport os\n\ndef baixar_arquivo(url, destino):\n    \"\"\"\n    Baixa um arquivo a partir de uma URL e salva com o nome especificado.\n\n    Par\u00e2metros:\n    - url (str): URL do arquivo a ser baixado.\n    - destino (str): Caminho e nome do arquivo onde ser\u00e1 salvo.\n\n    Retorna:\n    - True se o download foi bem-sucedido, False caso contr\u00e1rio.\n    \"\"\"\n    try:\n        if os.path.exists(destino):\n            print(f\"Arquivo j\u00e1 existe: {destino}\")\n            return True\n\n        resposta = requests.get(url, timeout=10)\n        resposta.raise_for_status()  # Levanta erro se status != 200\n\n        with open(destino, \"wb\") as f:\n            f.write(resposta.content)\n\n        print(f\"Download conclu\u00eddo: {destino}\")\n        return True\n    except Exception as e:\n        print(f\"Erro ao baixar o arquivo: {e}\")\n        return False\n\nurl = \"https://ultralytics.com/images/bus.jpg\"\ndestino = \"lab_images/sample_image.jpg\"\n\nbaixar_arquivo(url, destino)\n</pre> # Baixar uma imagem de exemplo  # no linux ou Google Colab # !wget -O sample_image.jpg https://ultralytics.com/images/bus.jpg # no macOS # !curl -o sample_image.jpg https://ultralytics.com/images/bus.jpg  # pode usar requests para baixar a imagem independente do sistema operacional  import requests import os  def baixar_arquivo(url, destino):     \"\"\"     Baixa um arquivo a partir de uma URL e salva com o nome especificado.      Par\u00e2metros:     - url (str): URL do arquivo a ser baixado.     - destino (str): Caminho e nome do arquivo onde ser\u00e1 salvo.      Retorna:     - True se o download foi bem-sucedido, False caso contr\u00e1rio.     \"\"\"     try:         if os.path.exists(destino):             print(f\"Arquivo j\u00e1 existe: {destino}\")             return True          resposta = requests.get(url, timeout=10)         resposta.raise_for_status()  # Levanta erro se status != 200          with open(destino, \"wb\") as f:             f.write(resposta.content)          print(f\"Download conclu\u00eddo: {destino}\")         return True     except Exception as e:         print(f\"Erro ao baixar o arquivo: {e}\")         return False  url = \"https://ultralytics.com/images/bus.jpg\" destino = \"lab_images/sample_image.jpg\"  baixar_arquivo(url, destino)  <pre>Download conclu\u00eddo: sample_image.jpg\n</pre> Out[\u00a0]: <pre>True</pre> In\u00a0[5]: Copied! <pre># Realizar infer\u00eancia na imagem\nresults = model('lab_images/bus.jpg')\n\n# Visualizar os resultados\nvisualize_detection('lab_images/bus.jpg', results)\n</pre> # Realizar infer\u00eancia na imagem results = model('lab_images/bus.jpg')  # Visualizar os resultados visualize_detection('lab_images/bus.jpg', results) <pre>\nimage 1/1 /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 74.2ms\nSpeed: 1.8ms preprocess, 74.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n</pre> <pre>Foram detectados 6 objetos\nDetec\u00e7\u00e3o 1: bus com confian\u00e7a 0.87\nDetec\u00e7\u00e3o 2: person com confian\u00e7a 0.87\nDetec\u00e7\u00e3o 3: person com confian\u00e7a 0.85\nDetec\u00e7\u00e3o 4: person com confian\u00e7a 0.83\nDetec\u00e7\u00e3o 5: person com confian\u00e7a 0.26\nDetec\u00e7\u00e3o 6: stop sign com confian\u00e7a 0.26\n</pre> In\u00a0[7]: Copied! <pre># Examinando o objeto de resultados, no library ultralytics\n\n# Exibindo informa\u00e7\u00f5es espec\u00edficas\nprint(\"\\nInforma\u00e7\u00f5es espec\u00edficas do resultado:\")\n\nprint(f\"Tipo: {type(results[0])}\")\nprint(f\"Quantidade de boxes: {len(results[0].boxes)}\")\n# Exibindo informa\u00e7\u00f5es de cada box\nprint(\"\\nInforma\u00e7\u00f5es de cada box:\")\nfor i, box in enumerate(results[0].boxes):\n    print(f\"Box {i}:\")\n    print(f\"  Coordenadas: {box.xyxy}\")\n    print(f\"  Classe: {box.cls}\")\n    print(f\"  Confian\u00e7a: {box.conf}\")\n\nresult = results[0] \n\n# Bounding boxes (coordenadas no formato [x1, y1, x2, y2])\nprint(\"Bounding boxes:\")\nprint(result.boxes.xyxy)\n\n# Classes detectadas\nprint(\"\\nClasses detectadas:\")\nprint(result.boxes.cls)\n\n# Mapeando classes para nomes\nprint(\"\\nNomes das classes:\")\nfor c in result.boxes.cls:\n    print(model.names[int(c)])\n\n# Scores de confian\u00e7a\nprint(\"\\nScores de confian\u00e7a:\")\nprint(result.boxes.conf)\n</pre> # Examinando o objeto de resultados, no library ultralytics  # Exibindo informa\u00e7\u00f5es espec\u00edficas print(\"\\nInforma\u00e7\u00f5es espec\u00edficas do resultado:\")  print(f\"Tipo: {type(results[0])}\") print(f\"Quantidade de boxes: {len(results[0].boxes)}\") # Exibindo informa\u00e7\u00f5es de cada box print(\"\\nInforma\u00e7\u00f5es de cada box:\") for i, box in enumerate(results[0].boxes):     print(f\"Box {i}:\")     print(f\"  Coordenadas: {box.xyxy}\")     print(f\"  Classe: {box.cls}\")     print(f\"  Confian\u00e7a: {box.conf}\")  result = results[0]   # Bounding boxes (coordenadas no formato [x1, y1, x2, y2]) print(\"Bounding boxes:\") print(result.boxes.xyxy)  # Classes detectadas print(\"\\nClasses detectadas:\") print(result.boxes.cls)  # Mapeando classes para nomes print(\"\\nNomes das classes:\") for c in result.boxes.cls:     print(model.names[int(c)])  # Scores de confian\u00e7a print(\"\\nScores de confian\u00e7a:\") print(result.boxes.conf) <pre>\nInforma\u00e7\u00f5es espec\u00edficas do resultado:\nTipo: &lt;class 'ultralytics.engine.results.Results'&gt;\nQuantidade de boxes: 6\n\nInforma\u00e7\u00f5es de cada box:\nBox 0:\n  Coordenadas: tensor([[ 22.8713, 231.2773, 805.0027, 756.8405]])\n  Classe: tensor([5.])\n  Confian\u00e7a: tensor([0.8734])\nBox 1:\n  Coordenadas: tensor([[ 48.5504, 398.5523, 245.3456, 902.7027]])\n  Classe: tensor([0.])\n  Confian\u00e7a: tensor([0.8657])\nBox 2:\n  Coordenadas: tensor([[669.4729, 392.1861, 809.7202, 877.0355]])\n  Classe: tensor([0.])\n  Confian\u00e7a: tensor([0.8528])\nBox 3:\n  Coordenadas: tensor([[221.5173, 405.7986, 344.9706, 857.5366]])\n  Classe: tensor([0.])\n  Confian\u00e7a: tensor([0.8252])\nBox 4:\n  Coordenadas: tensor([[  0.0000, 550.5250,  63.0069, 873.4429]])\n  Classe: tensor([0.])\n  Confian\u00e7a: tensor([0.2611])\nBox 5:\n  Coordenadas: tensor([[5.8161e-02, 2.5446e+02, 3.2557e+01, 3.2487e+02]])\n  Classe: tensor([11.])\n  Confian\u00e7a: tensor([0.2551])\nBounding boxes:\ntensor([[2.2871e+01, 2.3128e+02, 8.0500e+02, 7.5684e+02],\n        [4.8550e+01, 3.9855e+02, 2.4535e+02, 9.0270e+02],\n        [6.6947e+02, 3.9219e+02, 8.0972e+02, 8.7704e+02],\n        [2.2152e+02, 4.0580e+02, 3.4497e+02, 8.5754e+02],\n        [0.0000e+00, 5.5053e+02, 6.3007e+01, 8.7344e+02],\n        [5.8161e-02, 2.5446e+02, 3.2557e+01, 3.2487e+02]])\n\nClasses detectadas:\ntensor([ 5.,  0.,  0.,  0.,  0., 11.])\n\nNomes das classes:\nbus\nperson\nperson\nperson\nperson\nstop sign\n\nScores de confian\u00e7a:\ntensor([0.8734, 0.8657, 0.8528, 0.8252, 0.2611, 0.2551])\n</pre> In\u00a0[\u00a0]: Copied! <pre># Baixar um v\u00eddeo de exemplo\n# - https://drive.google.com/file/d/1RMkFUVCeuuU76-9IDNbVoQr8R1NUS0Di/view?usp=sharing\n\n# # Instale o gdown se necess\u00e1rio (no Colab j\u00e1 costuma estar instalado)\n# !pip install -q gdown\n\n# # Baixe o v\u00eddeo\n# !gdown 1RMkFUVCeuuU76-9IDNbVoQr8R1NUS0Di -O lab_images/race_car.mp4\n</pre> # Baixar um v\u00eddeo de exemplo # - https://drive.google.com/file/d/1RMkFUVCeuuU76-9IDNbVoQr8R1NUS0Di/view?usp=sharing  # # Instale o gdown se necess\u00e1rio (no Colab j\u00e1 costuma estar instalado) # !pip install -q gdown  # # Baixe o v\u00eddeo # !gdown 1RMkFUVCeuuU76-9IDNbVoQr8R1NUS0Di -O lab_images/race_car.mp4  <pre>Download conclu\u00eddo: sample_video.mp4\n</pre> Out[\u00a0]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre># Processar o v\u00eddeo\nresults_video = model('lab_images/pombos.mp4', \n                      conf=0.3, \n                      classes=[13, 14],\n                      save=True)\n\n\n# results_custom = model(\n#     'lab_images/bus.jpg',\n#     conf=0.5,        # Limiar de confian\u00e7a (padr\u00e3o 0.25)\n#     iou=0.7,         # Limiar IoU para NMS (padr\u00e3o 0.45)\n#     max_det=20,      # N\u00famero m\u00e1ximo de detec\u00e7\u00f5es\n#     classes=[13, 14]   # Filtrar apenas pessoas (0) e carros (2)\n# )\n\n# O v\u00eddeo processado estar\u00e1 dispon\u00edvel na pasta 'runs/detect/predict/'\n</pre> # Processar o v\u00eddeo results_video = model('lab_images/pombos.mp4',                        conf=0.3,                        classes=[13, 14],                       save=True)   # results_custom = model( #     'lab_images/bus.jpg', #     conf=0.5,        # Limiar de confian\u00e7a (padr\u00e3o 0.25) #     iou=0.7,         # Limiar IoU para NMS (padr\u00e3o 0.45) #     max_det=20,      # N\u00famero m\u00e1ximo de detec\u00e7\u00f5es #     classes=[13, 14]   # Filtrar apenas pessoas (0) e carros (2) # )  # O v\u00eddeo processado estar\u00e1 dispon\u00edvel na pasta 'runs/detect/predict/' <pre>\nWARNING \u26a0\ufe0f \ninference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\nerrors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n\nExample:\n    results = model(source=..., stream=True)  # generator of Results objects\n    for r in results:\n        boxes = r.boxes  # Boxes object for bbox outputs\n        masks = r.masks  # Masks object for segment masks outputs\n        probs = r.probs  # Class probabilities for classification outputs\n\nvideo 1/1 (frame 1/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 408.2ms\nvideo 1/1 (frame 2/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 389.1ms\nvideo 1/1 (frame 3/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 381.8ms\nvideo 1/1 (frame 4/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 397.8ms\nvideo 1/1 (frame 5/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 394.9ms\nvideo 1/1 (frame 6/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 394.5ms\nvideo 1/1 (frame 7/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 364.9ms\nvideo 1/1 (frame 8/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 366.8ms\nvideo 1/1 (frame 9/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 369.1ms\nvideo 1/1 (frame 10/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 362.9ms\nvideo 1/1 (frame 11/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 364.8ms\nvideo 1/1 (frame 12/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 360.9ms\nvideo 1/1 (frame 13/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 359.1ms\nvideo 1/1 (frame 14/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 364.3ms\nvideo 1/1 (frame 15/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 358.1ms\nvideo 1/1 (frame 16/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 360.6ms\nvideo 1/1 (frame 17/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 359.8ms\nvideo 1/1 (frame 18/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 363.5ms\nvideo 1/1 (frame 19/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 368.4ms\nvideo 1/1 (frame 20/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 372.2ms\nvideo 1/1 (frame 21/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 364.5ms\nvideo 1/1 (frame 22/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 366.7ms\nvideo 1/1 (frame 23/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 367.7ms\nvideo 1/1 (frame 24/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 361.5ms\nvideo 1/1 (frame 25/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 363.3ms\nvideo 1/1 (frame 26/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 361.4ms\nvideo 1/1 (frame 27/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 367.7ms\nvideo 1/1 (frame 28/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 369.9ms\nvideo 1/1 (frame 29/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 361.7ms\nvideo 1/1 (frame 30/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 371.0ms\nvideo 1/1 (frame 31/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 365.2ms\nvideo 1/1 (frame 32/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 23 birds, 401.3ms\nvideo 1/1 (frame 33/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 22 birds, 379.3ms\nvideo 1/1 (frame 34/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 22 birds, 366.8ms\nvideo 1/1 (frame 35/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 22 birds, 364.0ms\nvideo 1/1 (frame 36/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 364.7ms\nvideo 1/1 (frame 37/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 372.0ms\nvideo 1/1 (frame 38/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 361.8ms\nvideo 1/1 (frame 39/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 372.5ms\nvideo 1/1 (frame 40/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 365.2ms\nvideo 1/1 (frame 41/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 363.0ms\nvideo 1/1 (frame 42/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 372.0ms\nvideo 1/1 (frame 43/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 367.6ms\nvideo 1/1 (frame 44/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 362.6ms\nvideo 1/1 (frame 45/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 360.6ms\nvideo 1/1 (frame 46/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 359.4ms\nvideo 1/1 (frame 47/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 364.2ms\nvideo 1/1 (frame 48/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 362.0ms\nvideo 1/1 (frame 49/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 362.8ms\nvideo 1/1 (frame 50/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 371.1ms\nvideo 1/1 (frame 51/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 361.3ms\nvideo 1/1 (frame 52/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 379.4ms\nvideo 1/1 (frame 53/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 367.8ms\nvideo 1/1 (frame 54/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 363.1ms\nvideo 1/1 (frame 55/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 455.8ms\nvideo 1/1 (frame 56/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 368.6ms\nvideo 1/1 (frame 57/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 370.1ms\nvideo 1/1 (frame 58/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 359.3ms\nvideo 1/1 (frame 59/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 366.5ms\nvideo 1/1 (frame 60/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 364.2ms\nvideo 1/1 (frame 61/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 367.8ms\nvideo 1/1 (frame 62/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 376.7ms\nvideo 1/1 (frame 63/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 366.1ms\nvideo 1/1 (frame 64/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 368.6ms\nvideo 1/1 (frame 65/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 360.9ms\nvideo 1/1 (frame 66/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 360.7ms\nvideo 1/1 (frame 67/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 387.7ms\nvideo 1/1 (frame 68/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 363.2ms\nvideo 1/1 (frame 69/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 372.1ms\nvideo 1/1 (frame 70/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 360.5ms\nvideo 1/1 (frame 71/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 365.3ms\nvideo 1/1 (frame 72/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 381.0ms\nvideo 1/1 (frame 73/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 394.2ms\nvideo 1/1 (frame 74/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 411.0ms\nvideo 1/1 (frame 75/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 360.3ms\nvideo 1/1 (frame 76/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 376.8ms\nvideo 1/1 (frame 77/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 363.0ms\nvideo 1/1 (frame 78/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 364.0ms\nvideo 1/1 (frame 79/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 372.3ms\nvideo 1/1 (frame 80/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 363.9ms\nvideo 1/1 (frame 81/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 392.7ms\nvideo 1/1 (frame 82/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 392.4ms\nvideo 1/1 (frame 83/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 373.3ms\nvideo 1/1 (frame 84/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 369.3ms\nvideo 1/1 (frame 85/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 366.0ms\nvideo 1/1 (frame 86/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 366.2ms\nvideo 1/1 (frame 87/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 368.7ms\nvideo 1/1 (frame 88/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 363.9ms\nvideo 1/1 (frame 89/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 403.9ms\nvideo 1/1 (frame 90/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 398.9ms\nvideo 1/1 (frame 91/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 407.6ms\nvideo 1/1 (frame 92/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 424.6ms\nvideo 1/1 (frame 93/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 414.0ms\nvideo 1/1 (frame 94/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 419.7ms\nvideo 1/1 (frame 95/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 375.6ms\nvideo 1/1 (frame 96/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 366.7ms\nvideo 1/1 (frame 97/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 366.6ms\nvideo 1/1 (frame 98/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 365.8ms\nvideo 1/1 (frame 99/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 377.7ms\nvideo 1/1 (frame 100/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 481.5ms\nvideo 1/1 (frame 101/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 376.2ms\nvideo 1/1 (frame 102/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 367.7ms\nvideo 1/1 (frame 103/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 369.3ms\nvideo 1/1 (frame 104/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 364.8ms\nvideo 1/1 (frame 105/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 375.8ms\nvideo 1/1 (frame 106/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 364.7ms\nvideo 1/1 (frame 107/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 396.8ms\nvideo 1/1 (frame 108/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 378.0ms\nvideo 1/1 (frame 109/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 363.4ms\nvideo 1/1 (frame 110/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 374.5ms\nvideo 1/1 (frame 111/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 381.8ms\nvideo 1/1 (frame 112/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 387.9ms\nvideo 1/1 (frame 113/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 374.1ms\nvideo 1/1 (frame 114/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 364.6ms\nvideo 1/1 (frame 115/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 376.6ms\nvideo 1/1 (frame 116/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 370.9ms\nvideo 1/1 (frame 117/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 371.3ms\nvideo 1/1 (frame 118/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 363.0ms\nvideo 1/1 (frame 119/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 382.3ms\nvideo 1/1 (frame 120/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 376.3ms\nvideo 1/1 (frame 121/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 394.9ms\nvideo 1/1 (frame 122/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 396.3ms\nvideo 1/1 (frame 123/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 381.7ms\nvideo 1/1 (frame 124/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 388.4ms\nvideo 1/1 (frame 125/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 380.3ms\nvideo 1/1 (frame 126/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 384.1ms\nvideo 1/1 (frame 127/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 388.5ms\nvideo 1/1 (frame 128/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 387.7ms\nvideo 1/1 (frame 129/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 365.3ms\nvideo 1/1 (frame 130/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 386.2ms\nvideo 1/1 (frame 131/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 382.4ms\nvideo 1/1 (frame 132/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 369.5ms\nvideo 1/1 (frame 133/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 376.6ms\nvideo 1/1 (frame 134/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 374.4ms\nvideo 1/1 (frame 135/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 458.4ms\nvideo 1/1 (frame 136/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 446.4ms\nvideo 1/1 (frame 137/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 448.7ms\nvideo 1/1 (frame 138/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 426.0ms\nvideo 1/1 (frame 139/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 403.8ms\nvideo 1/1 (frame 140/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 417.6ms\nvideo 1/1 (frame 141/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 367.8ms\nvideo 1/1 (frame 142/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 363.9ms\nvideo 1/1 (frame 143/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 464.8ms\nvideo 1/1 (frame 144/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 371.4ms\nvideo 1/1 (frame 145/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 371.1ms\nvideo 1/1 (frame 146/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 416.5ms\nvideo 1/1 (frame 147/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 432.6ms\nvideo 1/1 (frame 148/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 434.4ms\nvideo 1/1 (frame 149/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 419.1ms\nvideo 1/1 (frame 150/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 394.5ms\nvideo 1/1 (frame 151/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 409.1ms\nvideo 1/1 (frame 152/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 408.9ms\nvideo 1/1 (frame 153/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 415.6ms\nvideo 1/1 (frame 154/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 381.9ms\nvideo 1/1 (frame 155/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 382.2ms\nvideo 1/1 (frame 156/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 392.7ms\nvideo 1/1 (frame 157/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 396.2ms\nvideo 1/1 (frame 158/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 384.2ms\nvideo 1/1 (frame 159/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 392.9ms\nvideo 1/1 (frame 160/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 381.0ms\nvideo 1/1 (frame 161/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 399.0ms\nvideo 1/1 (frame 162/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 396.8ms\nvideo 1/1 (frame 163/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 390.4ms\nvideo 1/1 (frame 164/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 373.7ms\nvideo 1/1 (frame 165/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 378.9ms\nvideo 1/1 (frame 166/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 369.0ms\nvideo 1/1 (frame 167/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 368.4ms\nvideo 1/1 (frame 168/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 372.1ms\nvideo 1/1 (frame 169/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 372.0ms\nvideo 1/1 (frame 170/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 371.7ms\nvideo 1/1 (frame 171/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 381.8ms\nvideo 1/1 (frame 172/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 404.6ms\nvideo 1/1 (frame 173/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 393.8ms\nvideo 1/1 (frame 174/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 389.1ms\nvideo 1/1 (frame 175/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 374.0ms\nvideo 1/1 (frame 176/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 377.4ms\nvideo 1/1 (frame 177/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 394.1ms\nvideo 1/1 (frame 178/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 388.9ms\nvideo 1/1 (frame 179/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 419.2ms\nvideo 1/1 (frame 180/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 481.0ms\nvideo 1/1 (frame 181/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 421.9ms\nvideo 1/1 (frame 182/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 398.7ms\nvideo 1/1 (frame 183/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 403.7ms\nvideo 1/1 (frame 184/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 377.5ms\nvideo 1/1 (frame 185/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 421.0ms\nvideo 1/1 (frame 186/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 379.2ms\nvideo 1/1 (frame 187/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 422.2ms\nvideo 1/1 (frame 188/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 434.0ms\nvideo 1/1 (frame 189/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 411.7ms\nvideo 1/1 (frame 190/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 406.6ms\nvideo 1/1 (frame 191/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 382.8ms\nvideo 1/1 (frame 192/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 512.4ms\nvideo 1/1 (frame 193/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 404.3ms\nvideo 1/1 (frame 194/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 395.4ms\nvideo 1/1 (frame 195/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 365.0ms\nvideo 1/1 (frame 196/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 366.1ms\nvideo 1/1 (frame 197/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 363.7ms\nvideo 1/1 (frame 198/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 372.0ms\nvideo 1/1 (frame 199/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 372.6ms\nvideo 1/1 (frame 200/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 367.3ms\nvideo 1/1 (frame 201/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 365.0ms\nvideo 1/1 (frame 202/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 366.9ms\nvideo 1/1 (frame 203/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 366.5ms\nvideo 1/1 (frame 204/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 369.0ms\nvideo 1/1 (frame 205/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 375.5ms\nvideo 1/1 (frame 206/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 24 birds, 368.7ms\nvideo 1/1 (frame 207/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 376.1ms\nvideo 1/1 (frame 208/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 364.5ms\nvideo 1/1 (frame 209/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 376.4ms\nvideo 1/1 (frame 210/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 361.9ms\nvideo 1/1 (frame 211/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 365.6ms\nvideo 1/1 (frame 212/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 379.3ms\nvideo 1/1 (frame 213/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 372.5ms\nvideo 1/1 (frame 214/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 372.5ms\nvideo 1/1 (frame 215/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 366.0ms\nvideo 1/1 (frame 216/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 384.2ms\nvideo 1/1 (frame 217/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 393.7ms\nvideo 1/1 (frame 218/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 370.5ms\nvideo 1/1 (frame 219/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 381.4ms\nvideo 1/1 (frame 220/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 10 birds, 373.1ms\nvideo 1/1 (frame 221/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 378.3ms\nvideo 1/1 (frame 222/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 366.8ms\nvideo 1/1 (frame 223/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 366.0ms\nvideo 1/1 (frame 224/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 392.6ms\nvideo 1/1 (frame 225/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 364.3ms\nvideo 1/1 (frame 226/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 372.1ms\nvideo 1/1 (frame 227/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 380.9ms\nvideo 1/1 (frame 228/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 385.6ms\nvideo 1/1 (frame 229/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 381.8ms\nvideo 1/1 (frame 230/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 373.3ms\nvideo 1/1 (frame 231/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 381.4ms\nvideo 1/1 (frame 232/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 364.1ms\nvideo 1/1 (frame 233/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 383.9ms\nvideo 1/1 (frame 234/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 391.5ms\nvideo 1/1 (frame 235/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 378.1ms\nvideo 1/1 (frame 236/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 366.9ms\nvideo 1/1 (frame 237/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 374.7ms\nvideo 1/1 (frame 238/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 367.3ms\nvideo 1/1 (frame 239/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 369.3ms\nvideo 1/1 (frame 240/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 369.6ms\nvideo 1/1 (frame 241/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 361.8ms\nvideo 1/1 (frame 242/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 366.8ms\nvideo 1/1 (frame 243/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 371.3ms\nvideo 1/1 (frame 244/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 364.2ms\nvideo 1/1 (frame 245/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 368.6ms\nvideo 1/1 (frame 246/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 366.8ms\nvideo 1/1 (frame 247/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 370.8ms\nvideo 1/1 (frame 248/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 369.1ms\nvideo 1/1 (frame 249/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 364.5ms\nvideo 1/1 (frame 250/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 367.8ms\nvideo 1/1 (frame 251/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 364.6ms\nvideo 1/1 (frame 252/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 365.8ms\nvideo 1/1 (frame 253/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 370.4ms\nvideo 1/1 (frame 254/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 362.1ms\nvideo 1/1 (frame 255/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 359.2ms\nvideo 1/1 (frame 256/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 361.8ms\nvideo 1/1 (frame 257/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 393.6ms\nvideo 1/1 (frame 258/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 353.0ms\nvideo 1/1 (frame 259/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 362.9ms\nvideo 1/1 (frame 260/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 569.6ms\nvideo 1/1 (frame 261/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 369.5ms\nvideo 1/1 (frame 262/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 359.5ms\nvideo 1/1 (frame 263/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 364.7ms\nvideo 1/1 (frame 264/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 360.4ms\nvideo 1/1 (frame 265/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 357.6ms\nvideo 1/1 (frame 266/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 359.1ms\nvideo 1/1 (frame 267/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 364.4ms\nvideo 1/1 (frame 268/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 360.5ms\nvideo 1/1 (frame 269/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 377.6ms\nvideo 1/1 (frame 270/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 396.2ms\nvideo 1/1 (frame 271/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 401.2ms\nvideo 1/1 (frame 272/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 628.0ms\nvideo 1/1 (frame 273/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 364.6ms\nvideo 1/1 (frame 274/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 400.0ms\nvideo 1/1 (frame 275/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 443.2ms\nvideo 1/1 (frame 276/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 359.8ms\nvideo 1/1 (frame 277/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 398.3ms\nvideo 1/1 (frame 278/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 401.9ms\nvideo 1/1 (frame 279/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 367.1ms\nvideo 1/1 (frame 280/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 371.6ms\nvideo 1/1 (frame 281/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 362.4ms\nvideo 1/1 (frame 282/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 369.0ms\nvideo 1/1 (frame 283/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 379.7ms\nvideo 1/1 (frame 284/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 400.0ms\nvideo 1/1 (frame 285/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 385.4ms\nvideo 1/1 (frame 286/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 433.0ms\nvideo 1/1 (frame 287/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 371.4ms\nvideo 1/1 (frame 288/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 369.8ms\nvideo 1/1 (frame 289/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 378.9ms\nvideo 1/1 (frame 290/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 378.7ms\nvideo 1/1 (frame 291/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 378.7ms\nvideo 1/1 (frame 292/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 374.3ms\nvideo 1/1 (frame 293/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 377.9ms\nvideo 1/1 (frame 294/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 411.3ms\nvideo 1/1 (frame 295/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 411.1ms\nvideo 1/1 (frame 296/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 421.0ms\nvideo 1/1 (frame 297/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 427.3ms\nvideo 1/1 (frame 298/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 375.3ms\nvideo 1/1 (frame 299/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 380.2ms\nvideo 1/1 (frame 300/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 593.9ms\nvideo 1/1 (frame 301/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 420.6ms\nvideo 1/1 (frame 302/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 458.9ms\nvideo 1/1 (frame 303/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 392.8ms\nvideo 1/1 (frame 304/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 365.1ms\nvideo 1/1 (frame 305/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 415.0ms\nvideo 1/1 (frame 306/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 386.1ms\nvideo 1/1 (frame 307/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 419.6ms\nvideo 1/1 (frame 308/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 427.5ms\nvideo 1/1 (frame 309/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 469.1ms\nvideo 1/1 (frame 310/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 423.3ms\nvideo 1/1 (frame 311/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 406.4ms\nvideo 1/1 (frame 312/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 413.8ms\nvideo 1/1 (frame 313/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 386.6ms\nvideo 1/1 (frame 314/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 398.0ms\nvideo 1/1 (frame 315/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 398.2ms\nvideo 1/1 (frame 316/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 389.1ms\nvideo 1/1 (frame 317/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 383.2ms\nvideo 1/1 (frame 318/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 406.1ms\nvideo 1/1 (frame 319/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 436.6ms\nvideo 1/1 (frame 320/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 427.4ms\nvideo 1/1 (frame 321/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 386.2ms\nvideo 1/1 (frame 322/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 399.9ms\nvideo 1/1 (frame 323/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 367.2ms\nvideo 1/1 (frame 324/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 370.6ms\nvideo 1/1 (frame 325/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 433.8ms\nvideo 1/1 (frame 326/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 373.0ms\nvideo 1/1 (frame 327/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 383.4ms\nvideo 1/1 (frame 328/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 372.2ms\nvideo 1/1 (frame 329/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 365.9ms\nvideo 1/1 (frame 330/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 396.3ms\nvideo 1/1 (frame 331/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 391.2ms\nvideo 1/1 (frame 332/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 384.5ms\nvideo 1/1 (frame 333/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 387.4ms\nvideo 1/1 (frame 334/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 380.2ms\nvideo 1/1 (frame 335/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 372.6ms\nvideo 1/1 (frame 336/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 369.9ms\nvideo 1/1 (frame 337/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 370.7ms\nvideo 1/1 (frame 338/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 369.7ms\nvideo 1/1 (frame 339/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 380.2ms\nvideo 1/1 (frame 340/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 378.7ms\nvideo 1/1 (frame 341/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 379.7ms\nvideo 1/1 (frame 342/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 384.0ms\nvideo 1/1 (frame 343/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 371.2ms\nvideo 1/1 (frame 344/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 382.9ms\nvideo 1/1 (frame 345/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 391.3ms\nvideo 1/1 (frame 346/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 375.9ms\nvideo 1/1 (frame 347/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 386.3ms\nvideo 1/1 (frame 348/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 425.8ms\nvideo 1/1 (frame 349/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 389.9ms\nvideo 1/1 (frame 350/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 379.9ms\nvideo 1/1 (frame 351/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 368.0ms\nvideo 1/1 (frame 352/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 365.2ms\nvideo 1/1 (frame 353/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 369.9ms\nvideo 1/1 (frame 354/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 370.8ms\nvideo 1/1 (frame 355/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 379.0ms\nvideo 1/1 (frame 356/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 368.6ms\nvideo 1/1 (frame 357/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 374.8ms\nvideo 1/1 (frame 358/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 371.1ms\nvideo 1/1 (frame 359/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 362.0ms\nvideo 1/1 (frame 360/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 371.5ms\nvideo 1/1 (frame 361/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 381.2ms\nvideo 1/1 (frame 362/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 374.4ms\nvideo 1/1 (frame 363/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 375.4ms\nvideo 1/1 (frame 364/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 378.5ms\nvideo 1/1 (frame 365/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 558.2ms\nvideo 1/1 (frame 366/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 23 birds, 403.0ms\nvideo 1/1 (frame 367/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 380.0ms\nvideo 1/1 (frame 368/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 391.5ms\nvideo 1/1 (frame 369/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 367.4ms\nvideo 1/1 (frame 370/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 381.5ms\nvideo 1/1 (frame 371/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 377.8ms\nvideo 1/1 (frame 372/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 459.2ms\nvideo 1/1 (frame 373/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 364.9ms\nvideo 1/1 (frame 374/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 386.3ms\nvideo 1/1 (frame 375/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 22 birds, 392.0ms\nvideo 1/1 (frame 376/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 426.5ms\nvideo 1/1 (frame 377/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 403.1ms\nvideo 1/1 (frame 378/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 393.6ms\nvideo 1/1 (frame 379/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 385.7ms\nvideo 1/1 (frame 380/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 378.7ms\nvideo 1/1 (frame 381/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 22 birds, 371.1ms\nvideo 1/1 (frame 382/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 23 birds, 388.7ms\nvideo 1/1 (frame 383/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 385.6ms\nvideo 1/1 (frame 384/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 22 birds, 370.4ms\nvideo 1/1 (frame 385/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 379.3ms\nvideo 1/1 (frame 386/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 377.2ms\nvideo 1/1 (frame 387/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 366.0ms\nvideo 1/1 (frame 388/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 367.5ms\nvideo 1/1 (frame 389/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 372.8ms\nvideo 1/1 (frame 390/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 370.3ms\nvideo 1/1 (frame 391/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 373.8ms\nvideo 1/1 (frame 392/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 365.1ms\nvideo 1/1 (frame 393/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 366.5ms\nvideo 1/1 (frame 394/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 381.8ms\nvideo 1/1 (frame 395/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 380.3ms\nvideo 1/1 (frame 396/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 374.5ms\nvideo 1/1 (frame 397/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 367.5ms\nvideo 1/1 (frame 398/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 365.8ms\nvideo 1/1 (frame 399/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 378.8ms\nvideo 1/1 (frame 400/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 368.1ms\nvideo 1/1 (frame 401/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 365.4ms\nvideo 1/1 (frame 402/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 367.7ms\nvideo 1/1 (frame 403/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 368.8ms\nvideo 1/1 (frame 404/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 22 birds, 370.0ms\nvideo 1/1 (frame 405/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 368.8ms\nvideo 1/1 (frame 406/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 366.4ms\nvideo 1/1 (frame 407/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 23 birds, 372.6ms\nvideo 1/1 (frame 408/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 366.1ms\nvideo 1/1 (frame 409/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 373.0ms\nvideo 1/1 (frame 410/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 371.0ms\nvideo 1/1 (frame 411/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 374.3ms\nvideo 1/1 (frame 412/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 370.0ms\nvideo 1/1 (frame 413/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 366.0ms\nvideo 1/1 (frame 414/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 496.7ms\nvideo 1/1 (frame 415/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 371.4ms\nvideo 1/1 (frame 416/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 369.9ms\nvideo 1/1 (frame 417/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 376.2ms\nvideo 1/1 (frame 418/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 370.3ms\nvideo 1/1 (frame 419/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 380.7ms\nvideo 1/1 (frame 420/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 371.6ms\nvideo 1/1 (frame 421/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 364.6ms\nvideo 1/1 (frame 422/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 368.8ms\nvideo 1/1 (frame 423/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 369.0ms\nvideo 1/1 (frame 424/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 368.2ms\nvideo 1/1 (frame 425/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 371.2ms\nvideo 1/1 (frame 426/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 365.5ms\nvideo 1/1 (frame 427/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 370.7ms\nvideo 1/1 (frame 428/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 367.7ms\nvideo 1/1 (frame 429/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 365.1ms\nvideo 1/1 (frame 430/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 371.6ms\nvideo 1/1 (frame 431/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 372.0ms\nvideo 1/1 (frame 432/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 376.7ms\nvideo 1/1 (frame 433/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 366.8ms\nvideo 1/1 (frame 434/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 368.5ms\nvideo 1/1 (frame 435/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 375.0ms\nvideo 1/1 (frame 436/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 378.9ms\nvideo 1/1 (frame 437/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 375.6ms\nvideo 1/1 (frame 438/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 370.9ms\nvideo 1/1 (frame 439/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 393.4ms\nvideo 1/1 (frame 440/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 372.7ms\nvideo 1/1 (frame 441/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 368.9ms\nvideo 1/1 (frame 442/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 374.5ms\nvideo 1/1 (frame 443/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 374.4ms\nvideo 1/1 (frame 444/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 375.2ms\nvideo 1/1 (frame 445/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 375.7ms\nvideo 1/1 (frame 446/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 366.7ms\nvideo 1/1 (frame 447/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 373.8ms\nvideo 1/1 (frame 448/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 372.8ms\nvideo 1/1 (frame 449/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 371.8ms\nvideo 1/1 (frame 450/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 379.5ms\nvideo 1/1 (frame 451/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 381.2ms\nvideo 1/1 (frame 452/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 367.1ms\nvideo 1/1 (frame 453/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 371.5ms\nvideo 1/1 (frame 454/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 372.1ms\nvideo 1/1 (frame 455/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 368.9ms\nvideo 1/1 (frame 456/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 370.6ms\nvideo 1/1 (frame 457/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 373.1ms\nvideo 1/1 (frame 458/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 371.4ms\nvideo 1/1 (frame 459/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 372.8ms\nvideo 1/1 (frame 460/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 375.5ms\nvideo 1/1 (frame 461/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 374.1ms\nvideo 1/1 (frame 462/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 375.7ms\nvideo 1/1 (frame 463/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 370.8ms\nvideo 1/1 (frame 464/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 375.3ms\nvideo 1/1 (frame 465/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 374.0ms\nvideo 1/1 (frame 466/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 387.2ms\nvideo 1/1 (frame 467/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 370.2ms\nvideo 1/1 (frame 468/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 374.4ms\nvideo 1/1 (frame 469/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 373.7ms\nvideo 1/1 (frame 470/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 500.1ms\nvideo 1/1 (frame 471/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 377.9ms\nvideo 1/1 (frame 472/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 382.3ms\nvideo 1/1 (frame 473/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 372.2ms\nvideo 1/1 (frame 474/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 380.7ms\nvideo 1/1 (frame 475/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 378.0ms\nvideo 1/1 (frame 476/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 370.8ms\nvideo 1/1 (frame 477/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 379.5ms\nvideo 1/1 (frame 478/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 380.6ms\nvideo 1/1 (frame 479/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 375.5ms\nvideo 1/1 (frame 480/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 370.8ms\nvideo 1/1 (frame 481/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 372.9ms\nvideo 1/1 (frame 482/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 371.5ms\nvideo 1/1 (frame 483/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 366.6ms\nvideo 1/1 (frame 484/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 375.1ms\nvideo 1/1 (frame 485/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 373.8ms\nvideo 1/1 (frame 486/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 372.7ms\nvideo 1/1 (frame 487/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 24 birds, 367.3ms\nvideo 1/1 (frame 488/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 368.3ms\nvideo 1/1 (frame 489/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 370.3ms\nvideo 1/1 (frame 490/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 367.1ms\nvideo 1/1 (frame 491/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 372.8ms\nvideo 1/1 (frame 492/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 365.5ms\nvideo 1/1 (frame 493/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 367.9ms\nvideo 1/1 (frame 494/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 376.1ms\nvideo 1/1 (frame 495/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 374.3ms\nvideo 1/1 (frame 496/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 377.4ms\nvideo 1/1 (frame 497/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 368.0ms\nvideo 1/1 (frame 498/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 375.9ms\nvideo 1/1 (frame 499/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 366.8ms\nvideo 1/1 (frame 500/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 22 birds, 370.1ms\nvideo 1/1 (frame 501/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 22 birds, 374.0ms\nvideo 1/1 (frame 502/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 367.1ms\nvideo 1/1 (frame 503/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 385.9ms\nvideo 1/1 (frame 504/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 366.8ms\nvideo 1/1 (frame 505/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 365.7ms\nvideo 1/1 (frame 506/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 374.1ms\nvideo 1/1 (frame 507/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 380.4ms\nvideo 1/1 (frame 508/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 375.3ms\nvideo 1/1 (frame 509/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 372.3ms\nvideo 1/1 (frame 510/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 23 birds, 378.4ms\nvideo 1/1 (frame 511/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 22 birds, 377.7ms\nvideo 1/1 (frame 512/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 22 birds, 376.8ms\nvideo 1/1 (frame 513/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 513.8ms\nvideo 1/1 (frame 514/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 22 birds, 638.6ms\nvideo 1/1 (frame 515/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 414.4ms\nvideo 1/1 (frame 516/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 23 birds, 391.1ms\nvideo 1/1 (frame 517/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 22 birds, 446.5ms\nvideo 1/1 (frame 518/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 24 birds, 408.2ms\nvideo 1/1 (frame 519/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 499.1ms\nvideo 1/1 (frame 520/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 448.2ms\nvideo 1/1 (frame 521/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 24 birds, 399.7ms\nvideo 1/1 (frame 522/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 25 birds, 424.6ms\nvideo 1/1 (frame 523/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 410.8ms\nvideo 1/1 (frame 524/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 25 birds, 500.0ms\nvideo 1/1 (frame 525/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 23 birds, 435.3ms\nvideo 1/1 (frame 526/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 24 birds, 430.7ms\nvideo 1/1 (frame 527/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 27 birds, 461.1ms\nvideo 1/1 (frame 528/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 435.8ms\nvideo 1/1 (frame 529/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 393.0ms\nvideo 1/1 (frame 530/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 408.9ms\nvideo 1/1 (frame 531/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 469.2ms\nvideo 1/1 (frame 532/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 25 birds, 417.0ms\nvideo 1/1 (frame 533/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 25 birds, 408.8ms\nvideo 1/1 (frame 534/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 25 birds, 445.9ms\nvideo 1/1 (frame 535/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 405.6ms\nvideo 1/1 (frame 536/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 422.1ms\nvideo 1/1 (frame 537/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 27 birds, 444.5ms\nvideo 1/1 (frame 538/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 27 birds, 403.7ms\nvideo 1/1 (frame 539/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 27 birds, 409.5ms\nvideo 1/1 (frame 540/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 25 birds, 410.3ms\nvideo 1/1 (frame 541/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 24 birds, 387.2ms\nvideo 1/1 (frame 542/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 391.1ms\nvideo 1/1 (frame 543/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 24 birds, 390.6ms\nvideo 1/1 (frame 544/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 25 birds, 388.4ms\nvideo 1/1 (frame 545/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 23 birds, 383.4ms\nvideo 1/1 (frame 546/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 22 birds, 393.0ms\nvideo 1/1 (frame 547/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 23 birds, 419.9ms\nvideo 1/1 (frame 548/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 449.2ms\nvideo 1/1 (frame 549/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 25 birds, 422.9ms\nvideo 1/1 (frame 550/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 377.8ms\nvideo 1/1 (frame 551/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 378.5ms\nvideo 1/1 (frame 552/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 28 birds, 377.2ms\nvideo 1/1 (frame 553/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 27 birds, 393.2ms\nvideo 1/1 (frame 554/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 27 birds, 396.4ms\nvideo 1/1 (frame 555/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 448.7ms\nvideo 1/1 (frame 556/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 407.7ms\nvideo 1/1 (frame 557/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 28 birds, 403.8ms\nvideo 1/1 (frame 558/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 27 birds, 440.2ms\nvideo 1/1 (frame 559/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 27 birds, 422.7ms\nvideo 1/1 (frame 560/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 27 birds, 451.2ms\nvideo 1/1 (frame 561/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 420.5ms\nvideo 1/1 (frame 562/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 28 birds, 440.1ms\nvideo 1/1 (frame 563/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 28 birds, 462.2ms\nvideo 1/1 (frame 564/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 28 birds, 450.0ms\nvideo 1/1 (frame 565/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 24 birds, 396.3ms\nvideo 1/1 (frame 566/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 24 birds, 469.1ms\nvideo 1/1 (frame 567/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 25 birds, 562.7ms\nvideo 1/1 (frame 568/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 482.4ms\nvideo 1/1 (frame 569/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 29 birds, 399.4ms\nvideo 1/1 (frame 570/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 31 birds, 409.9ms\nvideo 1/1 (frame 571/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 30 birds, 461.8ms\nvideo 1/1 (frame 572/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 24 birds, 421.7ms\nvideo 1/1 (frame 573/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 29 birds, 449.6ms\nvideo 1/1 (frame 574/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 28 birds, 483.2ms\nvideo 1/1 (frame 575/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 30 birds, 418.1ms\nvideo 1/1 (frame 576/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 28 birds, 415.1ms\nvideo 1/1 (frame 577/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 27 birds, 468.7ms\nvideo 1/1 (frame 578/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 27 birds, 420.5ms\nvideo 1/1 (frame 579/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 27 birds, 466.5ms\nvideo 1/1 (frame 580/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 27 birds, 412.3ms\nvideo 1/1 (frame 581/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 25 birds, 394.3ms\nvideo 1/1 (frame 582/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 24 birds, 406.1ms\nvideo 1/1 (frame 583/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 396.2ms\nvideo 1/1 (frame 584/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 30 birds, 452.9ms\nvideo 1/1 (frame 585/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 31 birds, 383.5ms\nvideo 1/1 (frame 586/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 29 birds, 400.0ms\nvideo 1/1 (frame 587/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 27 birds, 384.3ms\nvideo 1/1 (frame 588/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 385.2ms\nvideo 1/1 (frame 589/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 25 birds, 384.1ms\nvideo 1/1 (frame 590/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 29 birds, 400.1ms\nvideo 1/1 (frame 591/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 28 birds, 479.9ms\nvideo 1/1 (frame 592/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 26 birds, 378.8ms\nvideo 1/1 (frame 593/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 27 birds, 367.3ms\nvideo 1/1 (frame 594/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 27 birds, 375.3ms\nvideo 1/1 (frame 595/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 25 birds, 382.1ms\nvideo 1/1 (frame 596/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 24 birds, 404.6ms\nvideo 1/1 (frame 597/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 23 birds, 389.5ms\nvideo 1/1 (frame 598/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 23 birds, 408.8ms\nvideo 1/1 (frame 599/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 447.9ms\nvideo 1/1 (frame 600/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 469.1ms\nvideo 1/1 (frame 601/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 423.6ms\nvideo 1/1 (frame 602/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 384.9ms\nvideo 1/1 (frame 603/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 369.8ms\nvideo 1/1 (frame 604/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 374.7ms\nvideo 1/1 (frame 605/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 381.2ms\nvideo 1/1 (frame 606/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 22 birds, 362.0ms\nvideo 1/1 (frame 607/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 367.2ms\nvideo 1/1 (frame 608/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 22 birds, 377.4ms\nvideo 1/1 (frame 609/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 363.9ms\nvideo 1/1 (frame 610/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 379.2ms\nvideo 1/1 (frame 611/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 370.5ms\nvideo 1/1 (frame 612/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 372.3ms\nvideo 1/1 (frame 613/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 366.1ms\nvideo 1/1 (frame 614/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 363.5ms\nvideo 1/1 (frame 615/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 373.2ms\nvideo 1/1 (frame 616/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 375.5ms\nvideo 1/1 (frame 617/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 362.5ms\nvideo 1/1 (frame 618/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 365.9ms\nvideo 1/1 (frame 619/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 392.2ms\nvideo 1/1 (frame 620/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 375.4ms\nvideo 1/1 (frame 621/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 366.4ms\nvideo 1/1 (frame 622/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 372.4ms\nvideo 1/1 (frame 623/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 361.5ms\nvideo 1/1 (frame 624/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 364.4ms\nvideo 1/1 (frame 625/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 367.3ms\nvideo 1/1 (frame 626/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 366.7ms\nvideo 1/1 (frame 627/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 391.9ms\nvideo 1/1 (frame 628/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 374.9ms\nvideo 1/1 (frame 629/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 368.2ms\nvideo 1/1 (frame 630/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 372.1ms\nvideo 1/1 (frame 631/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 365.5ms\nvideo 1/1 (frame 632/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 370.8ms\nvideo 1/1 (frame 633/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 364.9ms\nvideo 1/1 (frame 634/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 372.6ms\nvideo 1/1 (frame 635/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 371.3ms\nvideo 1/1 (frame 636/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 367.2ms\nvideo 1/1 (frame 637/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 381.2ms\nvideo 1/1 (frame 638/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 389.8ms\nvideo 1/1 (frame 639/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 387.9ms\nvideo 1/1 (frame 640/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 371.4ms\nvideo 1/1 (frame 641/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 479.1ms\nvideo 1/1 (frame 642/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 534.4ms\nvideo 1/1 (frame 643/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 387.6ms\nvideo 1/1 (frame 644/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 415.8ms\nvideo 1/1 (frame 645/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 404.5ms\nvideo 1/1 (frame 646/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 378.1ms\nvideo 1/1 (frame 647/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 374.6ms\nvideo 1/1 (frame 648/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 412.9ms\nvideo 1/1 (frame 649/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 397.2ms\nvideo 1/1 (frame 650/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 379.3ms\nvideo 1/1 (frame 651/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 369.0ms\nvideo 1/1 (frame 652/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 374.5ms\nvideo 1/1 (frame 653/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 442.4ms\nvideo 1/1 (frame 654/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 438.0ms\nvideo 1/1 (frame 655/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 447.7ms\nvideo 1/1 (frame 656/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 408.3ms\nvideo 1/1 (frame 657/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 408.3ms\nvideo 1/1 (frame 658/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 411.2ms\nvideo 1/1 (frame 659/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 411.6ms\nvideo 1/1 (frame 660/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 405.7ms\nvideo 1/1 (frame 661/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 393.6ms\nvideo 1/1 (frame 662/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 390.4ms\nvideo 1/1 (frame 663/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 523.0ms\nvideo 1/1 (frame 664/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 435.2ms\nvideo 1/1 (frame 665/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 415.9ms\nvideo 1/1 (frame 666/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 407.9ms\nvideo 1/1 (frame 667/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 413.1ms\nvideo 1/1 (frame 668/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 536.1ms\nvideo 1/1 (frame 669/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 448.8ms\nvideo 1/1 (frame 670/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 518.4ms\nvideo 1/1 (frame 671/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 487.9ms\nvideo 1/1 (frame 672/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 430.3ms\nvideo 1/1 (frame 673/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 385.8ms\nvideo 1/1 (frame 674/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 399.2ms\nvideo 1/1 (frame 675/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 430.8ms\nvideo 1/1 (frame 676/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 634.1ms\nvideo 1/1 (frame 677/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 495.5ms\nvideo 1/1 (frame 678/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 506.8ms\nvideo 1/1 (frame 679/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 471.9ms\nvideo 1/1 (frame 680/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 437.6ms\nvideo 1/1 (frame 681/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 448.5ms\nvideo 1/1 (frame 682/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 417.2ms\nvideo 1/1 (frame 683/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 410.1ms\nvideo 1/1 (frame 684/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 418.3ms\nvideo 1/1 (frame 685/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 406.6ms\nvideo 1/1 (frame 686/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 22 birds, 426.1ms\nvideo 1/1 (frame 687/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 405.8ms\nvideo 1/1 (frame 688/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 412.7ms\nvideo 1/1 (frame 689/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 385.7ms\nvideo 1/1 (frame 690/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 411.7ms\nvideo 1/1 (frame 691/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 426.8ms\nvideo 1/1 (frame 692/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 388.0ms\nvideo 1/1 (frame 693/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 431.2ms\nvideo 1/1 (frame 694/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 428.2ms\nvideo 1/1 (frame 695/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 475.2ms\nvideo 1/1 (frame 696/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 460.7ms\nvideo 1/1 (frame 697/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 424.5ms\nvideo 1/1 (frame 698/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 450.0ms\nvideo 1/1 (frame 699/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 431.3ms\nvideo 1/1 (frame 700/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 382.5ms\nvideo 1/1 (frame 701/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 409.6ms\nvideo 1/1 (frame 702/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 396.9ms\nvideo 1/1 (frame 703/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 10 birds, 406.7ms\nvideo 1/1 (frame 704/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 399.5ms\nvideo 1/1 (frame 705/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 396.4ms\nvideo 1/1 (frame 706/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 388.5ms\nvideo 1/1 (frame 707/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 409.0ms\nvideo 1/1 (frame 708/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 394.4ms\nvideo 1/1 (frame 709/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 10 birds, 399.1ms\nvideo 1/1 (frame 710/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 395.7ms\nvideo 1/1 (frame 711/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 404.9ms\nvideo 1/1 (frame 712/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 394.5ms\nvideo 1/1 (frame 713/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 394.5ms\nvideo 1/1 (frame 714/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 389.9ms\nvideo 1/1 (frame 715/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 10 birds, 375.6ms\nvideo 1/1 (frame 716/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 9 birds, 418.2ms\nvideo 1/1 (frame 717/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 398.0ms\nvideo 1/1 (frame 718/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 393.0ms\nvideo 1/1 (frame 719/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 403.5ms\nvideo 1/1 (frame 720/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 389.6ms\nvideo 1/1 (frame 721/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 409.5ms\nvideo 1/1 (frame 722/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 390.7ms\nvideo 1/1 (frame 723/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 400.2ms\nvideo 1/1 (frame 724/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 399.0ms\nvideo 1/1 (frame 725/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 381.9ms\nvideo 1/1 (frame 726/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 403.8ms\nvideo 1/1 (frame 727/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 580.8ms\nvideo 1/1 (frame 728/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 381.1ms\nvideo 1/1 (frame 729/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 471.6ms\nvideo 1/1 (frame 730/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 414.5ms\nvideo 1/1 (frame 731/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 487.6ms\nvideo 1/1 (frame 732/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 501.1ms\nvideo 1/1 (frame 733/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 395.4ms\nvideo 1/1 (frame 734/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 440.9ms\nvideo 1/1 (frame 735/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 402.5ms\nvideo 1/1 (frame 736/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 443.9ms\nvideo 1/1 (frame 737/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 413.6ms\nvideo 1/1 (frame 738/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 421.6ms\nvideo 1/1 (frame 739/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 417.5ms\nvideo 1/1 (frame 740/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 415.6ms\nvideo 1/1 (frame 741/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 368.4ms\nvideo 1/1 (frame 742/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 402.7ms\nvideo 1/1 (frame 743/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 394.6ms\nvideo 1/1 (frame 744/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 381.4ms\nvideo 1/1 (frame 745/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 444.6ms\nvideo 1/1 (frame 746/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 439.2ms\nvideo 1/1 (frame 747/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 412.8ms\nvideo 1/1 (frame 748/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 398.2ms\nvideo 1/1 (frame 749/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 409.8ms\nvideo 1/1 (frame 750/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 403.5ms\nvideo 1/1 (frame 751/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 401.1ms\nvideo 1/1 (frame 752/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 405.1ms\nvideo 1/1 (frame 753/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 397.1ms\nvideo 1/1 (frame 754/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 385.2ms\nvideo 1/1 (frame 755/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 395.0ms\nvideo 1/1 (frame 756/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 396.1ms\nvideo 1/1 (frame 757/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 402.8ms\nvideo 1/1 (frame 758/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 378.4ms\nvideo 1/1 (frame 759/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 393.1ms\nvideo 1/1 (frame 760/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 384.0ms\nvideo 1/1 (frame 761/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 428.8ms\nvideo 1/1 (frame 762/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 379.4ms\nvideo 1/1 (frame 763/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 386.4ms\nvideo 1/1 (frame 764/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 443.5ms\nvideo 1/1 (frame 765/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 419.3ms\nvideo 1/1 (frame 766/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 467.5ms\nvideo 1/1 (frame 767/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 448.1ms\nvideo 1/1 (frame 768/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 444.8ms\nvideo 1/1 (frame 769/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 438.3ms\nvideo 1/1 (frame 770/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 408.0ms\nvideo 1/1 (frame 771/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 380.5ms\nvideo 1/1 (frame 772/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 371.6ms\nvideo 1/1 (frame 773/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 385.7ms\nvideo 1/1 (frame 774/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 365.8ms\nvideo 1/1 (frame 775/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 377.8ms\nvideo 1/1 (frame 776/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 377.7ms\nvideo 1/1 (frame 777/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 376.3ms\nvideo 1/1 (frame 778/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 378.7ms\nvideo 1/1 (frame 779/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 373.5ms\nvideo 1/1 (frame 780/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 378.3ms\nvideo 1/1 (frame 781/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 372.9ms\nvideo 1/1 (frame 782/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 376.5ms\nvideo 1/1 (frame 783/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 380.1ms\nvideo 1/1 (frame 784/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 366.8ms\nvideo 1/1 (frame 785/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 368.5ms\nvideo 1/1 (frame 786/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 368.0ms\nvideo 1/1 (frame 787/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 369.7ms\nvideo 1/1 (frame 788/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 376.4ms\nvideo 1/1 (frame 789/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 367.7ms\nvideo 1/1 (frame 790/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 368.6ms\nvideo 1/1 (frame 791/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 368.8ms\nvideo 1/1 (frame 792/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 371.4ms\nvideo 1/1 (frame 793/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 382.1ms\nvideo 1/1 (frame 794/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 373.7ms\nvideo 1/1 (frame 795/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 406.3ms\nvideo 1/1 (frame 796/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 399.4ms\nvideo 1/1 (frame 797/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 399.6ms\nvideo 1/1 (frame 798/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 393.5ms\nvideo 1/1 (frame 799/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 404.7ms\nvideo 1/1 (frame 800/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 411.0ms\nvideo 1/1 (frame 801/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 374.9ms\nvideo 1/1 (frame 802/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 385.0ms\nvideo 1/1 (frame 803/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 373.8ms\nvideo 1/1 (frame 804/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 369.3ms\nvideo 1/1 (frame 805/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 369.6ms\nvideo 1/1 (frame 806/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 383.9ms\nvideo 1/1 (frame 807/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 374.5ms\nvideo 1/1 (frame 808/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 381.3ms\nvideo 1/1 (frame 809/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 373.2ms\nvideo 1/1 (frame 810/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 381.6ms\nvideo 1/1 (frame 811/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 375.8ms\nvideo 1/1 (frame 812/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 383.2ms\nvideo 1/1 (frame 813/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 378.9ms\nvideo 1/1 (frame 814/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 389.1ms\nvideo 1/1 (frame 815/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 381.9ms\nvideo 1/1 (frame 816/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 383.3ms\nvideo 1/1 (frame 817/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 387.9ms\nvideo 1/1 (frame 818/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 400.3ms\nvideo 1/1 (frame 819/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 10 birds, 391.1ms\nvideo 1/1 (frame 820/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 380.8ms\nvideo 1/1 (frame 821/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 405.6ms\nvideo 1/1 (frame 822/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 383.5ms\nvideo 1/1 (frame 823/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 389.2ms\nvideo 1/1 (frame 824/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 387.9ms\nvideo 1/1 (frame 825/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 380.4ms\nvideo 1/1 (frame 826/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 376.7ms\nvideo 1/1 (frame 827/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 422.8ms\nvideo 1/1 (frame 828/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 404.8ms\nvideo 1/1 (frame 829/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 405.1ms\nvideo 1/1 (frame 830/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 383.7ms\nvideo 1/1 (frame 831/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 399.2ms\nvideo 1/1 (frame 832/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 385.2ms\nvideo 1/1 (frame 833/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 387.5ms\nvideo 1/1 (frame 834/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 381.2ms\nvideo 1/1 (frame 835/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 387.9ms\nvideo 1/1 (frame 836/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 391.9ms\nvideo 1/1 (frame 837/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 392.2ms\nvideo 1/1 (frame 838/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 392.8ms\nvideo 1/1 (frame 839/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 399.4ms\nvideo 1/1 (frame 840/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 388.9ms\nvideo 1/1 (frame 841/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 10 birds, 394.3ms\nvideo 1/1 (frame 842/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 412.4ms\nvideo 1/1 (frame 843/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 383.4ms\nvideo 1/1 (frame 844/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 379.7ms\nvideo 1/1 (frame 845/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 384.4ms\nvideo 1/1 (frame 846/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 387.1ms\nvideo 1/1 (frame 847/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 427.8ms\nvideo 1/1 (frame 848/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 418.0ms\nvideo 1/1 (frame 849/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 400.8ms\nvideo 1/1 (frame 850/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 496.7ms\nvideo 1/1 (frame 851/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 535.6ms\nvideo 1/1 (frame 852/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 387.2ms\nvideo 1/1 (frame 853/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 401.8ms\nvideo 1/1 (frame 854/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 408.9ms\nvideo 1/1 (frame 855/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 391.5ms\nvideo 1/1 (frame 856/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 399.4ms\nvideo 1/1 (frame 857/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 459.9ms\nvideo 1/1 (frame 858/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 461.4ms\nvideo 1/1 (frame 859/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 10 birds, 552.2ms\nvideo 1/1 (frame 860/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 472.3ms\nvideo 1/1 (frame 861/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 564.3ms\nvideo 1/1 (frame 862/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 416.3ms\nvideo 1/1 (frame 863/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 387.3ms\nvideo 1/1 (frame 864/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 403.6ms\nvideo 1/1 (frame 865/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 416.0ms\nvideo 1/1 (frame 866/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 401.2ms\nvideo 1/1 (frame 867/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 394.9ms\nvideo 1/1 (frame 868/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 409.5ms\nvideo 1/1 (frame 869/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 406.7ms\nvideo 1/1 (frame 870/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 418.5ms\nvideo 1/1 (frame 871/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 396.3ms\nvideo 1/1 (frame 872/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 398.3ms\nvideo 1/1 (frame 873/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 396.4ms\nvideo 1/1 (frame 874/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 385.3ms\nvideo 1/1 (frame 875/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 391.0ms\nvideo 1/1 (frame 876/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 397.6ms\nvideo 1/1 (frame 877/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 389.4ms\nvideo 1/1 (frame 878/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 598.8ms\nvideo 1/1 (frame 879/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 393.1ms\nvideo 1/1 (frame 880/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 389.4ms\nvideo 1/1 (frame 881/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 383.7ms\nvideo 1/1 (frame 882/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 383.6ms\nvideo 1/1 (frame 883/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 452.9ms\nvideo 1/1 (frame 884/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 395.6ms\nvideo 1/1 (frame 885/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 407.7ms\nvideo 1/1 (frame 886/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 405.8ms\nvideo 1/1 (frame 887/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 442.9ms\nvideo 1/1 (frame 888/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 468.6ms\nvideo 1/1 (frame 889/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 481.2ms\nvideo 1/1 (frame 890/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 446.4ms\nvideo 1/1 (frame 891/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 424.4ms\nvideo 1/1 (frame 892/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 397.6ms\nvideo 1/1 (frame 893/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 398.3ms\nvideo 1/1 (frame 894/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 373.9ms\nvideo 1/1 (frame 895/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 432.0ms\nvideo 1/1 (frame 896/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 449.4ms\nvideo 1/1 (frame 897/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 398.0ms\nvideo 1/1 (frame 898/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 403.9ms\nvideo 1/1 (frame 899/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 379.1ms\nvideo 1/1 (frame 900/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 371.1ms\nvideo 1/1 (frame 901/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 438.0ms\nvideo 1/1 (frame 902/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 405.2ms\nvideo 1/1 (frame 903/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 392.0ms\nvideo 1/1 (frame 904/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 396.0ms\nvideo 1/1 (frame 905/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 386.2ms\nvideo 1/1 (frame 906/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 539.3ms\nvideo 1/1 (frame 907/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 398.8ms\nvideo 1/1 (frame 908/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 390.8ms\nvideo 1/1 (frame 909/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 380.8ms\nvideo 1/1 (frame 910/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 369.1ms\nvideo 1/1 (frame 911/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 373.4ms\nvideo 1/1 (frame 912/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 379.1ms\nvideo 1/1 (frame 913/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 394.8ms\nvideo 1/1 (frame 914/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 378.3ms\nvideo 1/1 (frame 915/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 379.7ms\nvideo 1/1 (frame 916/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 393.2ms\nvideo 1/1 (frame 917/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 388.5ms\nvideo 1/1 (frame 918/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 397.1ms\nvideo 1/1 (frame 919/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 396.2ms\nvideo 1/1 (frame 920/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 400.4ms\nvideo 1/1 (frame 921/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 400.4ms\nvideo 1/1 (frame 922/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 395.5ms\nvideo 1/1 (frame 923/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 387.5ms\nvideo 1/1 (frame 924/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 392.7ms\nvideo 1/1 (frame 925/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 389.7ms\nvideo 1/1 (frame 926/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 411.1ms\nvideo 1/1 (frame 927/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 378.5ms\nvideo 1/1 (frame 928/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 380.0ms\nvideo 1/1 (frame 929/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 388.2ms\nvideo 1/1 (frame 930/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 392.4ms\nvideo 1/1 (frame 931/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 384.0ms\nvideo 1/1 (frame 932/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 395.7ms\nvideo 1/1 (frame 933/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 383.4ms\nvideo 1/1 (frame 934/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 387.0ms\nvideo 1/1 (frame 935/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 391.5ms\nvideo 1/1 (frame 936/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 384.6ms\nvideo 1/1 (frame 937/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 383.3ms\nvideo 1/1 (frame 938/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 405.4ms\nvideo 1/1 (frame 939/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 391.2ms\nvideo 1/1 (frame 940/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 385.1ms\nvideo 1/1 (frame 941/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 392.0ms\nvideo 1/1 (frame 942/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 397.4ms\nvideo 1/1 (frame 943/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 398.0ms\nvideo 1/1 (frame 944/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 388.2ms\nvideo 1/1 (frame 945/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 395.6ms\nvideo 1/1 (frame 946/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 392.0ms\nvideo 1/1 (frame 947/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 392.2ms\nvideo 1/1 (frame 948/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 388.9ms\nvideo 1/1 (frame 949/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 407.4ms\nvideo 1/1 (frame 950/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 373.5ms\nvideo 1/1 (frame 951/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 386.8ms\nvideo 1/1 (frame 952/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 388.1ms\nvideo 1/1 (frame 953/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 385.5ms\nvideo 1/1 (frame 954/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 382.9ms\nvideo 1/1 (frame 955/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 382.4ms\nvideo 1/1 (frame 956/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 376.7ms\nvideo 1/1 (frame 957/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 382.0ms\nvideo 1/1 (frame 958/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 373.1ms\nvideo 1/1 (frame 959/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 377.9ms\nvideo 1/1 (frame 960/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 410.8ms\nvideo 1/1 (frame 961/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 390.2ms\nvideo 1/1 (frame 962/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 399.2ms\nvideo 1/1 (frame 963/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 386.6ms\nvideo 1/1 (frame 964/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 373.8ms\nvideo 1/1 (frame 965/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 389.9ms\nvideo 1/1 (frame 966/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 392.1ms\nvideo 1/1 (frame 967/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 388.9ms\nvideo 1/1 (frame 968/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 391.5ms\nvideo 1/1 (frame 969/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 383.2ms\nvideo 1/1 (frame 970/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 386.9ms\nvideo 1/1 (frame 971/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 415.0ms\nvideo 1/1 (frame 972/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 392.7ms\nvideo 1/1 (frame 973/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 380.5ms\nvideo 1/1 (frame 974/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 383.9ms\nvideo 1/1 (frame 975/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 375.1ms\nvideo 1/1 (frame 976/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 376.0ms\nvideo 1/1 (frame 977/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 397.3ms\nvideo 1/1 (frame 978/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 389.4ms\nvideo 1/1 (frame 979/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 374.2ms\nvideo 1/1 (frame 980/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 388.7ms\nvideo 1/1 (frame 981/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 391.0ms\nvideo 1/1 (frame 982/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 396.0ms\nvideo 1/1 (frame 983/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 395.6ms\nvideo 1/1 (frame 984/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 387.6ms\nvideo 1/1 (frame 985/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 395.4ms\nvideo 1/1 (frame 986/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 383.8ms\nvideo 1/1 (frame 987/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 390.4ms\nvideo 1/1 (frame 988/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 9 birds, 376.0ms\nvideo 1/1 (frame 989/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 388.6ms\nvideo 1/1 (frame 990/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 375.1ms\nvideo 1/1 (frame 991/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 383.1ms\nvideo 1/1 (frame 992/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 394.9ms\nvideo 1/1 (frame 993/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 388.7ms\nvideo 1/1 (frame 994/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 482.6ms\nvideo 1/1 (frame 995/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 458.3ms\nvideo 1/1 (frame 996/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 436.6ms\nvideo 1/1 (frame 997/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 380.4ms\nvideo 1/1 (frame 998/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 425.6ms\nvideo 1/1 (frame 999/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 378.6ms\nvideo 1/1 (frame 1000/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 381.9ms\nvideo 1/1 (frame 1001/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 400.6ms\nvideo 1/1 (frame 1002/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 556.5ms\nvideo 1/1 (frame 1003/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 414.3ms\nvideo 1/1 (frame 1004/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 9 birds, 389.1ms\nvideo 1/1 (frame 1005/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 428.9ms\nvideo 1/1 (frame 1006/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 455.6ms\nvideo 1/1 (frame 1007/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 452.6ms\nvideo 1/1 (frame 1008/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 445.9ms\nvideo 1/1 (frame 1009/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 390.6ms\nvideo 1/1 (frame 1010/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 380.9ms\nvideo 1/1 (frame 1011/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 387.9ms\nvideo 1/1 (frame 1012/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 9 birds, 382.5ms\nvideo 1/1 (frame 1013/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 378.4ms\nvideo 1/1 (frame 1014/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 10 birds, 383.2ms\nvideo 1/1 (frame 1015/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 10 birds, 389.5ms\nvideo 1/1 (frame 1016/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 10 birds, 393.3ms\nvideo 1/1 (frame 1017/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 381.9ms\nvideo 1/1 (frame 1018/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 388.8ms\nvideo 1/1 (frame 1019/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 438.7ms\nvideo 1/1 (frame 1020/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 431.1ms\nvideo 1/1 (frame 1021/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 384.3ms\nvideo 1/1 (frame 1022/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 393.1ms\nvideo 1/1 (frame 1023/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 442.9ms\nvideo 1/1 (frame 1024/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 389.4ms\nvideo 1/1 (frame 1025/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 412.9ms\nvideo 1/1 (frame 1026/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 457.2ms\nvideo 1/1 (frame 1027/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 384.6ms\nvideo 1/1 (frame 1028/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 406.0ms\nvideo 1/1 (frame 1029/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 9 birds, 412.4ms\nvideo 1/1 (frame 1030/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 384.5ms\nvideo 1/1 (frame 1031/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 9 birds, 399.7ms\nvideo 1/1 (frame 1032/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 8 birds, 395.8ms\nvideo 1/1 (frame 1033/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 412.1ms\nvideo 1/1 (frame 1034/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 399.6ms\nvideo 1/1 (frame 1035/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 422.1ms\nvideo 1/1 (frame 1036/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 414.8ms\nvideo 1/1 (frame 1037/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 399.3ms\nvideo 1/1 (frame 1038/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 389.8ms\nvideo 1/1 (frame 1039/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 380.5ms\nvideo 1/1 (frame 1040/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 393.9ms\nvideo 1/1 (frame 1041/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 386.0ms\nvideo 1/1 (frame 1042/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 378.9ms\nvideo 1/1 (frame 1043/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 390.7ms\nvideo 1/1 (frame 1044/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 392.0ms\nvideo 1/1 (frame 1045/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 400.3ms\nvideo 1/1 (frame 1046/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 383.7ms\nvideo 1/1 (frame 1047/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 392.3ms\nvideo 1/1 (frame 1048/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 399.1ms\nvideo 1/1 (frame 1049/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 390.9ms\nvideo 1/1 (frame 1050/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 384.7ms\nvideo 1/1 (frame 1051/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 387.3ms\nvideo 1/1 (frame 1052/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 386.6ms\nvideo 1/1 (frame 1053/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 408.4ms\nvideo 1/1 (frame 1054/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 384.7ms\nvideo 1/1 (frame 1055/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 456.5ms\nvideo 1/1 (frame 1056/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 380.6ms\nvideo 1/1 (frame 1057/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 392.2ms\nvideo 1/1 (frame 1058/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 394.2ms\nvideo 1/1 (frame 1059/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 388.5ms\nvideo 1/1 (frame 1060/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 388.6ms\nvideo 1/1 (frame 1061/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 397.7ms\nvideo 1/1 (frame 1062/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 383.5ms\nvideo 1/1 (frame 1063/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 391.5ms\nvideo 1/1 (frame 1064/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 393.1ms\nvideo 1/1 (frame 1065/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 397.2ms\nvideo 1/1 (frame 1066/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 415.4ms\nvideo 1/1 (frame 1067/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 401.8ms\nvideo 1/1 (frame 1068/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 398.3ms\nvideo 1/1 (frame 1069/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 391.4ms\nvideo 1/1 (frame 1070/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 392.2ms\nvideo 1/1 (frame 1071/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 390.6ms\nvideo 1/1 (frame 1072/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 389.8ms\nvideo 1/1 (frame 1073/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 391.2ms\nvideo 1/1 (frame 1074/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 388.3ms\nvideo 1/1 (frame 1075/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 398.4ms\nvideo 1/1 (frame 1076/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 384.8ms\nvideo 1/1 (frame 1077/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 397.7ms\nvideo 1/1 (frame 1078/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 398.1ms\nvideo 1/1 (frame 1079/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 393.6ms\nvideo 1/1 (frame 1080/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 383.2ms\nvideo 1/1 (frame 1081/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 411.7ms\nvideo 1/1 (frame 1082/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 409.1ms\nvideo 1/1 (frame 1083/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 405.6ms\nvideo 1/1 (frame 1084/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 396.4ms\nvideo 1/1 (frame 1085/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 406.5ms\nvideo 1/1 (frame 1086/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 407.9ms\nvideo 1/1 (frame 1087/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 405.7ms\nvideo 1/1 (frame 1088/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 421.8ms\nvideo 1/1 (frame 1089/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 395.4ms\nvideo 1/1 (frame 1090/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 399.5ms\nvideo 1/1 (frame 1091/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 392.6ms\nvideo 1/1 (frame 1092/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 398.8ms\nvideo 1/1 (frame 1093/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 492.2ms\nvideo 1/1 (frame 1094/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 399.6ms\nvideo 1/1 (frame 1095/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 393.0ms\nvideo 1/1 (frame 1096/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 387.6ms\nvideo 1/1 (frame 1097/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 398.4ms\nvideo 1/1 (frame 1098/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 411.5ms\nvideo 1/1 (frame 1099/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 385.0ms\nvideo 1/1 (frame 1100/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 395.3ms\nvideo 1/1 (frame 1101/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 9 birds, 391.5ms\nvideo 1/1 (frame 1102/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 397.3ms\nvideo 1/1 (frame 1103/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 397.9ms\nvideo 1/1 (frame 1104/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 388.4ms\nvideo 1/1 (frame 1105/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 387.7ms\nvideo 1/1 (frame 1106/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 395.0ms\nvideo 1/1 (frame 1107/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 391.5ms\nvideo 1/1 (frame 1108/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 399.4ms\nvideo 1/1 (frame 1109/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 386.6ms\nvideo 1/1 (frame 1110/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 397.1ms\nvideo 1/1 (frame 1111/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 385.7ms\nvideo 1/1 (frame 1112/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 393.9ms\nvideo 1/1 (frame 1113/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 388.8ms\nvideo 1/1 (frame 1114/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 400.5ms\nvideo 1/1 (frame 1115/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 400.4ms\nvideo 1/1 (frame 1116/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 397.8ms\nvideo 1/1 (frame 1117/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 386.3ms\nvideo 1/1 (frame 1118/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 406.6ms\nvideo 1/1 (frame 1119/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 389.8ms\nvideo 1/1 (frame 1120/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 397.9ms\nvideo 1/1 (frame 1121/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 394.0ms\nvideo 1/1 (frame 1122/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 394.1ms\nvideo 1/1 (frame 1123/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 388.5ms\nvideo 1/1 (frame 1124/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 388.6ms\nvideo 1/1 (frame 1125/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 389.2ms\nvideo 1/1 (frame 1126/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 392.1ms\nvideo 1/1 (frame 1127/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 391.0ms\nvideo 1/1 (frame 1128/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 401.8ms\nvideo 1/1 (frame 1129/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 388.2ms\nvideo 1/1 (frame 1130/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 399.2ms\nvideo 1/1 (frame 1131/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 430.3ms\nvideo 1/1 (frame 1132/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 426.0ms\nvideo 1/1 (frame 1133/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 413.5ms\nvideo 1/1 (frame 1134/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 400.5ms\nvideo 1/1 (frame 1135/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 404.8ms\nvideo 1/1 (frame 1136/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 395.0ms\nvideo 1/1 (frame 1137/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 395.6ms\nvideo 1/1 (frame 1138/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 405.8ms\nvideo 1/1 (frame 1139/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 399.0ms\nvideo 1/1 (frame 1140/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 398.4ms\nvideo 1/1 (frame 1141/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 397.9ms\nvideo 1/1 (frame 1142/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 399.5ms\nvideo 1/1 (frame 1143/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 399.3ms\nvideo 1/1 (frame 1144/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 390.0ms\nvideo 1/1 (frame 1145/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 398.9ms\nvideo 1/1 (frame 1146/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 393.0ms\nvideo 1/1 (frame 1147/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 395.6ms\nvideo 1/1 (frame 1148/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 394.3ms\nvideo 1/1 (frame 1149/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 413.6ms\nvideo 1/1 (frame 1150/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 398.4ms\nvideo 1/1 (frame 1151/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 396.5ms\nvideo 1/1 (frame 1152/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 391.5ms\nvideo 1/1 (frame 1153/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 397.3ms\nvideo 1/1 (frame 1154/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 392.5ms\nvideo 1/1 (frame 1155/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 400.2ms\nvideo 1/1 (frame 1156/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 391.2ms\nvideo 1/1 (frame 1157/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 523.5ms\nvideo 1/1 (frame 1158/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 457.0ms\nvideo 1/1 (frame 1159/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 439.5ms\nvideo 1/1 (frame 1160/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 429.3ms\nvideo 1/1 (frame 1161/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 450.3ms\nvideo 1/1 (frame 1162/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 439.9ms\nvideo 1/1 (frame 1163/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 448.1ms\nvideo 1/1 (frame 1164/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 400.6ms\nvideo 1/1 (frame 1165/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 404.9ms\nvideo 1/1 (frame 1166/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 427.5ms\nvideo 1/1 (frame 1167/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 477.3ms\nvideo 1/1 (frame 1168/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 418.3ms\nvideo 1/1 (frame 1169/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 431.6ms\nvideo 1/1 (frame 1170/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 418.7ms\nvideo 1/1 (frame 1171/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 407.1ms\nvideo 1/1 (frame 1172/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 424.7ms\nvideo 1/1 (frame 1173/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 440.5ms\nvideo 1/1 (frame 1174/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 412.5ms\nvideo 1/1 (frame 1175/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 511.1ms\nvideo 1/1 (frame 1176/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 536.1ms\nvideo 1/1 (frame 1177/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 428.1ms\nvideo 1/1 (frame 1178/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 437.5ms\nvideo 1/1 (frame 1179/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 685.7ms\nvideo 1/1 (frame 1180/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 448.3ms\nvideo 1/1 (frame 1181/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 498.1ms\nvideo 1/1 (frame 1182/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 403.3ms\nvideo 1/1 (frame 1183/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 393.8ms\nvideo 1/1 (frame 1184/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 457.2ms\nvideo 1/1 (frame 1185/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 434.9ms\nvideo 1/1 (frame 1186/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 431.9ms\nvideo 1/1 (frame 1187/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 480.0ms\nvideo 1/1 (frame 1188/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 415.9ms\nvideo 1/1 (frame 1189/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 395.1ms\nvideo 1/1 (frame 1190/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 406.5ms\nvideo 1/1 (frame 1191/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 397.9ms\nvideo 1/1 (frame 1192/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 417.4ms\nvideo 1/1 (frame 1193/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 424.6ms\nvideo 1/1 (frame 1194/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 399.3ms\nvideo 1/1 (frame 1195/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 410.4ms\nvideo 1/1 (frame 1196/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 396.8ms\nvideo 1/1 (frame 1197/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 420.1ms\nvideo 1/1 (frame 1198/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 395.8ms\nvideo 1/1 (frame 1199/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 403.2ms\nvideo 1/1 (frame 1200/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 396.5ms\nvideo 1/1 (frame 1201/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 9 birds, 435.1ms\nvideo 1/1 (frame 1202/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 390.2ms\nvideo 1/1 (frame 1203/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 396.8ms\nvideo 1/1 (frame 1204/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 421.4ms\nvideo 1/1 (frame 1205/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 394.5ms\nvideo 1/1 (frame 1206/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 407.6ms\nvideo 1/1 (frame 1207/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 19 birds, 393.5ms\nvideo 1/1 (frame 1208/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 398.8ms\nvideo 1/1 (frame 1209/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 20 birds, 391.8ms\nvideo 1/1 (frame 1210/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 395.5ms\nvideo 1/1 (frame 1211/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 391.7ms\nvideo 1/1 (frame 1212/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 398.6ms\nvideo 1/1 (frame 1213/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 401.5ms\nvideo 1/1 (frame 1214/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 403.9ms\nvideo 1/1 (frame 1215/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 395.7ms\nvideo 1/1 (frame 1216/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 399.2ms\nvideo 1/1 (frame 1217/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 391.0ms\nvideo 1/1 (frame 1218/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 400.3ms\nvideo 1/1 (frame 1219/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 393.3ms\nvideo 1/1 (frame 1220/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 394.8ms\nvideo 1/1 (frame 1221/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 395.6ms\nvideo 1/1 (frame 1222/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 411.3ms\nvideo 1/1 (frame 1223/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 400.2ms\nvideo 1/1 (frame 1224/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 408.4ms\nvideo 1/1 (frame 1225/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 400.6ms\nvideo 1/1 (frame 1226/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 402.3ms\nvideo 1/1 (frame 1227/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 397.0ms\nvideo 1/1 (frame 1228/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 414.7ms\nvideo 1/1 (frame 1229/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 394.5ms\nvideo 1/1 (frame 1230/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 10 birds, 406.2ms\nvideo 1/1 (frame 1231/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 404.8ms\nvideo 1/1 (frame 1232/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 405.8ms\nvideo 1/1 (frame 1233/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 406.4ms\nvideo 1/1 (frame 1234/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 399.0ms\nvideo 1/1 (frame 1235/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 412.0ms\nvideo 1/1 (frame 1236/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 390.0ms\nvideo 1/1 (frame 1237/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 394.5ms\nvideo 1/1 (frame 1238/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 391.7ms\nvideo 1/1 (frame 1239/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 393.0ms\nvideo 1/1 (frame 1240/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 393.0ms\nvideo 1/1 (frame 1241/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 398.6ms\nvideo 1/1 (frame 1242/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 388.6ms\nvideo 1/1 (frame 1243/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 394.0ms\nvideo 1/1 (frame 1244/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 392.5ms\nvideo 1/1 (frame 1245/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 392.6ms\nvideo 1/1 (frame 1246/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 395.2ms\nvideo 1/1 (frame 1247/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 10 birds, 396.6ms\nvideo 1/1 (frame 1248/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 396.3ms\nvideo 1/1 (frame 1249/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 402.3ms\nvideo 1/1 (frame 1250/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 393.5ms\nvideo 1/1 (frame 1251/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 393.8ms\nvideo 1/1 (frame 1252/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 390.0ms\nvideo 1/1 (frame 1253/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 386.3ms\nvideo 1/1 (frame 1254/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 392.7ms\nvideo 1/1 (frame 1255/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 385.7ms\nvideo 1/1 (frame 1256/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 388.2ms\nvideo 1/1 (frame 1257/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 10 birds, 388.6ms\nvideo 1/1 (frame 1258/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 399.0ms\nvideo 1/1 (frame 1259/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 381.5ms\nvideo 1/1 (frame 1260/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 412.8ms\nvideo 1/1 (frame 1261/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 516.4ms\nvideo 1/1 (frame 1262/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 392.5ms\nvideo 1/1 (frame 1263/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 387.9ms\nvideo 1/1 (frame 1264/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 387.9ms\nvideo 1/1 (frame 1265/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 409.4ms\nvideo 1/1 (frame 1266/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 389.7ms\nvideo 1/1 (frame 1267/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 400.4ms\nvideo 1/1 (frame 1268/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 21 birds, 385.3ms\nvideo 1/1 (frame 1269/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 391.0ms\nvideo 1/1 (frame 1270/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 385.9ms\nvideo 1/1 (frame 1271/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 382.5ms\nvideo 1/1 (frame 1272/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 387.0ms\nvideo 1/1 (frame 1273/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 396.3ms\nvideo 1/1 (frame 1274/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 387.6ms\nvideo 1/1 (frame 1275/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 395.2ms\nvideo 1/1 (frame 1276/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 397.0ms\nvideo 1/1 (frame 1277/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 394.2ms\nvideo 1/1 (frame 1278/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 387.3ms\nvideo 1/1 (frame 1279/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 390.5ms\nvideo 1/1 (frame 1280/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 390.3ms\nvideo 1/1 (frame 1281/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 455.8ms\nvideo 1/1 (frame 1282/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 412.4ms\nvideo 1/1 (frame 1283/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 14 birds, 403.6ms\nvideo 1/1 (frame 1284/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 11 birds, 387.9ms\nvideo 1/1 (frame 1285/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 399.4ms\nvideo 1/1 (frame 1286/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 391.9ms\nvideo 1/1 (frame 1287/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 398.8ms\nvideo 1/1 (frame 1288/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 12 birds, 388.7ms\nvideo 1/1 (frame 1289/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 394.6ms\nvideo 1/1 (frame 1290/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 16 birds, 393.4ms\nvideo 1/1 (frame 1291/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 397.5ms\nvideo 1/1 (frame 1292/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 18 birds, 389.5ms\nvideo 1/1 (frame 1293/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 13 birds, 393.2ms\nvideo 1/1 (frame 1294/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 17 birds, 399.4ms\nvideo 1/1 (frame 1295/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 526.4ms\nvideo 1/1 (frame 1296/1296) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/pombos.mp4: 384x640 15 birds, 603.0ms\nSpeed: 1.6ms preprocess, 397.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\nResults saved to runs/detect/predict3\n</pre> <pre>\nO Kernel deu pane ao executar o c\u00f3digo na c\u00e9lula atual ou em uma c\u00e9lula anterior. \n\nAnalise o c\u00f3digo nas c\u00e9lulas para identificar uma poss\u00edvel causa da pane. \n\nClique &lt;a href='https://aka.ms/vscodeJupyterKernelCrash'&gt;aqui&lt;/a&gt; para obter mais informa\u00e7\u00f5es. \n\nConsulte Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; para obter mais detalhes.</pre> In\u00a0[12]: Copied! <pre># C\u00f3digo para executar em ambiente local (n\u00e3o funciona no notebook remoto)\n\n# Inicializar a c\u00e2mera\ncap = cv2.VideoCapture(1)  # 0 para a webcam padr\u00e3o\n# cap = cv2.VideoCapture(\"race_car.mp4\")  # Para usar um v\u00eddeo espec\u00edfico\nmodel = YOLO('model/yolov8x.pt')  # Carregar o modelo YOLOv8n\n# Verificar se a c\u00e2mera foi aberta corretamente\nif not cap.isOpened():\n    print(\"Erro ao abrir a c\u00e2mera\")\nelse:\n    while True:\n        # Capturar frame\n        ret, frame = cap.read()\n        \n        if not ret:\n            print(\"Erro ao capturar o frame\")\n            break\n            \n        # Realizar detec\u00e7\u00e3o\n        results = model(frame)\n        # results = model(frame, conf=0.5)\n        \n        # Desenhar as detec\u00e7\u00f5es\n        annotated_frame = results[0].plot()\n        \n        # Exibir a imagem\n        cv2.imshow('YOLOv8 Detection', annotated_frame)\n        \n        # Sair se pressionar 'q'\n        if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n            break\n            \n    # Liberar recursos\n    cap.release()\n    cv2.destroyAllWindows()\n</pre> # C\u00f3digo para executar em ambiente local (n\u00e3o funciona no notebook remoto)  # Inicializar a c\u00e2mera cap = cv2.VideoCapture(1)  # 0 para a webcam padr\u00e3o # cap = cv2.VideoCapture(\"race_car.mp4\")  # Para usar um v\u00eddeo espec\u00edfico model = YOLO('model/yolov8x.pt')  # Carregar o modelo YOLOv8n # Verificar se a c\u00e2mera foi aberta corretamente if not cap.isOpened():     print(\"Erro ao abrir a c\u00e2mera\") else:     while True:         # Capturar frame         ret, frame = cap.read()                  if not ret:             print(\"Erro ao capturar o frame\")             break                      # Realizar detec\u00e7\u00e3o         results = model(frame)         # results = model(frame, conf=0.5)                  # Desenhar as detec\u00e7\u00f5es         annotated_frame = results[0].plot()                  # Exibir a imagem         cv2.imshow('YOLOv8 Detection', annotated_frame)                  # Sair se pressionar 'q'         if cv2.waitKey(1) &amp; 0xFF == ord('q'):             break                  # Liberar recursos     cap.release()     cv2.destroyAllWindows()  <pre>Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8x.pt to 'model/yolov8x.pt'...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 131M/131M [00:12&lt;00:00, 10.6MB/s] \n</pre> <pre>\n0: 384x640 1 person, 409.1ms\nSpeed: 2.2ms preprocess, 409.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 391.8ms\nSpeed: 1.5ms preprocess, 391.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 369.9ms\nSpeed: 1.6ms preprocess, 369.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 458.2ms\nSpeed: 5.6ms preprocess, 458.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 373.0ms\nSpeed: 1.8ms preprocess, 373.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 374.9ms\nSpeed: 1.7ms preprocess, 374.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 371.4ms\nSpeed: 1.5ms preprocess, 371.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 368.6ms\nSpeed: 1.5ms preprocess, 368.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 366.0ms\nSpeed: 1.5ms preprocess, 366.0ms inference, 10.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 367.1ms\nSpeed: 1.4ms preprocess, 367.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 372.0ms\nSpeed: 1.5ms preprocess, 372.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 371.2ms\nSpeed: 1.3ms preprocess, 371.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 424.8ms\nSpeed: 1.3ms preprocess, 424.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 384.6ms\nSpeed: 1.3ms preprocess, 384.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 375.2ms\nSpeed: 1.9ms preprocess, 375.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 458.7ms\nSpeed: 1.4ms preprocess, 458.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 411.0ms\nSpeed: 1.4ms preprocess, 411.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 403.0ms\nSpeed: 1.8ms preprocess, 403.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 397.2ms\nSpeed: 1.4ms preprocess, 397.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 374.9ms\nSpeed: 1.8ms preprocess, 374.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 362.8ms\nSpeed: 1.4ms preprocess, 362.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 359.6ms\nSpeed: 1.4ms preprocess, 359.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 373.9ms\nSpeed: 1.6ms preprocess, 373.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 369.4ms\nSpeed: 1.4ms preprocess, 369.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 371.2ms\nSpeed: 1.3ms preprocess, 371.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 392.5ms\nSpeed: 1.3ms preprocess, 392.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 toothbrush, 362.3ms\nSpeed: 1.4ms preprocess, 362.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 378.6ms\nSpeed: 1.3ms preprocess, 378.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 372.9ms\nSpeed: 1.4ms preprocess, 372.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 373.4ms\nSpeed: 1.6ms preprocess, 373.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 368.7ms\nSpeed: 1.6ms preprocess, 368.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 366.9ms\nSpeed: 1.5ms preprocess, 366.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 368.4ms\nSpeed: 1.4ms preprocess, 368.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 378.6ms\nSpeed: 1.6ms preprocess, 378.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 358.0ms\nSpeed: 1.7ms preprocess, 358.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 517.6ms\nSpeed: 1.7ms preprocess, 517.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 toothbrush, 373.9ms\nSpeed: 1.3ms preprocess, 373.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 2 toothbrushs, 364.7ms\nSpeed: 1.4ms preprocess, 364.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 368.4ms\nSpeed: 1.6ms preprocess, 368.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 369.3ms\nSpeed: 1.3ms preprocess, 369.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 remote, 1 toothbrush, 381.5ms\nSpeed: 1.6ms preprocess, 381.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 369.7ms\nSpeed: 1.6ms preprocess, 369.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 368.6ms\nSpeed: 1.9ms preprocess, 368.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 363.1ms\nSpeed: 1.7ms preprocess, 363.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 358.3ms\nSpeed: 2.0ms preprocess, 358.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 374.9ms\nSpeed: 4.3ms preprocess, 374.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 369.1ms\nSpeed: 1.5ms preprocess, 369.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 371.1ms\nSpeed: 1.3ms preprocess, 371.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 356.9ms\nSpeed: 1.3ms preprocess, 356.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 cell phone, 375.9ms\nSpeed: 1.3ms preprocess, 375.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 bottle, 366.4ms\nSpeed: 1.8ms preprocess, 366.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 bottle, 392.1ms\nSpeed: 3.1ms preprocess, 392.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 bottle, 368.5ms\nSpeed: 1.3ms preprocess, 368.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 bottle, 363.2ms\nSpeed: 1.4ms preprocess, 363.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 bottle, 376.6ms\nSpeed: 1.3ms preprocess, 376.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 bottle, 371.9ms\nSpeed: 1.7ms preprocess, 371.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 bottle, 371.2ms\nSpeed: 1.9ms preprocess, 371.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 bottle, 362.4ms\nSpeed: 2.0ms preprocess, 362.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 bottle, 362.8ms\nSpeed: 1.9ms preprocess, 362.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 bottle, 363.3ms\nSpeed: 1.7ms preprocess, 363.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 bottle, 365.8ms\nSpeed: 1.7ms preprocess, 365.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 bottle, 366.3ms\nSpeed: 1.7ms preprocess, 366.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 bottle, 369.4ms\nSpeed: 1.7ms preprocess, 369.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 bottle, 363.5ms\nSpeed: 1.7ms preprocess, 363.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 bottle, 1 cell phone, 375.6ms\nSpeed: 1.7ms preprocess, 375.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 cell phone, 368.4ms\nSpeed: 1.6ms preprocess, 368.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 cell phone, 403.6ms\nSpeed: 1.8ms preprocess, 403.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 cell phone, 385.0ms\nSpeed: 1.7ms preprocess, 385.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 cell phone, 401.1ms\nSpeed: 1.8ms preprocess, 401.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 cell phone, 397.8ms\nSpeed: 1.2ms preprocess, 397.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 cell phone, 373.2ms\nSpeed: 1.3ms preprocess, 373.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 cell phone, 372.5ms\nSpeed: 1.3ms preprocess, 372.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 cell phone, 368.1ms\nSpeed: 2.2ms preprocess, 368.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 cell phone, 374.2ms\nSpeed: 1.3ms preprocess, 374.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 cell phone, 370.9ms\nSpeed: 1.4ms preprocess, 370.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 cell phone, 372.5ms\nSpeed: 1.3ms preprocess, 372.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 bottle, 1 cell phone, 369.3ms\nSpeed: 1.4ms preprocess, 369.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 bottle, 1 cell phone, 373.2ms\nSpeed: 1.3ms preprocess, 373.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 375.3ms\nSpeed: 1.2ms preprocess, 375.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 376.6ms\nSpeed: 1.4ms preprocess, 376.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 358.9ms\nSpeed: 1.6ms preprocess, 358.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 381.1ms\nSpeed: 1.7ms preprocess, 381.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 371.3ms\nSpeed: 1.3ms preprocess, 371.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 381.7ms\nSpeed: 1.4ms preprocess, 381.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 366.6ms\nSpeed: 1.6ms preprocess, 366.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 362.8ms\nSpeed: 1.5ms preprocess, 362.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 380.5ms\nSpeed: 1.6ms preprocess, 380.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 376.4ms\nSpeed: 1.4ms preprocess, 376.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 386.1ms\nSpeed: 1.3ms preprocess, 386.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 371.5ms\nSpeed: 1.6ms preprocess, 371.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 377.7ms\nSpeed: 1.3ms preprocess, 377.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 364.4ms\nSpeed: 1.9ms preprocess, 364.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 370.0ms\nSpeed: 1.8ms preprocess, 370.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 364.3ms\nSpeed: 1.7ms preprocess, 364.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 372.5ms\nSpeed: 2.3ms preprocess, 372.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 362.7ms\nSpeed: 2.2ms preprocess, 362.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 369.0ms\nSpeed: 1.9ms preprocess, 369.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 368.7ms\nSpeed: 1.7ms preprocess, 368.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 375.2ms\nSpeed: 3.5ms preprocess, 375.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 421.0ms\nSpeed: 1.9ms preprocess, 421.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 370.4ms\nSpeed: 1.7ms preprocess, 370.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 374.0ms\nSpeed: 1.4ms preprocess, 374.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 364.0ms\nSpeed: 1.4ms preprocess, 364.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 396.2ms\nSpeed: 1.4ms preprocess, 396.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 379.2ms\nSpeed: 1.7ms preprocess, 379.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 373.9ms\nSpeed: 1.4ms preprocess, 373.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 370.6ms\nSpeed: 1.3ms preprocess, 370.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 363.8ms\nSpeed: 1.3ms preprocess, 363.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 397.7ms\nSpeed: 1.3ms preprocess, 397.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 368.3ms\nSpeed: 1.5ms preprocess, 368.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 375.7ms\nSpeed: 1.3ms preprocess, 375.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 372.4ms\nSpeed: 1.6ms preprocess, 372.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 391.3ms\nSpeed: 1.8ms preprocess, 391.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 379.3ms\nSpeed: 1.4ms preprocess, 379.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 386.6ms\nSpeed: 1.7ms preprocess, 386.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 backpack, 2 potted plants, 1 tv, 391.4ms\nSpeed: 1.8ms preprocess, 391.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 1 chair, 2 potted plants, 1 tv, 1 teddy bear, 378.0ms\nSpeed: 1.5ms preprocess, 378.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 2 potted plants, 1 bed, 1 tv, 1 teddy bear, 394.0ms\nSpeed: 1.4ms preprocess, 394.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 2 potted plants, 1 bed, 1 tv, 1 teddy bear, 377.8ms\nSpeed: 1.9ms preprocess, 377.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 2 potted plants, 1 bed, 1 tv, 1 teddy bear, 385.8ms\nSpeed: 1.6ms preprocess, 385.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 giraffe, 1 handbag, 1 chair, 2 potted plants, 1 bed, 1 tv, 2 teddy bears, 379.4ms\nSpeed: 1.6ms preprocess, 379.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 giraffe, 1 handbag, 2 potted plants, 1 bed, 1 tv, 2 teddy bears, 375.8ms\nSpeed: 1.6ms preprocess, 375.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 2 potted plants, 1 bed, 1 tv, 1 teddy bear, 380.8ms\nSpeed: 1.4ms preprocess, 380.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 2 potted plants, 1 bed, 1 tv, 1 remote, 1 teddy bear, 376.6ms\nSpeed: 1.3ms preprocess, 376.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 1 chair, 2 potted plants, 1 tv, 1 remote, 1 teddy bear, 373.8ms\nSpeed: 1.6ms preprocess, 373.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 giraffe, 1 handbag, 1 chair, 2 potted plants, 1 tv, 1 remote, 1 teddy bear, 386.0ms\nSpeed: 1.4ms preprocess, 386.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 1 chair, 2 potted plants, 1 bed, 1 tv, 2 teddy bears, 372.4ms\nSpeed: 1.3ms preprocess, 372.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 giraffe, 1 handbag, 1 chair, 2 potted plants, 1 bed, 1 tv, 1 teddy bear, 379.3ms\nSpeed: 1.5ms preprocess, 379.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 1 chair, 2 potted plants, 1 bed, 1 tv, 2 teddy bears, 373.9ms\nSpeed: 1.9ms preprocess, 373.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 2 potted plants, 1 bed, 1 tv, 1 teddy bear, 375.5ms\nSpeed: 1.5ms preprocess, 375.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 2 potted plants, 1 bed, 1 tv, 1 teddy bear, 371.7ms\nSpeed: 1.3ms preprocess, 371.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 giraffe, 1 handbag, 1 wine glass, 2 potted plants, 1 tv, 2 teddy bears, 381.5ms\nSpeed: 1.5ms preprocess, 381.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 2 potted plants, 1 bed, 1 tv, 1 teddy bear, 386.4ms\nSpeed: 1.4ms preprocess, 386.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 1 chair, 2 potted plants, 1 bed, 1 tv, 2 teddy bears, 370.9ms\nSpeed: 1.7ms preprocess, 370.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 2 potted plants, 1 bed, 1 tv, 1 teddy bear, 378.9ms\nSpeed: 1.7ms preprocess, 378.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 giraffe, 1 handbag, 1 chair, 2 potted plants, 1 bed, 1 tv, 1 teddy bear, 368.0ms\nSpeed: 1.4ms preprocess, 368.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 2 potted plants, 1 bed, 1 tv, 2 teddy bears, 378.5ms\nSpeed: 1.5ms preprocess, 378.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 1 chair, 2 potted plants, 1 bed, 1 tv, 1 teddy bear, 371.9ms\nSpeed: 1.6ms preprocess, 371.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 2 potted plants, 1 bed, 1 tv, 1 teddy bear, 373.0ms\nSpeed: 1.9ms preprocess, 373.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 1 chair, 2 potted plants, 1 bed, 1 tv, 1 teddy bear, 363.4ms\nSpeed: 1.7ms preprocess, 363.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 1 chair, 2 potted plants, 1 tv, 1 teddy bear, 372.1ms\nSpeed: 2.1ms preprocess, 372.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 1 chair, 2 potted plants, 1 tv, 2 teddy bears, 395.4ms\nSpeed: 1.5ms preprocess, 395.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 1 chair, 2 potted plants, 1 bed, 1 tv, 1 teddy bear, 367.0ms\nSpeed: 1.5ms preprocess, 367.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 1 chair, 2 potted plants, 1 bed, 1 tv, 2 teddy bears, 378.5ms\nSpeed: 1.4ms preprocess, 378.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 1 chair, 2 potted plants, 1 tv, 1 teddy bear, 372.4ms\nSpeed: 1.5ms preprocess, 372.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 1 chair, 2 potted plants, 1 tv, 1 teddy bear, 377.8ms\nSpeed: 2.2ms preprocess, 377.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 2 potted plants, 1 tv, 1 teddy bear, 426.5ms\nSpeed: 1.4ms preprocess, 426.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 2 potted plants, 1 bed, 1 tv, 1 teddy bear, 375.4ms\nSpeed: 1.7ms preprocess, 375.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 2 potted plants, 1 bed, 1 tv, 2 teddy bears, 378.0ms\nSpeed: 2.4ms preprocess, 378.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 2 potted plants, 1 bed, 1 tv, 1 teddy bear, 367.2ms\nSpeed: 1.5ms preprocess, 367.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 2 potted plants, 1 bed, 1 tv, 2 teddy bears, 380.1ms\nSpeed: 1.4ms preprocess, 380.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 giraffe, 1 handbag, 2 potted plants, 1 bed, 1 tv, 2 teddy bears, 369.5ms\nSpeed: 1.3ms preprocess, 369.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 giraffe, 1 handbag, 2 potted plants, 1 bed, 1 tv, 3 teddy bears, 387.4ms\nSpeed: 1.5ms preprocess, 387.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 giraffe, 1 handbag, 1 chair, 2 potted plants, 1 tv, 1 teddy bear, 382.1ms\nSpeed: 1.4ms preprocess, 382.1ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 2 potted plants, 1 bed, 1 tv, 1 teddy bear, 373.2ms\nSpeed: 2.0ms preprocess, 373.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 1 bottle, 2 potted plants, 1 bed, 1 tv, 2 teddy bears, 381.7ms\nSpeed: 1.5ms preprocess, 381.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 1 bottle, 1 potted plant, 1 bed, 1 tv, 365.9ms\nSpeed: 1.4ms preprocess, 365.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 handbag, 1 potted plant, 1 bed, 1 tv, 405.2ms\nSpeed: 1.4ms preprocess, 405.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 378.1ms\nSpeed: 1.7ms preprocess, 378.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 384.8ms\nSpeed: 1.8ms preprocess, 384.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 385.1ms\nSpeed: 1.3ms preprocess, 385.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 378.3ms\nSpeed: 1.4ms preprocess, 378.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 395.5ms\nSpeed: 2.3ms preprocess, 395.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 380.5ms\nSpeed: 1.6ms preprocess, 380.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 379.3ms\nSpeed: 1.7ms preprocess, 379.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 383.6ms\nSpeed: 1.4ms preprocess, 383.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 383.1ms\nSpeed: 1.6ms preprocess, 383.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 377.3ms\nSpeed: 1.5ms preprocess, 377.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 389.0ms\nSpeed: 1.7ms preprocess, 389.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 376.7ms\nSpeed: 1.5ms preprocess, 376.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 388.6ms\nSpeed: 1.8ms preprocess, 388.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 384.8ms\nSpeed: 1.4ms preprocess, 384.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 377.1ms\nSpeed: 1.5ms preprocess, 377.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 375.8ms\nSpeed: 1.6ms preprocess, 375.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 365.2ms\nSpeed: 1.7ms preprocess, 365.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 386.0ms\nSpeed: 2.0ms preprocess, 386.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 persons, 540.7ms\nSpeed: 1.4ms preprocess, 540.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 386.7ms\nSpeed: 1.7ms preprocess, 386.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 378.1ms\nSpeed: 1.3ms preprocess, 378.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 388.9ms\nSpeed: 1.7ms preprocess, 388.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 377.6ms\nSpeed: 1.6ms preprocess, 377.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 372.0ms\nSpeed: 1.4ms preprocess, 372.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 374.1ms\nSpeed: 1.4ms preprocess, 374.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 374.9ms\nSpeed: 1.7ms preprocess, 374.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 375.9ms\nSpeed: 1.4ms preprocess, 375.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 374.6ms\nSpeed: 2.0ms preprocess, 374.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 376.4ms\nSpeed: 1.7ms preprocess, 376.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Realizar infer\u00eancia com par\u00e2metros personalizados\nresults_custom = model(\n    'lab_images/bus.jpg',\n    conf=0.5,        # Limiar de confian\u00e7a (padr\u00e3o 0.25)\n    iou=0.7,         # Limiar IoU para NMS (padr\u00e3o 0.45)\n    max_det=20,      # N\u00famero m\u00e1ximo de detec\u00e7\u00f5es\n    classes=[13, 14]   # Filtrar apenas pessoas (0) e carros (2)\n)\n\n# Visualizar os resultados\nvisualize_detection('lab_images/bus.jpg', results_custom)\n</pre> # Realizar infer\u00eancia com par\u00e2metros personalizados results_custom = model(     'lab_images/bus.jpg',     conf=0.5,        # Limiar de confian\u00e7a (padr\u00e3o 0.25)     iou=0.7,         # Limiar IoU para NMS (padr\u00e3o 0.45)     max_det=20,      # N\u00famero m\u00e1ximo de detec\u00e7\u00f5es     classes=[13, 14]   # Filtrar apenas pessoas (0) e carros (2) )  # Visualizar os resultados visualize_detection('lab_images/bus.jpg', results_custom) <pre>\nimage 1/1 /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/bus.jpg: 640x480 3 persons, 75.2ms\nSpeed: 2.7ms preprocess, 75.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n</pre> <pre>Foram detectados 3 objetos\nDetec\u00e7\u00e3o 1: person com confian\u00e7a 0.87\nDetec\u00e7\u00e3o 2: person com confian\u00e7a 0.85\nDetec\u00e7\u00e3o 3: person com confian\u00e7a 0.83\n</pre> In\u00a0[13]: Copied! <pre># Listar todas as classes que o modelo pode detectar\nfor idx, class_name in model.names.items():\n    print(f\"{idx}: {class_name}\")\n</pre> # Listar todas as classes que o modelo pode detectar for idx, class_name in model.names.items():     print(f\"{idx}: {class_name}\") <pre>0: person\n1: bicycle\n2: car\n3: motorcycle\n4: airplane\n5: bus\n6: train\n7: truck\n8: boat\n9: traffic light\n10: fire hydrant\n11: stop sign\n12: parking meter\n13: bench\n14: bird\n15: cat\n16: dog\n17: horse\n18: sheep\n19: cow\n20: elephant\n21: bear\n22: zebra\n23: giraffe\n24: backpack\n25: umbrella\n26: handbag\n27: tie\n28: suitcase\n29: frisbee\n30: skis\n31: snowboard\n32: sports ball\n33: kite\n34: baseball bat\n35: baseball glove\n36: skateboard\n37: surfboard\n38: tennis racket\n39: bottle\n40: wine glass\n41: cup\n42: fork\n43: knife\n44: spoon\n45: bowl\n46: banana\n47: apple\n48: sandwich\n49: orange\n50: broccoli\n51: carrot\n52: hot dog\n53: pizza\n54: donut\n55: cake\n56: chair\n57: couch\n58: potted plant\n59: bed\n60: dining table\n61: toilet\n62: tv\n63: laptop\n64: mouse\n65: remote\n66: keyboard\n67: cell phone\n68: microwave\n69: oven\n70: toaster\n71: sink\n72: refrigerator\n73: book\n74: clock\n75: vase\n76: scissors\n77: teddy bear\n78: hair drier\n79: toothbrush\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[5]: Copied! <pre>from ultralytics import YOLO\nimport ultralytics\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n</pre> from ultralytics import YOLO import ultralytics import cv2 import matplotlib.pyplot as plt import numpy as np In\u00a0[2]: Copied! <pre>ultralytics.checks()\n</pre> ultralytics.checks() <pre>Ultralytics 8.3.134 \ud83d\ude80 Python-3.9.6 torch-2.3.0 CPU (Apple M2)\nSetup complete \u2705 (8 CPUs, 16.0 GB RAM, 294.2/460.4 GB disk)\n</pre> In\u00a0[7]: Copied! <pre>imagem= cv2.imread(\"lab_images/bus.jpg\", cv2.IMREAD_COLOR)\n\n\nmodel = YOLO('model/yolov8n.pt')\nresults = model.predict(imagem, conf=0.25)\n\nimage_result = results[0].plot()\nimage_result = cv2.cvtColor(image_result, cv2.COLOR_BGR2RGB)\n\nplt.imshow(image_result)\n</pre> imagem= cv2.imread(\"lab_images/bus.jpg\", cv2.IMREAD_COLOR)   model = YOLO('model/yolov8n.pt') results = model.predict(imagem, conf=0.25)  image_result = results[0].plot() image_result = cv2.cvtColor(image_result, cv2.COLOR_BGR2RGB)  plt.imshow(image_result) <pre>\n0: 640x480 4 persons, 1 bus, 1 stop sign, 67.2ms\nSpeed: 2.8ms preprocess, 67.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n</pre> Out[7]: <pre>&lt;matplotlib.image.AxesImage at 0x135d5ee50&gt;</pre>"},{"location":"aulas/IA/lab11/yolo/#yolo-basico-com-ultralytics","title":"YOLO B\u00e1sico com Ultralytics\u00b6","text":"<p>Neste notebook, vamos explorar os conceitos b\u00e1sicos do YOLO utilizando a biblioteca Ultralytics. Vamos aprender a:</p> <ol> <li>Instalar e configurar a biblioteca</li> <li>Carregar modelos pr\u00e9-treinados</li> <li>Realizar infer\u00eancia em imagens e v\u00eddeos</li> <li>Interpretar e visualizar resultados</li> </ol>"},{"location":"aulas/IA/lab11/yolo/#1-instalacao-e-setup","title":"1. Instala\u00e7\u00e3o e Setup\u00b6","text":""},{"location":"aulas/IA/lab11/yolo/#2-carregando-um-modelo-pre-treinado","title":"2. Carregando um modelo pr\u00e9-treinado\u00b6","text":""},{"location":"aulas/IA/lab11/yolo/#3-funcao-auxiliar-para-visualizacao","title":"3. Fun\u00e7\u00e3o auxiliar para visualiza\u00e7\u00e3o\u00b6","text":""},{"location":"aulas/IA/lab11/yolo/#4-deteccao-em-uma-imagem-local","title":"4. Detec\u00e7\u00e3o em uma imagem local\u00b6","text":""},{"location":"aulas/IA/lab11/yolo/#5-explorando-os-resultados","title":"5. Explorando os resultados\u00b6","text":""},{"location":"aulas/IA/lab11/yolo/#6-deteccao-em-video","title":"6. Detec\u00e7\u00e3o em v\u00eddeo\u00b6","text":""},{"location":"aulas/IA/lab11/yolo/#7-deteccao-com-webcam-se-estiver-disponivel","title":"7. Detec\u00e7\u00e3o com webcam (se estiver dispon\u00edvel)\u00b6","text":""},{"location":"aulas/IA/lab11/yolo/#8-ajustando-os-parametros-de-inferencia","title":"8. Ajustando os par\u00e2metros de infer\u00eancia\u00b6","text":""},{"location":"aulas/IA/lab11/yolo/#9-lista-de-classes-coco-conjunto-de-dados-padrao","title":"9. Lista de classes COCO (conjunto de dados padr\u00e3o)\u00b6","text":""},{"location":"aulas/IA/lab11/yolo1/","title":"Yolo1","text":"In\u00a0[\u00a0]: Copied! <pre># # Instala\u00e7\u00e3o dos pacotes necess\u00e1rios\n# !pip install ultralytics\n# !pip install opencv-python matplotlib\n</pre> # # Instala\u00e7\u00e3o dos pacotes necess\u00e1rios # !pip install ultralytics # !pip install opencv-python matplotlib In\u00a0[\u00a0]: Copied! <pre># Importa\u00e7\u00e3o das bibliotecas\nfrom ultralytics import YOLO\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom IPython.display import display, Image\n</pre> # Importa\u00e7\u00e3o das bibliotecas from ultralytics import YOLO import cv2 import matplotlib.pyplot as plt import numpy as np import os from IPython.display import display, Image In\u00a0[2]: Copied! <pre># Carregar um modelo para detec\u00e7\u00e3o de objetos (padr\u00e3o)\ndetect_model = YOLO('model/yolov8n.pt')\n\n# Carregar um modelo para segmenta\u00e7\u00e3o de inst\u00e2ncias\nsegment_model = YOLO('model/yolov8n-seg.pt')\n\n# Carregar um modelo para classifica\u00e7\u00e3o de imagens\nclassify_model = YOLO('model/yolov8n-cls.pt')\n\n# Carregar um modelo para pose estimation\npose_model = YOLO('model/yolov8n-pose.pt')\n</pre> # Carregar um modelo para detec\u00e7\u00e3o de objetos (padr\u00e3o) detect_model = YOLO('model/yolov8n.pt')  # Carregar um modelo para segmenta\u00e7\u00e3o de inst\u00e2ncias segment_model = YOLO('model/yolov8n-seg.pt')  # Carregar um modelo para classifica\u00e7\u00e3o de imagens classify_model = YOLO('model/yolov8n-cls.pt')  # Carregar um modelo para pose estimation pose_model = YOLO('model/yolov8n-pose.pt') In\u00a0[9]: Copied! <pre># Fun\u00e7\u00e3o para mostrar imagens lado a lado\ndef compare_tasks(image_path):\n    # Detec\u00e7\u00e3o\n    detect_results = detect_model(image_path)\n    detect_img = detect_results[0].plot()\n    \n    # classifica\u00e7\u00e3o\n    classify_results = classify_model(image_path)\n    classify_img = classify_results[0].plot()\n    if classify_results[0].boxes:\n        class_name = classify_results[0].names[classify_results[0].boxes.cls[0]]\n        cv2.putText(detect_img, f\"Class: {class_name}\", (50, 50), \n                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n    else:\n        cv2.putText(detect_img, \"Nenhuma classe detectada\", (50, 50), \n                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n    \n    # Segmenta\u00e7\u00e3o\n    segment_results = segment_model(image_path)\n    segment_img = segment_results[0].plot()\n    \n    # Pose (pode n\u00e3o funcionar em todas as imagens)\n    try:\n        pose_results = pose_model(image_path)\n        pose_img = pose_results[0].plot()\n    except:\n        pose_img = np.zeros_like(detect_img)\n        cv2.putText(pose_img, \"Pose n\u00e3o detectada\", (50, 100), \n                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n    \n    # Exibir imagens\n    plt.figure(figsize=(18, 8))\n    \n    plt.subplot(1, 4, 1)\n    plt.imshow(cv2.cvtColor(detect_img, cv2.COLOR_BGR2RGB))\n    plt.title(\"Detec\u00e7\u00e3o de Objetos\")\n    plt.axis('off')\n    \n    plt.subplot(1, 4, 2)\n    plt.imshow(cv2.cvtColor(segment_img, cv2.COLOR_BGR2RGB))\n    plt.title(\"Segmenta\u00e7\u00e3o de Inst\u00e2ncias\")\n    plt.axis('off')\n    \n    plt.subplot(1, 4, 3)\n    plt.imshow(cv2.cvtColor(pose_img, cv2.COLOR_BGR2RGB))\n    plt.title(\"Estimativa de Pose\")\n    plt.axis('off')\n    \n    plt.subplot(1, 4, 4)\n    plt.imshow(cv2.cvtColor(classify_img, cv2.COLOR_BGR2RGB))\n    plt.title(\"Classifica\u00e7\u00e3o de Imagem\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n</pre> # Fun\u00e7\u00e3o para mostrar imagens lado a lado def compare_tasks(image_path):     # Detec\u00e7\u00e3o     detect_results = detect_model(image_path)     detect_img = detect_results[0].plot()          # classifica\u00e7\u00e3o     classify_results = classify_model(image_path)     classify_img = classify_results[0].plot()     if classify_results[0].boxes:         class_name = classify_results[0].names[classify_results[0].boxes.cls[0]]         cv2.putText(detect_img, f\"Class: {class_name}\", (50, 50),                      cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)     else:         cv2.putText(detect_img, \"Nenhuma classe detectada\", (50, 50),                      cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)          # Segmenta\u00e7\u00e3o     segment_results = segment_model(image_path)     segment_img = segment_results[0].plot()          # Pose (pode n\u00e3o funcionar em todas as imagens)     try:         pose_results = pose_model(image_path)         pose_img = pose_results[0].plot()     except:         pose_img = np.zeros_like(detect_img)         cv2.putText(pose_img, \"Pose n\u00e3o detectada\", (50, 100),                      cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)          # Exibir imagens     plt.figure(figsize=(18, 8))          plt.subplot(1, 4, 1)     plt.imshow(cv2.cvtColor(detect_img, cv2.COLOR_BGR2RGB))     plt.title(\"Detec\u00e7\u00e3o de Objetos\")     plt.axis('off')          plt.subplot(1, 4, 2)     plt.imshow(cv2.cvtColor(segment_img, cv2.COLOR_BGR2RGB))     plt.title(\"Segmenta\u00e7\u00e3o de Inst\u00e2ncias\")     plt.axis('off')          plt.subplot(1, 4, 3)     plt.imshow(cv2.cvtColor(pose_img, cv2.COLOR_BGR2RGB))     plt.title(\"Estimativa de Pose\")     plt.axis('off')          plt.subplot(1, 4, 4)     plt.imshow(cv2.cvtColor(classify_img, cv2.COLOR_BGR2RGB))     plt.title(\"Classifica\u00e7\u00e3o de Imagem\")     plt.axis('off')          plt.tight_layout()     plt.show() In\u00a0[10]: Copied! <pre># Comparar diferentes tarefas em uma imagem de pessoas\ncompare_tasks('lab_images/bus.jpg')\n</pre> # Comparar diferentes tarefas em uma imagem de pessoas compare_tasks('lab_images/bus.jpg') <pre>\nimage 1/1 /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 156.9ms\nSpeed: 3.1ms preprocess, 156.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n\nimage 1/1 /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/bus.jpg: 224x224 minibus 0.50, police_van 0.29, trolleybus 0.05, golfcart 0.02, jinrikisha 0.02, 9.0ms\nSpeed: 6.2ms preprocess, 9.0ms inference, 0.2ms postprocess per image at shape (1, 3, 224, 224)\n\nimage 1/1 /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/bus.jpg: 640x480 4 persons, 1 bus, 1 skateboard, 90.4ms\nSpeed: 2.0ms preprocess, 90.4ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 480)\n\nimage 1/1 /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/bus.jpg: 640x480 4 persons, 64.2ms\nSpeed: 2.1ms preprocess, 64.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Baixar um v\u00eddeo de exemplo\n!wget https://ultralytics.com/assets/people_walking.mp4 -O test_video.mp4\n</pre> # Baixar um v\u00eddeo de exemplo !wget https://ultralytics.com/assets/people_walking.mp4 -O test_video.mp4 In\u00a0[14]: Copied! <pre># Fine-tuning de um modelo pr\u00e9-treinado\nmodel = YOLO('model/yolov8n.pt')  # carregamos um modelo pequeno para o exemplo\n\n# Tracking de objetos no v\u00eddeo\ntrack_results = model.track(\n    'lab_images/people-walking.mp4',\n    conf=0.3,\n    iou=0.5,\n    show=False,\n    tracker=\"bytetrack.yaml\",  # Algoritmo de tracking\n    classes=0,  # Apenas pessoas (classe 0)\n    save=True\n)\n\nprint(\"Tracking completo! V\u00eddeo salvo em runs/detect/track/\")\n</pre> # Fine-tuning de um modelo pr\u00e9-treinado model = YOLO('model/yolov8n.pt')  # carregamos um modelo pequeno para o exemplo  # Tracking de objetos no v\u00eddeo track_results = model.track(     'lab_images/people-walking.mp4',     conf=0.3,     iou=0.5,     show=False,     tracker=\"bytetrack.yaml\",  # Algoritmo de tracking     classes=0,  # Apenas pessoas (classe 0)     save=True )  print(\"Tracking completo! V\u00eddeo salvo em runs/detect/track/\") <pre>Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'lab_images/yolov8n.pt'...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.25M/6.25M [00:01&lt;00:00, 6.35MB/s]</pre> <pre>\nWARNING \u26a0\ufe0f \ninference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\nerrors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n\nExample:\n    results = model(source=..., stream=True)  # generator of Results objects\n    for r in results:\n        boxes = r.boxes  # Boxes object for bbox outputs\n        masks = r.masks  # Masks object for segment masks outputs\n        probs = r.probs  # Class probabilities for classification outputs\n\n</pre> <pre>\n</pre> <pre>video 1/1 (frame 1/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 63.4ms\nvideo 1/1 (frame 2/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 60.3ms\nvideo 1/1 (frame 3/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 47.4ms\nvideo 1/1 (frame 4/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 49.2ms\nvideo 1/1 (frame 5/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 71.2ms\nvideo 1/1 (frame 6/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 76.9ms\nvideo 1/1 (frame 7/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 1 person, 66.5ms\nvideo 1/1 (frame 8/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 68.7ms\nvideo 1/1 (frame 9/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 78.3ms\nvideo 1/1 (frame 10/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 79.0ms\nvideo 1/1 (frame 11/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 61.0ms\nvideo 1/1 (frame 12/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 71.8ms\nvideo 1/1 (frame 13/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 79.0ms\nvideo 1/1 (frame 14/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 68.5ms\nvideo 1/1 (frame 15/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 65.8ms\nvideo 1/1 (frame 16/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 75.6ms\nvideo 1/1 (frame 17/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 69.0ms\nvideo 1/1 (frame 18/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 64.8ms\nvideo 1/1 (frame 19/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 73.6ms\nvideo 1/1 (frame 20/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 73.1ms\nvideo 1/1 (frame 21/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 60.5ms\nvideo 1/1 (frame 22/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 67.4ms\nvideo 1/1 (frame 23/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 78.4ms\nvideo 1/1 (frame 24/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 77.1ms\nvideo 1/1 (frame 25/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 76.9ms\nvideo 1/1 (frame 26/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 64.7ms\nvideo 1/1 (frame 27/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 70.8ms\nvideo 1/1 (frame 28/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 74.8ms\nvideo 1/1 (frame 29/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 75.7ms\nvideo 1/1 (frame 30/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 73.4ms\nvideo 1/1 (frame 31/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 80.1ms\nvideo 1/1 (frame 32/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 72.9ms\nvideo 1/1 (frame 33/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 78.0ms\nvideo 1/1 (frame 34/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 70.2ms\nvideo 1/1 (frame 35/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 84.8ms\nvideo 1/1 (frame 36/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 75.6ms\nvideo 1/1 (frame 37/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 73.1ms\nvideo 1/1 (frame 38/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 57.6ms\nvideo 1/1 (frame 39/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 68.1ms\nvideo 1/1 (frame 40/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 68.0ms\nvideo 1/1 (frame 41/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 71.1ms\nvideo 1/1 (frame 42/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 59.9ms\nvideo 1/1 (frame 43/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 81.1ms\nvideo 1/1 (frame 44/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 71.0ms\nvideo 1/1 (frame 45/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 59.9ms\nvideo 1/1 (frame 46/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 74.4ms\nvideo 1/1 (frame 47/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 84.9ms\nvideo 1/1 (frame 48/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 78.9ms\nvideo 1/1 (frame 49/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 62.2ms\nvideo 1/1 (frame 50/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 81.6ms\nvideo 1/1 (frame 51/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 82.7ms\nvideo 1/1 (frame 52/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 78.8ms\nvideo 1/1 (frame 53/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 69.7ms\nvideo 1/1 (frame 54/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 80.8ms\nvideo 1/1 (frame 55/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 70.9ms\nvideo 1/1 (frame 56/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 78.9ms\nvideo 1/1 (frame 57/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 77.7ms\nvideo 1/1 (frame 58/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 72.7ms\nvideo 1/1 (frame 59/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 80.9ms\nvideo 1/1 (frame 60/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 147.1ms\nvideo 1/1 (frame 61/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 82.5ms\nvideo 1/1 (frame 62/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 76.4ms\nvideo 1/1 (frame 63/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 74.4ms\nvideo 1/1 (frame 64/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 91.9ms\nvideo 1/1 (frame 65/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 70.4ms\nvideo 1/1 (frame 66/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 72.6ms\nvideo 1/1 (frame 67/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 63.3ms\nvideo 1/1 (frame 68/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 88.4ms\nvideo 1/1 (frame 69/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 74.6ms\nvideo 1/1 (frame 70/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 83.5ms\nvideo 1/1 (frame 71/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 74.0ms\nvideo 1/1 (frame 72/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 70.3ms\nvideo 1/1 (frame 73/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 59.5ms\nvideo 1/1 (frame 74/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 2 persons, 71.0ms\nvideo 1/1 (frame 75/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 67.3ms\nvideo 1/1 (frame 76/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 69.0ms\nvideo 1/1 (frame 77/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.3ms\nvideo 1/1 (frame 78/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.8ms\nvideo 1/1 (frame 79/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 67.0ms\nvideo 1/1 (frame 80/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 67.0ms\nvideo 1/1 (frame 81/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 1 person, 62.4ms\nvideo 1/1 (frame 82/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 60.9ms\nvideo 1/1 (frame 83/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 70.4ms\nvideo 1/1 (frame 84/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 71.9ms\nvideo 1/1 (frame 85/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.7ms\nvideo 1/1 (frame 86/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 63.7ms\nvideo 1/1 (frame 87/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 68.3ms\nvideo 1/1 (frame 88/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.9ms\nvideo 1/1 (frame 89/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 70.6ms\nvideo 1/1 (frame 90/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 70.1ms\nvideo 1/1 (frame 91/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 69.0ms\nvideo 1/1 (frame 92/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 71.8ms\nvideo 1/1 (frame 93/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 62.7ms\nvideo 1/1 (frame 94/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 64.8ms\nvideo 1/1 (frame 95/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 79.3ms\nvideo 1/1 (frame 96/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 69.9ms\nvideo 1/1 (frame 97/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 77.6ms\nvideo 1/1 (frame 98/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 70.3ms\nvideo 1/1 (frame 99/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 71.1ms\nvideo 1/1 (frame 100/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 66.6ms\nvideo 1/1 (frame 101/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 81.4ms\nvideo 1/1 (frame 102/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 65.3ms\nvideo 1/1 (frame 103/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 84.8ms\nvideo 1/1 (frame 104/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 69.7ms\nvideo 1/1 (frame 105/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 68.4ms\nvideo 1/1 (frame 106/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 67.3ms\nvideo 1/1 (frame 107/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 70.1ms\nvideo 1/1 (frame 108/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 160.2ms\nvideo 1/1 (frame 109/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 54.9ms\nvideo 1/1 (frame 110/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 84.4ms\nvideo 1/1 (frame 111/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 81.6ms\nvideo 1/1 (frame 112/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.0ms\nvideo 1/1 (frame 113/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 75.1ms\nvideo 1/1 (frame 114/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 67.8ms\nvideo 1/1 (frame 115/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.5ms\nvideo 1/1 (frame 116/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 86.9ms\nvideo 1/1 (frame 117/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.4ms\nvideo 1/1 (frame 118/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 62.1ms\nvideo 1/1 (frame 119/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 81.6ms\nvideo 1/1 (frame 120/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 69.5ms\nvideo 1/1 (frame 121/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 69.2ms\nvideo 1/1 (frame 122/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.0ms\nvideo 1/1 (frame 123/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 60.9ms\nvideo 1/1 (frame 124/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 82.2ms\nvideo 1/1 (frame 125/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.3ms\nvideo 1/1 (frame 126/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 59.3ms\nvideo 1/1 (frame 127/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 76.1ms\nvideo 1/1 (frame 128/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 70.6ms\nvideo 1/1 (frame 129/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 61.6ms\nvideo 1/1 (frame 130/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 71.7ms\nvideo 1/1 (frame 131/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.4ms\nvideo 1/1 (frame 132/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 77.1ms\nvideo 1/1 (frame 133/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 61.0ms\nvideo 1/1 (frame 134/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.5ms\nvideo 1/1 (frame 135/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 62.4ms\nvideo 1/1 (frame 136/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 71.9ms\nvideo 1/1 (frame 137/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 76.6ms\nvideo 1/1 (frame 138/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.5ms\nvideo 1/1 (frame 139/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.8ms\nvideo 1/1 (frame 140/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 67.9ms\nvideo 1/1 (frame 141/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.2ms\nvideo 1/1 (frame 142/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 104.1ms\nvideo 1/1 (frame 143/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 78.3ms\nvideo 1/1 (frame 144/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 81.2ms\nvideo 1/1 (frame 145/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 75.9ms\nvideo 1/1 (frame 146/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 81.4ms\nvideo 1/1 (frame 147/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 68.5ms\nvideo 1/1 (frame 148/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 79.4ms\nvideo 1/1 (frame 149/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 83.2ms\nvideo 1/1 (frame 150/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 69.5ms\nvideo 1/1 (frame 151/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.0ms\nvideo 1/1 (frame 152/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 77.1ms\nvideo 1/1 (frame 153/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.4ms\nvideo 1/1 (frame 154/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 64.1ms\nvideo 1/1 (frame 155/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 78.9ms\nvideo 1/1 (frame 156/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.7ms\nvideo 1/1 (frame 157/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.7ms\nvideo 1/1 (frame 158/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 61.7ms\nvideo 1/1 (frame 159/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.9ms\nvideo 1/1 (frame 160/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 66.5ms\nvideo 1/1 (frame 161/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 70.5ms\nvideo 1/1 (frame 162/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.1ms\nvideo 1/1 (frame 163/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.2ms\nvideo 1/1 (frame 164/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.3ms\nvideo 1/1 (frame 165/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 76.7ms\nvideo 1/1 (frame 166/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 63.8ms\nvideo 1/1 (frame 167/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 84.3ms\nvideo 1/1 (frame 168/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 62.5ms\nvideo 1/1 (frame 169/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.0ms\nvideo 1/1 (frame 170/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.5ms\nvideo 1/1 (frame 171/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.7ms\nvideo 1/1 (frame 172/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 77.2ms\nvideo 1/1 (frame 173/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 71.5ms\nvideo 1/1 (frame 174/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 59.6ms\nvideo 1/1 (frame 175/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 67.1ms\nvideo 1/1 (frame 176/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 81.4ms\nvideo 1/1 (frame 177/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 62.5ms\nvideo 1/1 (frame 178/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 75.1ms\nvideo 1/1 (frame 179/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 75.2ms\nvideo 1/1 (frame 180/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 78.6ms\nvideo 1/1 (frame 181/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 67.2ms\nvideo 1/1 (frame 182/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 94.8ms\nvideo 1/1 (frame 183/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 83.8ms\nvideo 1/1 (frame 184/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.9ms\nvideo 1/1 (frame 185/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 76.7ms\nvideo 1/1 (frame 186/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 78.1ms\nvideo 1/1 (frame 187/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.7ms\nvideo 1/1 (frame 188/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 78.8ms\nvideo 1/1 (frame 189/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.9ms\nvideo 1/1 (frame 190/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 70.3ms\nvideo 1/1 (frame 191/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.6ms\nvideo 1/1 (frame 192/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 58.1ms\nvideo 1/1 (frame 193/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 69.6ms\nvideo 1/1 (frame 194/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 65.8ms\nvideo 1/1 (frame 195/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.7ms\nvideo 1/1 (frame 196/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 97.1ms\nvideo 1/1 (frame 197/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 71.9ms\nvideo 1/1 (frame 198/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 68.5ms\nvideo 1/1 (frame 199/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 69.1ms\nvideo 1/1 (frame 200/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 70.8ms\nvideo 1/1 (frame 201/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.4ms\nvideo 1/1 (frame 202/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 80.3ms\nvideo 1/1 (frame 203/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 67.4ms\nvideo 1/1 (frame 204/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 78.4ms\nvideo 1/1 (frame 205/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 78.4ms\nvideo 1/1 (frame 206/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 70.6ms\nvideo 1/1 (frame 207/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 79.0ms\nvideo 1/1 (frame 208/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 76.8ms\nvideo 1/1 (frame 209/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 78.7ms\nvideo 1/1 (frame 210/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 76.5ms\nvideo 1/1 (frame 211/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 65.2ms\nvideo 1/1 (frame 212/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 81.8ms\nvideo 1/1 (frame 213/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.9ms\nvideo 1/1 (frame 214/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 77.4ms\nvideo 1/1 (frame 215/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 76.4ms\nvideo 1/1 (frame 216/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 79.6ms\nvideo 1/1 (frame 217/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 66.8ms\nvideo 1/1 (frame 218/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 65.7ms\nvideo 1/1 (frame 219/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 82.8ms\nvideo 1/1 (frame 220/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 68.4ms\nvideo 1/1 (frame 221/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 89.9ms\nvideo 1/1 (frame 222/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 61.2ms\nvideo 1/1 (frame 223/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 82.6ms\nvideo 1/1 (frame 224/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 105.2ms\nvideo 1/1 (frame 225/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.6ms\nvideo 1/1 (frame 226/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 128.4ms\nvideo 1/1 (frame 227/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 81.5ms\nvideo 1/1 (frame 228/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 87.8ms\nvideo 1/1 (frame 229/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 99.3ms\nvideo 1/1 (frame 230/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 58.8ms\nvideo 1/1 (frame 231/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 172.4ms\nvideo 1/1 (frame 232/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 121.4ms\nvideo 1/1 (frame 233/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 119.8ms\nvideo 1/1 (frame 234/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 103.5ms\nvideo 1/1 (frame 235/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 101.1ms\nvideo 1/1 (frame 236/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 101.5ms\nvideo 1/1 (frame 237/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 81.2ms\nvideo 1/1 (frame 238/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.8ms\nvideo 1/1 (frame 239/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.9ms\nvideo 1/1 (frame 240/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 83.3ms\nvideo 1/1 (frame 241/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 63.3ms\nvideo 1/1 (frame 242/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.6ms\nvideo 1/1 (frame 243/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.5ms\nvideo 1/1 (frame 244/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.6ms\nvideo 1/1 (frame 245/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.1ms\nvideo 1/1 (frame 246/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 75.1ms\nvideo 1/1 (frame 247/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 69.6ms\nvideo 1/1 (frame 248/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 61.2ms\nvideo 1/1 (frame 249/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.0ms\nvideo 1/1 (frame 250/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 65.9ms\nvideo 1/1 (frame 251/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 70.5ms\nvideo 1/1 (frame 252/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 70.8ms\nvideo 1/1 (frame 253/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 65.7ms\nvideo 1/1 (frame 254/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 69.7ms\nvideo 1/1 (frame 255/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 89.6ms\nvideo 1/1 (frame 256/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 63.1ms\nvideo 1/1 (frame 257/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 106.5ms\nvideo 1/1 (frame 258/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 98.5ms\nvideo 1/1 (frame 259/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 99.8ms\nvideo 1/1 (frame 260/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 85.2ms\nvideo 1/1 (frame 261/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.4ms\nvideo 1/1 (frame 262/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 61.9ms\nvideo 1/1 (frame 263/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 75.8ms\nvideo 1/1 (frame 264/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.2ms\nvideo 1/1 (frame 265/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 106.4ms\nvideo 1/1 (frame 266/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 107.0ms\nvideo 1/1 (frame 267/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 79.0ms\nvideo 1/1 (frame 268/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 66.5ms\nvideo 1/1 (frame 269/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 207.8ms\nvideo 1/1 (frame 270/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 87.4ms\nvideo 1/1 (frame 271/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 110.9ms\nvideo 1/1 (frame 272/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 76.9ms\nvideo 1/1 (frame 273/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 100.3ms\nvideo 1/1 (frame 274/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 96.4ms\nvideo 1/1 (frame 275/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 87.5ms\nvideo 1/1 (frame 276/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 95.9ms\nvideo 1/1 (frame 277/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 91.8ms\nvideo 1/1 (frame 278/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 95.1ms\nvideo 1/1 (frame 279/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 78.6ms\nvideo 1/1 (frame 280/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 91.9ms\nvideo 1/1 (frame 281/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 92.7ms\nvideo 1/1 (frame 282/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 98.4ms\nvideo 1/1 (frame 283/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.2ms\nvideo 1/1 (frame 284/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 83.4ms\nvideo 1/1 (frame 285/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 64.2ms\nvideo 1/1 (frame 286/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 87.0ms\nvideo 1/1 (frame 287/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 71.5ms\nvideo 1/1 (frame 288/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 69.9ms\nvideo 1/1 (frame 289/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.7ms\nvideo 1/1 (frame 290/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 77.4ms\nvideo 1/1 (frame 291/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 60.2ms\nvideo 1/1 (frame 292/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 81.1ms\nvideo 1/1 (frame 293/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 76.3ms\nvideo 1/1 (frame 294/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 81.8ms\nvideo 1/1 (frame 295/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 84.6ms\nvideo 1/1 (frame 296/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 80.7ms\nvideo 1/1 (frame 297/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 83.2ms\nvideo 1/1 (frame 298/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 76.1ms\nvideo 1/1 (frame 299/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.3ms\nvideo 1/1 (frame 300/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.3ms\nvideo 1/1 (frame 301/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 77.3ms\nvideo 1/1 (frame 302/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.8ms\nvideo 1/1 (frame 303/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 69.6ms\nvideo 1/1 (frame 304/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 66.8ms\nvideo 1/1 (frame 305/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 75.6ms\nvideo 1/1 (frame 306/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 123.8ms\nvideo 1/1 (frame 307/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 76.0ms\nvideo 1/1 (frame 308/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 118.4ms\nvideo 1/1 (frame 309/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 113.5ms\nvideo 1/1 (frame 310/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 108.3ms\nvideo 1/1 (frame 311/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 145.3ms\nvideo 1/1 (frame 312/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 131.2ms\nvideo 1/1 (frame 313/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 88.7ms\nvideo 1/1 (frame 314/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 79.0ms\nvideo 1/1 (frame 315/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 98.5ms\nvideo 1/1 (frame 316/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 91.5ms\nvideo 1/1 (frame 317/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 95.7ms\nvideo 1/1 (frame 318/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 115.2ms\nvideo 1/1 (frame 319/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 157.0ms\nvideo 1/1 (frame 320/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 89.5ms\nvideo 1/1 (frame 321/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 80.1ms\nvideo 1/1 (frame 322/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 87.1ms\nvideo 1/1 (frame 323/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 77.6ms\nvideo 1/1 (frame 324/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.7ms\nvideo 1/1 (frame 325/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.6ms\nvideo 1/1 (frame 326/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 79.7ms\nvideo 1/1 (frame 327/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 99.4ms\nvideo 1/1 (frame 328/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 75.0ms\nvideo 1/1 (frame 329/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 1 person, 61.5ms\nvideo 1/1 (frame 330/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 1 person, 70.1ms\nvideo 1/1 (frame 331/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.5ms\nvideo 1/1 (frame 332/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 1 person, 87.2ms\nvideo 1/1 (frame 333/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 65.1ms\nvideo 1/1 (frame 334/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 71.8ms\nvideo 1/1 (frame 335/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 70.9ms\nvideo 1/1 (frame 336/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 70.7ms\nvideo 1/1 (frame 337/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 71.0ms\nvideo 1/1 (frame 338/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 71.2ms\nvideo 1/1 (frame 339/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 62.1ms\nvideo 1/1 (frame 340/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 75.4ms\nvideo 1/1 (frame 341/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.7ms\nvideo 1/1 (frame 342/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.6ms\nvideo 1/1 (frame 343/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.5ms\nvideo 1/1 (frame 344/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 67.8ms\nvideo 1/1 (frame 345/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 61.3ms\nvideo 1/1 (frame 346/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 91.1ms\nvideo 1/1 (frame 347/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 86.7ms\nvideo 1/1 (frame 348/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 87.1ms\nvideo 1/1 (frame 349/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 104.8ms\nvideo 1/1 (frame 350/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 95.5ms\nvideo 1/1 (frame 351/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 119.9ms\nvideo 1/1 (frame 352/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 110.6ms\nvideo 1/1 (frame 353/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 122.9ms\nvideo 1/1 (frame 354/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 106.3ms\nvideo 1/1 (frame 355/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 93.8ms\nvideo 1/1 (frame 356/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 92.1ms\nvideo 1/1 (frame 357/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 89.2ms\nvideo 1/1 (frame 358/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 78.5ms\nvideo 1/1 (frame 359/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 108.4ms\nvideo 1/1 (frame 360/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 103.1ms\nvideo 1/1 (frame 361/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.4ms\nvideo 1/1 (frame 362/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 90.9ms\nvideo 1/1 (frame 363/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 89.2ms\nvideo 1/1 (frame 364/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 88.9ms\nvideo 1/1 (frame 365/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 109.2ms\nvideo 1/1 (frame 366/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 93.9ms\nvideo 1/1 (frame 367/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 124.3ms\nvideo 1/1 (frame 368/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 119.0ms\nvideo 1/1 (frame 369/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 139.3ms\nvideo 1/1 (frame 370/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 120.6ms\nvideo 1/1 (frame 371/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 106.0ms\nvideo 1/1 (frame 372/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 96.3ms\nvideo 1/1 (frame 373/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 91.0ms\nvideo 1/1 (frame 374/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 86.2ms\nvideo 1/1 (frame 375/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 104.8ms\nvideo 1/1 (frame 376/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 93.2ms\nvideo 1/1 (frame 377/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 125.7ms\nvideo 1/1 (frame 378/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 117.1ms\nvideo 1/1 (frame 379/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 130.8ms\nvideo 1/1 (frame 380/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 115.9ms\nvideo 1/1 (frame 381/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 98.7ms\nvideo 1/1 (frame 382/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 93.4ms\nvideo 1/1 (frame 383/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 98.5ms\nvideo 1/1 (frame 384/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 69.1ms\nvideo 1/1 (frame 385/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 76.0ms\nvideo 1/1 (frame 386/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.3ms\nvideo 1/1 (frame 387/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 71.8ms\nvideo 1/1 (frame 388/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 81.4ms\nvideo 1/1 (frame 389/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 75.0ms\nvideo 1/1 (frame 390/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 75.0ms\nvideo 1/1 (frame 391/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 84.3ms\nvideo 1/1 (frame 392/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 80.7ms\nvideo 1/1 (frame 393/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 94.5ms\nvideo 1/1 (frame 394/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 91.3ms\nvideo 1/1 (frame 395/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 87.3ms\nvideo 1/1 (frame 396/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 103.0ms\nvideo 1/1 (frame 397/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 83.7ms\nvideo 1/1 (frame 398/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 76.0ms\nvideo 1/1 (frame 399/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 75.7ms\nvideo 1/1 (frame 400/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.7ms\nvideo 1/1 (frame 401/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 77.1ms\nvideo 1/1 (frame 402/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 70.1ms\nvideo 1/1 (frame 403/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 71.4ms\nvideo 1/1 (frame 404/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.3ms\nvideo 1/1 (frame 405/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 75.9ms\nvideo 1/1 (frame 406/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 77.4ms\nvideo 1/1 (frame 407/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 75.3ms\nvideo 1/1 (frame 408/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 87.8ms\nvideo 1/1 (frame 409/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 98.3ms\nvideo 1/1 (frame 410/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 99.7ms\nvideo 1/1 (frame 411/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 103.0ms\nvideo 1/1 (frame 412/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 110.3ms\nvideo 1/1 (frame 413/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 99.7ms\nvideo 1/1 (frame 414/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 100.3ms\nvideo 1/1 (frame 415/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 97.2ms\nvideo 1/1 (frame 416/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 102.1ms\nvideo 1/1 (frame 417/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 98.1ms\nvideo 1/1 (frame 418/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 104.6ms\nvideo 1/1 (frame 419/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 98.2ms\nvideo 1/1 (frame 420/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 110.9ms\nvideo 1/1 (frame 421/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 122.7ms\nvideo 1/1 (frame 422/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 98.4ms\nvideo 1/1 (frame 423/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 110.7ms\nvideo 1/1 (frame 424/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 121.1ms\nvideo 1/1 (frame 425/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 133.1ms\nvideo 1/1 (frame 426/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 112.1ms\nvideo 1/1 (frame 427/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 109.6ms\nvideo 1/1 (frame 428/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 92.0ms\nvideo 1/1 (frame 429/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 80.8ms\nvideo 1/1 (frame 430/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 79.7ms\nvideo 1/1 (frame 431/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 85.2ms\nvideo 1/1 (frame 432/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 95.6ms\nvideo 1/1 (frame 433/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 84.6ms\nvideo 1/1 (frame 434/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 88.7ms\nvideo 1/1 (frame 435/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 103.2ms\nvideo 1/1 (frame 436/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 101.8ms\nvideo 1/1 (frame 437/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 116.0ms\nvideo 1/1 (frame 438/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 109.9ms\nvideo 1/1 (frame 439/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 108.1ms\nvideo 1/1 (frame 440/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 109.7ms\nvideo 1/1 (frame 441/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 83.9ms\nvideo 1/1 (frame 442/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 81.7ms\nvideo 1/1 (frame 443/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 77.3ms\nvideo 1/1 (frame 444/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 76.8ms\nvideo 1/1 (frame 445/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 86.0ms\nvideo 1/1 (frame 446/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 78.6ms\nvideo 1/1 (frame 447/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 80.2ms\nvideo 1/1 (frame 448/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 173.0ms\nvideo 1/1 (frame 449/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 175.9ms\nvideo 1/1 (frame 450/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 183.1ms\nvideo 1/1 (frame 451/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 138.0ms\nvideo 1/1 (frame 452/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 144.6ms\nvideo 1/1 (frame 453/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 158.8ms\nvideo 1/1 (frame 454/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 119.4ms\nvideo 1/1 (frame 455/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 293.7ms\nvideo 1/1 (frame 456/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 153.3ms\nvideo 1/1 (frame 457/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 98.1ms\nvideo 1/1 (frame 458/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 106.7ms\nvideo 1/1 (frame 459/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 77.5ms\nvideo 1/1 (frame 460/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 107.1ms\nvideo 1/1 (frame 461/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.6ms\nvideo 1/1 (frame 462/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 106.9ms\nvideo 1/1 (frame 463/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 80.5ms\nvideo 1/1 (frame 464/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 94.5ms\nvideo 1/1 (frame 465/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 83.6ms\nvideo 1/1 (frame 466/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 92.4ms\nvideo 1/1 (frame 467/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 81.0ms\nvideo 1/1 (frame 468/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 75.1ms\nvideo 1/1 (frame 469/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 62.6ms\nvideo 1/1 (frame 470/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 77.9ms\nvideo 1/1 (frame 471/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 82.1ms\nvideo 1/1 (frame 472/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.7ms\nvideo 1/1 (frame 473/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 95.4ms\nvideo 1/1 (frame 474/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.6ms\nvideo 1/1 (frame 475/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 81.9ms\nvideo 1/1 (frame 476/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 85.2ms\nvideo 1/1 (frame 477/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 69.9ms\nvideo 1/1 (frame 478/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 71.1ms\nvideo 1/1 (frame 479/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.3ms\nvideo 1/1 (frame 480/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 88.7ms\nvideo 1/1 (frame 481/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.1ms\nvideo 1/1 (frame 482/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 88.1ms\nvideo 1/1 (frame 483/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 79.0ms\nvideo 1/1 (frame 484/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 80.5ms\nvideo 1/1 (frame 485/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 89.1ms\nvideo 1/1 (frame 486/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 76.2ms\nvideo 1/1 (frame 487/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 93.8ms\nvideo 1/1 (frame 488/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 91.9ms\nvideo 1/1 (frame 489/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 75.2ms\nvideo 1/1 (frame 490/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 84.4ms\nvideo 1/1 (frame 491/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 82.3ms\nvideo 1/1 (frame 492/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 176.2ms\nvideo 1/1 (frame 493/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 81.7ms\nvideo 1/1 (frame 494/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 80.9ms\nvideo 1/1 (frame 495/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 71.4ms\nvideo 1/1 (frame 496/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.1ms\nvideo 1/1 (frame 497/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.3ms\nvideo 1/1 (frame 498/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 71.9ms\nvideo 1/1 (frame 499/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 70.6ms\nvideo 1/1 (frame 500/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 66.7ms\nvideo 1/1 (frame 501/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 61.4ms\nvideo 1/1 (frame 502/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 76.6ms\nvideo 1/1 (frame 503/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 67.9ms\nvideo 1/1 (frame 504/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.4ms\nvideo 1/1 (frame 505/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 72.7ms\nvideo 1/1 (frame 506/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 70.9ms\nvideo 1/1 (frame 507/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 68.2ms\nvideo 1/1 (frame 508/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 74.2ms\nvideo 1/1 (frame 509/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 60.8ms\nvideo 1/1 (frame 510/510) /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/lab_images/people-walking.mp4: 384x640 (no detections), 73.0ms\nSpeed: 3.7ms preprocess, 83.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\nResults saved to runs/detect/track\nTracking completo! V\u00eddeo salvo em runs/detect/track/\n</pre>"},{"location":"aulas/IA/lab11/yolo1/#yolo","title":"Yolo\u00b6","text":"<p>Neste notebook, exploraremos t\u00e9cnicas avan\u00e7adas do YOLO utilizando a biblioteca Ultralytics, incluindo:</p> <ol> <li>Tarefas m\u00faltiplas (detec\u00e7\u00e3o, segmenta\u00e7\u00e3o e classifica\u00e7\u00e3o)</li> <li>Tracking de objetos em v\u00eddeos</li> </ol>"},{"location":"aulas/IA/lab11/yolo1/#1-explorando-diferentes-modelos-e-tarefas","title":"1. Explorando diferentes modelos e tarefas\u00b6","text":"<p>YOLO v8 suporta m\u00faltiplas tarefas al\u00e9m da detec\u00e7\u00e3o de objetos:</p>"},{"location":"aulas/IA/lab11/yolo1/#comparacao-de-diferentes-tarefas-em-uma-mesma-imagem","title":"Compara\u00e7\u00e3o de diferentes tarefas em uma mesma imagem\u00b6","text":""},{"location":"aulas/IA/lab11/yolo1/#9-tracking-de-objetos-em-videos","title":"9. Tracking de objetos em v\u00eddeos\u00b6","text":"<p>O YOLO tamb\u00e9m pode ser utilizado para tracking de objetos em v\u00eddeos.</p>"},{"location":"aulas/IA/lab11/yolo2/","title":"Yolo2","text":"In\u00a0[\u00a0]: Copied! <pre>from ultralytics import YOLO\nimport ultralytics\n\nmodel = YOLO('yolov11n.pt')\n</pre> from ultralytics import YOLO import ultralytics  model = YOLO('yolov11n.pt') In\u00a0[\u00a0]: Copied! <pre># certifique se que o reposit\u00f3rio do dataset est\u00e1 clonado\n# e que o diret\u00f3rio 'dataset-pothole' est\u00e1 presente\n\n# !git clone https://github.com/michelpf/dataset-pothole\n</pre> # certifique se que o reposit\u00f3rio do dataset est\u00e1 clonado # e que o diret\u00f3rio 'dataset-pothole' est\u00e1 presente  # !git clone https://github.com/michelpf/dataset-pothole In\u00a0[\u00a0]: Copied! <pre>!touch model/configs_modelo.yaml\n</pre> !touch model/configs_modelo.yaml In\u00a0[12]: Copied! <pre>%%writefile configs_modelo.yaml\npath: 'dataset-pothole/dataset'\ntrain: 'train/'\nval: 'test/'\n\nnc: 1\nnames: [\"pothole\"]\n</pre> %%writefile configs_modelo.yaml path: 'dataset-pothole/dataset' train: 'train/' val: 'test/'  nc: 1 names: [\"pothole\"] <pre>Overwriting configs_modelo.yaml\n</pre> In\u00a0[13]: Copied! <pre>arquivo_config  = \"configs_modelo.yaml\"\n</pre>  arquivo_config  = \"configs_modelo.yaml\" <p>O treinamento do modelo tamb\u00e9m realiza em conjunto a valida\u00e7\u00e3o com os dados separados do treinamento.</p> In\u00a0[15]: Copied! <pre># Configura\u00e7\u00f5es de treinamento\nEPOCHS = 50  # Aumentado para melhor converg\u00eancia\nIMG_SIZE = 640  # Tamanho padr\u00e3o do YOLOv8\nBATCH_SIZE = 16\nLEARNING_RATE = 0.01\n\n# Hiperpar\u00e2metros espec\u00edficos\nhyperparameters = {\n    'epochs': EPOCHS,\n    'imgsz': IMG_SIZE,\n    'batch': BATCH_SIZE,\n    'lr0': LEARNING_RATE,\n    'workers': 8,\n    'patience': 10,  # Early stopping\n    'save_period': 10,  # Salvar checkpoint a cada 10 \u00e9pocas\n    'cache': True,  # Cache das imagens\n    'mosaic': 1.0,  # Data augmentation\n    'mixup': 0.1,\n    'copy_paste': 0.1\n}\n\n\nresultados = model.train(\n    data=arquivo_config,\n    name='yolov8_pothole_custom',\n    **hyperparameters\n)\n\n# treinamento simples \n\n# resultados = model.train(data=arquivo_config, epochs=3, imgsz=720, name='yolov8_pothole')\n</pre> # Configura\u00e7\u00f5es de treinamento EPOCHS = 50  # Aumentado para melhor converg\u00eancia IMG_SIZE = 640  # Tamanho padr\u00e3o do YOLOv8 BATCH_SIZE = 16 LEARNING_RATE = 0.01  # Hiperpar\u00e2metros espec\u00edficos hyperparameters = {     'epochs': EPOCHS,     'imgsz': IMG_SIZE,     'batch': BATCH_SIZE,     'lr0': LEARNING_RATE,     'workers': 8,     'patience': 10,  # Early stopping     'save_period': 10,  # Salvar checkpoint a cada 10 \u00e9pocas     'cache': True,  # Cache das imagens     'mosaic': 1.0,  # Data augmentation     'mixup': 0.1,     'copy_paste': 0.1 }   resultados = model.train(     data=arquivo_config,     name='yolov8_pothole_custom',     **hyperparameters )  # treinamento simples   # resultados = model.train(data=arquivo_config, epochs=3, imgsz=720, name='yolov8_pothole') <pre>New https://pypi.org/project/ultralytics/8.3.153 available \ud83d\ude03 Update with 'pip install -U ultralytics'\nUltralytics 8.3.134 \ud83d\ude80 Python-3.9.6 torch-2.3.0 CPU (Apple M2)\nengine/trainer: agnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=configs_modelo.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=3, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=720, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8_pothole2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/yolov8_pothole2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \nModel summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\nWARNING \u26a0\ufe0f imgsz=[720] must be multiple of max stride 32, updating to [736]\ntrain: Fast image access \u2705 (ping: 0.0\u00b10.0 ms, read: 110.6\u00b143.8 MB/s, size: 31.9 KB)\n</pre> <pre>train: Scanning /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/dataset-pothole/dataset/train... 1562 images, 0 backgrounds, 0 corrupt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1562/1562 [00:00&lt;00:00, 2661.57it/s]\n</pre> <pre>train: New cache created: /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/dataset-pothole/dataset/train.cache\nval: Fast image access \u2705 (ping: 0.0\u00b10.0 ms, read: 121.4\u00b122.3 MB/s, size: 31.1 KB)\n</pre> <pre>val: Scanning /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/dataset-pothole/dataset/test... 421 images, 0 backgrounds, 0 corrupt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 421/421 [00:00&lt;00:00, 2937.88it/s]</pre> <pre>val: New cache created: /Users/arnaldoalvesvianajunior/shift-fiap/10-lab18-yolo/dataset-pothole/dataset/test.cache\n</pre> <pre>\n</pre> <pre>Plotting labels to runs/detect/yolov8_pothole2/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n</pre> <pre>2025/06/11 17:29:58 INFO mlflow.tracking.fluent: Experiment with name '/Shared/Ultralytics' does not exist. Creating a new experiment.\n2025/06/11 17:29:59 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n2025/06/11 17:29:59 WARNING mlflow.utils.autologging_utils: MLflow transformers autologging is known to be compatible with 4.35.2 &lt;= transformers &lt;= 4.51.2, but the installed version is 4.51.3. If you encounter errors during autologging, try upgrading / downgrading transformers to a compatible version, or try upgrading MLflow.\n2025/06/11 17:30:02 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n2025/06/11 17:30:09 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n2025/06/11 17:30:09 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n2025/06/11 17:30:10 INFO mlflow.tracking.fluent: Autologging successfully enabled for transformers.\n</pre> <pre>MLflow: logging run_id(9f8d1c2250d941ff880ff4c5d5fa221e) to runs/mlflow\nMLflow: view at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs/mlflow'\nMLflow: disable with 'yolo settings mlflow=False'\nImage sizes 736 train, 736 val\nUsing 0 dataloader workers\nLogging results to runs/detect/yolov8_pothole2\nStarting training for 3 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n</pre> <pre>        1/3         0G      2.269      3.503      2.163         52        736:   5%|\u258c         | 5/98 [01:48&lt;33:38, 21.70s/it]\n</pre> <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 resultados = model.train(data=arquivo_config, epochs=3, imgsz=720, name='yolov8_pothole')\n\nFile ~/Library/Python/3.9/lib/python/site-packages/ultralytics/engine/model.py:793, in Model.train(self, trainer, **kwargs)\n    790     self.model = self.trainer.model\n    792 self.trainer.hub_session = self.session  # attach optional HUB session\n--&gt; 793 self.trainer.train()\n    794 # Update model and cfg after training\n    795 if RANK in {-1, 0}:\n\nFile ~/Library/Python/3.9/lib/python/site-packages/ultralytics/engine/trainer.py:211, in BaseTrainer.train(self)\n    208         ddp_cleanup(self, str(file))\n    210 else:\n--&gt; 211     self._do_train(world_size)\n\nFile ~/Library/Python/3.9/lib/python/site-packages/ultralytics/engine/trainer.py:399, in BaseTrainer._do_train(self, world_size)\n    394     self.tloss = (\n    395         (self.tloss * i + self.loss_items) / (i + 1) if self.tloss is not None else self.loss_items\n    396     )\n    398 # Backward\n--&gt; 399 self.scaler.scale(self.loss).backward()\n    401 # Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\n    402 if ni - last_opt_step &gt;= self.accumulate:\n\nFile ~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:525, in Tensor.backward(self, gradient, retain_graph, create_graph, inputs)\n    515 if has_torch_function_unary(self):\n    516     return handle_torch_function(\n    517         Tensor.backward,\n    518         (self,),\n   (...)\n    523         inputs=inputs,\n    524     )\n--&gt; 525 torch.autograd.backward(\n    526     self, gradient, retain_graph, create_graph, inputs=inputs\n    527 )\n\nFile ~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:267, in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\n    262     retain_graph = create_graph\n    264 # The reason we repeat the same comment below is that\n    265 # some Python versions print out the first line of a multi-line function\n    266 # calls in the traceback and some print out the last line\n--&gt; 267 _engine_run_backward(\n    268     tensors,\n    269     grad_tensors_,\n    270     retain_graph,\n    271     create_graph,\n    272     inputs,\n    273     allow_unreachable=True,\n    274     accumulate_grad=True,\n    275 )\n\nFile ~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:744, in _engine_run_backward(t_outputs, *args, **kwargs)\n    742     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n    743 try:\n--&gt; 744     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    745         t_outputs, *args, **kwargs\n    746     )  # Calls into the C++ engine to run the backward pass\n    747 finally:\n    748     if attach_logging_hooks:\n\nKeyboardInterrupt: </pre> In\u00a0[\u00a0]: Copied! <pre>import cv2\nimport matplotlib.pyplot as plt\n\nimage = cv2.imread(\"runs/detect/yolov8_pothole/F1_curve.png\", cv2.IMREAD_COLOR)\nplt.imshow(image)\n</pre> import cv2 import matplotlib.pyplot as plt  image = cv2.imread(\"runs/detect/yolov8_pothole/F1_curve.png\", cv2.IMREAD_COLOR) plt.imshow(image) In\u00a0[\u00a0]: Copied! <pre>image = cv2.imread(\"runs/detect/yolov8_pothole/results.png\", cv2.IMREAD_COLOR)\nplt.figure(figsize=(10,5))\nplt.imshow(image)\n</pre> image = cv2.imread(\"runs/detect/yolov8_pothole/results.png\", cv2.IMREAD_COLOR) plt.figure(figsize=(10,5)) plt.imshow(image) In\u00a0[\u00a0]: Copied! <pre>dir_resultados = \"runs/detect/yolov8_pothole5/weights/best.pt\"\n</pre> dir_resultados = \"runs/detect/yolov8_pothole5/weights/best.pt\" In\u00a0[\u00a0]: Copied! <pre>image = cv2.imread(\"imagens/buraco2.jpeg\")\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</pre> image = cv2.imread(\"imagens/buraco2.jpeg\") image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) In\u00a0[\u00a0]: Copied! <pre>model = YOLO(dir_resultados)\nresults = model.predict(source=image, conf=0.25)\n</pre> model = YOLO(dir_resultados) results = model.predict(source=image, conf=0.25) In\u00a0[\u00a0]: Copied! <pre>image_result = results[0].plot()\n\nplt.imshow(image_result)\n</pre> image_result = results[0].plot()  plt.imshow(image_result) In\u00a0[\u00a0]: Copied! <pre>image = cv2.imread(\"imagens/buraco.jpg\")\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</pre> image = cv2.imread(\"imagens/buraco.jpg\") image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) In\u00a0[\u00a0]: Copied! <pre>results = model.predict(source=image, conf=0.25)\n</pre> results = model.predict(source=image, conf=0.25) In\u00a0[\u00a0]: Copied! <pre>image_result = results[0].plot()\n\nplt.imshow(image_result)\n</pre> image_result = results[0].plot()  plt.imshow(image_result)"},{"location":"aulas/IA/lab11/yolo2/#treinamento-de-modelo-customizado","title":"Treinamento de modelo customizado\u00b6","text":"<p>Para treinar o modelo YOLOv8 basta usar apenas o comando train. Os par\u00e2metros necess\u00e1rios para essa fun\u00e7\u00e3o s\u00e3o:</p> <ul> <li>configura\u00e7\u00e3o de um arquivo yaml com as informa\u00e7\u00f5es de caminho das imagens e classes</li> <li>\u00e9pocas</li> <li>tamanho da imagem</li> <li>nome do seu modelo</li> </ul> <p>Neste exemplo vamos treinar um modelo capaz de detectar buracos em ruas e estradas.</p> <p>Utilizaremos o dataset Potholes Detection for YOLOV4 que foi organizado no reposit\u00f3rio abaixo.</p> <p>Os datasets padr\u00e3o YOLO s\u00e3o compat\u00edveis com todas as vers\u00f5es, mesmo este dataset originalmente ter sido utilizado para a vers\u00e3o V4.-</p> <p>Referencia: github/michelpf/fiap-ml-visao-computacional</p>"},{"location":"aulas/IA/lab11/yolo2/#clonagem-e-preparacao-do-dataset","title":"Clonagem e Prepara\u00e7\u00e3o do Dataset\u00b6","text":"<p>O dataset utilizado cont\u00e9m imagens de buracos anotadas no formato YOLO, originalmente desenvolvido para YOLOv4 mas compat\u00edvel com todas as vers\u00f5es YOLO.</p> <p>Estrutura esperada:</p> <pre><code>dataset-pothole/\n\u251c\u2500\u2500 dataset/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u2514\u2500\u2500 labels/\n\u2502   \u2514\u2500\u2500 test/\n\u2502       \u251c\u2500\u2500 images/\n\u2502       \u2514\u2500\u2500 labels/\n</code></pre>"},{"location":"aulas/IA/lab11/yolo2/#criacao-do-arquivo-de-configuracao","title":"Cria\u00e7\u00e3o do Arquivo de Configura\u00e7\u00e3o\u00b6","text":""},{"location":"aulas/IA/lab12/inferencia/","title":"Testando infer\u00eancia no TensorFlow Serving via REST API","text":"In\u00a0[1]: Copied! <pre># Importando depend\u00eancias\nimport numpy as np\nimport requests\nimport json\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n</pre> # Importando depend\u00eancias import numpy as np import requests import json from tensorflow import keras import matplotlib.pyplot as plt In\u00a0[2]: Copied! <pre># Carregar dados de teste\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_test = x_test / 255.0\nprint(f'Tamanho do x_test: {x_test.shape}')\n</pre> # Carregar dados de teste (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() x_test = x_test / 255.0 print(f'Tamanho do x_test: {x_test.shape}') <pre>Tamanho do x_test: (10000, 28, 28)\n</pre> In\u00a0[3]: Copied! <pre>def predict_mnist_rest(images, url=\"http://localhost:8501/v1/models/mnist:predict\"):\n    # images: array shape (N,28,28)\n    payload = json.dumps({\"instances\": images.tolist()})\n    response = requests.post(url, data=payload, headers={\"content-type\": \"application/json\"})\n    result = response.json()\n    return np.array(result[\"predictions\"])\n</pre> def predict_mnist_rest(images, url=\"http://localhost:8501/v1/models/mnist:predict\"):     # images: array shape (N,28,28)     payload = json.dumps({\"instances\": images.tolist()})     response = requests.post(url, data=payload, headers={\"content-type\": \"application/json\"})     result = response.json()     return np.array(result[\"predictions\"]) In\u00a0[4]: Copied! <pre>idx = 2093\nimg = x_test[idx:idx+1]  # shape (1,28,28)\npred = predict_mnist_rest(img)\npred_label = np.argmax(pred[0])\nprint(f\"Predito: {pred_label} | Real: {y_test[idx]}\")\nplt.imshow(x_test[idx], cmap='gray')\nplt.title(f\"Predito: {pred_label} | Real: {y_test[idx]}\")\nplt.axis('off')\nplt.show()\n</pre> idx = 2093 img = x_test[idx:idx+1]  # shape (1,28,28) pred = predict_mnist_rest(img) pred_label = np.argmax(pred[0]) print(f\"Predito: {pred_label} | Real: {y_test[idx]}\") plt.imshow(x_test[idx], cmap='gray') plt.title(f\"Predito: {pred_label} | Real: {y_test[idx]}\") plt.axis('off') plt.show() <pre>Predito: 8 | Real: 8\n</pre> In\u00a0[10]: Copied! <pre>batch = x_test[44:200]\ntrue_labels = y_test[44:200]\npreds = predict_mnist_rest(batch)\npred_labels = np.argmax(preds, axis=1)\n\nprint(\"Labels preditos:\", pred_labels)\nprint(\"Labels reais   :\", true_labels)\nacc = np.mean(pred_labels == true_labels)\nprint(f\"Acur\u00e1cia neste batch: {acc:.2%}\")\n</pre> batch = x_test[44:200] true_labels = y_test[44:200] preds = predict_mnist_rest(batch) pred_labels = np.argmax(preds, axis=1)  print(\"Labels preditos:\", pred_labels) print(\"Labels reais   :\", true_labels) acc = np.mean(pred_labels == true_labels) print(f\"Acur\u00e1cia neste batch: {acc:.2%}\") <pre>Labels preditos: [3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9 1 7 3 2 9 7 7\n 6 2 7 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9 6 0 5 4 9 9 2 1 9 4 8 7 3 9 7 4 4 4\n 9 2 5 4 7 6 7 9 0 5 8 5 6 6 5 7 8 1 0 1 6 4 6 7 3 1 7 1 8 2 0 2 9 9 5 5 1\n 5 6 0 3 4 4 6 5 4 6 5 4 5 1 4 4 7 2 3 2 7 1 8 1 8 1 8 5 0 8 9 2 5 0 1 1 1\n 0 9 0 3 1 6 4 2]\nLabels reais   : [3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9 1 7 3 2 9 7 7\n 6 2 7 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9 6 0 5 4 9 9 2 1 9 4 8 7 3 9 7 4 4 4\n 9 2 5 4 7 6 7 9 0 5 8 5 6 6 5 7 8 1 0 1 6 4 6 7 3 1 7 1 8 2 0 2 9 9 5 5 1\n 5 6 0 3 4 4 6 5 4 6 5 4 5 1 4 4 7 2 3 2 7 1 8 1 8 1 8 5 0 8 9 2 5 0 1 1 1\n 0 9 0 3 1 6 4 2]\nAcur\u00e1cia neste batch: 100.00%\n</pre> In\u00a0[11]: Copied! <pre>erros = np.where(pred_labels != true_labels)[0]\nif len(erros) == 0:\n    print(\"Sem erros neste batch!\")\nelse:\n    for i in erros:\n        plt.imshow(batch[i], cmap='gray')\n        plt.title(f\"Predito: {pred_labels[i]} | Real: {true_labels[i]}\")\n        plt.axis('off')\n        plt.show()\n</pre> erros = np.where(pred_labels != true_labels)[0] if len(erros) == 0:     print(\"Sem erros neste batch!\") else:     for i in erros:         plt.imshow(batch[i], cmap='gray')         plt.title(f\"Predito: {pred_labels[i]} | Real: {true_labels[i]}\")         plt.axis('off')         plt.show() <pre>Sem erros neste batch!\n</pre>"},{"location":"aulas/IA/lab12/inferencia/#testando-inferencia-no-tensorflow-serving-via-rest-api","title":"Testando infer\u00eancia no TensorFlow Serving via REST API\u00b6","text":"<p>Este notebook consome o modelo MNIST servido localmente via TensorFlow Serving REST (Bitnami).</p>"},{"location":"aulas/IA/lab12/inferencia/#carregando-dados-de-teste-mnist","title":"Carregando dados de teste (MNIST)\u00b6","text":""},{"location":"aulas/IA/lab12/inferencia/#funcao-para-inferencia-via-rest-api","title":"Fun\u00e7\u00e3o para infer\u00eancia via REST API\u00b6","text":""},{"location":"aulas/IA/lab12/inferencia/#testando-uma-unica-imagem","title":"Testando uma \u00fanica imagem\u00b6","text":""},{"location":"aulas/IA/lab12/inferencia/#testando-varias-imagens-batch-prediction","title":"Testando v\u00e1rias imagens (batch prediction)\u00b6","text":""},{"location":"aulas/IA/lab12/inferencia/#visualizando-erros-se-houver","title":"Visualizando erros (se houver)\u00b6","text":""},{"location":"aulas/IA/lab12/main/","title":"salva o modelo no formato HDF5","text":"In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nfrom tensorflow import keras\n</pre> import tensorflow as tf from tensorflow import keras In\u00a0[\u00a0]: Copied! <pre>(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n</pre> (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 In\u00a0[\u00a0]: Copied! <pre>model = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\n</pre> model = keras.Sequential([     keras.layers.Flatten(input_shape=(28, 28)),     keras.layers.Dense(128, activation='relu'),     keras.layers.Dense(10, activation='softmax') ]) In\u00a0[\u00a0]: Copied! <pre>model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n</pre> model.compile(     optimizer='adam',     loss='sparse_categorical_crossentropy',     metrics=['accuracy'] ) In\u00a0[\u00a0]: Copied! <pre>model.fit(x_train, y_train, epochs=2, validation_split=0.1)\n</pre> model.fit(x_train, y_train, epochs=2, validation_split=0.1) In\u00a0[\u00a0]: Copied! <pre># Salve o modelo no formato SavedModel (obrigat\u00f3rio pro TensorFlow Serving)\nmodel.export(\"mnist_model/1\")\n</pre> # Salve o modelo no formato SavedModel (obrigat\u00f3rio pro TensorFlow Serving) model.export(\"mnist_model/1\")"},{"location":"aulas/IA/lab12/main/#salva-o-modelo-no-formato-hdf5","title":"salva o modelo no formato HDF5\u00b6","text":"<p>model.save(\"mnist_model.h5\") ## mais antigo</p>"},{"location":"aulas/IA/lab12/main/#salve-o-modelo-no-formato-keras","title":"salve o modelo no formato keras\u00b6","text":"<p>model.save(\"mnist_model.keras\") ## mais novo</p>"},{"location":"aulas/IA/lab12/readme/","title":"TensorFlow Serving - End-to-End","text":"<p>Projeto exemplo de treinamento, exporta\u00e7\u00e3o e deploy de um modelo MNIST com TensorFlow 2 e serving via Docker (Bitnami ou oficial).</p>"},{"location":"aulas/IA/lab12/readme/#estrutura-do-projeto","title":"Estrutura do Projeto","text":"<p><pre><code>.\n\u251c\u2500\u2500 main.py               # Script de treinamento e exporta\u00e7\u00e3o do modelo\n\u251c\u2500\u2500 mnist_model/          # Diret\u00f3rio do modelo exportado (formato SavedModel)\n\u2502   \u2514\u2500\u2500 1/\n\u2502       \u251c\u2500\u2500 assets\n\u2502       \u251c\u2500\u2500 fingerprint.pb\n\u2502       \u251c\u2500\u2500 saved_model.pb\n\u2502       \u2514\u2500\u2500 variables/\n\u2502           \u251c\u2500\u2500 variables.data-00000-of-00001\n\u2502           \u2514\u2500\u2500 variables.index\n\u251c\u2500\u2500 teste.py              # Script Python para testar a API REST\n\u2514\u2500\u2500 test_inference.ipynb  # (Opcional) Notebook de teste via API REST\n</code></pre> Fa\u00e7a o download dos arquivos do projeto: </p> <ul> <li>arquivo main.py</li> <li>arquivo teste.py</li> <li>arquivo inferencia.ipynb</li> </ul>"},{"location":"aulas/IA/lab12/readme/#1-treinando-e-exportando-o-modelo","title":"1. Treinando e exportando o modelo","text":"<pre><code>python3 main.py\n</code></pre> <p>Isso gera a pasta <code>mnist_model/1/</code> com o modelo no formato correto para serving.</p>"},{"location":"aulas/IA/lab12/readme/#2-servindo-o-modelo-com-docker","title":"2. Servindo o modelo com Docker","text":"<p>Comando completo:</p> <pre><code>docker run -d --name tfserving_mnist -p 8501:8501 \\\n  -v \"$PWD/mnist_model:/models/mnist\" \\\n  tensorflow/serving \\\n  --rest_api_port=8501 \\\n  --model_name=mnist \\\n  --model_base_path=/models/mnist\n</code></pre>"},{"location":"aulas/IA/lab12/readme/#importante","title":"IMPORTANTE:","text":"<p>No Mac M1/M2 (ARM64): se der erro, adicione a flag <code>--platform=linux/arm64/v8</code> ap\u00f3s o docker run.</p> <p>No Windows, Linux, Mac Intel: n\u00e3o precisa dessa flag.</p> <p>Voc\u00ea pode usar a imagem oficial (tensorflow/serving) ou Bitnami (bitnami/tensorflow-serving:latest). Ambas s\u00e3o compat\u00edveis.</p> <p>Comando completo para Mac M1/M2:</p> <pre><code>docker run --platform=linux/arm64/v8 -d --name tfserving_mnist -p 8501:8501 \\\n  -v \"$PWD/mnist_model:/bitnami/tensorflow-serving/models/mnist\" \\\n  bitnami/tensorflow-serving:latest \\\n  tensorflow_model_server \\\n    --rest_api_port=8501 \\\n    --model_name=mnist \\\n    --model_base_path=/bitnami/tensorflow-serving/models/mnist\n</code></pre>"},{"location":"aulas/IA/lab12/readme/#explicacao-do-comando-linha-a-linha","title":"Explica\u00e7\u00e3o do comando, linha a linha:","text":"<ul> <li><code>docker run</code>: Cria e executa um novo container.</li> <li><code>--platform=linux/arm64/v8</code>: Especifica a arquitetura ARM64 (necess\u00e1rio em Macs M1/M2).</li> <li><code>-d</code>: Roda em segundo plano (background).</li> <li><code>--name tfserving_mnist</code>: D\u00e1 um nome ao container para facilitar logs, remo\u00e7\u00e3o etc.</li> <li><code>-p 8501:8501</code>: Mapeia a porta 8501 do container para a 8501 do host (API REST).</li> <li><code>-v \"$PWD/mnist_model:/bitnami/tensorflow-serving/models/mnist\"</code>: Monta o diret\u00f3rio local do modelo dentro do container no local esperado pelo TensorFlow Serving.</li> <li><code>bitnami/tensorflow-serving:latest</code>: Imagem Docker utilizada.</li> <li><code>tensorflow_model_server</code>: Comando que roda o servidor do TensorFlow Serving.</li> <li><code>--rest_api_port=8501</code>: Porta REST da API.</li> <li><code>--model_name=mnist</code>: Nome l\u00f3gico do modelo (usado nos endpoints).</li> <li><code>--model_base_path=/bitnami/tensorflow-serving/models/mnist</code>:Caminho do modelo dentro do container (deve bater com o volume acima).</li> </ul>"},{"location":"aulas/IA/lab12/readme/#3-verificando-o-status-do-modelo","title":"3. Verificando o status do modelo","text":"<p>Ap\u00f3s rodar o container, teste no terminal:</p> <pre><code>curl http://localhost:8501/v1/models/mnist\n</code></pre> <p>Deve retornar:</p> <pre><code>{\n \"model_version_status\": [\n  {\n   \"version\": \"1\",\n   \"state\": \"AVAILABLE\",\n   \"status\": {\n    \"error_code\": \"OK\",\n    \"error_message\": \"\"\n   }\n  }\n ]\n}\n</code></pre>"},{"location":"aulas/IA/lab12/readme/#4-testando-a-inferencia-com-python","title":"4. Testando a infer\u00eancia com Python","text":"<p>Use o script <code>teste.py</code>:</p> <pre><code>python3 teste.py\n</code></pre> <p>Sa\u00edda esperada (exemplo):</p> <pre><code>Predito: 7 Verdadeiro: 7\n</code></pre>"},{"location":"aulas/IA/lab12/readme/#5-teste-via-notebook","title":"5. Teste via notebook","text":"<p>Rode o <code>test_inference.ipynb</code> para explorar batch prediction, visualiza\u00e7\u00e3o, an\u00e1lise de erro, etc.</p>"},{"location":"aulas/IA/lab12/readme/#observacoes","title":"Observa\u00e7\u00f5es","text":"<ul> <li>Para atualizar o modelo, basta exportar uma nova vers\u00e3o na pasta <code>mnist_model/2/</code> e reiniciar o container.</li> <li>Se quiser automatizar o deploy, use um script <code>deploy.sh</code>.</li> <li>Para projetos maiores, considere Docker Compose e/ou integra\u00e7\u00e3o com aplica\u00e7\u00f5es web.</li> </ul>"},{"location":"aulas/IA/lab12/teste/","title":"Teste","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport requests\nimport json\n</pre> import numpy as np import requests import json In\u00a0[\u00a0]: Copied! <pre># Carregue o conjunto de teste do MNIST\nfrom tensorflow import keras\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_test = x_test / 255.0\n</pre> # Carregue o conjunto de teste do MNIST from tensorflow import keras (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() x_test = x_test / 255.0 In\u00a0[\u00a0]: Copied! <pre># Pegue uma imagem de teste\nimg = x_test[0]\nimg_input = np.expand_dims(img, axis=0)  # shape (1, 28, 28)\n</pre> # Pegue uma imagem de teste img = x_test[0] img_input = np.expand_dims(img, axis=0)  # shape (1, 28, 28) In\u00a0[\u00a0]: Copied! <pre>data = json.dumps({\"instances\": img_input.tolist()})\n</pre> data = json.dumps({\"instances\": img_input.tolist()}) In\u00a0[\u00a0]: Copied! <pre>url = \"http://localhost:8501/v1/models/mnist:predict\"\nresponse = requests.post(url, data=data, headers={\"content-type\": \"application/json\"})\n</pre> url = \"http://localhost:8501/v1/models/mnist:predict\" response = requests.post(url, data=data, headers={\"content-type\": \"application/json\"}) In\u00a0[\u00a0]: Copied! <pre>print(\"Resposta da API:\", response.json()) # Exibe a resposta completa da API\n</pre> print(\"Resposta da API:\", response.json()) # Exibe a resposta completa da API In\u00a0[\u00a0]: Copied! <pre># Extraia o d\u00edgito previsto\npred_digit = np.argmax(response.json()['predictions'][0])\nprint(\"Predito:\", pred_digit, \"Verdadeiro:\", y_test[0])\n</pre> # Extraia o d\u00edgito previsto pred_digit = np.argmax(response.json()['predictions'][0]) print(\"Predito:\", pred_digit, \"Verdadeiro:\", y_test[0])"},{"location":"aulas/checkpoint/","title":"Orienta\u00e7\u00f5es gerais","text":""},{"location":"aulas/checkpoint/#atencao-integridade-academica","title":"Aten\u00e7\u00e3o: integridade acad\u00eamica","text":"<ul> <li>Desonestidade intelectual n\u00e3o ser\u00e1 tolerada.</li> <li>Autoavalia\u00e7\u00e3o: uma autoavalia\u00e7\u00e3o incompat\u00edvel com o material entregue ser\u00e1 interpretada como desonestidade intelectual.</li> <li>Pl\u00e1gio: atividades podem incluir pesquisa e cita\u00e7\u00f5es, mas todo conte\u00fado (texto, imagem, c\u00f3digo, gr\u00e1fico, etc.) que n\u00e3o seja autoral deve ter refer\u00eancias.  </li> <li>Inclua uma se\u00e7\u00e3o \u201cRefer\u00eancias\u201d com os hiperlinks utilizados (ou refer\u00eancias bibliogr\u00e1ficas, quando aplic\u00e1vel).</li> <li>C\u00f3pia entre grupos: n\u00e3o \u00e9 permitida a troca de trabalhos. Cada grupo \u00e9 respons\u00e1vel pelo pr\u00f3prio desenvolvimento intelectual.  </li> <li>Atividades duplicadas resultar\u00e3o na anula\u00e7\u00e3o de ambas.</li> </ul>"},{"location":"aulas/checkpoint/#como-funcionam-os-checkpoints","title":"Como funcionam os checkpoints","text":""},{"location":"aulas/checkpoint/#o-que-sao","title":"O que s\u00e3o","text":"<p>Os checkpoints s\u00e3o instrumentos de avalia\u00e7\u00e3o da disciplina, desenhados para verificar a consolida\u00e7\u00e3o dos conte\u00fados e habilidades trabalhados ao longo do semestre.</p>"},{"location":"aulas/checkpoint/#quantos-existem","title":"Quantos existem","text":"<p>S\u00e3o 3 checkpoints por semestre.</p>"},{"location":"aulas/checkpoint/#formato","title":"Formato","text":"<p>Cada checkpoint pode assumir um dos formatos abaixo (conforme comunicado na publica\u00e7\u00e3o): - Prova (avalia\u00e7\u00e3o individual) - Projeto (desafio pr\u00e1tico, individual ou em grupo \u2014 conforme a regra do checkpoint)</p> <p>O formato, regras de colabora\u00e7\u00e3o e crit\u00e9rios de corre\u00e7\u00e3o ser\u00e3o sempre informados no enunciado do checkpoint.</p>"},{"location":"aulas/checkpoint/#quando-acontecem","title":"Quando acontecem","text":"<ul> <li>Os checkpoints s\u00e3o divulgados com anteced\u00eancia, de acordo com a agenda da disciplina.</li> <li>Em geral, devem ser realizados fora do hor\u00e1rio de aula, exceto quando o checkpoint for explicitamente aplicado em sala.</li> </ul>"},{"location":"aulas/checkpoint/#onde-e-como-sera-feito","title":"Onde e como ser\u00e1 feito","text":"<ul> <li>Quando for projeto, a entrega normalmente ocorrer\u00e1 via GitHub Classroom (reposit\u00f3rio privado), onde o estudante/grupo dever\u00e1 registrar a evolu\u00e7\u00e3o por meio de commits.</li> <li>Quando for prova, a aplica\u00e7\u00e3o e o formato de entrega seguir\u00e3o as instru\u00e7\u00f5es publicadas no checkpoint.</li> </ul>"},{"location":"aulas/checkpoint/#criterios-e-rubrica","title":"Crit\u00e9rios e rubrica","text":"<ul> <li>Cada checkpoint vem acompanhado de uma rubrica de avalia\u00e7\u00e3o (crit\u00e9rios e pesos), pensada para guiar o desenvolvimento e tornar a corre\u00e7\u00e3o transparente.</li> <li>O estudante deve usar a rubrica como \u201cchecklist\u201d do que \u00e9 esperado.</li> </ul>"},{"location":"aulas/checkpoint/#entrega-quando-aplicavel","title":"Entrega (quando aplic\u00e1vel)","text":""},{"location":"aulas/checkpoint/#prazo","title":"Prazo","text":"<ul> <li>A entrega deve ser feita at\u00e9 a data/hora definida no enunciado.</li> <li>Entregas fora do prazo seguem a pol\u00edtica definida pelo professor/institui\u00e7\u00e3o (quando aplic\u00e1vel).</li> </ul>"},{"location":"aulas/checkpoint/#autoavaliacao","title":"Autoavalia\u00e7\u00e3o","text":"<ul> <li>No momento da entrega, o estudante poder\u00e1 responder um formul\u00e1rio de autoavalia\u00e7\u00e3o (ex.: Google Forms), descrevendo:</li> <li>o que foi implementado,</li> <li>o que ficou pendente,</li> <li>evid\u00eancias (links/prints/commits) quando solicitado.</li> </ul>"},{"location":"aulas/checkpoint/#como-sera-avaliado","title":"Como ser\u00e1 avaliado","text":"<ul> <li>A avalia\u00e7\u00e3o pode ocorrer:</li> <li>em sala, na data prevista na agenda, e/ou</li> <li>por an\u00e1lise do reposit\u00f3rio/entrega, conforme regras do checkpoint.</li> <li>Quando houver avalia\u00e7\u00e3o pr\u00e1tica/oral, o estudante dever\u00e1 demonstrar:</li> <li>a evolu\u00e7\u00e3o do trabalho (ex.: hist\u00f3rico de commits),</li> <li>o funcionamento do que foi implementado,</li> <li>dom\u00ednio t\u00e9cnico do que foi entregue.</li> <li>A autoavalia\u00e7\u00e3o ser\u00e1 utilizada como apoio, mas n\u00e3o substitui as evid\u00eancias do reposit\u00f3rio/entrega.</li> </ul>"},{"location":"aulas/checkpoint/#o-que-devo-fazer-agora","title":"O que devo fazer agora?","text":"<p>Acompanhe a agenda da disciplina e aguarde a libera\u00e7\u00e3o do pr\u00f3ximo checkpoint. Assim que publicado, leia o enunciado completo e a rubrica antes de iniciar.</p>"},{"location":"aulas/checkpoint/cp-correcao/","title":"Avalia\u00e7\u00e3o Cruzada entre Grupos \u2013 Projeto de Jogo \ud83c\udfae","text":""},{"location":"aulas/checkpoint/cp-correcao/#objetivo-da-atividade","title":"Objetivo da Atividade","text":"<p>A proposta desta atividade \u00e9 estimular a avalia\u00e7\u00e3o cr\u00edtica e colaborativa entre os grupos, promovendo o aprendizado por meio da an\u00e1lise dos projetos dos colegas.</p> <p>Cada grupo ir\u00e1 avaliar 3 projetos de outros grupos, seguindo crit\u00e9rios previamente definidos. O objetivo \u00e9 desenvolver um olhar t\u00e9cnico e anal\u00edtico sobre o trabalho de colegas, aprendendo com diferentes abordagens e solu\u00e7\u00f5es.</p>"},{"location":"aulas/checkpoint/cp-correcao/#por-que-estamos-fazendo-isso-proposito","title":"Por que estamos fazendo isso? (\ud83e\udde0 Prop\u00f3sito)","text":"<p>Avaliar projetos de colegas n\u00e3o \u00e9 apenas uma forma de dar notas. \u00c9 um processo que:</p> <ul> <li>Desenvolve sua capacidade de an\u00e1lise cr\u00edtica.</li> <li>Ajuda a reconhecer boas pr\u00e1ticas de programa\u00e7\u00e3o e design.</li> <li>Ensina a argumentar tecnicamente sobre decis\u00f5es de projeto.</li> <li>Inspira melhorias no seu pr\u00f3prio trabalho.</li> </ul>"},{"location":"aulas/checkpoint/cp-correcao/#como-avaliar-com-responsabilidade-avaliacao-tecnica","title":"Como avaliar com responsabilidade (\u2705 Avalia\u00e7\u00e3o T\u00e9cnica)","text":"<p>Sua avalia\u00e7\u00e3o deve ser t\u00e9cnica, objetiva e respeitosa. Evite coment\u00e1rios gen\u00e9ricos como \u201cachei legal\u201d ou \u201cficou top\u201d. Use os crit\u00e9rios da rubrica para justificar suas observa\u00e7\u00f5es.</p> <p>Exemplos de bom coment\u00e1rio: - \u201cA ideia \u00e9 criativa, mas a interface ainda confunde o jogador no in\u00edcio.\u201d - \u201cO sistema de pontua\u00e7\u00e3o funciona, mas poderia ter feedback visual.\u201d</p> <p>Evite: - \u201cT\u00e1 ruim.\u201d - \u201cSem gra\u00e7a.\u201d - \u201cMuito bom!\u201d (sem justificar)</p>"},{"location":"aulas/checkpoint/cp-correcao/#o-que-devo-fazer","title":"O que devo fazer?","text":"<p>\u2705 Cada grupo dever\u00e1:</p> <ol> <li>Acessar o formul\u00e1rio de avalia\u00e7\u00e3o (link ser\u00e1 disponibilizado).</li> <li>Selecionar o grupo/projeto que est\u00e1 avaliando.</li> <li>Responder ao formul\u00e1rio 3 vezes, uma para cada projeto avaliado.</li> </ol> <p>\ud83d\udeab Voc\u00ea N\u00c3O deve avaliar o pr\u00f3prio projeto.</p>"},{"location":"aulas/checkpoint/cp-correcao/#quais-projetos-seu-grupo-deve-avaliar","title":"Quais projetos seu grupo deve avaliar?","text":"<p>Cada grupo ir\u00e1 receber uma lista com os 3 projetos a serem avaliados. Verifique com aten\u00e7\u00e3o no TEAMS:</p>"},{"location":"aulas/checkpoint/cp-correcao/#criterios-de-avaliacao","title":"Crit\u00e9rios de Avalia\u00e7\u00e3o","text":"<p>Os projetos devem ser avaliados com base nos seguintes crit\u00e9rios (cada um com nota de 0 a 10):</p> Crit\u00e9rio Descri\u00e7\u00e3o Jogabilidade O jogo \u00e9 divertido, intuitivo e f\u00e1cil de jogar? Segue a rubrica O jogo segue o que foi solicitado pela rubrica do projeto? Desempenho t\u00e9cnico / Bugs O jogo funciona bem, sem travamentos ou erros? C\u00f3digo top! O c\u00f3digo foi bem elaborado seguindo boas pr\u00e1ticas de programa\u00e7\u00e3o? <p>Al\u00e9m da pontua\u00e7\u00e3o, coment\u00e1rios construtivos s\u00e3o bem-vindos!</p>"},{"location":"aulas/checkpoint/cp-correcao/#regras-eticas-da-avaliacao-importante","title":"Regras \u00e9ticas da avalia\u00e7\u00e3o (\ud83d\udea8 Importante)","text":"<ul> <li>\u274c N\u00e3o pode avaliar o pr\u00f3prio grupo.</li> <li>\u274c N\u00e3o \u00e9 permitido combinar notas entre grupos.</li> <li>\u2705 Seja honesto e t\u00e9cnico. Notas sem justificativa podem ser desconsideradas.</li> <li>\u2705 Todos os membros do grupo devem participar das avalia\u00e7\u00f5es.</li> </ul>"},{"location":"aulas/checkpoint/cp-correcao/#gestao-de-tempo","title":"Gest\u00e3o de Tempo (\u23f1\ufe0f)","text":"<p>A aula ter\u00e1 momentos bem definidos:</p> <ul> <li>Tempo para jogar/testar os projetos (15\u201320 min por projeto)</li> <li>Tempo para preencher os formul\u00e1rios</li> </ul> <p>Boa avalia\u00e7\u00e3o!</p>"},{"location":"aulas/checkpoint/cp/","title":"Cp","text":""},{"location":"aulas/checkpoint/cp/#checkpoint","title":"CHECKPOINT","text":"<ul> <li>O objetivo do checkpoint \u00e9 avaliar a compreens\u00e3o dos estudantes em rela\u00e7\u00e3o ao conte\u00fado ministrado pela disciplina.</li> </ul> <p>Base Obrigat\u00f3ria: O projeto deve ser desenvolvido com base no c\u00f3digo de exemplo do jogo da mem\u00f3ria dispon\u00edvel em: </p> <ul> <li>Wokwi Jogo da Mem\u00f3ria</li> </ul> <p>Materiais Necess\u00e1rios:</p> <ul> <li>\u25b6\ufe0f Arduino UNO</li> <li>\u25b6\ufe0f LEDs</li> <li>\u25b6\ufe0f Bot\u00f5es</li> <li>\u25b6\ufe0f Buzzer</li> <li>\u25b6\ufe0f Resistores, jumpers e protoboard</li> </ul> <p>obs: Todos os testes e simula\u00e7\u00f5es devem ser realizados nos simuladores<code>Wokwi</code> ou <code>Tinkercad</code>.</p>"},{"location":"aulas/checkpoint/cp/#ideia-geral","title":"Ideia Geral","text":"<p>Neste checkpoint, o desafio \u00e9 desenvolver o prot\u00f3tipo do jogo da mem\u00f3ria <code>Genius</code> usando Arduino, com as seguintes caracter\u00edsticas:</p> <ul> <li>4 (ou mais) LEDs de cores diferentes</li> <li>4 (ou mais) Bot\u00f5es</li> <li>1 Buzzer</li> <li>Possuir Interface de comunica\u00e7\u00e3o serial</li> </ul> <p>Vamos explorar mais detalhadamente o funcionamento do prot\u00f3tipo e os crit\u00e9rios de avalia\u00e7\u00e3o.</p>"},{"location":"aulas/checkpoint/cp/#como-o-jogo-funciona","title":"Como o Jogo Funciona:","text":"<ul> <li>O jogo come\u00e7a com uma sequ\u00eancia aleat\u00f3ria de LEDs piscando.</li> <li>O jogador deve repetir a sequ\u00eancia pressionando os bot\u00f5es correspondentes.</li> <li>Se acertar, avan\u00e7a para o pr\u00f3ximo n\u00edvel; se errar, o jogo termina.</li> </ul> <p>Essa <code>base do jogo</code> j\u00e1 est\u00e1 pronta! no Wokwi Jogo da Mem\u00f3ria que voc\u00ea deve usar de base para o seu projeto. </p>"},{"location":"aulas/checkpoint/cp/#atencao-aos-requisitos-funcionais","title":"Aten\u00e7\u00e3o aos requisitos funcionais","text":"<p>Requisitos Funcionais B\u00e1sicos:</p> <ul> <li>LEDs: M\u00ednimo de 4 LEDs, cada um de uma cor diferente.</li> <li>Bot\u00f5es: M\u00ednimo de 4 bot\u00f5es, cada um associado a um LED espec\u00edfico.</li> <li>Buzzer: Deve emitir uma nota musical \u00fanica para cada cor de LED, tanto na sequ\u00eancia gerada pelo jogo quanto ao pressionar os bot\u00f5es.</li> <li>Fases do Jogo: O jogo deve ter pelo menos 4 n\u00edveis de dificuldade.</li> <li>Monitor Serial: O jogador deve conseguir interagir com o jogo tanto pelos bot\u00f5es f\u00edsicos quanto pelo monitor serial do Arduino.</li> </ul> <p>Requisitos Funcionais Avan\u00e7ados:</p> <ul> <li>FASES DO JOGO: Implementar uma quantidade \"infinita\" de n\u00edveis, aumentando a dificuldade progressivamente.</li> <li>Nivel de dificuldade Criar a fun\u00e7\u00e3o <code>nivelDificuldade</code> para ajustar a velocidade dos LEDs (iniciante, m\u00e9dio, hard).</li> <li>Salvar Pontua\u00e7\u00f5es Usar a mem\u00f3ria EEPROM do Arduino para armazenar as maiores pontua\u00e7\u00f5es, permitindo que os jogadores consultem e superem seus recordes.</li> <li>OUTRAS ID\u00c9IAS: O grupo pode propor outras funcionalidades avan\u00e7adas, mas deve ser aprovado pelo professor.</li> </ul>"},{"location":"aulas/checkpoint/cp/#rubrica","title":"Rubrica:","text":"Nota Itens 5 Atende aos requisitos funcionais b\u00e1sicos 6 Atende aos requisitos funcionais b\u00e1sicos + 1 Requisito Funcional Avan\u00e7ado 7 Atende aos requisitos funcionais b\u00e1sicos + 2 Requisitos Funcionais Avan\u00e7ados 8 Atende aos requisitos funcionais b\u00e1sicos + 3 Requisitos Funcionais Avan\u00e7ados 10 Atende aos requisitos funcionais b\u00e1sicos + 4 Requisitos Funcionais Avan\u00e7ados"},{"location":"aulas/checkpoint/cp1-simulado/","title":"Cp1 simulado","text":""},{"location":"aulas/checkpoint/cp1-simulado/#checkpoint-1-simulado","title":"CHECKPOINT 1 SIMULADO","text":"<ul> <li>Leia com aten\u00e7\u00e3o as instru\u00e7\u00f5es gerais sobre checkpoint;</li> <li>clique aqui no link para criar uma c\u00f3pia do repo. do CP em seu github;</li> <li>Leia com aten\u00e7\u00e3o as intru\u00e7\u00f5es do CP e desenvolva o projeto;</li> <li>O estudante deve realizar a entrega ANTES da data definida para avalia\u00e7\u00e3o do checkpoint;</li> <li>O CP \u00e9 individual.</li> </ul> cp INFOS link Reposit\u00f3rio https://classroom.github.com/a/Vdu2uFfz"},{"location":"aulas/checkpoint/cp2-desafio/","title":"Cp2 desafio","text":""},{"location":"aulas/checkpoint/cp2-desafio/#desafio-2tds","title":"Desafio 2TDS","text":"<p>Desenvolver uma solu\u00e7\u00e3o ao desafio proposto. Disponibilizado pelo TEAMS.</p>"},{"location":"aulas/checkpoint/cp2-desafio/#atualizacao-para-data-de-entrega","title":"ATUALIZA\u00c7\u00c3O PARA DATA DE ENTREGA","text":"<ul> <li><code>APRESENTA\u00c7\u00c3O DO PROJETO SER\u00c1 DURANTE O HOR\u00c1RIO DE AULA NA ULTIMA SEMANA ANTES DA DA GS ENTRE 20/05 E 24/05.</code></li> <li><code>A ORDEM DE APRESENTA\u00c7\u00c3O SER\u00c1 DEFINIDA POR SORTEIO ANTES DO INICIO DA AULA E DIVULGADA NO CANAL DA DISCIPLINA NO TEAMS.</code></li> </ul>"},{"location":"aulas/checkpoint/cp2-desafio/#dinamicas-das-proximas-aulas","title":"Dinamicas das pr\u00f3ximas aulas:","text":"<ul> <li> <p>Aula: Aula est\u00fadio para desenvolvimento do projeto.</p> <ul> <li>Entregas de acompanhamento semanal com feedback do projeto, via github.</li> </ul> </li> <li> <p>Teoria e pr\u00e1tica.</p> <ul> <li>Aula com novos conte\u00fados t\u00e9cnicos.</li> </ul> </li> </ul>"},{"location":"aulas/checkpoint/cp2-desafio/#feedback-continuo","title":"Feedback Cont\u00ednuo:","text":"<p>Os grupos receber\u00e3o feedback ap\u00f3s cada atualiza\u00e7\u00e3o do GitHub por meio de issues que ser\u00e3o abertas, e que valer\u00e3o parte da nota final a cada semana,  para garantir que estejam no caminho certo e para ajudar na resolu\u00e7\u00e3o de quaisquer problemas t\u00e9cnicos ou conceituais.</p>"},{"location":"aulas/checkpoint/cp2-desafio/#acompanhamento-semanal","title":"Acompanhamento semanal","text":"Data Atividade Nota parcial Semana 1 Lan\u00e7amento CP2 - Cria\u00e7\u00e3o do GitHub: Progresso documentado e c\u00f3digo atualizado, mostrando implementa\u00e7\u00e3o aproximada de <code>20%</code>. 0 - 3 pontos Semana 3 Atualiza\u00e7\u00f5es do GitHub: Progresso documentado e c\u00f3digo atualizado, mostrando implementa\u00e7\u00e3o aproximada de <code>50%</code>. 0 - 3 pontos Semana 5 Atualiza\u00e7\u00f5es do GitHub: Progresso documentado e c\u00f3digo atualizado, mostrando implementa\u00e7\u00e3o aproximada de <code>80%</code>. 0 - 4 pontos"},{"location":"aulas/checkpoint/cp2-desafio/#composicao-da-nota-final","title":"Composi\u00e7\u00e3o da nota final","text":"Data Crit\u00e9rio Nota Semana 6 Apresenta\u00e7\u00e3o (at\u00e9 2 pontos) e argui\u00e7\u00e3o (individual at\u00e9 3 pontos) 0 - 5 pontos Semana 6 Avalia\u00e7\u00e3o da Documenta\u00e7\u00e3o t\u00e9cnica do projeto  (Qualidade t\u00e9cnica de c\u00f3digo e organiza\u00e7\u00e3o do reposit\u00f3rio github) 0 - 5 pontos Da Semana 1 at\u00e9 Semana 5 Acompanhamento semanal (para os grupos que participarem ser\u00e1 a nota de CP3) 0 - 10 pontos <ul> <li> <p>CP2:</p> <ul> <li>Dia Semana 6, composta de <code>Apresenta\u00e7\u00e3o e argui\u00e7\u00e3o do projeto</code> + <code>valia\u00e7\u00e3o da Documenta\u00e7\u00e3o t\u00e9cnica do projeto</code>.</li> </ul> </li> <li> <p>CP3:</p> <ul> <li>Os grupos que realizarem o acompanhamento semanal ter\u00e1 a somat\u00f3ria do <code>acompanhamento semanal</code> convertida nota de CP3.</li> <li>Os grupos que <code>n\u00e3o participarem</code> do acompanhamento semanal <code>n\u00e3o ganham nota</code> e podem fazer o CP3 na Semana 7. O <code>CP3 Ser\u00e1 uma avalia\u00e7\u00e3o individual e presencial durante o periodo da aula</code>.  </li> </ul> </li> </ul>"},{"location":"aulas/checkpoint/cp3/","title":"CHECKPOINT 3","text":"<ul> <li>Leia com aten\u00e7\u00e3o as instru\u00e7\u00f5es gerais sobre checkpoint;</li> <li>clique aqui no link para criar uma c\u00f3pia do repo. do CP em seu github;</li> <li>Leia com aten\u00e7\u00e3o as intru\u00e7\u00f5es do CP e desenvolva o projeto;</li> <li>AO finalizar o CP, preencha o forms de auto avalia\u00e7\u00e3o;</li> <li>O estudante deve realizar a entrega ANTES da data definida para avalia\u00e7\u00e3o do checkpoint;</li> <li>O CP \u00e9 individual.</li> </ul> cp INFOS link Reposit\u00f3rio https://classroom.github.com/ Forms avalia\u00e7\u00e3o https://forms.gle/iSDD8EbZKesbfgAt9"},{"location":"aulas/checkpoint/cp4-arduino/","title":"CHECKPOINT 4","text":"<ul> <li>O CP ser\u00e1 realizado em aula, de forma individual. </li> <li>A data de aplica\u00e7\u00e3o est\u00e1 dispon\u00edvel no site, em agenda.</li> </ul>"},{"location":"aulas/checkpoint/cp4-arduino/#conteudo-do-cp","title":"Conte\u00fado do CP","text":"<ul> <li>Conceitos de Programa\u00e7\u00e3o C para arduino</li> <li>Conceitos de Eletrica e Eletr\u00f4nica</li> </ul>"},{"location":"aulas/checkpoint/cp5/","title":"Cp5","text":""},{"location":"aulas/checkpoint/cp5/#cp5-projeto-de-internet-das-coisas","title":"CP5 - Projeto de Internet das Coisas","text":"<ul> <li>O objetivo do checkpoint \u00e9 avaliar sua compreens\u00e3o acerca do conte\u00fado ministrado pela disciplina. </li> </ul>"},{"location":"aulas/checkpoint/cp5/#ideia-geral","title":"Ideia geral","text":"<p>Constru\u00e7\u00e3o de uma solu\u00e7\u00e3o de IoT que abrange todas as pontas da comunica\u00e7\u00e3o entre usu\u00e1rio e dispositivos</p> <p></p> <p>A desafio consiste no desenvolvimento de uma solu\u00e7\u00e3o simples mas completa que ilustra as partes mais importantes de uma arquitetura de IoT: os dispositivos e os usu\u00e1rios. A solu\u00e7\u00e3o tamb\u00e9m n\u00e3o contempla as ferramentas de seguran\u00e7a que seriam necess\u00e1rias ao implantar um sistema real.</p>"},{"location":"aulas/checkpoint/cp5/#entrega-e-apresentacao-do-projeto","title":"Entrega e Apresenta\u00e7\u00e3o do projeto:","text":"<ul> <li>Individual ou em Grupo (at\u00e9 5 alunos)</li> <li>Entrega da documenta\u00e7\u00e3o via github, apresentar o reposit\u00f3rio ORGANIZADO do projeto com toda a doumento e instru\u00e7\u00f5es de como replicar o projeto. (em breve o formulario para preenchechimento)</li> <li>A apresenta\u00e7\u00e3o ser\u00e1 presencial, e ocupar\u00e1 as aulas do dia 01/11/2023(2TDSA) 27/10/2023(2TDSG)</li> </ul> <p>Haver\u00e1 tempo durante as aulas para tirar d\u00favidas quanto ao desenvolvimento do trabalho. </p>"},{"location":"aulas/checkpoint/cp5/#requisitos-minimos","title":"Requisitos m\u00ednimos:","text":"<p>O sistema \u00e9 composto por pelo menos 1 <code>coisa inteligente</code>, que posssui:</p> <ul> <li> <p>Pelo menos 3 sensores, sendo eles:</p> <ul> <li>pelo menos 1 sensor deve ser digital;</li> <li>pelo menos 1 sensor deve ser anal\u00f3gico.</li> </ul> </li> <li> <p>Pelo menos 3 atuadores, sendo eles:</p> <ul> <li>pelo menos 1 atuador deve ser digital;</li> <li>pelo menos 1 atuador deve ser por PWM (analogWrite())</li> </ul> </li> </ul> <p>Tip</p> <p>Pense em dispositivos ou solu\u00e7\u00f5es que s\u00e3o ou podem ser utilizados em seu dia-a-dia, utilize como refer\u00eancia os sensores/atuadores dispon\u00edveis no kit IoT. </p>"},{"location":"aulas/checkpoint/cp5/#rubrica","title":"Rubrica","text":"<p>(R0 - NOTA at\u00e9 1 ponto) Idea\u00e7\u00e3o:</p> <ul> <li> <p>Explique de forma clara qual o objetivo do seu projeto, o que ele faz e como funciona.</p> </li> <li> <p>Fa\u00e7a um esbo\u00e7o da arquitetura do seu projeto. </p> </li> </ul> <p>(R1 - NOTA at\u00e9 1 ponto1) Programa em linguagem do <code>Arduino</code> que realize pelo menos:</p> <ul> <li> <p>A leitura de <code>pelo menos tr\u00eas sensores</code> a cada <code>200ms</code>, disponibilizando as leituras na porta serial, no formato JSON <code>{\"&lt;SENS1&gt;\":&lt;VALOR1&gt;,..., \"&lt;SENSn&gt;\":&lt;VALORn&gt;}</code>, onde <code>&lt;SENS1&gt;</code> e <code>&lt;SENSn&gt;</code> s\u00e3o os nomes dos sensores, e <code>&lt;VALOR1&gt;</code> e <code>&lt;VALORn&gt;</code> s\u00e3o os valores lidos, respectivamente. Deve ser utilizado sensores anal\u00f3gicos e digitais.</p> </li> <li> <p>a execu\u00e7\u00e3o de comandos em <code>pelo menos tr\u00eas atuadores</code>, lendo esses comandos pela porta serial. Os comandos devem ser na forma <code>{\u201c&lt;CMD&gt;\u201d:&lt;VALOR&gt;}</code>, onde CMD \u00e9 o nome do comando e VALOR \u00e9 o par\u00e2metro enviado. Por exemplo, se <code>&lt;CMD&gt;</code> for led, ent\u00e3o <code>&lt;VALOR&gt;</code> pode ser \u201cOn/Off\u201d (liga/desliga) ou \u201c0~255\u201d (dimmer), por exemplo. Deve possuir atuadores digitais e anal\u00f3gicos (PWM).</p> </li> <li> <p>para mais sensores e atuadores siga as instru\u00e7\u00f5es acima.</p> </li> </ul> <p>(R2 - NOTA at\u00e9 2 pontos) Comunica\u00e7\u00e3o com o <code>broker MQTT</code> de sua escolha: deve ser desenvolvido um programa Node-RED e Arduino que conecte o dispositivo de IoT \u00e0 central de mensagens (MQTT Broker) atrav\u00e9s dos seguintes t\u00f3picos:</p> <ul> <li>Realiza <code>PUB</code> sempre que for realizada a leitura dos sensores, convertendo e enviando a informa\u00e7\u00e3o de cada sensor no formato JSON <code>{\"value\": &lt;VALOR&gt;}</code> ao t\u00f3pico: </li> </ul> <p><code>fiap/iot/turma/&lt;TT&gt;/grupo/&lt;NNN&gt;/devtype/&lt;TYPE&gt;/devid/&lt;ID&gt;/sensor/&lt;SENS&gt;</code></p> <p>onde:</p> <pre><code>* &lt;TT&gt; corresponde \u00e0 turma (2TDSA, 2TDSG)\n* &lt;NNN&gt; corresponde ao nome do grupo\n* &lt;TYPE&gt; corresponde ao tipo de dispositivo (arduino ou esp8266)\n* &lt;ID&gt; corresponde \u00e0 identifica\u00e7\u00e3o do dispositivo (pode ser o MAC Address do ESP8266, ou o n\u00famero de s\u00e9rie do Arduino, ou qualquer identifica\u00e7\u00e3o que quiser)\n* &lt;SENS&gt; corresponde ao nome do sensor\n</code></pre> <ul> <li>Para cada sensor, fa\u00e7a a subscri\u00e7\u00e3o <code>SUB</code> ao seguinte t\u00f3pico, que ir\u00e1 receber um JSON da forma <code>{\"value\": &lt;VALOR&gt;}</code> :</li> </ul> <p><code>fiap/iot/turma/&lt;TT&gt;/grupo/&lt;NNN&gt;/devtype/&lt;TYPE&gt;/devid/&lt;ID&gt;/cmd/&lt;CMD&gt;</code></p> <p>onde <code>&lt;CMD&gt;</code> corresponde ao nome do comando a ser executado. Ao receber o comando, ele deve ser convertido e enviado \u00e0 placa IoT para ser executado.</p> <p>Este item pode ser realizado de duas formas:</p> <pre><code>- No caso da placa de IoT ser um Arduino, o item 2 deve ser executado na forma de um Gateway programado em Node-RED, que se conecta ao Arduino atrav\u00e9s da porta serial.\n\n- No caso da placa de IoT ser um ESP32, ESP8266 Node-MCU, este item deve constar na programa\u00e7\u00e3o da placa (em linguagem Arduino), e n\u00e3o \u00e9 necess\u00e1rio um gateway adicional.\n</code></pre> <p>(R3 - NOTA at\u00e9 2 pontos) Aplica\u00e7\u00e3o Web desenvolvida em <code>Node-RED</code> ou <code>Flask</code> que se conecte ao Broker MQTT e contenha uma API ReSTful com os seguintes endpoints </p> <ul> <li> <p>Recupera uma lista de IDs de dispositivos do tipo especificado, no formato JSON <code>[GET]http://&lt;HOST&gt;:&lt;PORTA&gt;/&lt;PROJNAME&gt;/fiap/iot/turma/&lt;TT&gt;/grupo/&lt;NNN&gt;/devtype/&lt;TYPE&gt;/devid/all</code></p> </li> <li> <p>Recupera a lista dos sensores dispon\u00edveis para o dispositivo, no formato JSON <code>[GET]http://&lt;HOST&gt;:&lt;PORTA&gt;/&lt;PROJNAME&gt;/fiap/iot/turma/&lt;TT&gt;/grupo/&lt;NNN&gt;/devtype/&lt;TYPE&gt;/devid/&lt;ID&gt;/sensor/all</code></p> </li> <li> <p>Recupera o \u00faltimo valor lido no sensor, no formato JSON {\"value\": } <code>[GET]http://&lt;HOST&gt;:&lt;PORTA&gt;/&lt;PROJNAME&gt;/fiap/iot/turma/&lt;TT&gt;/grupo/&lt;NNN&gt;/devtype/&lt;TYPE&gt;/devid/&lt;ID&gt;/sensor/&lt;SENS&gt;</code> <li> <p>Recupera a lista dos comandos dispon\u00edveis para o dispositivo, no formato JSON <code>[GET]http://&lt;HOST&gt;:&lt;PORTA&gt;/&lt;PROJNAME&gt;/fiap/iot/turma/&lt;TT&gt;/grupo/&lt;NNN&gt;/devtype/&lt;TYPE&gt;/devid/&lt;ID&gt;/cmd/all</code></p> </li> <li> <p>Executa um comando, passando um argumento da forma {\"value\": }  <code>[POST]http://&lt;HOST&gt;:&lt;PORTA&gt;/&lt;PROJNAME&gt;/fiap/iot/turma/&lt;TT&gt;/grupo/&lt;NNN&gt;/devtype/&lt;TYPE&gt;/devid/&lt;ID&gt;/cmd/&lt;CMD&gt;</code> <p>(R4 - NOTA at\u00e9 2 pontos) Dashboard no Node-RED cuja interface gr\u00e1fica demonstre o funcionamento de todas os endpoints do exerc\u00edcio anterior, ou seja:</p> <ul> <li>Apresente controles que permitam enviar comandos para o Arduino, ESP32 ou ESP8266 Node-MCU</li> <li>Apresente indicadores que permitam saber o valor dos sensores do Arduino, ESP32 ou ESP8266 Node-MCU</li> <li>(at\u00e9 1 ponto) Capricho na usabilidade da interface. </li> </ul> <p>(R5 - NOTA at\u00e9 2 pontos) Desenvolvimento de alguma <code>feature especial</code> de pesquisa extra para o projeto, podendo ser:</p> <ul> <li>Uso de comunica\u00e7\u00e3o Bluetooth entre o Arduino e o Gateway Node-RED</li> <li>Uso de mais de um dispositivo como ESP32, ESP8266 ou Node-MCU</li> <li>Uso da Raspberry como Gateway Node-RED</li> <li>Uso de um dos sensores especiais: validar com o professor.</li> <li>Prot\u00f3tipo funcional com parte mec\u00e2nica usando a infraestrutra do laborat\u00f3rio MakerLab (corte a laser, impressora 3D entre outros...)</li> </ul>"},{"location":"aulas/checkpoint/cp6/","title":"CHECKPOINT 6","text":"<p>Data de apresenta\u00e7\u00e3o: 05/11/2025 - presencial em aula </p>"},{"location":"aulas/checkpoint/cp6/#objetivo","title":"Objetivo","text":"<p>Desenvolver um aplicativo mobile inteligente que utilize IA Generativa para analisar informa\u00e7\u00f5es extra\u00eddas de cupons fiscais e gerar insights financeiros personalizados.</p>"},{"location":"aulas/checkpoint/cp6/#desafio","title":"Desafio","text":"<p>O desafio consiste em criar um app mobile funcional capaz de:</p> <ul> <li>Capturar uma imagem de um cupom fiscal (via c\u00e2mera do dispositivo).  </li> <li>Extrair informa\u00e7\u00f5es relevantes utilizando IA generativa e APIs do Firebase AI Logic, como:</li> <li>Valor total da compra  </li> <li>Data e hora da transa\u00e7\u00e3o  </li> <li>Nome do estabelecimento  </li> <li>Categoria da despesa (alimenta\u00e7\u00e3o, transporte, lazer etc.)  </li> <li>Persistir os dados extra\u00eddos em um banco de dados Firebase (Firestore).  </li> <li>Implementar um assistente de IA Generativa integrado com o Firebase AI Logic, que:</li> <li>Analise os dados salvos e gere insights financeiros personalizados.  </li> <li>Produza respostas em linguagem natural.</li> </ul>"},{"location":"aulas/checkpoint/cp6/#documentacao-oficial","title":"Documenta\u00e7\u00e3o Oficial","text":"<p>A integra\u00e7\u00e3o com o Firebase deve seguir obrigatoriamente a documenta\u00e7\u00e3o oficial:</p> <ul> <li>https://firebase.google.com/docs/ai-logic</li> </ul>"},{"location":"aulas/checkpoint/cp6/#ia-em-duas-dimensoes","title":"IA em Duas Dimens\u00f5es","text":""},{"location":"aulas/checkpoint/cp6/#1-no-aplicativo","title":"1. No Aplicativo","text":"<p>O app utilizar\u00e1 o modelo Gemini 2.5 Flash para interpretar os dados extra\u00eddos dos cupons (imagem-texto) e gerar an\u00e1lises (texto-texto).  </p> <p>Exemplo: \u201cSuas despesas em alimenta\u00e7\u00e3o aumentaram 15% em rela\u00e7\u00e3o ao m\u00eas anterior.\u201d</p>"},{"location":"aulas/checkpoint/cp6/#2-no-desenvolvimento","title":"2. No Desenvolvimento","text":"<p>Utilizar o VS Code com o GitHub Copilot ativado para auxiliar na escrita de c\u00f3digo, refatora\u00e7\u00e3o e documenta\u00e7\u00e3o (texto-c\u00f3digo, c\u00f3digo-c\u00f3digo). Essa pr\u00e1tica ser\u00e1 avaliada como parte da integra\u00e7\u00e3o da IA no processo de desenvolvimento.</p>"},{"location":"aulas/checkpoint/cp6/#funcionalidades-esperadas","title":"Funcionalidades Esperadas","text":"<ul> <li>Captura de imagem (cupom fiscal)  </li> <li>Extra\u00e7\u00e3o autom\u00e1tica dos dados https://firebase.google.com/docs/ai-logic/analyze-images?api=dev </li> <li>Armazenamento das informa\u00e7\u00f5es extra\u00eddas no Firebase  </li> <li>Consulta e exibi\u00e7\u00e3o dos registros no app  </li> <li>Gera\u00e7\u00e3o de insights com IA Generativa https://firebase.google.com/docs/ai-logic/chat?api=dev </li> <li>Interface amig\u00e1vel e responsiva  </li> </ul>"},{"location":"aulas/checkpoint/cp6/#features-extras-opcional","title":"Features Extras (opcional)","text":"<ul> <li>Exibir gr\u00e1ficos de gastos mensais por categoria  </li> <li>Implementar notifica\u00e7\u00f5es de alertas de gastos </li> <li>Criar um chatbot financeiro dentro do app  </li> </ul>"},{"location":"aulas/checkpoint/cp6/#apresentacao-e-entregaveis","title":"Apresenta\u00e7\u00e3o e Entreg\u00e1veis","text":"<ul> <li>Demonstra\u00e7\u00e3o do app funcional </li> <li>Reposit\u00f3rio no GitHub com c\u00f3digo e README</li> </ul> <p>Data de apresenta\u00e7\u00e3o: 05/11/2025 - presencial em aula</p>"},{"location":"aulas/iot/","title":"introdu\u00e7ao IoT","text":"<p>esse \u00e9 o index</p>"},{"location":"aulas/iot/esp32/01-Introducao-ESP32/","title":"Introdu\u00e7\u00e3o ao ESP32","text":""},{"location":"aulas/iot/esp32/01-Introducao-ESP32/#o-que-e-o-esp32","title":"O que \u00e9 o ESP32?","text":"<p>O ESP32 \u00e9 um microcontrolador de baixo custo e baixo consumo de energia que integra Wi-Fi e Bluetooth em um \u00fanico chip. Desenvolvido pela Espressif Systems, o ESP32 se tornou um dos componentes mais populares para projetos de Internet das Coisas (IoT) devido \u00e0 sua versatilidade e poder computacional.</p>"},{"location":"aulas/iot/esp32/01-Introducao-ESP32/#caracteristicas-principais","title":"Caracter\u00edsticas Principais","text":"<ul> <li>Processador: Dual-core Tensilica Xtensa LX6 de 32 bits (at\u00e9 240MHz)</li> <li>Mem\u00f3ria: 520 KB de SRAM</li> <li>Conectividade: Wi-Fi 802.11 b/g/n (2.4 GHz) e Bluetooth 4.2 (BLE)</li> <li>GPIO: At\u00e9 36 pinos</li> <li>Perif\u00e9ricos: ADC, DAC, I\u00b2C, SPI, UART, CAN, PWM, etc.</li> <li>Seguran\u00e7a: Criptografia por hardware</li> </ul>"},{"location":"aulas/iot/esp32/01-Introducao-ESP32/#diferencas-entre-esp32-e-arduino","title":"Diferen\u00e7as entre ESP32 e Arduino","text":"Caracter\u00edstica ESP32 Arduino UNO Processador Dual-core 32-bit at\u00e9 240MHz Single-core 8-bit 16MHz Mem\u00f3ria RAM 520 KB 2 KB WiFi Integrado Necessita shield Bluetooth Integrado Necessita shield GPIO At\u00e9 36 pinos 14 pinos digitais, 6 anal\u00f3gicos Pre\u00e7o \\(3-\\)10 \\(20-\\)25"},{"location":"aulas/iot/esp32/01-Introducao-ESP32/#modelos-comuns-de-esp32","title":"Modelos Comuns de ESP32","text":"<ol> <li>ESP32-DevKitC: Placa de desenvolvimento b\u00e1sica</li> <li>ESP32-WROOM-32: M\u00f3dulo com antena PCB integrada</li> <li>ESP32-WROVER: M\u00f3dulo com antena externa e mem\u00f3ria PSRAM adicional</li> <li>TTGO T-Display: ESP32 com display LCD colorido</li> <li>M5Stack: ESP32 em formato modular com display e sensores</li> </ol>"},{"location":"aulas/iot/esp32/01-Introducao-ESP32/#por-que-usar-esp32-para-iot","title":"Por que usar ESP32 para IoT?","text":"<ul> <li>Conectividade Integrada: WiFi e Bluetooth prontos para uso</li> <li>Baixo Consumo: Modos de deep sleep para aplica\u00e7\u00f5es com bateria</li> <li>Alto Desempenho: Processador dual-core permite aplica\u00e7\u00f5es mais complexas</li> <li>Baixo Custo: Excelente custo-benef\u00edcio para projetos IoT</li> <li>Ecossistema Rico: Ampla comunidade e muitas bibliotecas dispon\u00edveis</li> </ul>"},{"location":"aulas/iot/esp32/01-Introducao-ESP32/#aplicacoes-comuns","title":"Aplica\u00e7\u00f5es Comuns","text":"<ul> <li>Automa\u00e7\u00e3o residencial</li> <li>Monitoramento remoto</li> <li>Controle industrial</li> <li>Wearables e dispositivos m\u00e9dicos</li> <li>Esta\u00e7\u00f5es meteorol\u00f3gicas</li> <li>Sistemas de seguran\u00e7a</li> </ul> <p>Na pr\u00f3xima aula, configuraremos o ambiente de desenvolvimento para programar o ESP32 usando a IDE do Arduino.</p>"},{"location":"aulas/iot/esp32/02-Configuracao-Ambiente/","title":"Configura\u00e7\u00e3o do Ambiente de Desenvolvimento para ESP32","text":"<p>Nesta aula, configuraremos o ambiente necess\u00e1rio para programar o ESP32 usando a IDE do Arduino, que \u00e9 uma das formas mais acess\u00edveis para iniciar o desenvolvimento.</p>"},{"location":"aulas/iot/esp32/02-Configuracao-Ambiente/#requisitos","title":"Requisitos","text":"<ul> <li>Computador com Windows, macOS ou Linux</li> <li>Placa ESP32 (DevKit, NodeMCU-ESP32, etc.)</li> <li>Cabo USB adequado para sua placa</li> <li>Conex\u00e3o com a internet</li> </ul>"},{"location":"aulas/iot/esp32/02-Configuracao-Ambiente/#instalacao-da-ide-do-arduino","title":"Instala\u00e7\u00e3o da IDE do Arduino","text":"<ol> <li>Acesse o site oficial do Arduino: https://www.arduino.cc/en/software</li> <li>Baixe a vers\u00e3o adequada para seu sistema operacional</li> <li>Instale o software seguindo as instru\u00e7\u00f5es para seu sistema</li> </ol>"},{"location":"aulas/iot/esp32/02-Configuracao-Ambiente/#adicionando-suporte-ao-esp32-na-ide-do-arduino","title":"Adicionando Suporte ao ESP32 na IDE do Arduino","text":""},{"location":"aulas/iot/esp32/02-Configuracao-Ambiente/#metodo-1-usando-o-gerenciador-de-placas","title":"M\u00e9todo 1: Usando o Gerenciador de Placas","text":"<ol> <li>Abra a IDE do Arduino</li> <li>V\u00e1 para Arquivo &gt; Prefer\u00eancias (ou Arduino &gt; Prefer\u00eancias no macOS)</li> <li>No campo \"URLs Adicionais para Gerenciadores de Placas\", adicione:    <pre><code>https://raw.githubusercontent.com/espressif/arduino-esp32/gh-pages/package_esp32_index.json\n</code></pre></li> <li>Clique em \"OK\"</li> <li>V\u00e1 para Ferramentas &gt; Placa &gt; Gerenciador de Placas</li> <li>Pesquise por \"esp32\"</li> <li>Instale o pacote \"ESP32 by Espressif Systems\"</li> </ol>"},{"location":"aulas/iot/esp32/02-Configuracao-Ambiente/#metodo-2-instalacao-manual-alternativa","title":"M\u00e9todo 2: Instala\u00e7\u00e3o Manual (alternativa)","text":"<p>Se o m\u00e9todo 1 n\u00e3o funcionar, voc\u00ea pode instalar manualmente:</p> <ol> <li>Clone o reposit\u00f3rio do ESP32 para Arduino:    <pre><code>git clone https://github.com/espressif/arduino-esp32.git\n</code></pre></li> <li>Execute o script de instala\u00e7\u00e3o na pasta do reposit\u00f3rio (espec\u00edfico para cada sistema operacional)</li> </ol>"},{"location":"aulas/iot/esp32/02-Configuracao-Ambiente/#testando-a-instalacao","title":"Testando a Instala\u00e7\u00e3o","text":"<ol> <li>Conecte sua placa ESP32 ao computador via USB</li> <li>Na IDE do Arduino, selecione:</li> <li>Ferramentas &gt; Placa &gt; ESP32 Arduino &gt; [Seu modelo de ESP32]</li> <li> <p>Ferramentas &gt; Porta &gt; [Porta COM onde o ESP32 est\u00e1 conectado]</p> </li> <li> <p>Abra um exemplo simples:</p> </li> <li> <p>Arquivo &gt; Exemplos &gt; 01.Basics &gt; Blink</p> </li> <li> <p>Modifique o c\u00f3digo para usar o LED interno do ESP32 (pino 2 na maioria das placas):</p> </li> </ol> <pre><code>// LED_BUILTIN pode n\u00e3o funcionar para ESP32, use o pino 2 diretamente\nvoid setup() {\n  pinMode(2, OUTPUT);\n}\n\nvoid loop() {\n  digitalWrite(2, HIGH);\n  delay(1000);\n  digitalWrite(2, LOW);\n  delay(1000);\n}\n</code></pre> <ol> <li>Clique no bot\u00e3o \"Carregar\" (a seta para a direita)</li> <li>Aguarde a compila\u00e7\u00e3o e o upload</li> <li>Verifique se o LED na placa est\u00e1 piscando</li> </ol>"},{"location":"aulas/iot/esp32/02-Configuracao-Ambiente/#resolucao-de-problemas-comuns","title":"Resolu\u00e7\u00e3o de Problemas Comuns","text":""},{"location":"aulas/iot/esp32/02-Configuracao-Ambiente/#placa-nao-aparece-na-lista-de-portas","title":"Placa n\u00e3o aparece na lista de portas","text":"<ul> <li>Verifique o cabo USB (alguns cabos s\u00e3o apenas para alimenta\u00e7\u00e3o)</li> <li>Instale os drivers necess\u00e1rios:</li> <li>CP210x: Drivers Silicon Labs</li> <li>CH340: Drivers CH340</li> </ul>"},{"location":"aulas/iot/esp32/02-Configuracao-Ambiente/#erro-de-permissao-no-linux","title":"Erro de permiss\u00e3o no Linux","text":"<p><pre><code>sudo usermod -a -G dialout $USER\n</code></pre> (Necess\u00e1rio fazer logout e login novamente)</p>"},{"location":"aulas/iot/esp32/02-Configuracao-Ambiente/#falha-na-comunicacao-durante-upload","title":"Falha na comunica\u00e7\u00e3o durante upload","text":"<ul> <li>Mantenha pressionado o bot\u00e3o \"BOOT\" durante o in\u00edcio do upload</li> <li>Em algumas placas, \u00e9 necess\u00e1rio pressionar o bot\u00e3o \"RESET\" ap\u00f3s iniciar o upload</li> </ul>"},{"location":"aulas/iot/esp32/02-Configuracao-Ambiente/#editores-alternativos","title":"Editores Alternativos","text":"<p>Al\u00e9m da IDE do Arduino, voc\u00ea tamb\u00e9m pode usar:</p> <ol> <li>Visual Studio Code com extens\u00e3o PlatformIO:</li> <li>Mais recursos</li> <li>Melhor editor de c\u00f3digo</li> <li> <p>Gerenciamento de bibliotecas autom\u00e1tico</p> </li> <li> <p>ESP-IDF (Espressif IoT Development Framework):</p> </li> <li>Framework oficial da Espressif</li> <li>Acesso a todos os recursos do ESP32</li> <li>Curva de aprendizado mais \u00edngreme</li> </ol> <p>Na pr\u00f3xima aula, come\u00e7aremos com projetos b\u00e1sicos para conhecer melhor o ESP32.</p>"},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/","title":"Primeiros Projetos com ESP32","text":"<p>Nesta aula, vamos explorar projetos b\u00e1sicos para nos familiarizarmos com as funcionalidades do ESP32. Estes projetos ajudar\u00e3o a entender os conceitos fundamentais antes de avan\u00e7armos para aplica\u00e7\u00f5es IoT mais complexas.</p>"},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/#projeto-1-controle-de-leds-com-esp32","title":"Projeto 1: Controle de LEDs com ESP32","text":""},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/#materiais","title":"Materiais","text":"<ul> <li>ESP32 DevKit</li> <li>3 LEDs (vermelho, verde e azul)</li> <li>3 resistores de 220\u03a9</li> <li>Jumpers</li> <li>Protoboard</li> </ul>"},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/#circuito","title":"Circuito","text":"<p>Conecte os LEDs aos pinos GPIO do ESP32: - LED Vermelho: pino 25 - LED Verde: pino 26 - LED Azul: pino 27</p> <p>N\u00e3o esque\u00e7a de conectar os resistores em s\u00e9rie com os LEDs para limitar a corrente.</p>"},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/#codigo","title":"C\u00f3digo","text":"<pre><code>const int ledVermelho = 25;\nconst int ledVerde = 26;\nconst int ledAzul = 27;\n\nvoid setup() {\n  pinMode(ledVermelho, OUTPUT);\n  pinMode(ledVerde, OUTPUT);\n  pinMode(ledAzul, OUTPUT);\n}\n\nvoid loop() {\n  // Padr\u00e3o de sequ\u00eancia de LEDs\n  digitalWrite(ledVermelho, HIGH);\n  digitalWrite(ledVerde, LOW);\n  digitalWrite(ledAzul, LOW);\n  delay(1000);\n\n  digitalWrite(ledVermelho, LOW);\n  digitalWrite(ledVerde, HIGH);\n  digitalWrite(ledAzul, LOW);\n  delay(1000);\n\n  digitalWrite(ledVermelho, LOW);\n  digitalWrite(ledVerde, LOW);\n  digitalWrite(ledAzul, HIGH);\n  delay(1000);\n\n  // Todos acesos\n  digitalWrite(ledVermelho, HIGH);\n  digitalWrite(ledVerde, HIGH);\n  digitalWrite(ledAzul, HIGH);\n  delay(1000);\n\n  // Todos apagados\n  digitalWrite(ledVermelho, LOW);\n  digitalWrite(ledVerde, LOW);\n  digitalWrite(ledAzul, LOW);\n  delay(1000);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/#projeto-2-leitura-de-sensores-analogicos","title":"Projeto 2: Leitura de Sensores Anal\u00f3gicos","text":""},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/#materiais_1","title":"Materiais","text":"<ul> <li>ESP32 DevKit</li> <li>Potenci\u00f4metro de 10k\u03a9</li> <li>LED</li> <li>Resistor de 220\u03a9</li> <li>Jumpers</li> <li>Protoboard</li> </ul>"},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/#circuito_1","title":"Circuito","text":"<ul> <li>Conecte o potenci\u00f4metro ao pino anal\u00f3gico GPIO34 (ADC1_CH6)</li> <li>Conecte o LED ao pino GPIO2 atrav\u00e9s do resistor</li> </ul>"},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/#codigo_1","title":"C\u00f3digo","text":"<pre><code>const int potPin = 34;  // Pino do potenci\u00f4metro\nconst int ledPin = 2;   // Pino do LED\n\n// Vari\u00e1veis para PWM\nconst int freq = 5000;\nconst int ledChannel = 0;\nconst int resolution = 8;  // Resolu\u00e7\u00e3o de 8 bits (0-255)\n\nvoid setup() {\n  Serial.begin(115200);\n\n  // Configura\u00e7\u00e3o do PWM\n  ledcSetup(ledChannel, freq, resolution);\n  ledcAttachPin(ledPin, ledChannel);\n}\n\nvoid loop() {\n  // Leitura do valor anal\u00f3gico (ESP32 tem ADC de 12 bits: 0-4095)\n  int sensorValue = analogRead(potPin);\n\n  // Converte o valor lido (0-4095) para o intervalo do PWM (0-255)\n  int brightness = map(sensorValue, 0, 4095, 0, 255);\n\n  // Aplica o PWM ao LED\n  ledcWrite(ledChannel, brightness);\n\n  // Imprime os valores no monitor serial\n  Serial.print(\"Valor Anal\u00f3gico: \");\n  Serial.print(sensorValue);\n  Serial.print(\" | Brilho: \");\n  Serial.println(brightness);\n\n  delay(100);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/#projeto-3-botoes-e-interrupcoes","title":"Projeto 3: Bot\u00f5es e Interrup\u00e7\u00f5es","text":""},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/#materiais_2","title":"Materiais","text":"<ul> <li>ESP32 DevKit</li> <li>Bot\u00e3o push</li> <li>Resistor de 10k\u03a9 (pull-up)</li> <li>LED</li> <li>Resistor de 220\u03a9</li> <li>Jumpers</li> <li>Protoboard</li> </ul>"},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/#circuito_2","title":"Circuito","text":"<ul> <li>Conecte o bot\u00e3o ao pino GPIO13</li> <li>Use resistor de 10k\u03a9 como pull-up</li> <li>Conecte o LED ao pino GPIO2 atrav\u00e9s do resistor de 220\u03a9</li> </ul>"},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/#codigo_2","title":"C\u00f3digo","text":"<pre><code>const int buttonPin = 13;  // Pino do bot\u00e3o\nconst int ledPin = 2;      // Pino do LED\n\n// Vari\u00e1veis para controle de debounce\nvolatile bool ledState = false;\nvolatile unsigned long lastDebounceTime = 0;\nunsigned long debounceDelay = 200;  // Tempo de debounce em ms\n\n// Fun\u00e7\u00e3o de interrup\u00e7\u00e3o chamada quando o bot\u00e3o \u00e9 pressionado\nvoid IRAM_ATTR buttonISR() {\n  if ((millis() - lastDebounceTime) &gt; debounceDelay) {\n    ledState = !ledState;\n    lastDebounceTime = millis();\n  }\n}\n\nvoid setup() {\n  Serial.begin(115200);\n\n  pinMode(buttonPin, INPUT_PULLUP);  // Configurar pino do bot\u00e3o com pull-up interno\n  pinMode(ledPin, OUTPUT);\n\n  // Anexa a interrup\u00e7\u00e3o ao pino do bot\u00e3o - FALLING para detectar quando o bot\u00e3o \u00e9 pressionado\n  attachInterrupt(digitalPinToInterrupt(buttonPin), buttonISR, FALLING);\n}\n\nvoid loop() {\n  // Atualiza estado do LED baseado no estado armazenado pela interrup\u00e7\u00e3o\n  digitalWrite(ledPin, ledState);\n\n  // Adicionando um pequeno delay para estabilidade\n  delay(50);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/#projeto-4-medindo-temperatura-e-umidade-com-dht11dht22","title":"Projeto 4: Medindo Temperatura e Umidade com DHT11/DHT22","text":""},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/#materiais_3","title":"Materiais","text":"<ul> <li>ESP32 DevKit</li> <li>Sensor DHT11 ou DHT22</li> <li>Resistor de 10k\u03a9 (pull-up)</li> <li>Jumpers</li> <li>Protoboard</li> </ul>"},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/#circuito_3","title":"Circuito","text":"<ul> <li>Conecte o pino de dados do DHT ao pino GPIO4</li> <li>Conecte VCC ao 3.3V do ESP32</li> <li>Conecte GND ao GND do ESP32</li> <li>Use o resistor de 10k\u03a9 como pull-up entre dados e VCC</li> </ul>"},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/#codigo_3","title":"C\u00f3digo","text":"<p>Primeiro, instale a biblioteca DHT do Adafruit: 1. Na IDE do Arduino, v\u00e1 para Sketch &gt; Incluir Biblioteca &gt; Gerenciar Bibliotecas 2. Pesquise por \"DHT\" e instale a biblioteca \"DHT sensor library by Adafruit\" 3. Tamb\u00e9m instale a \"Adafruit Unified Sensor\" (depend\u00eancia)</p> <pre><code>#include \"DHT.h\"\n\n#define DHTPIN 4       // Pino digital conectado ao DHT\n#define DHTTYPE DHT11  // DHT11 ou DHT22, dependendo do seu sensor\n\nDHT dht(DHTPIN, DHTTYPE);\n\nvoid setup() {\n  Serial.begin(115200);\n  Serial.println(\"Teste do sensor DHT!\");\n\n  dht.begin();\n}\n\nvoid loop() {\n  // Aguarde alguns segundos entre as medi\u00e7\u00f5es\n  delay(2000);\n\n  // A leitura da temperatura e umidade pode levar at\u00e9 250ms\n  float h = dht.readHumidity();\n  float t = dht.readTemperature();\n  float f = dht.readTemperature(true); // True = Fahrenheit\n\n  // Verifica se alguma leitura falhou\n  if (isnan(h) || isnan(t) || isnan(f)) {\n    Serial.println(\"Falha na leitura do sensor DHT!\");\n    return;\n  }\n\n  // Calcula o \u00edndice de calor\n  float hif = dht.computeHeatIndex(f, h);\n  float hic = dht.computeHeatIndex(t, h, false);\n\n  Serial.print(\"Umidade: \");\n  Serial.print(h);\n  Serial.print(\"%\\t\");\n  Serial.print(\"Temperatura: \");\n  Serial.print(t);\n  Serial.print(\"\u00b0C \");\n  Serial.print(f);\n  Serial.print(\"\u00b0F\\t\");\n  Serial.print(\"\u00cdndice de Calor: \");\n  Serial.print(hic);\n  Serial.print(\"\u00b0C \");\n  Serial.print(hif);\n  Serial.println(\"\u00b0F\");\n}\n</code></pre>"},{"location":"aulas/iot/esp32/03-Primeiros-Projetos/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<p>Esses projetos b\u00e1sicos fornecem uma introdu\u00e7\u00e3o pr\u00e1tica ao ESP32 e suas funcionalidades fundamentais. Na pr\u00f3xima aula, exploraremos a conectividade WiFi do ESP32, um dos recursos mais importantes para aplica\u00e7\u00f5es IoT.</p> <p>Experimente modificar os c\u00f3digos e combinar esses projetos para criar suas pr\u00f3prias aplica\u00e7\u00f5es!</p>"},{"location":"aulas/iot/esp32/04-Conectividade-WiFi/","title":"Conectividade WiFi com ESP32","text":"<p>Nesta aula, exploraremos um dos recursos mais importantes do ESP32 para IoT: a conectividade WiFi. Aprenderemos a conectar o ESP32 a redes WiFi, implementar um servidor web simples, realizar requisi\u00e7\u00f5es HTTP e trabalhar com servi\u00e7os online.</p>"},{"location":"aulas/iot/esp32/04-Conectividade-WiFi/#conectando-o-esp32-a-uma-rede-wifi","title":"Conectando o ESP32 a uma Rede WiFi","text":""},{"location":"aulas/iot/esp32/04-Conectividade-WiFi/#codigo-basico-de-conexao-wifi","title":"C\u00f3digo B\u00e1sico de Conex\u00e3o WiFi","text":"<pre><code>#include &lt;WiFi.h&gt;\n\nconst char* ssid     = \"SuaRedeWiFi\";\nconst char* password = \"SuaSenhaWiFi\";\n\nvoid setup() {\n  Serial.begin(115200);\n  delay(10);\n\n  // Mensagem inicial\n  Serial.println();\n  Serial.println(\"Conectando a:\");\n  Serial.println(ssid);\n\n  // Inicia a conex\u00e3o WiFi\n  WiFi.begin(ssid, password);\n\n  // Aguarda a conex\u00e3o\n  while (WiFi.status() != WL_CONNECTED) {\n    delay(500);\n    Serial.print(\".\");\n  }\n\n  // Conex\u00e3o estabelecida\n  Serial.println(\"\");\n  Serial.println(\"WiFi conectado\");\n  Serial.println(\"Endere\u00e7o IP: \");\n  Serial.println(WiFi.localIP());\n}\n\nvoid loop() {\n  // Verifica periodicamente o status da conex\u00e3o\n  if (WiFi.status() != WL_CONNECTED) {\n    Serial.println(\"Conex\u00e3o WiFi perdida. Reconectando...\");\n    WiFi.begin(ssid, password);\n\n    while (WiFi.status() != WL_CONNECTED) {\n      delay(500);\n      Serial.print(\".\");\n    }\n\n    Serial.println(\"Reconectado ao WiFi\");\n  }\n\n  delay(30000); // Verifica a cada 30 segundos\n}\n</code></pre>"},{"location":"aulas/iot/esp32/04-Conectividade-WiFi/#criando-um-servidor-web-simples","title":"Criando um Servidor Web Simples","text":"<p>O ESP32 pode funcionar como um servidor web, permitindo o controle e monitoramento atrav\u00e9s de uma p\u00e1gina web.</p>"},{"location":"aulas/iot/esp32/04-Conectividade-WiFi/#codigo-do-servidor-web-basico","title":"C\u00f3digo do Servidor Web B\u00e1sico","text":"<pre><code>#include &lt;WiFi.h&gt;\n#include &lt;WebServer.h&gt;\n\nconst char* ssid     = \"SuaRedeWiFi\";\nconst char* password = \"SuaSenhaWiFi\";\n\n// Define a porta do servidor web (80 \u00e9 a porta padr\u00e3o HTTP)\nWebServer server(80);\n\n// Pino do LED interno\nconst int ledPin = 2;\nbool ledStatus = false;\n\nvoid setup() {\n  Serial.begin(115200);\n  pinMode(ledPin, OUTPUT);\n  digitalWrite(ledPin, LOW);\n\n  // Conecta ao WiFi\n  WiFi.begin(ssid, password);\n  Serial.print(\"Conectando ao WiFi\");\n\n  while (WiFi.status() != WL_CONNECTED) {\n    delay(500);\n    Serial.print(\".\");\n  }\n\n  Serial.println(\"\");\n  Serial.println(\"WiFi conectado\");\n  Serial.println(\"Endere\u00e7o IP: \");\n  Serial.println(WiFi.localIP());\n\n  // Configurar as rotas do servidor\n  server.on(\"/\", handleRoot);\n  server.on(\"/toggle\", handleToggle);\n\n  // Iniciar o servidor\n  server.begin();\n  Serial.println(\"Servidor HTTP iniciado\");\n}\n\nvoid loop() {\n  // Manipula as requisi\u00e7\u00f5es do cliente\n  server.handleClient();\n}\n\n// Fun\u00e7\u00e3o para p\u00e1gina principal\nvoid handleRoot() {\n  String html = \"&lt;!DOCTYPE html&gt;&lt;html&gt;\";\n  html += \"&lt;head&gt;&lt;meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1\\\"&gt;\";\n  html += \"&lt;style&gt;body { font-family: Arial; text-align: center; margin: 0px auto; padding: 20px; }\";\n  html += \"button { background-color: #4CAF50; color: white; padding: 15px 32px; font-size: 16px; margin: 4px; cursor: pointer; border: none; border-radius: 4px; }\";\n  html += \"&lt;/style&gt;&lt;/head&gt;&lt;body&gt;\";\n  html += \"&lt;h1&gt;ESP32 Web Server&lt;/h1&gt;\";\n\n  if (ledStatus) {\n    html += \"&lt;p&gt;Status do LED: LIGADO&lt;/p&gt;\";\n    html += \"&lt;button onclick=\\\"window.location.href='/toggle'\\\"&gt;DESLIGAR&lt;/button&gt;\";\n  } else {\n    html += \"&lt;p&gt;Status do LED: DESLIGADO&lt;/p&gt;\";\n    html += \"&lt;button onclick=\\\"window.location.href='/toggle'\\\"&gt;LIGAR&lt;/button&gt;\";\n  }\n\n  html += \"&lt;/body&gt;&lt;/html&gt;\";\n  server.send(200, \"text/html\", html);\n}\n\n// Fun\u00e7\u00e3o para alternar o estado do LED\nvoid handleToggle() {\n  ledStatus = !ledStatus;\n  digitalWrite(ledPin, ledStatus ? HIGH : LOW);\n  server.sendHeader(\"Location\", \"/\");\n  server.send(303);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/04-Conectividade-WiFi/#fazendo-requisicoes-http-cliente","title":"Fazendo Requisi\u00e7\u00f5es HTTP (Cliente)","text":"<p>O ESP32 tamb\u00e9m pode atuar como cliente, realizando requisi\u00e7\u00f5es HTTP para servi\u00e7os web.</p>"},{"location":"aulas/iot/esp32/04-Conectividade-WiFi/#codigo-para-requisicoes-get","title":"C\u00f3digo para Requisi\u00e7\u00f5es GET","text":"<pre><code>#include &lt;WiFi.h&gt;\n#include &lt;HTTPClient.h&gt;\n\nconst char* ssid     = \"SuaRedeWiFi\";\nconst char* password = \"SuaSenhaWiFi\";\n\n// URL para a qual faremos a requisi\u00e7\u00e3o\nconst char* serverUrl = \"http://jsonplaceholder.typicode.com/todos/1\";\n\nvoid setup() {\n  Serial.begin(115200);\n  WiFi.begin(ssid, password);\n\n  Serial.print(\"Conectando ao WiFi\");\n  while (WiFi.status() != WL_CONNECTED) {\n    delay(500);\n    Serial.print(\".\");\n  }\n\n  Serial.println(\"\");\n  Serial.println(\"WiFi conectado\");\n  Serial.println(\"Endere\u00e7o IP: \");\n  Serial.println(WiFi.localIP());\n}\n\nvoid loop() {\n  // Verifica se est\u00e1 conectado ao WiFi\n  if (WiFi.status() == WL_CONNECTED) {\n    HTTPClient http;\n\n    Serial.print(\"Realizando requisi\u00e7\u00e3o HTTP GET para: \");\n    Serial.println(serverUrl);\n\n    // Inicia a conex\u00e3o com o servidor\n    http.begin(serverUrl);\n\n    // Envia a requisi\u00e7\u00e3o HTTP GET\n    int httpResponseCode = http.GET();\n\n    if (httpResponseCode &gt; 0) {\n      Serial.print(\"C\u00f3digo de resposta HTTP: \");\n      Serial.println(httpResponseCode);\n\n      if (httpResponseCode == HTTP_CODE_OK) {\n        String payload = http.getString();\n        Serial.println(\"Resposta:\");\n        Serial.println(payload);\n      }\n    } else {\n      Serial.print(\"Erro na requisi\u00e7\u00e3o HTTP. C\u00f3digo de erro: \");\n      Serial.println(httpResponseCode);\n    }\n\n    // Libera os recursos\n    http.end();\n  } else {\n    Serial.println(\"Erro na conex\u00e3o WiFi\");\n  }\n\n  // Espera 60 segundos para a pr\u00f3xima requisi\u00e7\u00e3o\n  delay(60000);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/04-Conectividade-WiFi/#enviando-dados-para-um-servico-web-post","title":"Enviando Dados para um Servi\u00e7o Web (POST)","text":""},{"location":"aulas/iot/esp32/04-Conectividade-WiFi/#codigo-para-requisicoes-post","title":"C\u00f3digo para Requisi\u00e7\u00f5es POST","text":"<pre><code>#include &lt;WiFi.h&gt;\n#include &lt;HTTPClient.h&gt;\n#include &lt;ArduinoJson.h&gt;\n\nconst char* ssid     = \"SuaRedeWiFi\";\nconst char* password = \"SuaSenhaWiFi\";\n\n// URL para onde enviaremos os dados\nconst char* serverUrl = \"http://jsonplaceholder.typicode.com/posts\";\n\n// Simula\u00e7\u00e3o de leitura de sensores\nfloat getTemperature() {\n  return random(2000, 3000) / 100.0; // Simula temperatura entre 20-30\u00b0C\n}\n\nfloat getHumidity() {\n  return random(4000, 8000) / 100.0; // Simula umidade entre 40-80%\n}\n\nvoid setup() {\n  Serial.begin(115200);\n  WiFi.begin(ssid, password);\n\n  Serial.print(\"Conectando ao WiFi\");\n  while (WiFi.status() != WL_CONNECTED) {\n    delay(500);\n    Serial.print(\".\");\n  }\n\n  Serial.println(\"\");\n  Serial.println(\"WiFi conectado\");\n  Serial.println(\"Endere\u00e7o IP: \");\n  Serial.println(WiFi.localIP());\n\n  // Inicializa o gerador de n\u00fameros aleat\u00f3rios\n  randomSeed(analogRead(0));\n}\n\nvoid loop() {\n  // Verifica se est\u00e1 conectado ao WiFi\n  if (WiFi.status() == WL_CONNECTED) {\n    HTTPClient http;\n\n    // Cria um objeto JSON para armazenar os dados\n    StaticJsonDocument&lt;200&gt; doc;\n\n    // Adiciona os dados dos \"sensores\"\n    doc[\"deviceId\"] = \"ESP32-\" + String((uint32_t)ESP.getEfuseMac(), HEX);\n    doc[\"temperature\"] = getTemperature();\n    doc[\"humidity\"] = getHumidity();\n    doc[\"timestamp\"] = millis();\n\n    // Serializa o JSON para uma String\n    String jsonString;\n    serializeJson(doc, jsonString);\n\n    Serial.println(\"Enviando dados via POST:\");\n    Serial.println(jsonString);\n\n    // Inicia a conex\u00e3o com o servidor\n    http.begin(serverUrl);\n\n    // Especifica o tipo de conte\u00fado\n    http.addHeader(\"Content-Type\", \"application/json\");\n\n    // Envia a requisi\u00e7\u00e3o HTTP POST com os dados JSON\n    int httpResponseCode = http.POST(jsonString);\n\n    if (httpResponseCode &gt; 0) {\n      Serial.print(\"C\u00f3digo de resposta HTTP: \");\n      Serial.println(httpResponseCode);\n\n      String response = http.getString();\n      Serial.println(\"Resposta:\");\n      Serial.println(response);\n    } else {\n      Serial.print(\"Erro na requisi\u00e7\u00e3o HTTP POST. C\u00f3digo de erro: \");\n      Serial.println(httpResponseCode);\n    }\n\n    // Libera os recursos\n    http.end();\n  } else {\n    Serial.println(\"Erro na conex\u00e3o WiFi\");\n  }\n\n  // Espera 30 segundos para a pr\u00f3xima requisi\u00e7\u00e3o\n  delay(30000);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/04-Conectividade-WiFi/#utilizando-wifimanager-para-configuracao-facil","title":"Utilizando WiFiManager para Configura\u00e7\u00e3o F\u00e1cil","text":"<p>O WiFiManager permite configurar as credenciais WiFi sem precisar recompilar o c\u00f3digo.</p>"},{"location":"aulas/iot/esp32/04-Conectividade-WiFi/#instalacao-da-biblioteca-wifimanager","title":"Instala\u00e7\u00e3o da Biblioteca WiFiManager","text":"<ol> <li>Na IDE do Arduino, acesse Sketch &gt; Incluir Biblioteca &gt; Gerenciar Bibliotecas...</li> <li>Pesquise por \"WiFiManager\"</li> <li>Instale a biblioteca \"WiFiManager by tzapu\"</li> </ol>"},{"location":"aulas/iot/esp32/04-Conectividade-WiFi/#codigo-com-wifimanager","title":"C\u00f3digo com WiFiManager","text":"<pre><code>#include &lt;WiFiManager.h&gt;\n\nvoid setup() {\n  Serial.begin(115200);\n\n  // Inicializa o WiFiManager\n  WiFiManager wifiManager;\n\n  // Mensagem informativa\n  Serial.println(\"Conectando \u00e0 rede WiFi...\");\n  Serial.println(\"Se n\u00e3o conectar automaticamente, conecte-se \u00e0 rede 'ESP32-AutoConnectAP'\");\n  Serial.println(\"e acesse 192.168.4.1 no navegador para configurar\");\n\n  // Define o timeout para modo de configura\u00e7\u00e3o (30 segundos)\n  wifiManager.setConfigPortalTimeout(180);\n\n  // Tenta se conectar ou inicia o portal de configura\u00e7\u00e3o\n  bool res = wifiManager.autoConnect(\"ESP32-AutoConnectAP\", \"password123\");\n\n  if(!res) {\n    Serial.println(\"Falha ao conectar\");\n    // Reset e tenta novamente\n    ESP.restart();\n  } \n  else {\n    // Conectado com sucesso\n    Serial.println(\"WiFi conectado\");\n    Serial.print(\"Endere\u00e7o IP: \");\n    Serial.println(WiFi.localIP());\n  }\n}\n\nvoid loop() {\n  // Seu c\u00f3digo principal aqui\n  delay(1000);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/04-Conectividade-WiFi/#projeto-pratico-monitor-de-temperatura-e-umidade-com-dashboard-web","title":"Projeto Pr\u00e1tico: Monitor de Temperatura e Umidade com Dashboard Web","text":"<p>Combine os exemplos anteriores para criar um projeto que: 1. L\u00ea dados de um sensor DHT22 2. Fornece uma interface web para visualiza\u00e7\u00e3o em tempo real 3. Armazena leituras recentes para exibir um gr\u00e1fico</p> <p>O c\u00f3digo completo para este projeto est\u00e1 dispon\u00edvel no reposit\u00f3rio de exemplos.</p>"},{"location":"aulas/iot/esp32/04-Conectividade-WiFi/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<p>Na pr\u00f3xima aula, exploraremos a conectividade Bluetooth do ESP32, outro recurso importante para aplica\u00e7\u00f5es IoT que requerem comunica\u00e7\u00e3o de curto alcance.</p> <p>Desafio: Modifique o servidor web para incluir mais funcionalidades, como controlar m\u00faltiplos LEDs ou exibir dados de diferentes sensores.</p>"},{"location":"aulas/iot/esp32/05-Bluetooth/","title":"Bluetooth com ESP32","text":"<p>Nesta aula, vamos explorar as capacidades Bluetooth do ESP32, que oferecem duas tecnologias principais: Bluetooth Cl\u00e1ssico (BR/EDR) e Bluetooth Low Energy (BLE). O ESP32 suporta ambos os modos, tornando-o extremamente vers\u00e1til para diferentes aplica\u00e7\u00f5es IoT.</p>"},{"location":"aulas/iot/esp32/05-Bluetooth/#bluetooth-classico-vs-bluetooth-low-energy-ble","title":"Bluetooth Cl\u00e1ssico vs Bluetooth Low Energy (BLE)","text":"Caracter\u00edstica Bluetooth Cl\u00e1ssico Bluetooth Low Energy Consumo de energia Maior Muito baixo Taxa de transfer\u00eancia At\u00e9 3 Mbps At\u00e9 1 Mbps Alcance ~10m ~100m Aplica\u00e7\u00f5es Streaming de \u00e1udio, transfer\u00eancia de arquivos IoT, sensores, beacons Topologia Ponto a ponto Ponto a ponto, broadcast Tempo de conex\u00e3o Lento (~100ms) R\u00e1pido (~6ms)"},{"location":"aulas/iot/esp32/05-Bluetooth/#1-bluetooth-serial-classico","title":"1. Bluetooth Serial (Cl\u00e1ssico)","text":"<p>O Bluetooth Serial permite que o ESP32 se comunique como um dispositivo de porta serial Bluetooth, semelhante aos m\u00f3dulos HC-05/HC-06 usados com Arduino.</p>"},{"location":"aulas/iot/esp32/05-Bluetooth/#exemplo-esp32-como-servidor-bluetooth-serial","title":"Exemplo: ESP32 como Servidor Bluetooth Serial","text":"<pre><code>#include \"BluetoothSerial.h\"\n\n// Verifica se Bluetooth Serial est\u00e1 habilitado\n#if !defined(CONFIG_BT_ENABLED) || !defined(CONFIG_BLUEDROID_ENABLED)\n#error Bluetooth n\u00e3o est\u00e1 habilitado! Por favor habilite nas configura\u00e7\u00f5es da placa.\n#endif\n\nBluetoothSerial SerialBT;\nString mensagem = \"\";\nchar caractereRecebido;\n\nconst int ledPin = 2; // LED interno do ESP32\n\nvoid setup() {\n  Serial.begin(115200);\n  pinMode(ledPin, OUTPUT);\n\n  SerialBT.begin(\"ESP32_BT_Serial\"); // Nome do dispositivo Bluetooth\n  Serial.println(\"Dispositivo Bluetooth iniciado. Voc\u00ea pode emparelhar agora!\");\n}\n\nvoid loop() {\n  // Verifica se h\u00e1 dados dispon\u00edveis do Bluetooth\n  if (SerialBT.available()) {\n    caractereRecebido = SerialBT.read();\n\n    if (caractereRecebido != '\\n'){\n      // Acumula os caracteres recebidos na mensagem\n      mensagem += caractereRecebido;\n    }\n    else {\n      // Processa a mensagem completa\n      mensagem.trim();\n      Serial.println(\"Mensagem recebida: \" + mensagem);\n\n      // Verifica comandos\n      if (mensagem == \"ON\") {\n        digitalWrite(ledPin, HIGH);\n        SerialBT.println(\"LED LIGADO\");\n        Serial.println(\"LED LIGADO\");\n      } \n      else if (mensagem == \"OFF\") {\n        digitalWrite(ledPin, LOW);\n        SerialBT.println(\"LED DESLIGADO\");\n        Serial.println(\"LED DESLIGADO\");\n      }\n      else if (mensagem == \"STATUS\") {\n        String status = digitalRead(ledPin) ? \"LIGADO\" : \"DESLIGADO\";\n        SerialBT.println(\"Status do LED: \" + status);\n        Serial.println(\"Status do LED: \" + status);\n      }\n      else {\n        SerialBT.println(\"Comando n\u00e3o reconhecido. Use ON, OFF ou STATUS\");\n      }\n\n      // Limpa a mensagem para a pr\u00f3xima leitura\n      mensagem = \"\";\n    }\n  }\n\n  // Verifica se h\u00e1 dados do Serial (monitor) para enviar via Bluetooth\n  if (Serial.available()) {\n    SerialBT.write(Serial.read());\n  }\n\n  delay(20);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/05-Bluetooth/#como-testar","title":"Como testar:","text":"<ol> <li>Carregue o c\u00f3digo no ESP32</li> <li>No seu smartphone, instale um aplicativo de terminal Bluetooth, como:</li> <li>\"Serial Bluetooth Terminal\" (Android)</li> <li>\"Bluetooth Terminal\" (iOS)</li> <li>Procure por dispositivos Bluetooth e conecte-se ao \"ESP32_BT_Serial\"</li> <li>Envie comandos: ON, OFF, STATUS</li> </ol>"},{"location":"aulas/iot/esp32/05-Bluetooth/#2-bluetooth-low-energy-ble","title":"2. Bluetooth Low Energy (BLE)","text":"<p>BLE \u00e9 projetado para aplica\u00e7\u00f5es que exigem transmiss\u00e3o de pequenas quantidades de dados com baixo consumo de energia.</p>"},{"location":"aulas/iot/esp32/05-Bluetooth/#conceitos-importantes-do-ble","title":"Conceitos Importantes do BLE:","text":"<ul> <li>Perif\u00e9rico: Dispositivo que anuncia sua presen\u00e7a e disponibiliza dados (ESP32 neste exemplo)</li> <li>Central: Dispositivo que se conecta ao perif\u00e9rico (smartphone, tablet)</li> <li>Servi\u00e7o: Cole\u00e7\u00e3o de caracter\u00edsticas relacionadas</li> <li>Caracter\u00edstica: Valor que pode ser lido, escrito ou notificado</li> <li>UUID: Identificador \u00fanico para servi\u00e7os e caracter\u00edsticas</li> </ul>"},{"location":"aulas/iot/esp32/05-Bluetooth/#exemplo-esp32-como-servidor-ble-periferico","title":"Exemplo: ESP32 como Servidor BLE (Perif\u00e9rico)","text":"<pre><code>#include &lt;BLEDevice.h&gt;\n#include &lt;BLEServer.h&gt;\n#include &lt;BLEUtils.h&gt;\n#include &lt;BLE2902.h&gt;\n\n// UUIDs para nosso servi\u00e7o e caracter\u00edsticas\n#define SERVICE_UUID        \"4fafc201-1fb5-459e-8fcc-c5c9c331914b\"\n#define CHARACTERISTIC_UUID \"beb5483e-36e1-4688-b7f5-ea07361b26a8\"\n\n// Vari\u00e1veis para controle do dispositivo\nBLEServer* pServer = NULL;\nBLECharacteristic* pCharacteristic = NULL;\nbool deviceConnected = false;\nbool oldDeviceConnected = false;\nint value = 0;\n\n// Classe para controlar eventos de conex\u00e3o\nclass MyServerCallbacks: public BLEServerCallbacks {\n    void onConnect(BLEServer* pServer) {\n      deviceConnected = true;\n      Serial.println(\"Dispositivo conectado\");\n    };\n\n    void onDisconnect(BLEServer* pServer) {\n      deviceConnected = false;\n      Serial.println(\"Dispositivo desconectado\");\n    }\n};\n\n// Classe para controlar eventos da caracter\u00edstica\nclass MyCallbacks: public BLECharacteristicCallbacks {\n    void onWrite(BLECharacteristic *pCharacteristic) {\n      std::string value = pCharacteristic-&gt;getValue();\n\n      if (value.length() &gt; 0) {\n        Serial.println(\"*********\");\n        Serial.print(\"Novo valor: \");\n\n        for (int i = 0; i &lt; value.length(); i++) {\n          Serial.print(value[i]);\n        }\n\n        Serial.println();\n\n        // Verifica comandos\n        if (value == \"ON\") {\n          digitalWrite(2, HIGH);\n          Serial.println(\"LED ligado\");\n        } \n        else if (value == \"OFF\") {\n          digitalWrite(2, LOW);\n          Serial.println(\"LED desligado\");\n        }\n\n        Serial.println(\"*********\");\n      }\n    }\n};\n\nvoid setup() {\n  Serial.begin(115200);\n  pinMode(2, OUTPUT);\n\n  // Cria o dispositivo BLE\n  BLEDevice::init(\"ESP32_BLE\");\n\n  // Cria o servidor BLE\n  pServer = BLEDevice::createServer();\n  pServer-&gt;setCallbacks(new MyServerCallbacks());\n\n  // Cria o servi\u00e7o BLE\n  BLEService *pService = pServer-&gt;createService(SERVICE_UUID);\n\n  // Cria a caracter\u00edstica BLE\n  pCharacteristic = pService-&gt;createCharacteristic(\n                      CHARACTERISTIC_UUID,\n                      BLECharacteristic::PROPERTY_READ   |\n                      BLECharacteristic::PROPERTY_WRITE  |\n                      BLECharacteristic::PROPERTY_NOTIFY\n                    );\n\n  // Cria um descritor para permitir notifica\u00e7\u00f5es\n  pCharacteristic-&gt;addDescriptor(new BLE2902());\n\n  // Define os callbacks para a caracter\u00edstica\n  pCharacteristic-&gt;setCallbacks(new MyCallbacks());\n\n  // Valor inicial\n  pCharacteristic-&gt;setValue(\"Hello BLE\");\n\n  // Inicia o servi\u00e7o\n  pService-&gt;start();\n\n  // Come\u00e7a a anunciar\n  BLEAdvertising *pAdvertising = BLEDevice::getAdvertising();\n  pAdvertising-&gt;addServiceUUID(SERVICE_UUID);\n  pAdvertising-&gt;setScanResponse(false);\n  pAdvertising-&gt;setMinPreferred(0x0);  // Ajuda com iPhone\n  BLEDevice::startAdvertising();\n\n  Serial.println(\"BLE pronto, aguardando conex\u00f5es...\");\n}\n\nvoid loop() {\n  // Notifica o cliente periodicamente se conectado\n  if (deviceConnected) {\n    // Converte o valor para string\n    char txString[8];\n    sprintf(txString, \"%d\", value);\n\n    // Define o valor e notifica\n    pCharacteristic-&gt;setValue(txString);\n    pCharacteristic-&gt;notify();\n\n    value++;\n    delay(1000);\n  }\n\n  // Lida com a desconex\u00e3o\n  if (!deviceConnected &amp;&amp; oldDeviceConnected) {\n    delay(500); // Tempo para o BT stack se atualizar\n    pServer-&gt;startAdvertising(); // Reinicia an\u00fancios\n    Serial.println(\"Reiniciando an\u00fancios\");\n    oldDeviceConnected = deviceConnected;\n  }\n\n  // Lida com a conex\u00e3o\n  if (deviceConnected &amp;&amp; !oldDeviceConnected) {\n    oldDeviceConnected = deviceConnected;\n  }\n}\n</code></pre>"},{"location":"aulas/iot/esp32/05-Bluetooth/#como-testar-ble","title":"Como testar BLE:","text":"<ol> <li>Carregue o c\u00f3digo no ESP32</li> <li>No seu smartphone, instale um aplicativo de teste BLE, como:</li> <li>\"nRF Connect\" (Android/iOS)</li> <li>\"BLE Scanner\" (Android/iOS)</li> <li>Escaneie dispositivos BLE dispon\u00edveis</li> <li>Conecte-se ao \"ESP32_BLE\"</li> <li>Procure o servi\u00e7o com o UUID especificado</li> <li>Interaja com a caracter\u00edstica:</li> <li>Leia o valor atual</li> <li>Escreva \"ON\" ou \"OFF\" para controlar o LED</li> <li>Ative notifica\u00e7\u00f5es para receber atualiza\u00e7\u00f5es peri\u00f3dicas</li> </ol>"},{"location":"aulas/iot/esp32/05-Bluetooth/#3-projeto-sensor-de-temperatura-ble","title":"3. Projeto: Sensor de Temperatura BLE","text":"<p>Este projeto combina BLE com um sensor de temperatura para criar um dispositivo que transmite leituras de temperatura via Bluetooth LE.</p>"},{"location":"aulas/iot/esp32/05-Bluetooth/#materiais-necessarios","title":"Materiais necess\u00e1rios:","text":"<ul> <li>ESP32</li> <li>Sensor de temperatura (DHT11, DHT22 ou DS18B20)</li> <li>Jumpers</li> <li>Resistor pull-up de 4.7k\u03a9 (para DS18B20) ou 10k\u03a9 (para DHT11/22)</li> </ul>"},{"location":"aulas/iot/esp32/05-Bluetooth/#codigo-para-sensor-ds18b20","title":"C\u00f3digo para sensor DS18B20:","text":"<pre><code>#include &lt;BLEDevice.h&gt;\n#include &lt;BLEServer.h&gt;\n#include &lt;BLEUtils.h&gt;\n#include &lt;BLE2902.h&gt;\n#include &lt;OneWire.h&gt;\n#include &lt;DallasTemperature.h&gt;\n\n// Pino do sensor DS18B20\n#define ONE_WIRE_BUS 4\n\n// Configura\u00e7\u00e3o do BLE\n#define SERVICE_UUID        \"4fafc201-1fb5-459e-8fcc-c5c9c331914b\"\n#define TEMP_CHAR_UUID      \"beb5483e-36e1-4688-b7f5-ea07361b26a8\"\n\n// Configura\u00e7\u00e3o do sensor\nOneWire oneWire(ONE_WIRE_BUS);\nDallasTemperature sensors(&amp;oneWire);\n\n// Vari\u00e1veis BLE\nBLEServer* pServer = NULL;\nBLECharacteristic* pTemperatureCharacteristic = NULL;\nbool deviceConnected = false;\nbool oldDeviceConnected = false;\n\n// Callbacks de conex\u00e3o BLE\nclass MyServerCallbacks: public BLEServerCallbacks {\n    void onConnect(BLEServer* pServer) {\n      deviceConnected = true;\n      Serial.println(\"Cliente conectado\");\n    };\n\n    void onDisconnect(BLEServer* pServer) {\n      deviceConnected = false;\n      Serial.println(\"Cliente desconectado\");\n    }\n};\n\nvoid setup() {\n  Serial.begin(115200);\n  Serial.println(\"Iniciando sensor de temperatura BLE\");\n\n  // Inicializa o sensor\n  sensors.begin();\n\n  // Configura o BLE\n  BLEDevice::init(\"ESP32 Temp Sensor\");\n\n  // Cria o servidor BLE\n  pServer = BLEDevice::createServer();\n  pServer-&gt;setCallbacks(new MyServerCallbacks());\n\n  // Cria o servi\u00e7o BLE\n  BLEService *pService = pServer-&gt;createService(SERVICE_UUID);\n\n  // Cria a caracter\u00edstica de temperatura\n  pTemperatureCharacteristic = pService-&gt;createCharacteristic(\n                      TEMP_CHAR_UUID,\n                      BLECharacteristic::PROPERTY_READ   |\n                      BLECharacteristic::PROPERTY_NOTIFY\n                    );\n\n  // Adiciona descritor para notifica\u00e7\u00f5es\n  pTemperatureCharacteristic-&gt;addDescriptor(new BLE2902());\n\n  // Inicia o servi\u00e7o\n  pService-&gt;start();\n\n  // Inicia o an\u00fancio\n  BLEAdvertising *pAdvertising = BLEDevice::getAdvertising();\n  pAdvertising-&gt;addServiceUUID(SERVICE_UUID);\n  pAdvertising-&gt;setScanResponse(false);\n  pAdvertising-&gt;setMinPreferred(0x0);\n  BLEDevice::startAdvertising();\n\n  Serial.println(\"Sensor de temperatura BLE pronto\");\n}\n\nvoid loop() {\n  // L\u00ea a temperatura a cada 2 segundos\n  if (deviceConnected) {\n    // Requisita leitura da temperatura\n    sensors.requestTemperatures();\n\n    // Obt\u00e9m a temperatura em Celsius\n    float tempC = sensors.getTempCByIndex(0);\n\n    // Verifica se a leitura foi bem-sucedida\n    if (tempC != DEVICE_DISCONNECTED_C) {\n      Serial.print(\"Temperatura: \");\n      Serial.print(tempC);\n      Serial.println(\"\u00b0C\");\n\n      // Converte para string\n      char tempString[8];\n      sprintf(tempString, \"%.2f\", tempC);\n\n      // Define o valor e notifica\n      pTemperatureCharacteristic-&gt;setValue(tempString);\n      pTemperatureCharacteristic-&gt;notify();\n    } else {\n      Serial.println(\"Erro ao ler sensor de temperatura!\");\n    }\n  }\n\n  // Lida com desconex\u00e3o\n  if (!deviceConnected &amp;&amp; oldDeviceConnected) {\n    delay(500);\n    pServer-&gt;startAdvertising();\n    Serial.println(\"Reiniciando an\u00fancios\");\n    oldDeviceConnected = deviceConnected;\n  }\n\n  // Lida com nova conex\u00e3o\n  if (deviceConnected &amp;&amp; !oldDeviceConnected) {\n    oldDeviceConnected = deviceConnected;\n  }\n\n  delay(2000);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/05-Bluetooth/#4-bluetooth-beacon-ibeacon","title":"4. Bluetooth Beacon (iBeacon)","text":"<p>Os beacons s\u00e3o dispositivos BLE que transmitem continuamente seus identificadores. S\u00e3o usados para servi\u00e7os baseados em localiza\u00e7\u00e3o, rastreamento de ativos e marketing de proximidade.</p>"},{"location":"aulas/iot/esp32/05-Bluetooth/#exemplo-de-ibeacon-com-esp32","title":"Exemplo de iBeacon com ESP32:","text":"<pre><code>#include &lt;BLEDevice.h&gt;\n#include &lt;BLEUtils.h&gt;\n#include &lt;BLEBeacon.h&gt;\n\n// Defina seu pr\u00f3prio UUID para o beacon\n#define BEACON_UUID \"8ec76ea3-6668-48da-9866-75be8bc86f4d\"\n\n// Configura\u00e7\u00e3o do beacon\nuint16_t beaconUUID = 0xFFFF; // ID do fabricante para Apple iBeacon\nuint16_t major = 1;           // Valor Major - usado para agrupar beacons\nuint16_t minor = 100;         // Valor Minor - identifica um beacon espec\u00edfico\nint txPower = -59;            // Pot\u00eancia do sinal a 1 metro (calibrado)\n\nBLEAdvertising *pAdvertising;\nBLEAdvertisementData advertisementData;\n\nvoid setup() {\n  Serial.begin(115200);\n  Serial.println(\"Iniciando ESP32 iBeacon\");\n\n  // Inicializa o BLE\n  BLEDevice::init(\"ESP32 iBeacon\");\n\n  // Obt\u00e9m o objeto de advertising\n  pAdvertising = BLEDevice::getAdvertising();\n\n  // Configura os dados do iBeacon\n  setBeacon();\n\n  // Inicia o advertising\n  pAdvertising-&gt;start();\n\n  Serial.println(\"iBeacon iniciado\");\n}\n\nvoid setBeacon() {\n  BLEBeacon oBeacon = BLEBeacon();\n  oBeacon.setManufacturerId(beaconUUID);\n  oBeacon.setProximityUUID(BLEUUID(BEACON_UUID));\n  oBeacon.setMajor(major);\n  oBeacon.setMinor(minor);\n  oBeacon.setSignalPower(txPower);\n\n  BLEAdvertisementData oAdvertisementData = BLEAdvertisementData();\n  BLEAdvertisementData oScanResponseData = BLEAdvertisementData();\n\n  oAdvertisementData.setFlags(0x04); // BR_EDR_NOT_SUPPORTED 0x04\n\n  std::string strServiceData = \"\";\n  strServiceData += (char)26;     // Tamanho dos dados em bytes\n  strServiceData += (char)0xFF;   // Tipo de dados (FF = dados do fabricante)\n  strServiceData += oBeacon.getData(); \n\n  oAdvertisementData.addData(strServiceData);\n\n  pAdvertising-&gt;setAdvertisementData(oAdvertisementData);\n  pAdvertising-&gt;setScanResponseData(oScanResponseData);\n}\n\nvoid loop() {\n  // Para beacons, n\u00e3o \u00e9 necess\u00e1rio fazer nada no loop\n  // O BLE se encarrega de transmitir constantemente\n  delay(1000);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/05-Bluetooth/#como-usar-o-ibeacon","title":"Como usar o iBeacon:","text":"<ol> <li>Carregue o c\u00f3digo no ESP32</li> <li>Use um aplicativo de beacon scanner como:</li> <li>\"Beacon Scanner\" (Android)</li> <li>\"Locate Beacon\" (iOS)</li> <li>O ESP32 ser\u00e1 detectado como um iBeacon com os identificadores definidos</li> </ol>"},{"location":"aulas/iot/esp32/05-Bluetooth/#consideracoes-de-bateria-e-energia","title":"Considera\u00e7\u00f5es de Bateria e Energia","text":"<p>O Bluetooth Low Energy foi projetado para consumir muito menos energia que o Bluetooth cl\u00e1ssico:</p> <ul> <li>Use o modo BLE para dispositivos alimentados por bateria</li> <li>Para reduzir ainda mais o consumo:</li> <li>Aumente o intervalo de advertising (menos frequente)</li> <li>Diminua a pot\u00eancia de transmiss\u00e3o</li> <li>Use o Deep Sleep entre transmiss\u00f5es</li> </ul>"},{"location":"aulas/iot/esp32/05-Bluetooth/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<p>Na pr\u00f3xima aula, vamos explorar como trabalhar com sensores e atuadores comuns em projetos IoT com o ESP32, expandindo as possibilidades de seus projetos.</p> <p>Desafio: Modifique o exemplo do sensor de temperatura para incluir tamb\u00e9m umidade (com um DHT22) e adicione uma caracter\u00edstica de LED que possa ser controlada pelo smartphone.</p>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/","title":"Sensores e Atuadores com ESP32","text":"<p>Nesta aula, exploraremos como conectar e utilizar diversos sensores e atuadores com o ESP32. O ESP32 possui muitos pinos GPIO, ADC, DAC, capacitivos e interfaces como I2C, SPI e UART que permitem a conex\u00e3o com uma ampla variedade de dispositivos.</p>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#pinagem-do-esp32","title":"Pinagem do ESP32","text":"<p>Antes de come\u00e7armos, \u00e9 importante entender a pinagem do ESP32. Dependendo da placa que voc\u00ea est\u00e1 utilizando, os pinos f\u00edsicos podem variar, mas as funcionalidades s\u00e3o semelhantes.</p> <p></p> <p>Observa\u00e7\u00f5es importantes: - Nem todos os pinos podem ser usados como entrada/sa\u00edda de prop\u00f3sito geral - Os pinos GPIO6-GPIO11 s\u00e3o usados para o flash SPI interno (n\u00e3o dispon\u00edveis) - Os pinos GPIO0, GPIO2 e GPIO15 t\u00eam fun\u00e7\u00f5es especiais durante o boot - GPIO34-GPIO39 s\u00e3o apenas entradas (n\u00e3o possuem resistores pull-up/pull-down internos)</p>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#1-sensores-analogicos","title":"1. Sensores Anal\u00f3gicos","text":"<p>O ESP32 possui dois conversores anal\u00f3gico-digital (ADC) que permitem ler valores anal\u00f3gicos: - ADC1: Conectado aos pinos GPIO32-GPIO39 - ADC2: Conectado aos pinos GPIO0, GPIO2, GPIO4, GPIO12-GPIO15, GPIO25-GPIO27 (n\u00e3o dispon\u00edvel quando WiFi est\u00e1 ativo)</p>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#leitura-de-sensor-analogico-ldr","title":"Leitura de Sensor Anal\u00f3gico (LDR)","text":"<pre><code>// Sensor de luz (LDR) conectado ao pino GPIO34\nconst int ldrPin = 34;\n\nvoid setup() {\n  Serial.begin(115200);\n  delay(1000); // Aguarda estabiliza\u00e7\u00e3o do serial\n\n  // Configura resolu\u00e7\u00e3o do ADC (padr\u00e3o \u00e9 12 bits, mas voc\u00ea pode escolher 9-12)\n  analogReadResolution(12); // 0-4095\n}\n\nvoid loop() {\n  // L\u00ea o valor do LDR\n  int ldrValue = analogRead(ldrPin);\n\n  // Converte o valor para porcentagem (0-100%)\n  int lightPercent = map(ldrValue, 0, 4095, 0, 100);\n\n  // Imprime os valores\n  Serial.print(\"Valor ADC: \");\n  Serial.print(ldrValue);\n  Serial.print(\" | Luz (%): \");\n  Serial.println(lightPercent);\n\n  delay(1000);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#calibracao-de-sensores-analogicos","title":"Calibra\u00e7\u00e3o de Sensores Anal\u00f3gicos","text":"<p>A calibra\u00e7\u00e3o \u00e9 importante para obter medi\u00e7\u00f5es precisas:</p> <pre><code>// Constantes para calibra\u00e7\u00e3o\nconst int numReadings = 10;\nconst int minRawValue = 500;   // Valor raw m\u00ednimo esperado\nconst int maxRawValue = 3800;  // Valor raw m\u00e1ximo esperado\n\n// Fun\u00e7\u00e3o para calibrar sensor anal\u00f3gico\nfloat calibrateAnalogSensor(int pin) {\n  int sum = 0;\n\n  // Faz v\u00e1rias leituras e calcula a m\u00e9dia\n  for (int i = 0; i &lt; numReadings; i++) {\n    sum += analogRead(pin);\n    delay(10);\n  }\n  int rawValue = sum / numReadings;\n\n  // Limita o valor dentro da faixa esperada\n  rawValue = constrain(rawValue, minRawValue, maxRawValue);\n\n  // Mapeia para porcentagem (0-100)\n  float result = map(rawValue, minRawValue, maxRawValue, 0, 100);\n\n  return result;\n}\n</code></pre>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#2-saidas-analogicas-dac-e-pwm","title":"2. Sa\u00eddas Anal\u00f3gicas (DAC) e PWM","text":"<p>O ESP32 possui dois conversores digital-anal\u00f3gico (DAC) nos pinos 25 e 26. Al\u00e9m disso, qualquer pino GPIO pode ser usado para PWM.</p>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#exemplo-de-dac","title":"Exemplo de DAC","text":"<pre><code>// Usando o DAC do ESP32\nconst int dacPin = 25;\n\nvoid setup() {\n  Serial.begin(115200);\n}\n\nvoid loop() {\n  // Loop de 0 a 255 (8 bits)\n  for (int value = 0; value &lt;= 255; value++) {\n    // Escreve o valor no DAC\n    dacWrite(dacPin, value);\n\n    Serial.print(\"Valor DAC: \");\n    Serial.println(value);\n\n    delay(50);\n  }\n\n  // Loop decrescente de 255 a 0\n  for (int value = 255; value &gt;= 0; value--) {\n    dacWrite(dacPin, value);\n\n    Serial.print(\"Valor DAC: \");\n    Serial.println(value);\n\n    delay(50);\n  }\n}\n</code></pre>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#exemplo-de-pwm-led-com-brilho-variavel","title":"Exemplo de PWM (LED com Brilho Vari\u00e1vel)","text":"<pre><code>// Controlando um LED com PWM\nconst int ledPin = 16;\n\n// Configura\u00e7\u00f5es de PWM\nconst int freq = 5000;\nconst int ledChannel = 0;\nconst int resolution = 8; // 8 bits = 0-255\n\nvoid setup() {\n  // Configura\u00e7\u00e3o do PWM\n  ledcSetup(ledChannel, freq, resolution);\n\n  // Anexa o canal ao pino\n  ledcAttachPin(ledPin, ledChannel);\n}\n\nvoid loop() {\n  // Aumenta o brilho gradualmente\n  for (int dutyCycle = 0; dutyCycle &lt;= 255; dutyCycle++) {\n    ledcWrite(ledChannel, dutyCycle);\n    delay(15);\n  }\n\n  // Diminui o brilho gradualmente\n  for (int dutyCycle = 255; dutyCycle &gt;= 0; dutyCycle--) {\n    ledcWrite(ledChannel, dutyCycle);\n    delay(15);\n  }\n}\n</code></pre>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#3-sensores-digitais","title":"3. Sensores Digitais","text":""},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#dht11dht22-temperatura-e-umidade","title":"DHT11/DHT22 (Temperatura e Umidade)","text":"<pre><code>#include \"DHT.h\"\n\n#define DHTPIN 4      // Pino conectado ao sensor\n#define DHTTYPE DHT22 // DHT 22 (AM2302)\n\nDHT dht(DHTPIN, DHTTYPE);\n\nvoid setup() {\n  Serial.begin(115200);\n  Serial.println(\"Teste de sensor DHT!\");\n\n  dht.begin();\n}\n\nvoid loop() {\n  // Aguarda entre leituras\n  delay(2000);\n\n  // L\u00ea a umidade\n  float h = dht.readHumidity();\n  // L\u00ea a temperatura em Celsius\n  float t = dht.readTemperature();\n  // L\u00ea a temperatura em Fahrenheit\n  float f = dht.readTemperature(true);\n\n  // Verifica se h\u00e1 falha de leitura\n  if (isnan(h) || isnan(t) || isnan(f)) {\n    Serial.println(\"Falha ao ler do sensor DHT!\");\n    return;\n  }\n\n  // Calcula o \u00edndice de calor\n  float hic = dht.computeHeatIndex(t, h, false);\n\n  Serial.print(\"Umidade: \");\n  Serial.print(h);\n  Serial.print(\"%  Temperatura: \");\n  Serial.print(t);\n  Serial.print(\"\u00b0C \");\n  Serial.print(f);\n  Serial.print(\"\u00b0F  \u00cdndice de Calor: \");\n  Serial.print(hic);\n  Serial.println(\"\u00b0C\");\n}\n</code></pre>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#sensor-de-distancia-ultrassonico-hc-sr04","title":"Sensor de Dist\u00e2ncia Ultrass\u00f4nico HC-SR04","text":"<pre><code>const int trigPin = 5;\nconst int echoPin = 18;\n\n// Define constantes do som\n#define SOUND_SPEED 0.034 // em cm/us\n#define CM_TO_INCH 0.393701\n\nlong duration;\nfloat distanceCm;\nfloat distanceInch;\n\nvoid setup() {\n  Serial.begin(115200);\n  pinMode(trigPin, OUTPUT);\n  pinMode(echoPin, INPUT);\n}\n\nvoid loop() {\n  // Limpa o trigPin\n  digitalWrite(trigPin, LOW);\n  delayMicroseconds(2);\n\n  // Aciona o trigPin por 10 microsegundos\n  digitalWrite(trigPin, HIGH);\n  delayMicroseconds(10);\n  digitalWrite(trigPin, LOW);\n\n  // L\u00ea o echoPin e obt\u00e9m o tempo de viagem da onda\n  duration = pulseIn(echoPin, HIGH);\n\n  // Calcula a dist\u00e2ncia\n  distanceCm = duration * SOUND_SPEED/2;\n  distanceInch = distanceCm * CM_TO_INCH;\n\n  // Imprime no Serial Monitor\n  Serial.print(\"Dist\u00e2ncia (cm): \");\n  Serial.println(distanceCm);\n  Serial.print(\"Dist\u00e2ncia (inch): \");\n  Serial.println(distanceInch);\n\n  delay(1000);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#4-sensores-i2c","title":"4. Sensores I\u00b2C","text":"<p>O ESP32 suporta a interface I\u00b2C, que permite conectar m\u00faltiplos dispositivos usando apenas dois pinos.</p>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#bme280-temperatura-umidade-e-pressao","title":"BME280 (Temperatura, Umidade e Press\u00e3o)","text":"<pre><code>#include &lt;Wire.h&gt;\n#include &lt;Adafruit_Sensor.h&gt;\n#include &lt;Adafruit_BME280.h&gt;\n\n// Pinos I\u00b2C do ESP32\n#define SDA_PIN 21\n#define SCL_PIN 22\n\n// Endere\u00e7o I\u00b2C do BME280 (0x76 ou 0x77)\n#define BME_ADDR 0x76\n\nAdafruit_BME280 bme;\n\nvoid setup() {\n  Serial.begin(115200);\n\n  // Inicializa o I\u00b2C com os pinos definidos\n  Wire.begin(SDA_PIN, SCL_PIN);\n\n  // Inicializa o BME280\n  bool status = bme.begin(BME_ADDR);\n  if (!status) {\n    Serial.println(\"N\u00e3o foi poss\u00edvel encontrar o sensor BME280!\");\n    while (1);\n  }\n\n  Serial.println(\"Sensor BME280 encontrado!\");\n}\n\nvoid loop() {\n  // L\u00ea os valores\n  float temperature = bme.readTemperature();\n  float humidity = bme.readHumidity();\n  float pressure = bme.readPressure() / 100.0F; // em hPa\n  float altitude = bme.readAltitude(1013.25); // altitude estimada\n\n  // Imprime no Serial Monitor\n  Serial.print(\"Temperatura: \");\n  Serial.print(temperature);\n  Serial.println(\" \u00b0C\");\n\n  Serial.print(\"Umidade: \");\n  Serial.print(humidity);\n  Serial.println(\" %\");\n\n  Serial.print(\"Press\u00e3o: \");\n  Serial.print(pressure);\n  Serial.println(\" hPa\");\n\n  Serial.print(\"Altitude aprox: \");\n  Serial.print(altitude);\n  Serial.println(\" m\");\n\n  delay(2000);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#5-sensores-spi","title":"5. Sensores SPI","text":"<p>A interface SPI (Serial Peripheral Interface) permite comunica\u00e7\u00e3o r\u00e1pida com diversos dispositivos como displays, cart\u00f5es SD, e sensores.</p>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#leitor-de-cartao-sd","title":"Leitor de Cart\u00e3o SD","text":"<pre><code>#include &lt;SPI.h&gt;\n#include &lt;SD.h&gt;\n\n// Pino do CS para o m\u00f3dulo SD\n#define SD_CS 5\n\n// Nome do arquivo no SD\n#define FILENAME \"/data.txt\"\n\n// Vari\u00e1veis para simular sensores\nfloat temperatura, umidade;\n\nvoid setup() {\n  Serial.begin(115200);\n  delay(1000);\n\n  Serial.println(\"Inicializando cart\u00e3o SD...\");\n\n  // Inicializa o cart\u00e3o SD\n  if (!SD.begin(SD_CS)) {\n    Serial.println(\"Falha na inicializa\u00e7\u00e3o do cart\u00e3o SD!\");\n    return;\n  }\n\n  Serial.println(\"Cart\u00e3o SD inicializado com sucesso.\");\n\n  // Cria um cabe\u00e7alho no arquivo se ele n\u00e3o existir\n  if (!SD.exists(FILENAME)) {\n    File dataFile = SD.open(FILENAME, FILE_WRITE);\n    if (dataFile) {\n      dataFile.println(\"Time,Temperature,Humidity\");\n      dataFile.close();\n      Serial.println(\"Arquivo criado com sucesso!\");\n    } else {\n      Serial.println(\"Erro ao criar arquivo!\");\n    }\n  }\n}\n\nvoid loop() {\n  // Simula leituras de sensores\n  temperatura = random(2000, 3000) / 100.0;\n  umidade = random(4000, 9000) / 100.0;\n\n  // Obt\u00e9m o tempo atual desde o in\u00edcio do programa\n  unsigned long currentTime = millis() / 1000; // segundos\n\n  // Cria uma string com os dados\n  String dataString = String(currentTime) + \",\" + \n                      String(temperatura) + \",\" + \n                      String(umidade);\n\n  // Abre o arquivo para escrita\n  File dataFile = SD.open(FILENAME, FILE_APPEND);\n\n  if (dataFile) {\n    dataFile.println(dataString);\n    dataFile.close();\n    Serial.println(\"Dados gravados: \" + dataString);\n  } else {\n    Serial.println(\"Erro ao abrir o arquivo!\");\n  }\n\n  delay(5000); // Espera 5 segundos\n}\n</code></pre>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#6-display-oled-i2c","title":"6. Display OLED I\u00b2C","text":"<p>Displays s\u00e3o importantes para visualizar dados sem conex\u00e3o com um computador.</p> <pre><code>#include &lt;Wire.h&gt;\n#include &lt;Adafruit_GFX.h&gt;\n#include &lt;Adafruit_SSD1306.h&gt;\n\n#define SCREEN_WIDTH 128\n#define SCREEN_HEIGHT 64\n#define OLED_RESET    -1\n#define SCREEN_ADDRESS 0x3C\n\nAdafruit_SSD1306 display(SCREEN_WIDTH, SCREEN_HEIGHT, &amp;Wire, OLED_RESET);\n\nvoid setup() {\n  Serial.begin(115200);\n\n  // Inicializa o display OLED\n  if(!display.begin(SSD1306_SWITCHCAPVCC, SCREEN_ADDRESS)) {\n    Serial.println(F(\"Falha ao alocar SSD1306\"));\n    for(;;); // Loop infinito\n  }\n\n  // Limpa o buffer\n  display.clearDisplay();\n\n  // Configura o tamanho, cor e posi\u00e7\u00e3o do texto\n  display.setTextSize(1);\n  display.setTextColor(WHITE);\n  display.setCursor(0, 0);\n\n  // Adiciona texto ao buffer\n  display.println(F(\"Teste do Display OLED\"));\n  display.println(F(\"com ESP32\"));\n  display.println();\n  display.print(F(\"Temperatura: \"));\n  display.println(F(\"25.5 C\"));\n  display.print(F(\"Umidade: \"));\n  display.println(F(\"60%\"));\n\n  // Exibe o buffer na tela\n  display.display();\n}\n\nvoid loop() {\n  // No loop principal, podemos atualizar partes espec\u00edficas da tela\n  // Simula\u00e7\u00e3o de um contador\n  static int counter = 0;\n\n  // Limpa apenas a \u00e1rea onde o contador ser\u00e1 exibido\n  display.fillRect(0, 48, 128, 16, BLACK);\n  display.setCursor(0, 48);\n  display.print(F(\"Contador: \"));\n  display.println(counter);\n  display.display();\n\n  counter++;\n  delay(1000);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#7-sensores-touch-capacitivos","title":"7. Sensores Touch Capacitivos","text":"<p>O ESP32 possui 10 sensores touch capacitivos integrados, o que permite criar interfaces sens\u00edveis ao toque sem componentes adicionais.</p> <pre><code>// Definindo os pinos touch\nconst int touchPin = 4; // GPIO4 (T0)\n\n// Vari\u00e1vel para armazenar o valor da leitura touch\nint touchValue;\n\n// Threshold para detec\u00e7\u00e3o de toque\nconst int threshold = 40;\n\n// LED que ser\u00e1 controlado pelo touch\nconst int ledPin = 2;\n\nvoid setup() {\n  Serial.begin(115200);\n  pinMode(ledPin, OUTPUT);\n\n  delay(1000);\n  Serial.println(\"ESP32 Touch Test\");\n}\n\nvoid loop() {\n  // L\u00ea o valor do sensor touch\n  touchValue = touchRead(touchPin);\n\n  Serial.print(\"Valor Touch: \");\n  Serial.println(touchValue);\n\n  // Verifica se o valor est\u00e1 abaixo do threshold (quanto menor o valor, mais forte o toque)\n  if (touchValue &lt; threshold) {\n    digitalWrite(ledPin, HIGH);\n    Serial.println(\"Touch detectado!\");\n  } else {\n    digitalWrite(ledPin, LOW);\n  }\n\n  delay(500);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#projeto-pratico-estacao-meteorologica","title":"Projeto Pr\u00e1tico: Esta\u00e7\u00e3o Meteorol\u00f3gica","text":"<p>Combinaremos um sensor BME280 (temperatura, umidade e press\u00e3o), um display OLED e um cart\u00e3o SD para criar uma esta\u00e7\u00e3o meteorol\u00f3gica que exibe e registra dados ambientais.</p> <pre><code>#include &lt;Wire.h&gt;\n#include &lt;SPI.h&gt;\n#include &lt;SD.h&gt;\n#include &lt;Adafruit_GFX.h&gt;\n#include &lt;Adafruit_SSD1306.h&gt;\n#include &lt;Adafruit_BME280.h&gt;\n\n// Configura\u00e7\u00f5es do display OLED\n#define SCREEN_WIDTH 128\n#define SCREEN_HEIGHT 64\n#define OLED_RESET    -1\n#define SCREEN_ADDRESS 0x3C\n\n// Pino CS do cart\u00e3o SD\n#define SD_CS 5\n\n// Arquivo de log no SD\n#define FILENAME \"/clima.csv\"\n\n// Intervalo de log em milissegundos\n#define LOG_INTERVAL 60000\n\n// Objetos para sensores e display\nAdafruit_SSD1306 display(SCREEN_WIDTH, SCREEN_HEIGHT, &amp;Wire, OLED_RESET);\nAdafruit_BME280 bme;\n\n// Vari\u00e1veis para armazenar hora da \u00faltima leitura\nunsigned long lastLogTime = 0;\n\nvoid setup() {\n  Serial.begin(115200);\n  delay(1000);\n\n  // Inicializa o I\u00b2C\n  Wire.begin();\n\n  // Inicializa o display OLED\n  if(!display.begin(SSD1306_SWITCHCAPVCC, SCREEN_ADDRESS)) {\n    Serial.println(F(\"Falha ao inicializar o display OLED\"));\n    while(1);\n  }\n  display.clearDisplay();\n  display.setTextSize(1);\n  display.setTextColor(WHITE);\n  display.setCursor(0, 0);\n  display.println(F(\"Estacao Meteorologica\"));\n  display.println(F(\"Inicializando...\"));\n  display.display();\n\n  // Inicializa o sensor BME280\n  if (!bme.begin(0x76)) {\n    Serial.println(F(\"Erro ao encontrar o sensor BME280!\"));\n    display.println(F(\"Erro sensor BME280!\"));\n    display.display();\n    while (1);\n  }\n\n  // Inicializa o cart\u00e3o SD\n  display.println(F(\"Inicializando SD...\"));\n  display.display();\n\n  if (!SD.begin(SD_CS)) {\n    Serial.println(F(\"Falha na inicializa\u00e7\u00e3o do cart\u00e3o SD!\"));\n    display.println(F(\"Erro SD! Logs desabilitados\"));\n    display.display();\n    delay(2000);\n  } else {\n    // Cria o arquivo de log se n\u00e3o existir\n    if (!SD.exists(FILENAME)) {\n      File dataFile = SD.open(FILENAME, FILE_WRITE);\n      if (dataFile) {\n        dataFile.println(\"Timestamp,Temperatura,Umidade,Pressao,Altitude\");\n        dataFile.close();\n      }\n    }\n    display.println(F(\"SD OK!\"));\n    display.display();\n  }\n\n  delay(2000);\n}\n\nvoid loop() {\n  // L\u00ea os dados do sensor\n  float temperatura = bme.readTemperature();\n  float umidade = bme.readHumidity();\n  float pressao = bme.readPressure() / 100.0F;\n  float altitude = bme.readAltitude(1013.25);\n\n  // Atualiza o display\n  display.clearDisplay();\n  display.setCursor(0, 0);\n  display.println(F(\"Estacao Meteorologica\"));\n  display.println();\n\n  display.print(F(\"Temp: \"));\n  display.print(temperatura);\n  display.println(F(\" C\"));\n\n  display.print(F(\"Umid: \"));\n  display.print(umidade);\n  display.println(F(\" %\"));\n\n  display.print(F(\"Pres: \"));\n  display.print(pressao);\n  display.println(F(\" hPa\"));\n\n  display.print(F(\"Alt: \"));\n  display.print(altitude);\n  display.println(F(\" m\"));\n\n  // Mostra o tempo desde a \u00faltima grava\u00e7\u00e3o no SD\n  unsigned long timeNow = millis();\n  if (SD.begin(SD_CS)) {\n    display.print(F(\"Prox. log: \"));\n    if (timeNow - lastLogTime &lt; LOG_INTERVAL) {\n      display.print((LOG_INTERVAL - (timeNow - lastLogTime)) / 1000);\n      display.println(F(\" s\"));\n    } else {\n      display.println(F(\"Agora!\"));\n    }\n  } else {\n    display.println(F(\"SD desconectado!\"));\n  }\n\n  display.display();\n\n  // Verifica se \u00e9 hora de gravar no SD\n  if (timeNow - lastLogTime &gt;= LOG_INTERVAL) {\n    if (SD.begin(SD_CS)) {\n      // Cria a string de dados\n      String dataString = String(timeNow / 1000) + \",\" + \n                         String(temperatura) + \",\" + \n                         String(umidade) + \",\" + \n                         String(pressao) + \",\" + \n                         String(altitude);\n\n      // Abre o arquivo para escrita\n      File dataFile = SD.open(FILENAME, FILE_APPEND);\n      if (dataFile) {\n        dataFile.println(dataString);\n        dataFile.close();\n        Serial.println(\"Dados gravados no SD: \" + dataString);\n      }\n    }\n\n    // Atualiza o tempo da \u00faltima grava\u00e7\u00e3o\n    lastLogTime = timeNow;\n  }\n\n  delay(1000);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/06-Sensores-Atuadores/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<p>Na pr\u00f3xima aula, exploraremos o protocolo MQTT, fundamental para a comunica\u00e7\u00e3o IoT, permitindo que o ESP32 publique dados e receba comandos atrav\u00e9s da internet.</p> <p>Desafio: Adapte o projeto da esta\u00e7\u00e3o meteorol\u00f3gica para incluir um sensor adicional (como um sensor de luz ou de qualidade do ar) e customize a interface do display para mostrar os novos dados.</p>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/","title":"Protocolo MQTT com ESP32","text":"<p>Nesta aula, vamos explorar o protocolo MQTT (Message Queuing Telemetry Transport), um dos protocolos mais utilizados em aplica\u00e7\u00f5es IoT devido \u00e0 sua leveza, efici\u00eancia e confiabilidade.</p>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#o-que-e-mqtt","title":"O que \u00e9 MQTT?","text":"<p>MQTT \u00e9 um protocolo de mensagens leve, projetado para dispositivos com recursos limitados e redes de baixa largura de banda, alta lat\u00eancia ou inst\u00e1veis. Ele utiliza um modelo de comunica\u00e7\u00e3o publish/subscribe (publica\u00e7\u00e3o/assinatura), que \u00e9 mais eficiente que o modelo tradicional request/response (requisi\u00e7\u00e3o/resposta).</p>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#caracteristicas-principais-do-mqtt","title":"Caracter\u00edsticas Principais do MQTT","text":"<ul> <li>Leve: Requer m\u00ednimos recursos de hardware</li> <li>Bidirecional: Permite tanto enviar quanto receber dados</li> <li>Seguro: Suporta autentica\u00e7\u00e3o e criptografia TLS/SSL</li> <li>Desacoplado: Os clientes n\u00e3o precisam conhecer uns aos outros</li> <li>Escalon\u00e1vel: Pode suportar milhares de dispositivos conectados</li> <li>Eficiente: Pouco overhead de protocolo</li> </ul>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#conceitos-fundamentais-do-mqtt","title":"Conceitos Fundamentais do MQTT","text":""},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#broker","title":"Broker","text":"<p>O broker \u00e9 o servidor central que gerencia todas as mensagens entre os clientes. Ele recebe mensagens publicadas e as encaminha para os clientes que se inscreveram nos t\u00f3picos correspondentes.</p> <p>Brokers MQTT populares: - Mosquitto - HiveMQ - AWS IoT Core - Azure IoT Hub - CloudMQTT</p>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#publishsubscribe","title":"Publish/Subscribe","text":"<ul> <li>Publish (Publica\u00e7\u00e3o): Ato de enviar uma mensagem para um t\u00f3pico espec\u00edfico no broker</li> <li>Subscribe (Assinatura): Ato de informar ao broker que voc\u00ea deseja receber mensagens de um t\u00f3pico espec\u00edfico</li> </ul>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#topicos-topics","title":"T\u00f3picos (Topics)","text":"<p>Os t\u00f3picos s\u00e3o strings que definem canais de comunica\u00e7\u00e3o. Eles s\u00e3o organizados em hierarquias, separados por barras (/).</p> <p>Exemplos: - <code>casa/sala/temperatura</code> - <code>casa/quarto/lampada</code> - <code>carro/sensor/velocidade</code></p>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#qos-quality-of-service","title":"QoS (Quality of Service)","text":"<p>O MQTT oferece tr\u00eas n\u00edveis de garantia de entrega:</p> <ul> <li>QoS 0 (At most once): A mensagem \u00e9 enviada no m\u00e1ximo uma vez, sem confirma\u00e7\u00e3o</li> <li>QoS 1 (At least once): A mensagem \u00e9 enviada pelo menos uma vez, com confirma\u00e7\u00e3o</li> <li>QoS 2 (Exactly once): A mensagem \u00e9 entregue exatamente uma vez, atrav\u00e9s de um handshake de 4 etapas</li> </ul>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#configurando-um-broker-mqtt-local","title":"Configurando um Broker MQTT Local","text":"<p>Antes de programar o ESP32, vamos configurar um broker MQTT local para testes:</p>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#instalacao-do-mosquitto-broker-mqtt","title":"Instala\u00e7\u00e3o do Mosquitto (Broker MQTT)","text":""},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#windows","title":"Windows","text":"<ol> <li>Baixe o instalador em mosquitto.org/download</li> <li>Execute o instalador e siga as instru\u00e7\u00f5es</li> <li>Adicione mosquitto ao PATH do sistema</li> </ol>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#macos","title":"macOS","text":"<pre><code>brew install mosquitto\n</code></pre>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<pre><code>sudo apt-get update\nsudo apt-get install mosquitto mosquitto-clients\n</code></pre>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#iniciando-o-mosquitto","title":"Iniciando o Mosquitto","text":"<pre><code>mosquitto -v\n</code></pre> <p>Isso inicia o broker na porta padr\u00e3o 1883.</p>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#programando-o-esp32-com-mqtt","title":"Programando o ESP32 com MQTT","text":""},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#bibliotecas-necessarias","title":"Bibliotecas Necess\u00e1rias","text":"<p>Na IDE do Arduino, instale a biblioteca PubSubClient: 1. Ferramentas &gt; Gerenciar Bibliotecas 2. Procure por \"PubSubClient\" 3. Instale a biblioteca PubSubClient por Nick O'Leary</p>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#exemplo-1-publicando-dados-do-esp32","title":"Exemplo 1: Publicando Dados do ESP32","text":"<pre><code>#include &lt;WiFi.h&gt;\n#include &lt;PubSubClient.h&gt;\n\n// Configura\u00e7\u00f5es do WiFi\nconst char* ssid = \"SuaRedeWiFi\";\nconst char* password = \"SuaSenhaWiFi\";\n\n// Configura\u00e7\u00f5es do MQTT\nconst char* mqttServer = \"192.168.1.100\"; // Endere\u00e7o IP do seu broker\nconst int mqttPort = 1883;\nconst char* mqttUser = \"\"; // Se o broker requer autentica\u00e7\u00e3o\nconst char* mqttPassword = \"\"; // Se o broker requer autentica\u00e7\u00e3o\nconst char* mqttClientId = \"ESP32Client\";\n\n// T\u00f3picos MQTT\nconst char* topicTemperature = \"esp32/temperature\";\nconst char* topicHumidity = \"esp32/humidity\";\n\n// Vari\u00e1veis para armazenar leituras simuladas\nfloat temperature;\nfloat humidity;\n\n// Objetos WiFi e MQTT\nWiFiClient espClient;\nPubSubClient client(espClient);\n\n// Timestamp da \u00faltima publica\u00e7\u00e3o\nunsigned long lastPublish = 0;\nconst int publishInterval = 5000; // Intervalo de publica\u00e7\u00e3o (5 segundos)\n\nvoid setup() {\n  Serial.begin(115200);\n\n  // Conecta ao WiFi\n  setupWiFi();\n\n  // Configura o servidor MQTT\n  client.setServer(mqttServer, mqttPort);\n\n  // Configura o callback para recep\u00e7\u00e3o de mensagens (n\u00e3o utilizado neste exemplo)\n  client.setCallback(callback);\n}\n\nvoid loop() {\n  // Verifica e mant\u00e9m a conex\u00e3o com o broker MQTT\n  if (!client.connected()) {\n    reconnect();\n  }\n  client.loop();\n\n  // Publica a cada intervalo definido\n  unsigned long now = millis();\n  if (now - lastPublish &gt; publishInterval) {\n    lastPublish = now;\n\n    // Simula leituras de sensores\n    temperature = random(2000, 3000) / 100.0;\n    humidity = random(4000, 8000) / 100.0;\n\n    // Publica os dados\n    publishData();\n  }\n}\n\nvoid setupWiFi() {\n  delay(10);\n  Serial.println();\n  Serial.print(\"Conectando a \");\n  Serial.println(ssid);\n\n  WiFi.begin(ssid, password);\n\n  while (WiFi.status() != WL_CONNECTED) {\n    delay(500);\n    Serial.print(\".\");\n  }\n\n  Serial.println(\"\");\n  Serial.println(\"WiFi conectado\");\n  Serial.println(\"Endere\u00e7o IP: \");\n  Serial.println(WiFi.localIP());\n}\n\nvoid callback(char* topic, byte* payload, unsigned int length) {\n  // Este callback \u00e9 chamado quando uma mensagem \u00e9 recebida em t\u00f3picos inscritos\n  // N\u00e3o \u00e9 utilizado neste exemplo, mas \u00e9 necess\u00e1rio para a biblioteca\n  Serial.print(\"Mensagem recebida [\");\n  Serial.print(topic);\n  Serial.print(\"] \");\n\n  for (int i = 0; i &lt; length; i++) {\n    Serial.print((char)payload[i]);\n  }\n  Serial.println();\n}\n\nvoid reconnect() {\n  // Loop at\u00e9 reconectar\n  while (!client.connected()) {\n    Serial.print(\"Tentando conex\u00e3o MQTT...\");\n\n    // Tenta conectar\n    if (client.connect(mqttClientId, mqttUser, mqttPassword)) {\n      Serial.println(\"conectado\");\n\n      // Uma vez conectado, publica uma mensagem...\n      client.publish(\"esp32/status\", \"online\");\n\n      // ... e se inscreve em t\u00f3picos (neste exemplo n\u00e3o h\u00e1 inscri\u00e7\u00e3o)\n\n    } else {\n      Serial.print(\"falhou, rc=\");\n      Serial.print(client.state());\n      Serial.println(\" tentando novamente em 5 segundos\");\n      delay(5000);\n    }\n  }\n}\n\nvoid publishData() {\n  // Converte os valores para strings\n  char tempString[8];\n  char humString[8];\n  dtostrf(temperature, 1, 2, tempString);\n  dtostrf(humidity, 1, 2, humString);\n\n  // Publica a temperatura\n  Serial.print(\"Publicando temperatura: \");\n  Serial.println(tempString);\n  client.publish(topicTemperature, tempString);\n\n  // Publica a umidade\n  Serial.print(\"Publicando umidade: \");\n  Serial.println(humString);\n  client.publish(topicHumidity, humString);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#exemplo-2-subscrevendo-se-a-um-topico-e-controlando-um-led","title":"Exemplo 2: Subscrevendo-se a um T\u00f3pico e Controlando um LED","text":"<pre><code>#include &lt;WiFi.h&gt;\n#include &lt;PubSubClient.h&gt;\n\n// Configura\u00e7\u00f5es do WiFi\nconst char* ssid = \"SuaRedeWiFi\";\nconst char* password = \"SuaSenhaWiFi\";\n\n// Configura\u00e7\u00f5es do MQTT\nconst char* mqttServer = \"192.168.1.100\"; // Endere\u00e7o IP do seu broker\nconst int mqttPort = 1883;\nconst char* mqttUser = \"\"; // Se o broker requer autentica\u00e7\u00e3o\nconst char* mqttPassword = \"\"; // Se o broker requer autentica\u00e7\u00e3o\nconst char* mqttClientId = \"ESP32Client\";\n\n// T\u00f3picos MQTT\nconst char* topicLED = \"esp32/led\";\nconst char* topicStatus = \"esp32/status\";\n\n// Pino do LED\nconst int ledPin = 2;\n\n// Objetos WiFi e MQTT\nWiFiClient espClient;\nPubSubClient client(espClient);\n\nvoid setup() {\n  Serial.begin(115200);\n\n  // Configura o pino do LED\n  pinMode(ledPin, OUTPUT);\n  digitalWrite(ledPin, LOW);\n\n  // Conecta ao WiFi\n  setupWiFi();\n\n  // Configura o servidor MQTT\n  client.setServer(mqttServer, mqttPort);\n\n  // Configura o callback para recep\u00e7\u00e3o de mensagens\n  client.setCallback(callback);\n}\n\nvoid loop() {\n  // Verifica e mant\u00e9m a conex\u00e3o com o broker MQTT\n  if (!client.connected()) {\n    reconnect();\n  }\n  client.loop();\n}\n\nvoid setupWiFi() {\n  delay(10);\n  Serial.println();\n  Serial.print(\"Conectando a \");\n  Serial.println(ssid);\n\n  WiFi.begin(ssid, password);\n\n  while (WiFi.status() != WL_CONNECTED) {\n    delay(500);\n    Serial.print(\".\");\n  }\n\n  Serial.println(\"\");\n  Serial.println(\"WiFi conectado\");\n  Serial.println(\"Endere\u00e7o IP: \");\n  Serial.println(WiFi.localIP());\n}\n\nvoid callback(char* topic, byte* payload, unsigned int length) {\n  Serial.print(\"Mensagem recebida [\");\n  Serial.print(topic);\n  Serial.print(\"] \");\n\n  // Converte o payload para uma string\n  char message[length + 1];\n  for (int i = 0; i &lt; length; i++) {\n    message[i] = (char)payload[i];\n    Serial.print(message[i]);\n  }\n  message[length] = '\\0';\n  Serial.println();\n\n  // Verifica se a mensagem \u00e9 para o t\u00f3pico do LED\n  if (String(topic) == topicLED) {\n    if (String(message) == \"ON\") {\n      digitalWrite(ledPin, HIGH);\n      Serial.println(\"LED ligado\");\n      client.publish(topicStatus, \"LED est\u00e1 ON\");\n    } else if (String(message) == \"OFF\") {\n      digitalWrite(ledPin, LOW);\n      Serial.println(\"LED desligado\");\n      client.publish(topicStatus, \"LED est\u00e1 OFF\");\n    }\n  }\n}\n\nvoid reconnect() {\n  // Loop at\u00e9 reconectar\n  while (!client.connected()) {\n    Serial.print(\"Tentando conex\u00e3o MQTT...\");\n\n    // Tenta conectar\n    if (client.connect(mqttClientId, mqttUser, mqttPassword)) {\n      Serial.println(\"conectado\");\n\n      // Publica uma mensagem informando que est\u00e1 online\n      client.publish(topicStatus, \"ESP32 online\");\n\n      // Se inscreve no t\u00f3pico do LED\n      client.subscribe(topicLED);\n\n    } else {\n      Serial.print(\"falhou, rc=\");\n      Serial.print(client.state());\n      Serial.println(\" tentando novamente em 5 segundos\");\n      delay(5000);\n    }\n  }\n}\n</code></pre>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#testando-a-comunicacao-mqtt","title":"Testando a Comunica\u00e7\u00e3o MQTT","text":"<p>Ap\u00f3s carregar o c\u00f3digo no ESP32, podemos testar a comunica\u00e7\u00e3o MQTT usando ferramentas de linha de comando ou aplicativos gr\u00e1ficos.</p>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#usando-ferramentas-de-linha-de-comando","title":"Usando Ferramentas de Linha de Comando","text":""},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#subscribing-recebendo-mensagens","title":"Subscribing (Recebendo Mensagens)","text":"<pre><code>mosquitto_sub -h 192.168.1.100 -t \"esp32/temperature\" -v\n</code></pre>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#publishing-enviando-mensagens","title":"Publishing (Enviando Mensagens)","text":"<pre><code>mosquitto_pub -h 192.168.1.100 -t \"esp32/led\" -m \"ON\"\n</code></pre>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#usando-ferramentas-graficas","title":"Usando Ferramentas Gr\u00e1ficas","text":"<p>Existem v\u00e1rias ferramentas GUI para MQTT, como:</p> <ul> <li>MQTT Explorer: mqtt-explorer.com</li> <li>MQTT.fx: mqttfx.jensd.de</li> <li>MQTTLens: Extens\u00e3o para o Google Chrome</li> </ul>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#exemplo-3-projeto-de-monitoramento-com-esp32-e-mqtt","title":"Exemplo 3: Projeto de Monitoramento com ESP32 e MQTT","text":"<p>Vamos criar um projeto que combina um sensor DHT22 com MQTT para monitorar temperatura e umidade, e tamb\u00e9m permite controlar um rel\u00e9 via MQTT.</p> <pre><code>#include &lt;WiFi.h&gt;\n#include &lt;PubSubClient.h&gt;\n#include &lt;DHT.h&gt;\n#include &lt;ArduinoJson.h&gt;\n\n// Configura\u00e7\u00f5es do WiFi\nconst char* ssid = \"SuaRedeWiFi\";\nconst char* password = \"SuaSenhaWiFi\";\n\n// Configura\u00e7\u00f5es do MQTT\nconst char* mqttServer = \"192.168.1.100\";\nconst int mqttPort = 1883;\nconst char* mqttUser = \"\";\nconst char* mqttPassword = \"\";\nconst char* mqttClientId = \"ESP32Client\";\n\n// T\u00f3picos MQTT\nconst char* topicSensor = \"esp32/sensor\";\nconst char* topicRelay = \"esp32/relay\";\nconst char* topicStatus = \"esp32/status\";\n\n// Configura\u00e7\u00f5es do sensor DHT\n#define DHTPIN 4\n#define DHTTYPE DHT22\nDHT dht(DHTPIN, DHTTYPE);\n\n// Pino do rel\u00e9\nconst int relayPin = 16;\n\n// Objetos WiFi e MQTT\nWiFiClient espClient;\nPubSubClient client(espClient);\n\n// Timestamp da \u00faltima publica\u00e7\u00e3o\nunsigned long lastPublish = 0;\nconst int publishInterval = 30000; // 30 segundos\n\nvoid setup() {\n  Serial.begin(115200);\n\n  // Inicializa o sensor DHT\n  dht.begin();\n\n  // Configura o pino do rel\u00e9\n  pinMode(relayPin, OUTPUT);\n  digitalWrite(relayPin, LOW);\n\n  // Conecta ao WiFi\n  setupWiFi();\n\n  // Configura o servidor MQTT\n  client.setServer(mqttServer, mqttPort);\n  client.setCallback(callback);\n}\n\nvoid loop() {\n  // Verifica e mant\u00e9m a conex\u00e3o com o broker MQTT\n  if (!client.connected()) {\n    reconnect();\n  }\n  client.loop();\n\n  // Publica a cada intervalo definido\n  unsigned long now = millis();\n  if (now - lastPublish &gt; publishInterval) {\n    lastPublish = now;\n    publishSensorData();\n  }\n}\n\nvoid setupWiFi() {\n  delay(10);\n  Serial.println();\n  Serial.print(\"Conectando a \");\n  Serial.println(ssid);\n\n  WiFi.begin(ssid, password);\n\n  while (WiFi.status() != WL_CONNECTED) {\n    delay(500);\n    Serial.print(\".\");\n  }\n\n  Serial.println(\"\");\n  Serial.println(\"WiFi conectado\");\n  Serial.println(\"Endere\u00e7o IP: \");\n  Serial.println(WiFi.localIP());\n}\n\nvoid callback(char* topic, byte* payload, unsigned int length) {\n  Serial.print(\"Mensagem recebida [\");\n  Serial.print(topic);\n  Serial.print(\"] \");\n\n  // Converte o payload para uma string\n  char message[length + 1];\n  for (int i = 0; i &lt; length; i++) {\n    message[i] = (char)payload[i];\n    Serial.print(message[i]);\n  }\n  message[length] = '\\0';\n  Serial.println();\n\n  // Verifica se a mensagem \u00e9 para o t\u00f3pico do rel\u00e9\n  if (String(topic) == topicRelay) {\n    if (String(message) == \"ON\") {\n      digitalWrite(relayPin, HIGH);\n      Serial.println(\"Rel\u00e9 ligado\");\n      client.publish(topicStatus, \"Rel\u00e9 est\u00e1 ON\");\n    } else if (String(message) == \"OFF\") {\n      digitalWrite(relayPin, LOW);\n      Serial.println(\"Rel\u00e9 desligado\");\n      client.publish(topicStatus, \"Rel\u00e9 est\u00e1 OFF\");\n    }\n  }\n}\n\nvoid reconnect() {\n  // Loop at\u00e9 reconectar\n  while (!client.connected()) {\n    Serial.print(\"Tentando conex\u00e3o MQTT...\");\n\n    // Tenta conectar\n    if (client.connect(mqttClientId, mqttUser, mqttPassword)) {\n      Serial.println(\"conectado\");\n\n      // Publica uma mensagem informando que est\u00e1 online\n      client.publish(topicStatus, \"ESP32 online\");\n\n      // Se inscreve no t\u00f3pico do rel\u00e9\n      client.subscribe(topicRelay);\n\n    } else {\n      Serial.print(\"falhou, rc=\");\n      Serial.print(client.state());\n      Serial.println(\" tentando novamente em 5 segundos\");\n      delay(5000);\n    }\n  }\n}\n\nvoid publishSensorData() {\n  // L\u00ea os dados do sensor DHT\n  float h = dht.readHumidity();\n  float t = dht.readTemperature();\n\n  // Verifica se as leituras s\u00e3o v\u00e1lidas\n  if (isnan(h) || isnan(t)) {\n    Serial.println(\"Falha ao ler do sensor DHT!\");\n    return;\n  }\n\n  // Cria um documento JSON\n  StaticJsonDocument&lt;200&gt; doc;\n  doc[\"device_id\"] = mqttClientId;\n  doc[\"temperature\"] = t;\n  doc[\"humidity\"] = h;\n  doc[\"timestamp\"] = millis();\n\n  // Serializa para JSON\n  char jsonBuffer[512];\n  serializeJson(doc, jsonBuffer);\n\n  // Publica a mensagem\n  Serial.println(\"Publicando dados do sensor:\");\n  Serial.println(jsonBuffer);\n  client.publish(topicSensor, jsonBuffer);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#mqtt-com-seguranca-tlsssl","title":"MQTT com Seguran\u00e7a (TLS/SSL)","text":"<p>Para aplica\u00e7\u00f5es de produ\u00e7\u00e3o, \u00e9 importante usar MQTT com seguran\u00e7a. Isso envolve configurar o broker com certificados SSL/TLS e modificar o cliente para usar conex\u00f5es seguras.</p>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#exemplo-de-conexao-segura-esboco","title":"Exemplo de Conex\u00e3o Segura (Esbo\u00e7o)","text":"<pre><code>#include &lt;WiFiClientSecure.h&gt;\n#include &lt;PubSubClient.h&gt;\n\n// Certificado CA do servidor (substitua pelo seu)\nconst char* root_ca = \\\n\"-----BEGIN CERTIFICATE-----\\n\" \\\n\"MIIDSjCCAjKgAwIBAgIQRK+wgNajJ7qJMDmGLvhAazANBgkqhkiG9w0BAQUFADA/\\n\" \\\n\"... o restante do certificado ... \\n\" \\\n\"-----END CERTIFICATE-----\\n\";\n\nWiFiClientSecure espClient;\nPubSubClient client(espClient);\n\nvoid setup() {\n  // Configurar certificado CA\n  espClient.setCACert(root_ca);\n\n  // Configurar servidor MQTT seguro (porta 8883 \u00e9 padr\u00e3o para MQTT com TLS)\n  client.setServer(mqttServer, 8883);\n\n  // ... resto do c\u00f3digo\n}\n</code></pre>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#brokers-mqtt-publicos-para-testes","title":"Brokers MQTT P\u00fablicos para Testes","text":"<p>Para testar suas aplica\u00e7\u00f5es sem configurar um broker local ou se voc\u00ea n\u00e3o tem um servidor em casa, existem brokers p\u00fablicos:</p> <ul> <li>test.mosquitto.org: Broker p\u00fablico mantido pelos desenvolvedores do Mosquitto</li> <li>broker.hivemq.com: Broker p\u00fablico mantido por HiveMQ</li> <li>broker.emqx.io: Broker p\u00fablico mantido por EMQX</li> </ul> <p>Observe que esses brokers s\u00e3o para testes e n\u00e3o devem ser usados em produ\u00e7\u00e3o, pois qualquer pessoa pode se inscrever e publicar nos t\u00f3picos.</p>"},{"location":"aulas/iot/esp32/07-Protocolo-MQTT/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<p>Na pr\u00f3xima aula, integraremos o ESP32 com o Node-RED, uma plataforma de fluxo visual que facilita a conex\u00e3o de dispositivos de hardware, APIs e servi\u00e7os online para criar aplica\u00e7\u00f5es IoT completas.</p> <p>Desafio: Implemente um sistema que usa MQTT para controlar m\u00faltiplos dispositivos - por exemplo, diferentes LEDs que podem ser controlados individualmente por t\u00f3picos como <code>esp32/led/1</code>, <code>esp32/led/2</code>, etc.</p>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/","title":"Integra\u00e7\u00e3o do ESP32 com Node-RED","text":"<p>Nesta aula, exploraremos como integrar projetos ESP32 com o Node-RED, uma ferramenta poderosa para criar aplica\u00e7\u00f5es IoT de forma visual e com pouca programa\u00e7\u00e3o.</p>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#o-que-e-o-node-red","title":"O que \u00e9 o Node-RED?","text":"<p>Node-RED \u00e9 uma plataforma de desenvolvimento baseada em fluxos que permite conectar dispositivos de hardware, APIs e servi\u00e7os online de maneira visual. Desenvolvido originalmente pela IBM, \u00e9 uma ferramenta de c\u00f3digo aberto ideal para prototipagem r\u00e1pida de aplica\u00e7\u00f5es IoT.</p>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#principais-caracteristicas","title":"Principais caracter\u00edsticas:","text":"<ul> <li>Interface visual: Programa\u00e7\u00e3o baseada em fluxos com editor web</li> <li>N\u00f3s pr\u00e9-constru\u00eddos: Componentes prontos para comunica\u00e7\u00e3o, processamento e visualiza\u00e7\u00e3o</li> <li>Extensibilidade: Ecossistema de mais de 3.000 pacotes adicionais</li> <li>Leve: Pode ser executado em dispositivos como Raspberry Pi</li> <li>Multiplataforma: Dispon\u00edvel para Windows, macOS e Linux</li> </ul>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#instalacao-do-node-red","title":"Instala\u00e7\u00e3o do Node-RED","text":""},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#no-windows","title":"No Windows:","text":"<ol> <li>Instale o Node.js: nodejs.org</li> <li>Abra o prompt de comando como administrador e execute:    <pre><code>npm install -g --unsafe-perm node-red\n</code></pre></li> <li>Para iniciar o Node-RED, execute:    <pre><code>node-red\n</code></pre></li> </ol>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#no-macos","title":"No macOS:","text":"<ol> <li>Instale o Node.js: nodejs.org</li> <li>Abra o Terminal e execute:    <pre><code>sudo npm install -g --unsafe-perm node-red\n</code></pre></li> <li>Para iniciar o Node-RED, execute:    <pre><code>node-red\n</code></pre></li> </ol>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#no-linux-ubuntudebian","title":"No Linux (Ubuntu/Debian):","text":"<ol> <li>Instale o Node.js:    <pre><code>curl -sL https://deb.nodesource.com/setup_14.x | sudo -E bash -\nsudo apt-get install -y nodejs\n</code></pre></li> <li>Instale o Node-RED:    <pre><code>sudo npm install -g --unsafe-perm node-red\n</code></pre></li> <li>Para iniciar o Node-RED, execute:    <pre><code>node-red\n</code></pre></li> </ol>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#no-raspberry-pi","title":"No Raspberry Pi:","text":"<p>O Raspberry Pi tem um script de instala\u00e7\u00e3o espec\u00edfico: <pre><code>bash &lt;(curl -sL https://raw.githubusercontent.com/node-red/linux-installers/master/deb/update-nodejs-and-nodered)\n</code></pre></p>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#acessando-o-node-red","title":"Acessando o Node-RED","text":"<p>Ap\u00f3s iniciar o Node-RED, acesse a interface web atrav\u00e9s do navegador: <pre><code>http://localhost:1880\n</code></pre></p>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#instalacao-de-nos-adicionais","title":"Instala\u00e7\u00e3o de N\u00f3s Adicionais","text":"<p>Para trabalhar com MQTT e dashboards, precisamos instalar alguns n\u00f3s adicionais:</p> <ol> <li>No Node-RED, clique no menu (superior direito) &gt; Manage palette</li> <li>Na aba \"Install\", pesquise e instale:</li> <li><code>node-red-dashboard</code> (para criar interfaces de usu\u00e1rio)</li> <li><code>node-red-contrib-ui-led</code> (para LEDs virtuais)</li> <li><code>node-red-contrib-mqtt-broker</code> (broker MQTT local opcional)</li> </ol> <p>Alternativamente, voc\u00ea pode instalar via linha de comando: <pre><code>cd ~/.node-red\nnpm install node-red-dashboard node-red-contrib-ui-led node-red-contrib-mqtt-broker\n</code></pre></p>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#integracao-esp32-com-node-red-via-mqtt","title":"Integra\u00e7\u00e3o ESP32 com Node-RED via MQTT","text":"<p>Usaremos o protocolo MQTT para conectar o ESP32 ao Node-RED, aproveitando o que aprendemos na aula anterior.</p>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#1-configurando-um-fluxo-basico-com-mqtt","title":"1. Configurando um Fluxo B\u00e1sico com MQTT","text":"<p>Vamos criar um fluxo simples que recebe dados de um ESP32 e os exibe em um dashboard:</p> <ol> <li>Adicione um n\u00f3 MQTT Subscriber:</li> <li>Arraste um n\u00f3 <code>mqtt in</code> do painel esquerdo para o editor</li> <li>D\u00ea um duplo clique para configur\u00e1-lo</li> <li>Configure um novo broker MQTT clicando no \u00edcone de l\u00e1pis</li> <li>Informe o endere\u00e7o do broker (por exemplo: <code>localhost</code> ou o IP do seu broker)</li> <li>Configure o t\u00f3pico como <code>esp32/sensor/data</code></li> <li> <p>Clique em \"Done\"</p> </li> <li> <p>Adicione um n\u00f3 JSON:</p> </li> <li>Arraste um n\u00f3 <code>json</code> para o editor</li> <li>Conecte-o ao n\u00f3 MQTT</li> <li> <p>Configure-o para converter a string JSON em um objeto JavaScript</p> </li> <li> <p>Adicione n\u00f3s de debug:</p> </li> <li>Arraste um n\u00f3 <code>debug</code> para o editor</li> <li>Conecte-o ao n\u00f3 JSON</li> <li> <p>Configure-o para mostrar a mensagem completa</p> </li> <li> <p>Implante o fluxo:</p> </li> <li>Clique no bot\u00e3o \"Deploy\" no canto superior direito</li> </ol> <p></p>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#2-criando-um-dashboard-para-o-esp32","title":"2. Criando um Dashboard para o ESP32","text":"<p>Vamos criar um dashboard para visualizar os dados do ESP32 e controlar um LED:</p> <ol> <li>Configure o Dashboard:</li> <li>No painel direito, selecione a aba Dashboard</li> <li>Crie uma nova aba chamada \"ESP32 Monitor\"</li> <li> <p>Dentro desta aba, crie dois grupos: \"Sensores\" e \"Controles\"</p> </li> <li> <p>Adicione gr\u00e1ficos para temperatura e umidade:</p> </li> <li>Arraste um n\u00f3 <code>chart</code> para o editor</li> <li>Conecte-o ap\u00f3s o n\u00f3 JSON</li> <li>Configure-o para o grupo \"Sensores\"</li> <li>Defina o caminho do valor como <code>msg.payload.temperature</code></li> <li> <p>Configure o t\u00edtulo, eixos e apar\u00eancia</p> </li> <li> <p>Repita para umidade:</p> </li> <li>Adicione outro n\u00f3 <code>chart</code></li> <li> <p>Configure-o para exibir <code>msg.payload.humidity</code></p> </li> <li> <p>Adicione um switch para controlar o LED:</p> </li> <li>Arraste um n\u00f3 <code>switch</code> para o editor</li> <li>Configure-o para o grupo \"Controles\"</li> <li> <p>Configure as op\u00e7\u00f5es:</p> <ul> <li>Tipo: Switch</li> <li>R\u00f3tulo: \"LED Control\"</li> <li>On Payload: \"ON\"</li> <li>Off Payload: \"OFF\"</li> </ul> </li> <li> <p>Adicione um n\u00f3 MQTT Publisher:</p> </li> <li>Arraste um n\u00f3 <code>mqtt out</code> para o editor</li> <li>Conecte-o ao n\u00f3 switch</li> <li> <p>Configure-o para publicar no t\u00f3pico <code>esp32/control/led</code></p> </li> <li> <p>Implante o fluxo atualizado:</p> </li> <li> <p>Clique em \"Deploy\"</p> </li> <li> <p>Acesse o Dashboard:</p> </li> <li>Clique no \u00edcone de inicializa\u00e7\u00e3o do dashboard no painel direito</li> <li>Ou acesse: <code>http://localhost:1880/ui</code></li> </ol>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#exemplo-de-codigo-para-o-esp32","title":"Exemplo de C\u00f3digo para o ESP32","text":"<p>Este \u00e9 o c\u00f3digo que o ESP32 deve executar para comunicar-se com o Node-RED via MQTT, baseado no que aprendemos na aula anterior:</p> <pre><code>#include &lt;WiFi.h&gt;\n#include &lt;PubSubClient.h&gt;\n#include &lt;ArduinoJson.h&gt;\n#include &lt;DHT.h&gt;\n\n// Configura\u00e7\u00f5es do WiFi\nconst char* ssid = \"SuaRedeWiFi\";\nconst char* password = \"SuaSenhaWiFi\";\n\n// Configura\u00e7\u00f5es do MQTT\nconst char* mqttServer = \"SEU_IP_DO_BROKER\"; // Endere\u00e7o do broker MQTT\nconst int mqttPort = 1883;\nconst char* mqttUser = \"\";\nconst char* mqttPassword = \"\";\nconst char* mqttClientId = \"ESP32Client\";\n\n// T\u00f3picos MQTT\nconst char* topicData = \"esp32/sensor/data\";    // Para enviar dados dos sensores\nconst char* topicLed = \"esp32/control/led\";     // Para controlar o LED\nconst char* topicStatus = \"esp32/status\";       // Status do dispositivo\n\n// Configura\u00e7\u00e3o dos pinos\nconst int ledPin = 2;   // LED interno do ESP32 ou LED externo\n#define DHTPIN 4        // Pino do sensor DHT\n#define DHTTYPE DHT22   // Tipo do sensor (DHT22 ou DHT11)\n\n// Instanciar o sensor DHT\nDHT dht(DHTPIN, DHTTYPE);\n\n// Objetos WiFi e MQTT\nWiFiClient espClient;\nPubSubClient client(espClient);\n\n// Vari\u00e1veis para controle de tempo\nunsigned long lastMsg = 0;\nconst int publishInterval = 5000;  // Intervalo de 5 segundos\n\nvoid setup() {\n  Serial.begin(115200);\n\n  // Configurar o pino do LED\n  pinMode(ledPin, OUTPUT);\n  digitalWrite(ledPin, LOW);\n\n  // Inicializar o sensor DHT\n  dht.begin();\n\n  // Conectar ao WiFi\n  setupWifi();\n\n  // Configurar o servidor MQTT\n  client.setServer(mqttServer, mqttPort);\n  client.setCallback(callback);\n}\n\nvoid loop() {\n  // Verificar conex\u00e3o com MQTT e reconectar se necess\u00e1rio\n  if (!client.connected()) {\n    reconnect();\n  }\n  client.loop();\n\n  // Publicar dados periodicamente\n  unsigned long now = millis();\n  if (now - lastMsg &gt; publishInterval) {\n    lastMsg = now;\n    publishSensorData();\n  }\n}\n\nvoid setupWifi() {\n  delay(10);\n  Serial.println();\n  Serial.print(\"Conectando-se \u00e0 rede \");\n  Serial.println(ssid);\n\n  WiFi.begin(ssid, password);\n\n  while (WiFi.status() != WL_CONNECTED) {\n    delay(500);\n    Serial.print(\".\");\n  }\n\n  Serial.println(\"\");\n  Serial.println(\"WiFi conectado\");\n  Serial.println(\"Endere\u00e7o IP: \");\n  Serial.println(WiFi.localIP());\n}\n\nvoid callback(char* topic, byte* payload, unsigned int length) {\n  Serial.print(\"Mensagem recebida [\");\n  Serial.print(topic);\n  Serial.print(\"] \");\n\n  // Criar uma string a partir do payload\n  String message;\n  for (int i = 0; i &lt; length; i++) {\n    message += (char)payload[i];\n  }\n  Serial.println(message);\n\n  // Verificar o t\u00f3pico e agir\n  if (String(topic) == topicLed) {\n    if (message == \"ON\") {\n      digitalWrite(ledPin, HIGH);\n      Serial.println(\"LED LIGADO\");\n      client.publish(topicStatus, \"LED ligado\");\n    } else if (message == \"OFF\") {\n      digitalWrite(ledPin, LOW);\n      Serial.println(\"LED DESLIGADO\");\n      client.publish(topicStatus, \"LED est\u00e1 desligado\");\n    }\n  }\n}\n\nvoid reconnect() {\n  // Loop at\u00e9 reconectar\n  while (!client.connected()) {\n    Serial.print(\"Tentando conex\u00e3o MQTT...\");\n\n    // Tentar conectar\n    if (client.connect(mqttClientId, mqttUser, mqttPassword)) {\n      Serial.println(\"conectado\");\n\n      // Publicar mensagem informando que est\u00e1 online\n      client.publish(topicStatus, \"ESP32 conectado\");\n\n      // Inscrever no t\u00f3pico de controle do LED\n      client.subscribe(topicLed);\n    } else {\n      Serial.print(\"falhou, rc=\");\n      Serial.print(client.state());\n      Serial.println(\" tentando novamente em 5 segundos\");\n      delay(5000);\n    }\n  }\n}\n\nvoid publishSensorData() {\n  // Ler temperatura e umidade\n  float h = dht.readHumidity();\n  float t = dht.readTemperature();\n\n  // Verificar se as leituras s\u00e3o v\u00e1lidas\n  if (isnan(h) || isnan(t)) {\n    Serial.println(\"Falha ao ler do sensor DHT!\");\n    return;\n  }\n\n  // Criar um objeto JSON\n  StaticJsonDocument&lt;200&gt; doc;\n  doc[\"device\"] = mqttClientId;\n  doc[\"temperature\"] = t;\n  doc[\"humidity\"] = h;\n\n  // Adicionar status do LED\n  doc[\"led\"] = digitalRead(ledPin) ? \"ON\" : \"OFF\";\n\n  // Serializar para JSON\n  char jsonBuffer[256];\n  serializeJson(doc, jsonBuffer);\n\n  // Publicar no t\u00f3pico\n  Serial.print(\"Publicando: \");\n  Serial.println(jsonBuffer);\n  client.publish(topicData, jsonBuffer);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#fluxo-completo-no-node-red-exemplo","title":"Fluxo Completo no Node-RED (exemplo)","text":"<p>Voc\u00ea pode importar este fluxo completo para o Node-RED. V\u00e1 para menu &gt; Import &gt; Clipboard e cole o seguinte JSON:</p> <pre><code>[\n    {\n        \"id\": \"e36406da.8d2938\",\n        \"type\": \"tab\",\n        \"label\": \"ESP32 Dashboard\",\n        \"disabled\": false,\n        \"info\": \"\"\n    },\n    {\n        \"id\": \"8ddfcd77.4e175\",\n        \"type\": \"mqtt in\",\n        \"z\": \"e36406da.8d2938\",\n        \"name\": \"ESP32 Sensor Data\",\n        \"topic\": \"esp32/sensor/data\",\n        \"qos\": \"0\",\n        \"datatype\": \"auto\",\n        \"broker\": \"35b83581.2d44ca\",\n        \"x\": 140,\n        \"y\": 180,\n        \"wires\": [\n            [\n                \"523a4560.84ed3c\",\n                \"bcc7c29c.45c03\"\n            ]\n        ]\n    },\n    {\n        \"id\": \"523a4560.84ed3c\",\n        \"type\": \"json\",\n        \"z\": \"e36406da.8d2938\",\n        \"name\": \"\",\n        \"property\": \"payload\",\n        \"action\": \"obj\",\n        \"pretty\": false,\n        \"x\": 310,\n        \"y\": 180,\n        \"wires\": [\n            [\n                \"5c5950e3.a8a13\",\n                \"5a29a2ad.bbebfc\",\n                \"6ca8dcc6.c0dd64\"\n            ]\n        ]\n    },\n    {\n        \"id\": \"bcc7c29c.45c03\",\n        \"type\": \"debug\",\n        \"z\": \"e36406da.8d2938\",\n        \"name\": \"Raw MQTT Data\",\n        \"active\": true,\n        \"tosidebar\": true,\n        \"console\": false,\n        \"tostatus\": false,\n        \"complete\": \"payload\",\n        \"targetType\": \"msg\",\n        \"x\": 350,\n        \"y\": 120,\n        \"wires\": []\n    },\n    {\n        \"id\": \"5c5950e3.a8a13\",\n        \"type\": \"debug\",\n        \"z\": \"e36406da.8d2938\",\n        \"name\": \"Parsed JSON\",\n        \"active\": true,\n        \"tosidebar\": true,\n        \"console\": false,\n        \"tostatus\": false,\n        \"complete\": \"payload\",\n        \"targetType\": \"msg\",\n        \"x\": 490,\n        \"y\": 120,\n        \"wires\": []\n    },\n    {\n        \"id\": \"5a29a2ad.bbebfc\",\n        \"type\": \"ui_chart\",\n        \"z\": \"e36406da.8d2938\",\n        \"name\": \"Temperature\",\n        \"group\": \"b33ad5c5.80d188\",\n        \"order\": 1,\n        \"width\": \"0\",\n        \"height\": \"0\",\n        \"label\": \"Temperature (\u00b0C)\",\n        \"chartType\": \"line\",\n        \"legend\": \"false\",\n        \"xformat\": \"HH:mm:ss\",\n        \"interpolate\": \"linear\",\n        \"nodata\": \"\",\n        \"dot\": false,\n        \"ymin\": \"0\",\n        \"ymax\": \"50\",\n        \"removeOlder\": 1,\n        \"removeOlderPoints\": \"\",\n        \"removeOlderUnit\": \"3600\",\n        \"cutout\": 0,\n        \"useOneColor\": false,\n        \"useUTC\": false,\n        \"colors\": [\n            \"#1f77b4\",\n            \"#aec7e8\",\n            \"#ff7f0e\",\n            \"#2ca02c\",\n            \"#98df8a\",\n            \"#d62728\",\n            \"#ff9896\",\n            \"#9467bd\",\n            \"#c5b0d5\"\n        ],\n        \"outputs\": 1,\n        \"x\": 510,\n        \"y\": 180,\n        \"wires\": [\n            []\n        ]\n    },\n    {\n        \"id\": \"6ca8dcc6.c0dd64\",\n        \"type\": \"ui_chart\",\n        \"z\": \"e36406da.8d2938\",\n        \"name\": \"Humidity\",\n        \"group\": \"b33ad5c5.80d188\",\n        \"order\": 2,\n        \"width\": \"0\",\n        \"height\": \"0\",\n        \"label\": \"Humidity (%)\",\n        \"chartType\": \"line\",\n        \"legend\": \"false\",\n        \"xformat\": \"HH:mm:ss\",\n        \"interpolate\": \"linear\",\n        \"nodata\": \"\",\n        \"dot\": false,\n        \"ymin\": \"0\",\n        \"ymax\": \"100\",\n        \"removeOlder\": 1,\n        \"removeOlderPoints\": \"\",\n        \"removeOlderUnit\": \"3600\",\n        \"cutout\": 0,\n        \"useOneColor\": false,\n        \"useUTC\": false,\n        \"colors\": [\n            \"#1f77b4\",\n            \"#aec7e8\",\n            \"#ff7f0e\",\n            \"#2ca02c\",\n            \"#98df8a\",\n            \"#d62728\",\n            \"#ff9896\",\n            \"#9467bd\",\n            \"#c5b0d5\"\n        ],\n        \"outputs\": 1,\n        \"x\": 500,\n        \"y\": 240,\n        \"wires\": [\n            []\n        ]\n    },\n    {\n        \"id\": \"7aeebb91.789c94\",\n        \"type\": \"ui_switch\",\n        \"z\": \"e36406da.8d2938\",\n        \"name\": \"LED Control\",\n        \"label\": \"LED Control\",\n        \"tooltip\": \"\",\n        \"group\": \"71f6d4b5.4c0cbc\",\n        \"order\": 1,\n        \"width\": 0,\n        \"height\": 0,\n        \"passthru\": true,\n        \"decouple\": \"false\",\n        \"topic\": \"\",\n        \"style\": \"\",\n        \"onvalue\": \"ON\",\n        \"onvalueType\": \"str\",\n        \"onicon\": \"\",\n        \"oncolor\": \"\",\n        \"offvalue\": \"OFF\",\n        \"offvalueType\": \"str\",\n        \"officon\": \"\",\n        \"offcolor\": \"\",\n        \"x\": 130,\n        \"y\": 320,\n        \"wires\": [\n            [\n                \"92e36c6f.20c87\"\n            ]\n        ]\n    },\n    {\n        \"id\": \"92e36c6f.20c87\",\n        \"type\": \"mqtt out\",\n        \"z\": \"e36406da.8d2938\",\n        \"name\": \"LED Control\",\n        \"topic\": \"esp32/control/led\",\n        \"qos\": \"\",\n        \"retain\": \"\",\n        \"broker\": \"35b83581.2d44ca\",\n        \"x\": 330,\n        \"y\": 320,\n        \"wires\": []\n    },\n    {\n        \"id\": \"35b83581.2d44ca\",\n        \"type\": \"mqtt-broker\",\n        \"name\": \"Local Broker\",\n        \"broker\": \"localhost\",\n        \"port\": \"1883\",\n        \"clientid\": \"\",\n        \"usetls\": false,\n        \"compatmode\": false,\n        \"keepalive\": \"60\",\n        \"cleansession\": true,\n        \"birthTopic\": \"\",\n        \"birthQos\": \"0\",\n        \"birthPayload\": \"\",\n        \"closeTopic\": \"\",\n        \"closeQos\": \"0\",\n        \"closePayload\": \"\",\n        \"willTopic\": \"\",\n        \"willQos\": \"0\",\n        \"willPayload\": \"\"\n    },\n    {\n        \"id\": \"b33ad5c5.80d188\",\n        \"type\": \"ui_group\",\n        \"name\": \"Sensores\",\n        \"tab\": \"a2e1584c.deb448\",\n        \"order\": 1,\n        \"disp\": true,\n        \"width\": \"6\",\n        \"collapse\": false\n    },\n    {\n        \"id\": \"71f6d4b5.4c0cbc\",\n        \"type\": \"ui_group\",\n        \"name\": \"Controles\",\n        \"tab\": \"a2e1584c.deb448\",\n        \"order\": 2,\n        \"disp\": true,\n        \"width\": \"6\",\n        \"collapse\": false\n    },\n    {\n        \"id\": \"a2e1584c.deb448\",\n        \"type\": \"ui_tab\",\n        \"name\": \"ESP32 Monitor\",\n        \"icon\": \"dashboard\",\n        \"disabled\": false,\n        \"hidden\": false\n    }\n]\n</code></pre>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#aplicacoes-avancadas-do-node-red-com-esp32","title":"Aplica\u00e7\u00f5es Avan\u00e7adas do Node-RED com ESP32","text":""},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#1-armazenamento-de-dados-historicos","title":"1. Armazenamento de Dados Hist\u00f3ricos","text":"<p>O Node-RED pode armazenar dados hist\u00f3ricos do seu ESP32:</p> <ol> <li>Use o n\u00f3 <code>node-red-contrib-influxdb</code> para armazenar dados em um banco InfluxDB</li> <li>Use os n\u00f3s <code>file</code> para gravar dados em arquivos CSV</li> <li>Integre com outros bancos de dados como MySQL ou MongoDB</li> </ol>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#2-notificacoes-e-alertas","title":"2. Notifica\u00e7\u00f5es e Alertas","text":"<p>Configure alertas baseados nos dados do ESP32:</p> <ol> <li>Envio de e-mails quando valores ultrapassarem limites</li> <li>Notifica\u00e7\u00f5es push para smartphones</li> <li>Mensagens SMS ou integra\u00e7\u00e3o com servi\u00e7os de mensagens</li> </ol>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#3-integracoes-com-servicos-de-nuvem","title":"3. Integra\u00e7\u00f5es com Servi\u00e7os de Nuvem","text":"<p>Node-RED facilita a integra\u00e7\u00e3o do ESP32 com servi\u00e7os de nuvem:</p> <ol> <li>AWS IoT</li> <li>Google Cloud IoT</li> <li>Microsoft Azure IoT</li> <li>ThingSpeak, Adafruit IO ou outras plataformas IoT</li> </ol>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#4-controle-por-voz","title":"4. Controle por Voz","text":"<p>Integre seu ESP32 com assistentes de voz:</p> <ol> <li>Use n\u00f3s Node-RED para Google Assistant</li> <li>Integra\u00e7\u00e3o com Amazon Alexa</li> <li>Controle por voz via IFTTT</li> </ol>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#exemplo-dashboard-completo-com-node-red","title":"Exemplo: Dashboard Completo com Node-RED","text":"<p>Um dashboard completo para monitoramento do ESP32 pode incluir:</p> <ol> <li>Gr\u00e1ficos em tempo real:</li> <li>Temperatura e umidade</li> <li>N\u00edveis de bateria</li> <li> <p>Outros sensores</p> </li> <li> <p>Controles:</p> </li> <li>Bot\u00f5es para ligar/desligar LEDs</li> <li>Sliders para controlar motores ou servos</li> <li> <p>Campos de texto para enviar mensagens</p> </li> <li> <p>Indicadores de Status:</p> </li> <li>LEDs virtuais para mostrar status de conex\u00e3o</li> <li>Medidores para visualiza\u00e7\u00e3o de valores</li> <li> <p>Timestamp da \u00faltima atualiza\u00e7\u00e3o</p> </li> <li> <p>Funcionalidades Avan\u00e7adas:</p> </li> <li>Programa\u00e7\u00e3o de tarefas</li> <li>Regras condicionais</li> <li>Hist\u00f3rico de eventos</li> </ol>"},{"location":"aulas/iot/esp32/08-Integracao-NodeRED/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<p>Na pr\u00f3xima aula, exploraremos t\u00e9cnicas de gerenciamento de energia para o ESP32, permitindo que seus projetos IoT funcionem com baterias por longos per\u00edodos.</p> <p>Desafio: Expanda o dashboard Node-RED para incluir um painel de controle completo para seu ESP32, com gr\u00e1ficos hist\u00f3ricos, controles para v\u00e1rios pinos e alertas baseados em limites de temperatura ou umidade.</p>"},{"location":"aulas/iot/esp32/09-Gerenciamento-Energia/","title":"Gerenciamento de Energia com ESP32","text":"<p>Nesta aula, vamos explorar t\u00e9cnicas de gerenciamento de energia para o ESP32, fundamentais para projetos IoT alimentados por bateria. O ESP32 possui diversos modos de economia de energia que podem estender significativamente a vida \u00fatil da bateria.</p>"},{"location":"aulas/iot/esp32/09-Gerenciamento-Energia/#importancia-do-gerenciamento-de-energia","title":"Import\u00e2ncia do Gerenciamento de Energia","text":"<p>Em muitas aplica\u00e7\u00f5es IoT, os dispositivos: - S\u00e3o instalados em locais remotos - Funcionam com baterias ou energia solar - Precisam operar por meses ou anos sem manuten\u00e7\u00e3o</p> <p>Um ESP32 funcionando continuamente consome aproximadamente: - 160-260mA durante transmiss\u00e3o WiFi - 100-150mA durante processamento intenso - 20-30mA em modo ativo leve</p> <p>Com uma bateria de 1000mAh, isso significa apenas algumas horas de opera\u00e7\u00e3o. Com gerenciamento de energia eficiente, podemos estender isso para semanas, meses ou at\u00e9 anos.</p>"},{"location":"aulas/iot/esp32/09-Gerenciamento-Energia/#modos-de-economia-de-energia-do-esp32","title":"Modos de Economia de Energia do ESP32","text":"<p>O ESP32 possui v\u00e1rios modos de opera\u00e7\u00e3o com diferentes n\u00edveis de consumo:</p> Modo Consumo t\u00edpico CPU WiFi/BT RTC Mem\u00f3ria Ativo 160-260mA Ativo Ativo Ativo Todas ativas Modem Sleep 20-30mA Ativo Desligado Ativo Todas ativas Light Sleep 0.8mA Pausado Desligado Ativo Preservada Deep Sleep 10\u00b5A Desligado Desligado Ativo RTC retida Hiberna\u00e7\u00e3o 5\u00b5A Desligado Desligado Timer apenas Nada"},{"location":"aulas/iot/esp32/09-Gerenciamento-Energia/#1-deep-sleep-sono-profundo","title":"1. Deep Sleep (Sono Profundo)","text":"<p>O Deep Sleep \u00e9 o modo mais utilizado para economizar energia em projetos IoT. O ESP32 desliga a maioria dos sistemas, mantendo apenas o timer RTC para acordar o dispositivo ap\u00f3s um tempo determinado.</p>"},{"location":"aulas/iot/esp32/09-Gerenciamento-Energia/#exemplo-basico-de-deep-sleep","title":"Exemplo B\u00e1sico de Deep Sleep","text":"<pre><code>#define uS_TO_S_FACTOR 1000000  // Fator de convers\u00e3o de micro segundos para segundos\n#define TIME_TO_SLEEP  60       // Tempo de sleep em segundos\n\nvoid setup() {\n  Serial.begin(115200);\n\n  // Configura o timer para acordar\n  esp_sleep_enable_timer_wakeup(TIME_TO_SLEEP * uS_TO_S_FACTOR);\n\n  Serial.println(\"ESP32 entrar\u00e1 em deep sleep por \" + String(TIME_TO_SLEEP) + \" segundos\");\n  Serial.flush(); \n\n  // Entra em deep sleep\n  esp_deep_sleep_start();\n}\n\nvoid loop() {\n  // Nunca ser\u00e1 executado no deep sleep\n}\n</code></pre>"},{"location":"aulas/iot/esp32/09-Gerenciamento-Energia/#obtendo-a-causa-do-despertar","title":"Obtendo a Causa do Despertar","text":"<pre><code>void setup() {\n  Serial.begin(115200);\n  delay(1000);\n\n  // Verifica a causa do despertar\n  esp_sleep_wakeup_cause_t wakeup_reason;\n  wakeup_reason = esp_sleep_get_wakeup_cause();\n\n  switch(wakeup_reason) {\n    case ESP_SLEEP_WAKEUP_EXT0 : \n      Serial.println(\"Despertar causado por sinal externo (RTC_IO)\"); \n      break;\n    case ESP_SLEEP_WAKEUP_EXT1 : \n      Serial.println(\"Despertar causado por sinal externo (RTC_CNTL)\"); \n      break;\n    case ESP_SLEEP_WAKEUP_TIMER : \n      Serial.println(\"Despertar causado pelo timer\"); \n      break;\n    case ESP_SLEEP_WAKEUP_TOUCHPAD : \n      Serial.println(\"Despertar causado por touchpad\"); \n      break;\n    case ESP_SLEEP_WAKEUP_ULP : \n      Serial.println(\"Despertar causado pelo programa ULP\"); \n      break;\n    default : \n      Serial.println(\"Despertar n\u00e3o causado por deep sleep\"); \n      break;\n  }\n\n  // Configura deep sleep novamente\n  esp_sleep_enable_timer_wakeup(60 * uS_TO_S_FACTOR);\n\n  // Executa a l\u00f3gica do programa aqui\n  // ...\n\n  Serial.println(\"Voltando ao deep sleep\");\n  esp_deep_sleep_start();\n}\n\nvoid loop() {\n  // Nunca ser\u00e1 executado\n}\n</code></pre>"},{"location":"aulas/iot/esp32/09-Gerenciamento-Energia/#2-despertar-por-pino-externo","title":"2. Despertar por Pino Externo","text":"<p>O ESP32 pode acordar do deep sleep quando um pino espec\u00edfico muda de estado.</p> <pre><code>// Pino RTC GPIO usado para despertar (apenas os pinos RTC podem ser usados)\n#define WAKEUP_PIN GPIO_NUM_33\n\nvoid setup() {\n  Serial.begin(115200);\n  delay(1000);\n\n  // Configura despertar por pino externo\n  esp_sleep_enable_ext0_wakeup(WAKEUP_PIN, 1); // 1 = HIGH, 0 = LOW\n\n  // Tamb\u00e9m configura timer como backup\n  esp_sleep_enable_timer_wakeup(3600 * uS_TO_S_FACTOR); // 1 hora\n\n  Serial.println(\"ESP32 configurado para despertar com pino HIGH ou ap\u00f3s 1 hora\");\n  Serial.flush();\n\n  esp_deep_sleep_start();\n}\n\nvoid loop() {\n  // Nunca ser\u00e1 executado\n}\n</code></pre>"},{"location":"aulas/iot/esp32/09-Gerenciamento-Energia/#3-despertar-por-touchpad","title":"3. Despertar por Touchpad","text":"<p>O ESP32 pode acordar quando um sensor touch capacitivo \u00e9 tocado.</p> <pre><code>#define TOUCH_THRESHOLD 40\n\nvoid setup() {\n  Serial.begin(115200);\n  delay(1000);\n\n  // Configura despertar por touchpad (pino 4 = T0)\n  touchSleepWakeUpEnable(T0, TOUCH_THRESHOLD);\n\n  Serial.println(\"ESP32 entrar\u00e1 em deep sleep, toque no pino T0 para acordar\");\n  Serial.flush();\n\n  esp_deep_sleep_start();\n}\n\nvoid loop() {\n  // Nunca ser\u00e1 executado\n}\n</code></pre>"},{"location":"aulas/iot/esp32/09-Gerenciamento-Energia/#4-light-sleep-sono-leve","title":"4. Light Sleep (Sono Leve)","text":"<p>No Light Sleep, a CPU \u00e9 pausada, mas a mem\u00f3ria \u00e9 preservada. \u00c9 \u00fatil quando voc\u00ea precisa despertar rapidamente.</p> <pre><code>void setup() {\n  Serial.begin(115200);\n}\n\nvoid loop() {\n  // Executa alguma tarefa\n  Serial.println(\"Executando tarefa...\");\n\n  // Aguarda na serial\n  Serial.flush();\n\n  // Configura light sleep\n  esp_sleep_enable_timer_wakeup(5 * 1000000); // 5 segundos\n\n  Serial.println(\"Entrando em light sleep\");\n  Serial.flush();\n\n  // Entra em light sleep\n  esp_light_sleep_start();\n\n  // C\u00f3digo continua daqui quando o dispositivo acorda\n  Serial.println(\"ESP32 acordou do light sleep\");\n}\n</code></pre>"},{"location":"aulas/iot/esp32/09-Gerenciamento-Energia/#5-modo-modem-sleep","title":"5. Modo Modem Sleep","text":"<p>No Modem Sleep, o processador continua funcionando, mas o WiFi e o Bluetooth s\u00e3o desligados.</p> <pre><code>#include &lt;WiFi.h&gt;\n\nconst char* ssid     = \"SuaRedeWiFi\";\nconst char* password = \"SuaSenhaWiFi\";\n\nvoid setup() {\n  Serial.begin(115200);\n  WiFi.begin(ssid, password);\n\n  while (WiFi.status() != WL_CONNECTED) {\n    delay(500);\n    Serial.print(\".\");\n  }\n\n  Serial.println(\"\");\n  Serial.println(\"WiFi conectado\");\n}\n\nvoid loop() {\n  // Desliga o WiFi para economizar energia\n  WiFi.disconnect(true);\n  WiFi.mode(WIFI_OFF);\n  Serial.println(\"WiFi desligado\");\n\n  // Executa tarefas locais que n\u00e3o precisam de WiFi\n  delay(10000);\n\n  // Reativa o WiFi quando necess\u00e1rio\n  WiFi.mode(WIFI_STA);\n  WiFi.begin(ssid, password);\n\n  while (WiFi.status() != WL_CONNECTED) {\n    delay(500);\n    Serial.print(\".\");\n  }\n\n  Serial.println(\"\");\n  Serial.println(\"WiFi reconectado\");\n\n  // Executa tarefas que precisam de conex\u00e3o\n  delay(5000);\n}\n</code></pre>"},{"location":"aulas/iot/esp32/09-Gerenciamento-Energia/#6-sensores-de-baixo-consumo","title":"6. Sensores de Baixo Consumo","text":"<p>Para projetos de energia ultra baixa, considere usar sensores que consomem pouca energia:</p> <ol> <li>Sensores Digitais de Baixo Consumo:</li> <li>BME280 (temperatura, umidade, press\u00e3o): 3.6\u00b5A em standby</li> <li>DS18B20 (temperatura): 1\u00b5A em standby</li> <li> <p>PIR Motion (movimento): ~10\u00b5A em standby</p> </li> <li> <p>Reduzindo Consumo de Sensores Anal\u00f3gicos:</p> </li> <li>Ligue os sensores anal\u00f3gicos apenas quando necess\u00e1rio</li> <li>Use pinos GPIO para controlar a alimenta\u00e7\u00e3o dos sensores</li> </ol> <pre><code>#define SENSOR_POWER_PIN 13\n#define SENSOR_INPUT_PIN 34\n\nvoid setup() {\n  Serial.begin(115200);\n\n  // Configura o pino de alimenta\u00e7\u00e3o do sensor\n  pinMode(SENSOR_POWER_PIN, OUTPUT);\n\n  // Configura o pino de leitura\n  pinMode(SENSOR_INPUT_PIN, INPUT);\n}\n\nvoid loop() {\n  // Liga o sensor\n  digitalWrite(SENSOR_POWER_PIN, HIGH);\n\n  // Pequeno delay para estabiliza\u00e7\u00e3o do sensor\n  delay(10);\n\n  // L\u00ea o valor\n  int sensorValue = analogRead(SENSOR_INPUT_PIN);\n\n  // Desliga o sensor para economizar energia\n  digitalWrite(SENSOR_POWER_PIN, LOW);\n\n  Serial.print(\"Valor do sensor: \");\n  Serial.println(sensorValue);\n\n  // Entra em deep sleep\n  esp_sleep_enable_timer_wakeup(60 * 1000000); // 60 segundos\n  esp_deep_sleep_start();\n}\n</code></pre>"},{"location":"aulas/iot/esp32/09-Gerenciamento-Energia/#7-projeto-estacao-meteorologica-de-baixo-consumo","title":"7. Projeto: Esta\u00e7\u00e3o Meteorol\u00f3gica de Baixo Consumo","text":"<p>Este projeto combina v\u00e1rias t\u00e9cnicas de economia de energia:</p> <pre><code>#include &lt;WiFi.h&gt;\n#include &lt;PubSubClient.h&gt;\n#include &lt;Wire.h&gt;\n#include &lt;Adafruit_BME280.h&gt;\n#include &lt;ArduinoJson.h&gt;\n\n// Configura\u00e7\u00f5es\nconst char* ssid = \"SuaRedeWiFi\";\nconst char* password = \"SuaSenhaWiFi\";\nconst char* mqttServer = \"seuservidormqtt.com\";\nconst int mqttPort = 1883;\nconst char* mqttUser = \"seu_usuario\";\nconst char* mqttPassword = \"sua_senha\";\nconst char* mqttTopic = \"esp32/clima\";\n\n// Pinos\n#define I2C_SDA 21\n#define I2C_SCL 22\n#define BATT_ADC_PIN 35\n\n// Tempo entre leituras (15 minutos)\n#define SLEEP_TIME_SECONDS 900\n\n// Objetos\nAdafruit_BME280 bme;\nWiFiClient espClient;\nPubSubClient mqttClient(espClient);\n\n// Fun\u00e7\u00e3o para leitura da bateria\nfloat getBatteryVoltage() {\n  // L\u00ea o valor (ajuste os valores conforme seu divisor de tens\u00e3o)\n  float adc = analogRead(BATT_ADC_PIN);\n  float voltage = adc / 4095.0 * 3.3 * 2.0; // Assumindo um divisor de tens\u00e3o de 1:1\n  return voltage;\n}\n\n// Conecta ao WiFi\nvoid connectWiFi() {\n  WiFi.begin(ssid, password);\n\n  // Timeout de 20 segundos\n  int timeout = 20;\n  while (WiFi.status() != WL_CONNECTED &amp;&amp; timeout &gt; 0) {\n    delay(1000);\n    Serial.print(\".\");\n    timeout--;\n  }\n\n  if (WiFi.status() != WL_CONNECTED) {\n    Serial.println(\"Falha na conex\u00e3o WiFi. Entrando em deep sleep.\");\n    esp_deep_sleep_start();\n    return;\n  }\n\n  Serial.println(\"WiFi conectado\");\n}\n\n// Conecta ao broker MQTT\nbool connectMQTT() {\n  mqttClient.setServer(mqttServer, mqttPort);\n\n  Serial.println(\"Conectando ao MQTT...\");\n  if (mqttClient.connect(\"ESP32WeatherStation\", mqttUser, mqttPassword)) {\n    Serial.println(\"Conectado ao MQTT\");\n    return true;\n  } else {\n    Serial.print(\"Falha na conex\u00e3o MQTT, rc=\");\n    Serial.println(mqttClient.state());\n    return false;\n  }\n}\n\nvoid setup() {\n  Serial.begin(115200);\n  Serial.println(\"Esta\u00e7\u00e3o Meteorol\u00f3gica de Baixo Consumo\");\n\n  // Inicializa I2C\n  Wire.begin(I2C_SDA, I2C_SCL);\n\n  // Inicializa o sensor\n  if (!bme.begin(0x76)) {\n    Serial.println(\"N\u00e3o foi poss\u00edvel encontrar o sensor BME280!\");\n    esp_deep_sleep_start();\n  }\n\n  // L\u00ea dados do sensor\n  float temperature = bme.readTemperature();\n  float humidity = bme.readHumidity();\n  float pressure = bme.readPressure() / 100.0F; // hPa\n  float voltage = getBatteryVoltage();\n\n  Serial.printf(\"Temperatura: %.2f\u00b0C, Umidade: %.2f%%, Press\u00e3o: %.2fhPa, Bateria: %.2fV\\n\", \n                temperature, humidity, pressure, voltage);\n\n  // Conecta ao WiFi\n  connectWiFi();\n\n  // Conecta ao MQTT\n  if (connectMQTT()) {\n    // Cria o JSON com os dados\n    StaticJsonDocument&lt;200&gt; doc;\n    doc[\"device\"] = \"ESP32_Weather\";\n    doc[\"temperature\"] = temperature;\n    doc[\"humidity\"] = humidity;\n    doc[\"pressure\"] = pressure;\n    doc[\"battery\"] = voltage;\n\n    // Serializa o JSON\n    char buffer[256];\n    serializeJson(doc, buffer);\n\n    // Publica no MQTT\n    if (mqttClient.publish(mqttTopic, buffer)) {\n      Serial.println(\"Dados publicados com sucesso\");\n    } else {\n      Serial.println(\"Falha ao publicar dados\");\n    }\n\n    // Aguarda finaliza\u00e7\u00e3o do envio\n    mqttClient.loop();\n    delay(100);\n  }\n\n  // Desconecta para limpar\n  mqttClient.disconnect();\n  WiFi.disconnect(true);\n  WiFi.mode(WIFI_OFF);\n\n  // Configura alarme para pr\u00f3xima leitura\n  esp_sleep_enable_timer_wakeup(SLEEP_TIME_SECONDS * 1000000ULL);\n\n  Serial.printf(\"Entrando em deep sleep por %d segundos\\n\", SLEEP_TIME_SECONDS);\n  Serial.flush();\n\n  // Entra em deep sleep\n  esp_deep_sleep_start();\n}\n\nvoid loop() {\n  // Nunca ser\u00e1 executado\n}\n</code></pre>"},{"location":"aulas/iot/esp32/09-Gerenciamento-Energia/#8-calculando-o-consumo-de-energia","title":"8. Calculando o Consumo de Energia","text":"<p>Para estimar a dura\u00e7\u00e3o da bateria, use a seguinte f\u00f3rmula:</p> <pre><code>Dura\u00e7\u00e3o da bateria (horas) = Capacidade da bateria (mAh) / Consumo m\u00e9dio (mA)\n</code></pre> <p>Para o consumo m\u00e9dio, voc\u00ea precisa considerar:</p> <ol> <li>Tempo em cada estado:</li> <li>Tempo ativo (ta)</li> <li> <p>Tempo em sleep (ts)</p> </li> <li> <p>Corrente em cada estado:</p> </li> <li>Corrente ativa (Ia)</li> <li>Corrente em sleep (Is)</li> </ol> <pre><code>Consumo m\u00e9dio = (ta * Ia + ts * Is) / (ta + ts)\n</code></pre>"},{"location":"aulas/iot/esp32/09-Gerenciamento-Energia/#exemplo-de-calculo","title":"Exemplo de c\u00e1lculo:","text":"<p>Se seu ESP32: - Fica ativo por 2 segundos (consumindo 150mA) - Dorme por 58 segundos (consumindo 10\u00b5A) - Repete este ciclo a cada minuto - Usa uma bateria de 2000mAh</p> <p>O consumo m\u00e9dio seria: <pre><code>(2s * 150mA + 58s * 0.01mA) / 60s = (300mA*s + 0.58mA*s) / 60s = 5.01mA\n</code></pre></p> <p>Dura\u00e7\u00e3o estimada: <pre><code>2000mAh / 5.01mA = 399 horas \u2248 16.6 dias\n</code></pre></p>"},{"location":"aulas/iot/esp32/09-Gerenciamento-Energia/#dicas-adicionais-para-economia-de-energia","title":"Dicas Adicionais para Economia de Energia","text":"<ol> <li>Otimize o C\u00f3digo:</li> <li>Evite loops de espera ocupada (busy waiting)</li> <li>Minimize opera\u00e7\u00f5es matem\u00e1ticas complexas</li> <li> <p>Use vari\u00e1veis de tamanho adequado</p> </li> <li> <p>Hardware:</p> </li> <li>Use baterias LiPo ou Li-Ion para projetos prolongados</li> <li>Considere adicionar pain\u00e9is solares para recarregar</li> <li> <p>Use reguladores de tens\u00e3o eficientes (como LDO)</p> </li> <li> <p>Comunica\u00e7\u00e3o:</p> </li> <li>Minimize a quantidade de dados transmitidos</li> <li>Reduza a frequ\u00eancia das transmiss\u00f5es</li> <li> <p>Use protocolos leves como MQTT</p> </li> <li> <p>T\u00e9cnicas Avan\u00e7adas:</p> </li> <li>Use ULP (Ultra Low Power) coprocessor para monitoramento cont\u00ednuo</li> <li>Implemente l\u00f3gica adaptativa (mais tempo de sleep quando as condi\u00e7\u00f5es s\u00e3o est\u00e1veis)</li> <li>Monitore o n\u00edvel da bateria e ajuste o comportamento</li> </ol>"},{"location":"aulas/iot/esp32/09-Gerenciamento-Energia/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<p>Na pr\u00f3xima aula, abordaremos um projeto final integrado que utiliza tudo o que aprendemos nas aulas anteriores, combinando conectividade, sensores, e gerenciamento de energia para criar uma solu\u00e7\u00e3o IoT completa.</p> <p>Desafio: Modifique o projeto da Esta\u00e7\u00e3o Meteorol\u00f3gica para incluir algum tipo de energia alternativa (como painel solar) e adapta\u00e7\u00e3o din\u00e2mica do tempo de sono com base no n\u00edvel da bateria.</p>"},{"location":"aulas/iot/esp32/10-Projeto-Final/","title":"Projeto Final Integrado com ESP32","text":"<p>Nesta aula final, vamos consolidar todo o conhecimento adquirido ao longo do curso para desenvolver um projeto IoT completo e funcional. Este projeto integrar\u00e1 sensores, conectividade sem fio, comunica\u00e7\u00e3o com servidores, interface de usu\u00e1rio e gerenciamento de energia.</p>"},{"location":"aulas/iot/esp32/10-Projeto-Final/#visao-geral-sistema-de-monitoramento-ambiental-inteligente","title":"Vis\u00e3o Geral: Sistema de Monitoramento Ambiental Inteligente","text":"<p>Nosso projeto final ser\u00e1 um sistema de monitoramento ambiental completo que:</p> <ol> <li>Coleta dados de m\u00faltiplos sensores ambientais</li> <li>Processa e armazena localmente os dados</li> <li>Transmite informa\u00e7\u00f5es via WiFi usando MQTT</li> <li>Permite visualiza\u00e7\u00e3o em tempo real atrav\u00e9s de dashboard </li> <li>Gera alertas para condi\u00e7\u00f5es ambientais anormais</li> <li>Opera com baixo consumo de energia para uso com bateria</li> <li>Oferece configura\u00e7\u00e3o e monitoramento via Bluetooth</li> </ol>"},{"location":"aulas/iot/esp32/10-Projeto-Final/#componentes-necessarios","title":"Componentes Necess\u00e1rios","text":""},{"location":"aulas/iot/esp32/10-Projeto-Final/#hardware","title":"Hardware","text":"<ul> <li>ESP32 DevKit ou m\u00f3dulo similar</li> <li>Sensor BME280 (temperatura, umidade e press\u00e3o)</li> <li>Sensor de luminosidade BH1750</li> <li>Sensor de qualidade do ar (MQ-135 ou CCS811)</li> <li>Display OLED 128x64 (SSD1306)</li> <li>LEDs indicadores (2-3)</li> <li>Bateria LiPo 3.7V 2000mAh (opcional)</li> <li>M\u00f3dulo de carregamento TP4056 (opcional)</li> <li>Painel solar pequeno 5V/1W (opcional)</li> <li>Bot\u00e3o para configura\u00e7\u00e3o e reset</li> <li>Resistores, capacitores e jumpers</li> </ul>"},{"location":"aulas/iot/esp32/10-Projeto-Final/#software","title":"Software","text":"<ul> <li>Arduino IDE com bibliotecas ESP32</li> <li>Bibliotecas para os sensores</li> <li>Biblioteca PubSubClient para MQTT</li> <li>ArduinoJSON para formata\u00e7\u00e3o de dados</li> <li>Node-RED para dashboard</li> <li>Broker MQTT (local ou na nuvem)</li> </ul>"},{"location":"aulas/iot/esp32/10-Projeto-Final/#esquema-do-projeto","title":"Esquema do Projeto","text":""},{"location":"aulas/iot/esp32/10-Projeto-Final/#diagrama-de-conexao","title":"Diagrama de Conex\u00e3o","text":"<pre><code>                          +----------+\n                          |  ESP32   |\n                          +----------+\n                               |\n         +-------------------+-+------------------+\n         |                   |                    |\n  +------+------+    +-------+--------+    +-----+------+\n  |  Sensores   |    | Comunica\u00e7\u00e3o    |    | Interface  |\n  +-------------+    +----------------+    +------------+\n  | - BME280    |    | - WiFi (MQTT)  |    | - OLED     |\n  | - BH1750    |    | - Bluetooth    |    | - LEDs     |\n  | - MQ-135    |    |                |    | - Bot\u00e3o    |\n  +-------------+    +----------------+    +------------+\n                               |\n                     +---------+---------+\n                     | Energia           |\n                     +-------------------+\n                     | - Bateria         |\n                     | - Deep Sleep      |\n                     | - Painel Solar    |\n                     +-------------------+\n</code></pre>"},{"location":"aulas/iot/esp32/10-Projeto-Final/#pinagem-do-esp32","title":"Pinagem do ESP32","text":"Componente Pino ESP32 Observa\u00e7\u00f5es BME280 (SDA) GPIO21 I\u00b2C BME280 (SCL) GPIO22 I\u00b2C BH1750 (SDA) GPIO21 Compartilha I\u00b2C com BME280 BH1750 (SCL) GPIO22 Compartilha I\u00b2C com BME280 OLED (SDA) GPIO21 Compartilha I\u00b2C OLED (SCL) GPIO22 Compartilha I\u00b2C MQ-135 GPIO34 (ADC1_6) Sa\u00edda anal\u00f3gica LED Status GPIO2 LED padr\u00e3o da placa LED Alerta GPIO4 Vermelho para alertas Bot\u00e3o Config GPIO0 Tamb\u00e9m usado para programa\u00e7\u00e3o Sensor Bateria GPIO35 (ADC1_7) Divisor de tens\u00e3o para leitura"},{"location":"aulas/iot/esp32/10-Projeto-Final/#implementacao-do-codigo","title":"Implementa\u00e7\u00e3o do C\u00f3digo","text":"<p>Vamos dividir o c\u00f3digo em m\u00f3dulos para facilitar a compreens\u00e3o e manuten\u00e7\u00e3o:</p>"},{"location":"aulas/iot/esp32/10-Projeto-Final/#1-arquivo-principal","title":"1. Arquivo Principal","text":"<pre><code>/**\n * Projeto Final: Sistema de Monitoramento Ambiental Inteligente\n * Curso de IoT com ESP32\n */\n\n#include &lt;WiFi.h&gt;\n#include &lt;BluetoothSerial.h&gt;\n#include &lt;PubSubClient.h&gt;\n#include &lt;Wire.h&gt;\n#include &lt;Adafruit_Sensor.h&gt;\n#include &lt;Adafruit_BME280.h&gt;\n#include &lt;BH1750.h&gt;\n#include &lt;Adafruit_GFX.h&gt;\n#include &lt;Adafruit_SSD1306.h&gt;\n#include &lt;ArduinoJson.h&gt;\n#include \"config.h\"           // Arquivo com configura\u00e7\u00f5es\n#include \"display.h\"          // Fun\u00e7\u00f5es para o display\n#include \"sensors.h\"          // Fun\u00e7\u00f5es para leitura de sensores\n#include \"mqtt_handler.h\"     // Fun\u00e7\u00f5es para comunica\u00e7\u00e3o MQTT\n#include \"power_manager.h\"    // Fun\u00e7\u00f5es de gerenciamento de energia\n\n// Objetos globais\nAdafruit_BME280 bme;\nBH1750 lightMeter;\nAdafruit_SSD1306 display(SCREEN_WIDTH, SCREEN_HEIGHT, &amp;Wire, OLED_RESET);\nBluetoothSerial SerialBT;\nWiFiClient espClient;\nPubSubClient mqttClient(espClient);\n\n// Vari\u00e1veis globais\nSensorData sensorData;\nDeviceConfig config;\nbool alertMode = false;\nunsigned long lastPublishTime = 0;\nunsigned long lastSensorReadTime = 0;\nunsigned long lastDisplayUpdateTime = 0;\nint failedMqttConnections = 0;\n\nvoid setup() {\n  // Inicializa comunica\u00e7\u00e3o serial\n  Serial.begin(115200);\n  delay(100);\n  Serial.println(\"\\n--- Sistema de Monitoramento Ambiental Iniciando ---\");\n\n  // Configura pinos\n  pinMode(LED_STATUS_PIN, OUTPUT);\n  pinMode(LED_ALERT_PIN, OUTPUT);\n  pinMode(CONFIG_BUTTON_PIN, INPUT_PULLUP);\n\n  // LED de status - inicializa\u00e7\u00e3o\n  digitalWrite(LED_STATUS_PIN, HIGH);\n\n  // Inicializa I2C\n  Wire.begin(I2C_SDA_PIN, I2C_SCL_PIN);\n\n  // Carrega configura\u00e7\u00f5es salvas\n  loadConfig();\n\n  // Inicializa o display OLED\n  if(!initDisplay(display)) {\n    Serial.println(\"Falha ao inicializar o display OLED\");\n  }\n\n  // Exibe tela de inicializa\u00e7\u00e3o\n  showSplashScreen(display);\n\n  // Inicializa os sensores\n  if(!initSensors(bme, lightMeter)) {\n    Serial.println(\"Falha na inicializa\u00e7\u00e3o de sensores\");\n    showError(display, \"Erro: Sensores\");\n    delay(2000);\n  }\n\n  // Tenta conectar ao WiFi\n  connectToWiFi();\n\n  // Inicializa MQTT\n  setupMqtt(mqttClient, config.mqttServer, config.mqttPort);\n\n  // Inicializa Bluetooth se o modo de configura\u00e7\u00e3o estiver ativo\n  if (isConfigButtonPressed()) {\n    enterConfigMode();\n  }\n\n  // L\u00ea os sensores pela primeira vez\n  readAllSensors(bme, lightMeter, sensorData);\n\n  // Atualiza o display com leituras iniciais\n  updateDisplay(display, sensorData, WiFi.status() == WL_CONNECTED);\n\n  // Pisca LED para indicar inicializa\u00e7\u00e3o completa\n  blinkLed(LED_STATUS_PIN, 3, 100);\n  digitalWrite(LED_STATUS_PIN, LOW);\n\n  Serial.println(\"Sistema inicializado e pronto\");\n}\n\nvoid loop() {\n  // Verifica modo de configura\u00e7\u00e3o\n  if (isConfigButtonPressed()) {\n    enterConfigMode();\n  }\n\n  // L\u00ea sensores periodicamente\n  unsigned long currentMillis = millis();\n  if (currentMillis - lastSensorReadTime &gt;= SENSOR_READ_INTERVAL) {\n    lastSensorReadTime = currentMillis;\n\n    // L\u00ea todos os sensores\n    readAllSensors(bme, lightMeter, sensorData);\n\n    // Verifica condi\u00e7\u00f5es de alerta\n    checkAlertConditions(sensorData);\n\n    // Atualiza display se necess\u00e1rio\n    if (currentMillis - lastDisplayUpdateTime &gt;= DISPLAY_UPDATE_INTERVAL) {\n      lastDisplayUpdateTime = currentMillis;\n      updateDisplay(display, sensorData, mqttClient.connected());\n    }\n  }\n\n  // Gerencia conex\u00e3o MQTT e publica\u00e7\u00e3o de dados\n  if (WiFi.status() == WL_CONNECTED) {\n    // Mant\u00e9m a conex\u00e3o MQTT\n    if (!mqttClient.connected()) {\n      reconnectMqtt(mqttClient, config.mqttUser, config.mqttPassword);\n    }\n    mqttClient.loop();\n\n    // Publica dados periodicamente\n    if (currentMillis - lastPublishTime &gt;= MQTT_PUBLISH_INTERVAL) {\n      lastPublishTime = currentMillis;\n      publishSensorData(mqttClient, sensorData, alertMode);\n    }\n  } else {\n    // Reconecta ao WiFi se necess\u00e1rio\n    if (currentMillis % WIFI_RECONNECT_INTERVAL == 0) {\n      connectToWiFi();\n    }\n  }\n\n  // Gerenciamento de energia\n  if (config.enableDeepSleep &amp;&amp; \n      currentMillis - lastSensorReadTime &gt;= config.maxAwakeTime &amp;&amp;\n      !alertMode) {\n    prepareForSleep();\n    goToDeepSleep(config.deepSleepDuration);\n  }\n\n  // Pequeno delay para estabilidade\n  delay(10);\n}\n\nbool isConfigButtonPressed() {\n  return digitalRead(CONFIG_BUTTON_PIN) == LOW;\n}\n\nvoid enterConfigMode() {\n  Serial.println(\"Entrando no modo de configura\u00e7\u00e3o\");\n\n  // Pisca LED para indicar modo de configura\u00e7\u00e3o\n  blinkLed(LED_STATUS_PIN, 5, 200);\n\n  // Inicializa Bluetooth\n  SerialBT.begin(\"ESP32-Env-Monitor\");\n\n  // Atualiza display\n  showConfigScreen(display);\n\n  // Loop de configura\u00e7\u00e3o via Bluetooth\n  bool exitConfig = false;\n  while (!exitConfig) {\n    // Processa comandos Bluetooth\n    processBluetoothConfig(SerialBT, config);\n\n    // Mant\u00e9m LED piscando para indicar modo de configura\u00e7\u00e3o\n    digitalWrite(LED_STATUS_PIN, (millis() / 500) % 2);\n\n    // Sai do modo de configura\u00e7\u00e3o se o bot\u00e3o for pressionado novamente\n    if (digitalRead(CONFIG_BUTTON_PIN) == LOW) {\n      delay(50);  // Debounce\n      if (digitalRead(CONFIG_BUTTON_PIN) == LOW) {\n        exitConfig = true;\n      }\n    }\n\n    delay(100);\n  }\n\n  // Salva configura\u00e7\u00f5es e reinicia\n  saveConfig();\n  SerialBT.end();\n\n  // Reinicia o ESP32\n  ESP.restart();\n}\n\nvoid checkAlertConditions(SensorData &amp;data) {\n  bool previousAlertMode = alertMode;\n  alertMode = false;\n\n  // Verifica limites para temperatura\n  if (data.temperature &gt; config.tempHighLimit || data.temperature &lt; config.tempLowLimit) {\n    alertMode = true;\n  }\n\n  // Verifica limite para umidade\n  if (data.humidity &gt; config.humidityHighLimit || data.humidity &lt; config.humidityLowLimit) {\n    alertMode = true;\n  }\n\n  // Verifica limite para qualidade do ar\n  if (data.airQuality &gt; config.airQualityLimit) {\n    alertMode = true;\n  }\n\n  // Gerencia LED de alerta\n  digitalWrite(LED_ALERT_PIN, alertMode ? HIGH : LOW);\n\n  // Se o estado de alerta mudou, envie notifica\u00e7\u00e3o imediatamente\n  if (alertMode != previousAlertMode &amp;&amp; mqttClient.connected()) {\n    publishAlertStatus(mqttClient, data, alertMode);\n  }\n}\n\nvoid connectToWiFi() {\n  Serial.print(\"Conectando ao WiFi\");\n  display.clearDisplay();\n  display.setTextSize(1);\n  display.setTextColor(WHITE);\n  display.setCursor(0, 0);\n  display.println(\"Conectando WiFi...\");\n  display.println(config.wifiSSID);\n  display.display();\n\n  WiFi.begin(config.wifiSSID, config.wifiPassword);\n\n  int timeout = WIFI_CONNECTION_TIMEOUT / 500;\n  while (WiFi.status() != WL_CONNECTED &amp;&amp; timeout &gt; 0) {\n    delay(500);\n    Serial.print(\".\");\n    digitalWrite(LED_STATUS_PIN, !digitalRead(LED_STATUS_PIN));\n    timeout--;\n  }\n\n  if (WiFi.status() == WL_CONNECTED) {\n    Serial.println(\"\\nWiFi conectado\");\n    Serial.print(\"Endere\u00e7o IP: \");\n    Serial.println(WiFi.localIP());\n\n    display.println(\"\\nConectado!\");\n    display.println(WiFi.localIP().toString());\n    display.display();\n    delay(1000);\n  } else {\n    Serial.println(\"\\nFalha na conex\u00e3o WiFi\");\n    display.println(\"\\nFalha na conexao!\");\n    display.display();\n    delay(2000);\n  }\n}\n\nvoid blinkLed(int pin, int times, int delayMs) {\n  for (int i = 0; i &lt; times; i++) {\n    digitalWrite(pin, HIGH);\n    delay(delayMs);\n    digitalWrite(pin, LOW);\n    delay(delayMs);\n  }\n}\n</code></pre>"},{"location":"aulas/iot/esp32/10-Projeto-Final/#2-arquivo-de-configuracao-configh","title":"2. Arquivo de Configura\u00e7\u00e3o (config.h)","text":"<pre><code>#ifndef CONFIG_H\n#define CONFIG_H\n\n#include &lt;EEPROM.h&gt;\n\n// Defini\u00e7\u00f5es de pinos\n#define I2C_SDA_PIN 21\n#define I2C_SCL_PIN 22\n#define LED_STATUS_PIN 2\n#define LED_ALERT_PIN 4\n#define CONFIG_BUTTON_PIN 0\n#define MQ135_PIN 34\n#define BATTERY_ADC_PIN 35\n\n// Defini\u00e7\u00f5es do Display OLED\n#define SCREEN_WIDTH 128\n#define SCREEN_HEIGHT 64\n#define OLED_RESET -1\n\n// Intervalos de tempo (ms)\n#define SENSOR_READ_INTERVAL 5000\n#define DISPLAY_UPDATE_INTERVAL 10000\n#define MQTT_PUBLISH_INTERVAL 60000\n#define WIFI_CONNECTION_TIMEOUT 20000\n#define WIFI_RECONNECT_INTERVAL 300000\n\n// Endere\u00e7o EEPROM para salvar configura\u00e7\u00e3o\n#define CONFIG_EEPROM_ADDR 0\n#define EEPROM_SIZE 512\n\n// Estrutura para armazenar dados dos sensores\nstruct SensorData {\n  float temperature;\n  float humidity;\n  float pressure;\n  float lightLevel;\n  int airQuality;\n  float batteryVoltage;\n  unsigned long timestamp;\n};\n\n// Estrutura para armazenar configura\u00e7\u00f5es do dispositivo\nstruct DeviceConfig {\n  char deviceName[32];\n  char wifiSSID[32];\n  char wifiPassword[64];\n  char mqttServer[64];\n  int mqttPort;\n  char mqttUser[32];\n  char mqttPassword[32];\n  char mqttTopicPrefix[32];\n\n  float tempHighLimit;\n  float tempLowLimit;\n  float humidityHighLimit;\n  float humidityLowLimit;\n  int airQualityLimit;\n\n  bool enableDeepSleep;\n  unsigned long deepSleepDuration;\n  unsigned long maxAwakeTime;\n\n  uint32_t configVersion;\n};\n\n// Valores padr\u00e3o para configura\u00e7\u00e3o\nvoid setDefaultConfig(DeviceConfig &amp;config) {\n  strncpy(config.deviceName, \"ESP32-Env-Monitor\", sizeof(config.deviceName));\n  strncpy(config.wifiSSID, \"SeuWiFi\", sizeof(config.wifiSSID));\n  strncpy(config.wifiPassword, \"SuaSenha\", sizeof(config.wifiPassword));\n  strncpy(config.mqttServer, \"broker.hivemq.com\", sizeof(config.mqttServer));\n  config.mqttPort = 1883;\n  strncpy(config.mqttUser, \"\", sizeof(config.mqttUser));\n  strncpy(config.mqttPassword, \"\", sizeof(config.mqttPassword));\n  strncpy(config.mqttTopicPrefix, \"esp32/ambiente\", sizeof(config.mqttTopicPrefix));\n\n  config.tempHighLimit = 35.0;\n  config.tempLowLimit = 5.0;\n  config.humidityHighLimit = 80.0;\n  config.humidityLowLimit = 20.0;\n  config.airQualityLimit = 800;\n\n  config.enableDeepSleep = false;\n  config.deepSleepDuration = 900; // 15 minutos em segundos\n  config.maxAwakeTime = 300000;   // 5 minutos em ms\n\n  config.configVersion = 1;\n}\n\n// Carrega configura\u00e7\u00e3o da EEPROM\nvoid loadConfig() {\n  // Inicializa EEPROM\n  EEPROM.begin(EEPROM_SIZE);\n\n  // Configura\u00e7\u00e3o para uso externo\n  extern DeviceConfig config;\n\n  // Tenta ler da EEPROM\n  EEPROM.get(CONFIG_EEPROM_ADDR, config);\n\n  // Verifica se \u00e9 a primeira vez ou se os dados est\u00e3o corrompidos\n  if (config.configVersion != 1) {\n    Serial.println(\"Configura\u00e7\u00e3o n\u00e3o encontrada, usando valores padr\u00e3o\");\n    setDefaultConfig(config);\n    saveConfig();\n  } else {\n    Serial.println(\"Configura\u00e7\u00e3o carregada da EEPROM\");\n  }\n}\n\n// Salva configura\u00e7\u00e3o na EEPROM\nvoid saveConfig() {\n  extern DeviceConfig config;\n  EEPROM.put(CONFIG_EEPROM_ADDR, config);\n  EEPROM.commit();\n  Serial.println(\"Configura\u00e7\u00e3o salva na EEPROM\");\n}\n\n// Processa comandos de configura\u00e7\u00e3o via Bluetooth\nvoid processBluetoothConfig(BluetoothSerial &amp;bt, DeviceConfig &amp;config) {\n  if (bt.available()) {\n    String command = bt.readStringUntil('\\n');\n    command.trim();\n\n    // Formato do comando: SET:parametro:valor\n    if (command.startsWith(\"SET:\")) {\n      int firstSep = command.indexOf(':', 4);\n      if (firstSep &gt; 4) {\n        String param = command.substring(4, firstSep);\n        String value = command.substring(firstSep + 1);\n\n        if (param == \"WIFI_SSID\") {\n          value.toCharArray(config.wifiSSID, sizeof(config.wifiSSID));\n          bt.println(\"WiFi SSID atualizado para: \" + value);\n        }\n        else if (param == \"WIFI_PASS\") {\n          value.toCharArray(config.wifiPassword, sizeof(config.wifiPassword));\n          bt.println(\"Senha WiFi atualizada\");\n        }\n        else if (param == \"MQTT_SERVER\") {\n          value.toCharArray(config.mqttServer, sizeof(config.mqttServer));\n          bt.println(\"Servidor MQTT atualizado para: \" + value);\n        }\n        else if (param == \"MQTT_PORT\") {\n          config.mqttPort = value.toInt();\n          bt.println(\"Porta MQTT atualizada para: \" + value);\n        }\n        else if (param == \"TEMP_HIGH\") {\n          config.tempHighLimit = value.toFloat();\n          bt.println(\"Limite superior de temperatura: \" + value);\n        }\n        else if (param == \"TEMP_LOW\") {\n          config.tempLowLimit = value.toFloat();\n          bt.println(\"Limite inferior de temperatura: \" + value);\n        }\n        else if (param == \"HUMIDITY_HIGH\") {\n          config.humidityHighLimit = value.toFloat();\n          bt.println(\"Limite superior de umidade: \" + value);\n        }\n        else if (param == \"HUMIDITY_LOW\") {\n          config.humidityLowLimit = value.toFloat();\n          bt.println(\"Limite inferior de umidade: \" + value);\n        }\n        else if (param == \"AIR_QUALITY\") {\n          config.airQualityLimit = value.toInt();\n          bt.println(\"Limite de qualidade do ar: \" + value);\n        }\n        else if (param == \"DEEP_SLEEP\") {\n          config.enableDeepSleep = (value == \"true\" || value == \"1\");\n          bt.println(\"Deep sleep \" + String(config.enableDeepSleep ? \"ativado\" : \"desativado\"));\n        }\n        else if (param == \"SLEEP_DURATION\") {\n          config.deepSleepDuration = value.toInt();\n          bt.println(\"Dura\u00e7\u00e3o do deep sleep: \" + value + \" segundos\");\n        }\n        else {\n          bt.println(\"Par\u00e2metro desconhecido: \" + param);\n        }\n      }\n    }\n    else if (command == \"GET:CONFIG\") {\n      // Envia toda a configura\u00e7\u00e3o atual\n      bt.println(\"===== Configura\u00e7\u00e3o Atual =====\");\n      bt.println(\"WIFI_SSID:\" + String(config.wifiSSID));\n      bt.println(\"MQTT_SERVER:\" + String(config.mqttServer));\n      bt.println(\"MQTT_PORT:\" + String(config.mqttPort));\n      bt.println(\"MQTT_TOPIC:\" + String(config.mqttTopicPrefix));\n      bt.println(\"TEMP_HIGH:\" + String(config.tempHighLimit));\n      bt.println(\"TEMP_LOW:\" + String(config.tempLowLimit));\n      bt.println(\"HUMIDITY_HIGH:\" + String(config.humidityHighLimit));\n      bt.println(\"HUMIDITY_LOW:\" + String(config.humidityLowLimit));\n      bt.println(\"AIR_QUALITY:\" + String(config.airQualityLimit));\n      bt.println(\"DEEP_SLEEP:\" + String(config.enableDeepSleep ? \"true\" : \"false\"));\n      bt.println(\"SLEEP_DURATION:\" + String(config.deepSleepDuration));\n      bt.println(\"==============================\");\n    }\n    else if (command == \"SAVE\") {\n      saveConfig();\n      bt.println(\"Configura\u00e7\u00e3o salva na mem\u00f3ria\");\n    }\n    else if (command == \"RESET\") {\n      bt.println(\"Restaurando configura\u00e7\u00f5es padr\u00e3o...\");\n      setDefaultConfig(config);\n      saveConfig();\n      bt.println(\"Configura\u00e7\u00f5es padr\u00e3o restauradas\");\n    }\n    else if (command == \"EXIT\") {\n      bt.println(\"Saindo do modo de configura\u00e7\u00e3o...\");\n      return;\n    }\n    else if (command == \"HELP\") {\n      bt.println(\"===== Comandos Dispon\u00edveis =====\");\n      bt.println(\"SET:WIFI_SSID:valor - Define SSID WiFi\");\n      bt.println(\"SET:WIFI_PASS:valor - Define senha WiFi\");\n      bt.println(\"SET:MQTT_SERVER:valor - Define servidor MQTT\");\n      bt.println(\"SET:MQTT_PORT:valor - Define porta MQTT\");\n      bt.println(\"SET:TEMP_HIGH:valor - Limite superior temp\");\n      bt.println(\"SET:TEMP_LOW:valor - Limite inferior temp\");\n      bt.println(\"SET:HUMIDITY_HIGH:valor - Limite superior umidade\");\n      bt.println(\"SET:HUMIDITY_LOW:valor - Limite inferior umidade\");\n      bt.println(\"SET:AIR_QUALITY:valor - Limite qualidade ar\");\n      bt.println(\"SET:DEEP_SLEEP:true/false - Ativa/desativa sleep\");\n      bt.println(\"SET:SLEEP_DURATION:valor - Tempo de sleep (s)\");\n      bt.println(\"GET:CONFIG - Mostra configura\u00e7\u00e3o atual\");\n      bt.println(\"SAVE - Salva configura\u00e7\u00e3o na mem\u00f3ria\");\n      bt.println(\"RESET - Restaura configura\u00e7\u00f5es padr\u00e3o\");\n      bt.println(\"EXIT - Sai do modo de configura\u00e7\u00e3o\");\n      bt.println(\"================================\");\n    }\n    else {\n      bt.println(\"Comando n\u00e3o reconhecido. Digite HELP para ajuda.\");\n    }\n  }\n}\n\n#endif // CONFIG_H\n</code></pre>"},{"location":"aulas/iot/esp32/10-Projeto-Final/#3-arquivo-de-funcoes-para-display-displayh","title":"3. Arquivo de Fun\u00e7\u00f5es para Display (display.h)","text":"<pre><code>#ifndef DISPLAY_H\n#define DISPLAY_H\n\n#include &lt;Adafruit_SSD1306.h&gt;\n#include \"config.h\"\n\n// Inicializa o display OLED\nbool initDisplay(Adafruit_SSD1306 &amp;display) {\n  // Inicializa o display SSD1306\n  if(!display.begin(SSD1306_SWITCHCAPVCC, 0x3C)) {\n    Serial.println(F(\"Falha ao alocar SSD1306\"));\n    return false;\n  }\n\n  display.clearDisplay();\n  display.setTextSize(1);\n  display.setTextColor(WHITE);\n  display.setCursor(0, 0);\n  display.display();\n  return true;\n}\n\n// Exibe tela de inicializa\u00e7\u00e3o\nvoid showSplashScreen(Adafruit_SSD1306 &amp;display) {\n  display.clearDisplay();\n\n  // Logo ou t\u00edtulo\n  display.setTextSize(2);\n  display.setTextColor(WHITE);\n  display.setCursor(5, 5);\n  display.println(\"ESP32\");\n  display.setTextSize(1);\n  display.setCursor(5, 25);\n  display.println(\"Sistema de\");\n  display.setCursor(5, 35);\n  display.println(\"Monitoramento\");\n  display.setCursor(5, 45);\n  display.println(\"Ambiental\");\n\n  display.display();\n  delay(2000);\n}\n\n// Exibe tela de erro\nvoid showError(Adafruit_SSD1306 &amp;display, String errorMsg) {\n  display.clearDisplay();\n\n  display.setTextSize(1);\n  display.setTextColor(WHITE);\n  display.setCursor(0, 0);\n  display.println(\"ERRO!\");\n  display.println();\n  display.println(errorMsg);\n\n  display.display();\n}\n\n// Atualiza o display com os dados dos sensores\nvoid updateDisplay(Adafruit_SSD1306 &amp;display, SensorData &amp;data, bool connected) {\n  display.clearDisplay();\n\n  // Linha superior - status\n  display.setTextSize(1);\n  display.setCursor(0, 0);\n  display.print(connected ? \"Conectado\" : \"Offline\");\n\n  // Hora do dia ou tempo de opera\u00e7\u00e3o\n  unsigned long uptime = millis() / 1000;\n  int hours = uptime / 3600;\n  int minutes = (uptime % 3600) / 60;\n  int seconds = uptime % 60;\n\n  char timeStr[9];\n  sprintf(timeStr, \"%02d:%02d:%02d\", hours, minutes, seconds);\n\n  display.setCursor(128 - 6*8, 0); // Alinha \u00e0 direita\n  display.print(timeStr);\n\n  // Linha de separa\u00e7\u00e3o\n  display.drawLine(0, 9, 128, 9, WHITE);\n\n  // Dados dos sensores\n  display.setCursor(0, 12);\n  display.print(\"Temp: \");\n  display.print(data.temperature, 1);\n  display.print(\" C\");\n\n  display.setCursor(0, 22);\n  display.print(\"Umid: \");\n  display.print(data.humidity, 1);\n  display.print(\" %\");\n\n  display.setCursor(0, 32);\n  display.print(\"Pres: \");\n  display.print(data.pressure, 0);\n  display.print(\" hPa\");\n\n  display.setCursor(0, 42);\n  display.print(\"Luz: \");\n  display.print(data.lightLevel, 0);\n  display.print(\" lx\");\n\n  display.setCursor(0, 52);\n  display.print(\"Ar: \");\n  display.print(data.airQuality);\n\n  // Status da bateria, se dispon\u00edvel\n  if (data.batteryVoltage &gt; 0) {\n    display.setCursor(80, 52);\n    display.print(\"Bat:\");\n    display.print(data.batteryVoltage, 1);\n    display.print(\"V\");\n  }\n\n  display.display();\n}\n\n// Exibe tela de configura\u00e7\u00e3o\nvoid showConfigScreen(Adafruit_SSD1306 &amp;display) {\n  display.clearDisplay();\n\n  display.setTextSize(1);\n  display.setCursor(0, 0);\n  display.println(\"MODO CONFIGURACAO\");\n  display.drawLine(0, 9, 128, 9, WHITE);\n\n  display.setCursor(0, 15);\n  display.println(\"Conecte via Bluetooth\");\n  display.setCursor(0, 25);\n  display.println(\"Nome: ESP32-Env-Monitor\");\n\n  display.setCursor(0, 45);\n  display.println(\"Pressione o botao\");\n  display.setCursor(0, 55);\n  display.println(\"novamente para sair\");\n\n  display.display();\n}\n\n#endif // DISPLAY_H\n</code></pre>"},{"location":"aulas/iot/esp32/10-Projeto-Final/#4-arquivo-de-funcoes-para-sensores-sensorsh","title":"4. Arquivo de Fun\u00e7\u00f5es para Sensores (sensors.h)","text":"<pre><code>#ifndef SENSORS_H\n#define SENSORS_H\n\n#include &lt;Adafruit_BME280.h&gt;\n#include &lt;BH1750.h&gt;\n#include \"config.h\"\n\n// Inicializa todos os sensores\nbool initSensors(Adafruit_BME280 &amp;bme, BH1750 &amp;lightMeter) {\n  bool status = true;\n\n  // Inicializa BME280\n  if (!bme.begin(0x76)) {\n    Serial.println(\"N\u00e3o foi poss\u00edvel encontrar o sensor BME280!\");\n    status = false;\n  }\n\n  // Inicializa BH1750\n  if (!lightMeter.begin(BH1750::CONTINUOUS_HIGH_RES_MODE)) {\n    Serial.println(\"N\u00e3o foi poss\u00edvel encontrar o sensor BH1750!\");\n    status = false;\n  }\n\n  return status;\n}\n\n// L\u00ea o sensor MQ135 (qualidade do ar)\nint readMQ135() {\n  // Faz m\u00faltiplas leituras para estabilidade\n  int sum = 0;\n  for (int i = 0; i &lt; 10; i++) {\n    sum += analogRead(MQ135_PIN);\n    delay(10);\n  }\n  return sum / 10;\n}\n\n// L\u00ea a tens\u00e3o da bateria\nfloat readBatteryVoltage() {\n  // Faz m\u00faltiplas leituras para estabilidade\n  int sum = 0;\n  for (int i = 0; i &lt; 10; i++) {\n    sum += analogRead(BATTERY_ADC_PIN);\n    delay(10);\n  }\n  int rawValue = sum / 10;\n\n  // Converte para tens\u00e3o - ajuste estes valores conforme seu circuito\n  float voltage = rawValue * (3.3 / 4095.0);\n\n  // Se houver um divisor de tens\u00e3o (ex: duas resist\u00eancias iguais)\n  // voltage = voltage * 2.0;\n\n  return voltage;\n}\n\n// L\u00ea todos os sensores e atualiza a estrutura de dados\nvoid readAllSensors(Adafruit_BME280 &amp;bme, BH1750 &amp;lightMeter, SensorData &amp;data) {\n  // L\u00ea temperatura, umidade e press\u00e3o do BME280\n  data.temperature = bme.readTemperature();\n  data.humidity = bme.readHumidity();\n  data.pressure = bme.readPressure() / 100.0F; // hPa\n\n  // L\u00ea luminosidade do BH1750\n  data.lightLevel = lightMeter.readLightLevel();\n\n  // L\u00ea qualidade do ar do MQ135\n  data.airQuality = readMQ135();\n\n  // L\u00ea a tens\u00e3o da bateria\n  data.batteryVoltage = readBatteryVoltage();\n\n  // Adiciona timestamp\n  data.timestamp = millis();\n\n  // Exibe no Serial para debug\n  Serial.println(\"--- Leitura de Sensores ---\");\n  Serial.print(\"Temperatura: \"); Serial.print(data.temperature); Serial.println(\" \u00b0C\");\n  Serial.print(\"Umidade: \"); Serial.print(data.humidity); Serial.println(\" %\");\n  Serial.print(\"Press\u00e3o: \"); Serial.print(data.pressure); Serial.println(\" hPa\");\n  Serial.print(\"Luminosidade: \"); Serial.print(data.lightLevel); Serial.println(\" lx\");\n  Serial.print(\"Qualidade do ar: \"); Serial.println(data.airQuality);\n  Serial.print(\"Bateria: \"); Serial.print(data.batteryVoltage); Serial.println(\" V\");\n  Serial.println(\"--------------------------\");\n}\n\n#endif // SENSORS_H\n</code></pre>"},{"location":"aulas/iot/esp32/10-Projeto-Final/#5-arquivo-de-funcoes-para-mqtt-mqtt_handlerh","title":"5. Arquivo de Fun\u00e7\u00f5es para MQTT (mqtt_handler.h)","text":"<pre><code>#ifndef MQTT_HANDLER_H\n#define MQTT_HANDLER_H\n\n#include &lt;PubSubClient.h&gt;\n#include &lt;ArduinoJson.h&gt;\n#include \"config.h\"\n\n// Configura o cliente MQTT\nvoid setupMqtt(PubSubClient &amp;client, const char* server, int port) {\n  client.setServer(server, port);\n}\n\n// Reconecta ao servidor MQTT\nbool reconnectMqtt(PubSubClient &amp;client, const char* username, const char* password) {\n  extern DeviceConfig config;\n\n  Serial.print(\"Tentando conectar ao MQTT...\");\n\n  // Cria um ID de cliente aleat\u00f3rio\n  String clientId = \"ESP32Client-\";\n  clientId += String(random(0xffff), HEX);\n\n  // Tenta conectar\n  if (client.connect(clientId.c_str(), username, password)) {\n    Serial.println(\"conectado\");\n\n    // T\u00f3pico para receber comandos\n    String commandTopic = String(config.mqttTopicPrefix) + \"/command\";\n    client.subscribe(commandTopic.c_str());\n\n    // Publica mensagem informando que est\u00e1 online\n    String statusTopic = String(config.mqttTopicPrefix) + \"/status\";\n    client.publish(statusTopic.c_str(), \"online\", true);\n\n    return true;\n  } else {\n    Serial.print(\"falhou, rc=\");\n    Serial.print(client.state());\n    return false;\n  }\n}\n\n// Publica dados dos sensores via MQTT\nvoid publishSensorData(PubSubClient &amp;client, SensorData &amp;data, bool alert) {\n  extern DeviceConfig config;\n\n  if (!client.connected()) return;\n\n  // Cria o JSON com os dados\n  StaticJsonDocument&lt;512&gt; doc;\n\n  doc[\"device_id\"] = config.deviceName;\n  doc[\"temperature\"] = data.temperature;\n  doc[\"humidity\"] = data.humidity;\n  doc[\"pressure\"] = data.pressure;\n  doc[\"light\"] = data.lightLevel;\n  doc[\"air_quality\"] = data.airQuality;\n  doc[\"battery\"] = data.batteryVoltage;\n  doc[\"alert\"] = alert;\n  doc[\"timestamp\"] = data.timestamp;\n\n  // Serializa para JSON\n  char buffer[512];\n  serializeJson(doc, buffer);\n\n  // T\u00f3pico para dados\n  String dataTopic = String(config.mqttTopicPrefix) + \"/data\";\n\n  // Publica\n  bool success = client.publish(dataTopic.c_str(), buffer);\n  Serial.print(\"Publica\u00e7\u00e3o MQTT: \");\n  Serial.println(success ? \"OK\" : \"FALHA\");\n}\n\n// Publica status de alerta via MQTT\nvoid publishAlertStatus(PubSubClient &amp;client, SensorData &amp;data, bool isAlert) {\n  extern DeviceConfig config;\n\n  if (!client.connected()) return;\n\n  // Cria o JSON com os dados\n  StaticJsonDocument&lt;256&gt; doc;\n\n  doc[\"device_id\"] = config.deviceName;\n  doc[\"alert\"] = isAlert;\n  doc[\"temperature\"] = data.temperature;\n  doc[\"humidity\"] = data.humidity;\n  doc[\"air_quality\"] = data.airQuality;\n  doc[\"timestamp\"] = data.timestamp;\n\n  // Raz\u00e3o do alerta\n  if (isAlert) {\n    String reason = \"\";\n    if (data.temperature &gt; config.tempHighLimit)\n      reason += \"Temperatura alta, \";\n    if (data.temperature &lt; config.tempLowLimit)\n      reason += \"Temperatura baixa, \";\n    if (data.humidity &gt; config.humidityHighLimit)\n      reason += \"Umidade alta, \";\n    if (data.humidity &lt; config.humidityLowLimit)\n      reason += \"Umidade baixa, \";\n    if (data.airQuality &gt; config.airQualityLimit)\n      reason += \"Qualidade do ar ruim, \";\n\n    // Remove a \u00faltima v\u00edrgula e espa\u00e7o\n    if (reason.length() &gt; 0)\n      reason.remove(reason.length() - 2);\n\n    doc[\"reason\"] = reason;\n  }\n\n  // Serializa para JSON\n  char buffer[256];\n  serializeJson(doc, buffer);\n\n  // T\u00f3pico para alertas\n  String alertTopic = String(config.mqttTopicPrefix) + \"/alert\";\n\n  // Publica\n  client.publish(alertTopic.c_str(), buffer);\n}\n\n#endif // MQTT_HANDLER_H\n</code></pre>"},{"location":"aulas/iot/esp32/10-Projeto-Final/#6-arquivo-de-funcoes-para-gerenciamento-de-energia-power_managerh","title":"6. Arquivo de Fun\u00e7\u00f5es para Gerenciamento de Energia (power_manager.h)","text":"<pre><code>#ifndef POWER_MANAGER_H\n#define POWER_MANAGER_H\n\n#include \"config.h\"\n\n// Prepara o dispositivo para entrar em deep sleep\nvoid prepareForSleep() {\n  extern PubSubClient mqttClient;\n  extern DeviceConfig config;\n\n  Serial.println(\"Preparando para entrar em deep sleep...\");\n\n  // Notifica que vai entrar em sleep, se conectado ao MQTT\n  if (mqttClient.connected()) {\n    String statusTopic = String(config.mqttTopicPrefix) + \"/status\";\n    mqttClient.publish(statusTopic.c_str(), \"sleeping\", true);\n    mqttClient.disconnect();\n  }\n\n  // Desliga WiFi\n  WiFi.disconnect(true);\n  WiFi.mode(WIFI_OFF);\n\n  // Desliga outros componentes que consomem energia\n  // ...\n\n  Serial.flush();\n}\n\n// Entra em deep sleep por um tempo determinado\nvoid goToDeepSleep(unsigned long sleepTimeSeconds) {\n  Serial.println(\"Entrando em deep sleep por \" + String(sleepTimeSeconds) + \" segundos\");\n\n  // Configura o timer para acordar\n  esp_sleep_enable_timer_wakeup(sleepTimeSeconds * 1000000ULL);\n\n  // Opcional: habilitar wake-up por pino externo (ex: sensor de movimento)\n  // esp_sleep_enable_ext0_wakeup(GPIO_NUM_33, 1);\n\n  // Entra em deep sleep\n  esp_deep_sleep_start();\n}\n\n#endif // POWER_MANAGER_H\n</code></pre>"},{"location":"aulas/iot/esp32/10-Projeto-Final/#configuracao-do-node-red","title":"Configura\u00e7\u00e3o do Node-RED","text":"<p>Para completar este projeto, crie um dashboard no Node-RED que inclua:</p> <ol> <li>Gr\u00e1ficos para Dados Ambientais:</li> <li>Temperatura e umidade em um gr\u00e1fico de linha</li> <li>Press\u00e3o atmosf\u00e9rica</li> <li>N\u00edvel de luminosidade</li> <li> <p>Qualidade do ar</p> </li> <li> <p>Indicadores de Status:</p> </li> <li>LED virtual para mostrar se o dispositivo est\u00e1 online/offline</li> <li>Indicador de n\u00edvel de bateria</li> <li> <p>Tempo desde a \u00faltima atualiza\u00e7\u00e3o</p> </li> <li> <p>Alertas:</p> </li> <li>\u00c1rea para exibir alertas ativos</li> <li>Hist\u00f3rico de alertas</li> <li> <p>Configura\u00e7\u00e3o de notifica\u00e7\u00f5es</p> </li> <li> <p>Controles:</p> </li> <li>Op\u00e7\u00f5es para ajustar limites de alerta</li> <li>Bot\u00e3o para for\u00e7ar leitura imediata</li> <li>Op\u00e7\u00f5es de configura\u00e7\u00e3o remota</li> </ol>"},{"location":"aulas/iot/esp32/10-Projeto-Final/#expansoes-e-personalizacoes-possiveis","title":"Expans\u00f5es e Personaliza\u00e7\u00f5es Poss\u00edveis","text":"<p>Este projeto pode ser expandido e personalizado de v\u00e1rias maneiras:</p> <ol> <li>Adicionar Mais Sensores:</li> <li>Sensor de CO2 (MH-Z19)</li> <li>Sensor de Material Particulado (PMS5003)</li> <li>Sensor de Movimento (PIR HC-SR501)</li> <li> <p>Sensor de Ru\u00eddo</p> </li> <li> <p>Expandir Conectividade:</p> </li> <li>Integra\u00e7\u00e3o com plataformas na nuvem (AWS IoT, Google Cloud IoT)</li> <li>Backup de dados para cart\u00e3o SD</li> <li> <p>Conex\u00e3o com outros servi\u00e7os web (IFTTT, Webhooks)</p> </li> <li> <p>Melhorar a Interface F\u00edsica:</p> </li> <li>Adicionar um display LCD maior</li> <li>Adicionar bot\u00f5es para navega\u00e7\u00e3o</li> <li> <p>Caixa \u00e0 prova d'\u00e1gua para uso externo</p> </li> <li> <p>Otimizar Energia:</p> </li> <li>Sistema de energia solar completo</li> <li>Adapta\u00e7\u00e3o din\u00e2mica do ciclo de sono baseada na carga da bateria</li> <li>Usar ULP (Ultra Low Power coprocessor) para verifica\u00e7\u00f5es cont\u00ednuas</li> </ol>"},{"location":"aulas/iot/esp32/10-Projeto-Final/#conclusao","title":"Conclus\u00e3o","text":"<p>Este projeto final integra todos os conceitos aprendidos ao longo do curso: - Programa\u00e7\u00e3o do ESP32 - Uso de sensores anal\u00f3gicos e digitais - Conectividade WiFi e Bluetooth - Comunica\u00e7\u00e3o MQTT - Visualiza\u00e7\u00e3o de dados com Node-RED - Gerenciamento de energia - Configura\u00e7\u00e3o e diagn\u00f3stico remotos</p> <p>A estrutura modular permite que voc\u00ea adapte o sistema \u00e0s suas necessidades espec\u00edficas, adicionando mais sensores, alterando a l\u00f3gica de alerta ou implementando novas formas de visualiza\u00e7\u00e3o dos dados.</p> <p>Essa esta\u00e7\u00e3o de monitoramento ambiental \u00e9 apenas o come\u00e7o - com os conhecimentos adquiridos neste curso, voc\u00ea est\u00e1 preparado para desenvolver uma infinidade de projetos IoT usando o ESP32 como plataforma principal.</p> <p>Bom desenvolvimento!</p>"},{"location":"aulas/iot/ex0/","title":"Do Zero ao Her\u00f3i","text":""},{"location":"aulas/iot/ex0/#from-zero-to-hero","title":"From Zero to Hero!!","text":"<p>Espero que estejam animados para mergulhar no mundo dos Sistemas Embarcados e IoT com programa\u00e7\u00e3o em C/C++ usando o Arduino IDE. Para come\u00e7ar do jeito certo e com o p\u00e9 direito! Pratique com essa lista de exerc\u00edcios que vai ajud\u00e1-lo a dominar os conceitos b\u00e1sicos de C/C++, focando apenas na programa\u00e7\u00e3o e utilizando o Monitor Serial para entrada e sa\u00edda de dados, sem envolver por enquanto o hardware espec\u00edfico do Arduino.</p> <p>S\u00e3o 10 exerc\u00edcios abrangendo temas como vari\u00e1veis, opera\u00e7\u00f5es matem\u00e1ticas, estruturas de controle, estruturas de repeti\u00e7\u00e3o, fun\u00e7\u00f5es, vetores, manipula\u00e7\u00e3o de strings, ponteiros, structs e aloca\u00e7\u00e3o din\u00e2mica de mem\u00f3ria. </p> <p>Esses exerc\u00edcios <code>VALEM NOTA!!</code> e s\u00e3o ideais para praticar e aprimorar suas habilidades de programa\u00e7\u00e3o, independentemente do n\u00edvel de experi\u00eancia.</p>"},{"location":"aulas/iot/ex0/#exercicios","title":"Exercicios","text":"<p>Exercise</p> <p>\"Hello, World!\" no Monitor Serial Familiarize-se com o Arduino IDE e o Monitor Serial, escrevendo um programa simples que imprime \"Hello, World!\" no Monitor Serial.</p> <p>Progress</p> <p>Solu\u00e7\u00e3o...</p> <p>Dica: https://docs.arduino.cc/software/ide-v2/tutorials/ide-v2-serial-monitor.</p> <p>Exercise</p> <p>Vari\u00e1veis e Opera\u00e7\u00f5es Matem\u00e1ticas Crie um programa que recebe dois n\u00fameros inteiros do Monitor Serial, realiza opera\u00e7\u00f5es matem\u00e1ticas b\u00e1sicas (adi\u00e7\u00e3o, subtra\u00e7\u00e3o, multiplica\u00e7\u00e3o e divis\u00e3o) e exibe os resultados no Monitor Serial.</p> <p>Exercise</p> <p>Estruturas de Controle: if, else e switch-case Escreva um programa que receba um n\u00famero inteiro do Monitor Serial e, usando estruturas de controle, verifique se o n\u00famero \u00e9 par ou \u00edmpar, positivo ou negativo e imprima o resultado no Monitor Serial.</p> <p>Exercise</p> <p>Estruturas de Repeti\u00e7\u00e3o: for e while Desenvolva um programa que imprima no Monitor Serial os primeiros N n\u00fameros da sequ\u00eancia de Fibonacci, onde N \u00e9 um n\u00famero inteiro fornecido pelo usu\u00e1rio atrav\u00e9s do Monitor Serial.</p> <p>Exercise</p> <p>Fun\u00e7\u00f5es Crie um programa que utiliza fun\u00e7\u00f5es para converter temperaturas entre graus Celsius e Fahrenheit. O usu\u00e1rio deve inserir a temperatura e a escala desejada (C ou F) no Monitor Serial, e o programa deve retornar a temperatura convertida.</p> <p>Exercise</p> <p>Vetores e manipula\u00e7\u00e3o de dados Desenvolva um programa que recebe uma sequ\u00eancia de N n\u00fameros inteiros pelo Monitor Serial, armazena em um vetor, e calcula a m\u00e9dia, o maior e o menor n\u00famero. Imprima os resultados no Monitor Serial.</p> <p>Exercise</p> <p>Manipula\u00e7\u00e3o de Strings Escreva um programa que receba uma string do Monitor Serial e determine o n\u00famero de palavras, o n\u00famero de vogais e o n\u00famero de consoantes na string. Imprima os resultados no Monitor Serial.</p> <p>Exercise</p> <p>Ponteiros Crie um programa que recebe dois n\u00fameros inteiros do Monitor Serial e troque seus valores usando ponteiros. Imprima os valores antes e depois da troca no Monitor Serial.</p> <p>Exercise</p> <p>Estruturas (structs) e Tipos Definidos pelo Usu\u00e1rio Crie um programa que gerencia informa\u00e7\u00f5es de alunos, como nome, idade e notas. Utilize structs para armazenar as informa\u00e7\u00f5es e fun\u00e7\u00f5es para realizar opera\u00e7\u00f5es, como adicionar um aluno, remover um aluno, calcular a m\u00e9dia das notas e exibir as informa\u00e7\u00f5es dos alunos no Monitor Serial.</p>"},{"location":"aulas/iot/ex0/solucao/","title":"Solucao","text":""},{"location":"aulas/iot/ex0/solucao/#solucao-from-zero-to-hero","title":"Solu\u00e7\u00e3o From Zero to Hero!!","text":""},{"location":"aulas/iot/ex0/solucao/#exercicio-1","title":"Exercicio 1","text":"<p>Exercise</p> <p>\"Hello, World!\" no Monitor Serial</p> <p>Familiarize-se com o Arduino IDE e o Monitor Serial, escrevendo um programa simples que imprime \"Hello, World!\" no Monitor Serial.</p> <pre><code>void setup() {\n  // Inicia a comunica\u00e7\u00e3o serial com o monitor em 9600 bps\n  Serial.begin(9600);\n}\n\nvoid loop() {\n\n  Serial.println(\"Hello, World!\");\n  delay(1000); // Aguarda 1 segundo\n}\n</code></pre> <p>Progress</p> <p>Continuar...</p>"},{"location":"aulas/iot/ex0/solucao/#exercicio-2","title":"Exercicio 2","text":"<p>Exercise</p> <p>Vari\u00e1veis e Opera\u00e7\u00f5es Matem\u00e1ticas</p> <p>Crie um programa que recebe dois n\u00fameros inteiros do Monitor Serial, realiza opera\u00e7\u00f5es matem\u00e1ticas b\u00e1sicas (adi\u00e7\u00e3o, subtra\u00e7\u00e3o, multiplica\u00e7\u00e3o e divis\u00e3o) e exibe os resultados no Monitor Serial.</p> <pre><code>int num1, num2;\n\nvoid setup() {\n  Serial.begin(9600);\n}\n\nvoid loop() {\n    Serial.println(\"Digite o primeiro n\u00famero:\");\n    while (Serial.available() == 0) {} // Aguarda o primeiro n\u00famero\n    num1 = Serial.parseInt();\n\n    Serial.println(\"Digite o segundo n\u00famero:\");\n    while (Serial.available() == 0) {} // Aguarda o segundo n\u00famero\n    num2 = Serial.parseInt();\n\n    Serial.print(\"Soma: \");\n    Serial.println(num1 + num2);\n\n    Serial.print(\"Subtra\u00e7\u00e3o: \");\n    Serial.println(num1 - num2);\n\n    Serial.print(\"Multiplica\u00e7\u00e3o: \");\n    Serial.println(num1 * num2);\n\n    if (num2 != 0) {\n      Serial.print(\"Divis\u00e3o: \");\n      Serial.println(num1 / num2);\n    } else {\n      Serial.println(\"Divis\u00e3o por zero n\u00e3o permitida.\");\n    }\n    delay(1000); // Aguarda 1 segundo\n}\n</code></pre> <p>Progress</p> <p>Continuar...</p>"},{"location":"aulas/iot/ex0/solucao/#exercicio-3","title":"Exercicio 3","text":"<p>Exercise</p> <p>Estruturas de Controle: if, else e switch-case</p> <p>Escreva um programa que receba um n\u00famero inteiro do Monitor Serial e, usando estruturas de controle, verifique se o n\u00famero \u00e9 par ou \u00edmpar, positivo ou negativo e imprima o resultado no Monitor Serial.</p> <pre><code>void setup() {\n    Serial.begin(9600);\n    Serial.println(\"Digite um n\u00famero inteiro:\");\n}\n\nvoid loop() {\n\n    if (Serial.available() &gt; 0) {\n        int num = Serial.parseInt();\n\n        if (num % 2 == 0) {\n            Serial.println(\"O n\u00famero \u00e9 par.\");\n        } else {\n            Serial.println(\"O n\u00famero \u00e9 \u00edmpar.\");\n        }\n\n        if (num &gt; 0) {\n            Serial.println(\"O n\u00famero \u00e9 positivo.\");\n        } else if (num &lt; 0) {\n            Serial.println(\"O n\u00famero \u00e9 negativo.\");\n        } else {\n            Serial.println(\"O n\u00famero \u00e9 zero.\");\n        }\n        Serial.println(\"Digite um n\u00famero inteiro:\");  \n    }\n}\n</code></pre> <p>Progress</p> <p>Continuar...</p>"},{"location":"aulas/iot/ex0/solucao/#exercicio-4","title":"Exercicio 4","text":"<p>Exercise</p> <p>Estruturas de Repeti\u00e7\u00e3o: for e while</p> <p>Desenvolva um programa que imprima no Monitor Serial os primeiros N n\u00fameros da sequ\u00eancia de Fibonacci, onde N \u00e9 um n\u00famero inteiro fornecido pelo usu\u00e1rio atrav\u00e9s do Monitor Serial.</p> <pre><code>int n;\n\nvoid setup() {\n  Serial.begin(9600);\n  Serial.println(\"Digite o valor de N:\");\n}\n\nvoid loop() {\n  if (Serial.available() &gt; 0) {\n    n = Serial.parseInt();\n    int a = 0, b = 1, c;\n\n    Serial.print(\"Sequ\u00eancia de Fibonacci: \");\n    Serial.print(a);\n    Serial.print(\", \");\n    Serial.print(b);\n\n    for (int i = 2; i &lt; n; i++) {\n      c = a + b;\n      Serial.print(\", \");\n      Serial.print(c);\n      a = b;\n      b = c;\n    }\n    Serial.println();\n    Serial.println(\"Digite o valor de N:\");\n  }\n}\n</code></pre> <p>Progress</p> <p>Continuar...</p>"},{"location":"aulas/iot/ex0/solucao/#exercicio-5","title":"Exercicio 5","text":"<p>Exercise</p> <p>Fun\u00e7\u00f5es</p> <p>Crie um programa que utiliza fun\u00e7\u00f5es para converter temperaturas entre graus Celsius e Fahrenheit. O usu\u00e1rio deve inserir a temperatura e a escala desejada (C ou F) no Monitor Serial, e o programa deve retornar a temperatura convertida.</p> <pre><code>float converteFahrenheit(float celsius) {\n  return celsius * 9.0 / 5.0 + 32;\n}\n\nfloat converteCelsius(float fahrenheit) {\n    float valor = (fahrenheit - 32) * 5.0 / 9.0;\n  return valor;\n}\n\nvoid setup() {\n  Serial.begin(9600);\n  Serial.println(\"Digite a temperatura e a escala (C ou F):\");\n}\n\nvoid loop() {\n  if (Serial.available() &gt; 0) {\n    float temp = Serial.parseFloat();\n    char scale = Serial.read();\n\n    if (scale == 'C' || scale == 'c') {\n      Serial.print(\"Temperatura em Fahrenheit: \");\n      Serial.println(converteFahrenheit(temp));\n    } else if (scale == 'F' || scale == 'f') {\n      Serial.print(\"Temperatura em Celsius: \");\n      Serial.println(converteCelsius(temp));\n    } else {\n      Serial.println(\"Escala inv\u00e1lida.\");\n    }\n  }\n}\n</code></pre> <p>Progress</p> <p>Continuar...</p>"},{"location":"aulas/iot/ex0/solucao/#exercicio-6","title":"Exercicio 6","text":"<p>Exercise</p> <p>Vetores e manipula\u00e7\u00e3o de dados</p> <p>Desenvolva um programa que recebe uma sequ\u00eancia de N n\u00fameros inteiros pelo Monitor Serial, armazena em um vetor, e calcula a m\u00e9dia, o maior e o menor n\u00famero. Imprima os resultados no Monitor Serial.</p> <pre><code>#define TAMANHO_MAXIMO 100  // Defina um tamanho m\u00e1ximo para o vetor\n\nint vetor[TAMANHO_MAXIMO];  // Vetor de tamanho fixo\n\nvoid setup() {\n  Serial.begin(9600);\n  Serial.println(\"Digite o numero de elementos no vetor:\");\n}\n\nvoid lerElementos(int vetor[], int tamanho) {\n  for (int i = 0; i &lt; tamanho; i++) {\n    Serial.print(\"Digite o numero \");\n    Serial.print(i + 1);\n    Serial.print(\": \");\n\n    while (Serial.available() == 0) {} // Aguarda input\n    vetor[i] = Serial.parseInt();\n    Serial.println(vetor[i]);\n  }\n}\n\nfloat calcularMedia(int vetor[], int tamanho) {\n  int soma = 0;\n  for (int i = 0; i &lt; tamanho; i++) {\n    soma += vetor[i];\n  }\n  float media = (float)soma / tamanho;\n  return media;\n}\n\nint encontrarMaior(int vetor[], int tamanho) {\n  int maior = vetor[0];\n  for (int i = 1; i &lt; tamanho; i++) {\n    if (vetor[i] &gt; maior) {\n      maior = vetor[i];\n    }\n  }\n  return maior;\n}\n\nint encontrarMenor(int vetor[], int tamanho) {\n  int menor = vetor[0];\n  for (int i = 1; i &lt; tamanho; i++) {\n    if (vetor[i] &lt; menor) {\n      menor = vetor[i];\n    }\n  }\n  return menor;\n}\n\nvoid loop() {\n  if (Serial.available() &gt; 0) {\n    int n = Serial.parseInt();\n\n    if (n &gt; TAMANHO_MAXIMO) {\n      Serial.print(\"Numero de elementos escolhido excede o tamanho m\u00e1ximo permitido de \");\n      Serial.println(TAMANHO_MAXIMO);\n      n = TAMANHO_MAXIMO;\n    }\n\n    Serial.print(\"Numero de elementos: \");\n    Serial.println(n);\n\n    lerElementos(vetor, n);\n\n\n    float media = calcularMedia(vetor, n);\n    int maior = encontrarMaior(vetor, n);\n    int menor = encontrarMenor(vetor, n);\n\n    Serial.print(\"Media: \");\n    Serial.println(media);\n\n    Serial.print(\"Maior numero: \");\n    Serial.println(maior);\n\n    Serial.print(\"Menor numero: \");\n    Serial.println(menor);\n\n    Serial.println(\"Digite o numero de elementos no vetor:\");\n\n  }\n}\n</code></pre> <p>Progress</p> <p>Continuar...</p>"},{"location":"aulas/iot/ex0/solucao/#exercicio-7","title":"Exercicio 7","text":"<p>Exercise</p> <p>Manipula\u00e7\u00e3o de Strings</p> <p>Escreva um programa que receba uma string do Monitor Serial e determine o n\u00famero de palavras, o n\u00famero de vogais e o n\u00famero de consoantes na string. Imprima os resultados no Monitor Serial.</p> <p>Tip</p> <p>Esse ex\u00e9rcicio n\u00e3o funcionou no tinkercad, mas no wokwi deu certo.</p> <pre><code>#define TAMANHO_MAXIMO 100;  // Defina um tamanho m\u00e1ximo para o vetor\n\nvoid setup() {\n  Serial.begin(9600);\n  Serial.println(\"Digite uma string:\");\n}\n// O `\\0` \u00e9 o caractere nulo que indica o fim da string\n// || \u00e9 o operador l\u00f3gico OU e &amp;&amp; \u00e9 o operador l\u00f3gico E\n\n#define TAMANHO_MAXIMO 100\n\nvoid setup() {\n  Serial.begin(9600);\n  Serial.println(\"Digite uma string:\");\n}\n\nvoid loop() {\n\n  if (Serial.available() &gt; 0) {\n    char palavras[TAMANHO_MAXIMO]; // Array para armazenar a string com um tamanho m\u00e1ximo de 100 caracteres\n\n    lerString(palavras, TAMANHO_MAXIMO);\n\n    int numPalavras = contarPalavras(palavras);\n    int numVogais = contarVogais(palavras);\n    int numConsoantes = contarConsoantes(palavras);\n\n    // Imprime os resultados no Monitor Serial\n    Serial.print(\"N\u00famero de palavras: \");\n    Serial.println(numPalavras);\n    Serial.print(\"N\u00famero de vogais: \");\n    Serial.println(numVogais);\n    Serial.print(\"N\u00famero de consoantes: \");\n    Serial.println(numConsoantes);\n\n    // Aguarda nova entrada do usu\u00e1rio\n    Serial.println(\"Digite outra string:\");\n  }\n}\n\n// Fun\u00e7\u00e3o que l\u00ea uma string da Serial e armazena em um array de caracteres\nvoid lerString(char buffer[], int maxLength) {\n  int index = 0;\n\n  while (true) {\n    if (Serial.available() &gt; 0) {   // Verifica se h\u00e1 dados dispon\u00edveis na porta serial\n      char receivedChar = Serial.read(); // L\u00ea o caractere recebido\n\n      if (receivedChar == '\\n') {  // Verifica se o caractere \u00e9 uma nova linha (Enter)\n        buffer[index] = '\\0';    // Termina a string com um caractere nulo\n        Serial.print(\"Voc\u00ea digitou: \");\n        Serial.println(buffer);  // Imprime a string recebida\n        break;  // Sai do loop\n      } else {\n        buffer[index] = receivedChar; // Armazena o caractere no array\n        index++;  // Incrementa o \u00edndice\n\n        if (index &gt;= maxLength - 1) {  // Limita o tamanho da string para evitar estouro de mem\u00f3ria\n          Serial.println(\"String muito longa!\");\n          buffer[index] = '\\0';  // Termina a string com um caractere nulo\n          break;  // Sai do loop\n        }\n      }\n    }\n  }\n}\n\n// Fun\u00e7\u00e3o que conta o n\u00famero de palavras em uma string, considerando que uma palavra \u00e9 uma sequ\u00eancia de letras min\u00fasculas\nint contarPalavras(char input[]) {\n  int numPalavras = 0;\n  bool novaPalavra = true;\n  for (int i = 0; input[i] != '\\0'; i++) {\n    char c = input[i];\n    if (c &gt;= 'a' &amp;&amp; c &lt;= 'z') {\n      if (novaPalavra) {\n        numPalavras++;\n        novaPalavra = false;\n      }\n    } else {\n      novaPalavra = true;\n    }\n  }\n  return numPalavras;\n}\n\n// Fun\u00e7\u00e3o que conta o n\u00famero de vogais em uma string \nint contarVogais(char input[]) {\n  int numVogais = 0;\n  for (int i = 0; input[i] != '\\0'; i++) {\n    char c = input[i];\n    if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n      numVogais++;\n    }\n  }\n  return numVogais;\n}\n\n// Fun\u00e7\u00e3o que conta o n\u00famero de consoantes em uma string\nint contarConsoantes(char input[]) {\n  int numConsoantes = 0;\n  for (int i = 0; input[i] != '\\0'; i++) {\n    char c = input[i];\n    if (c &gt;= 'a' &amp;&amp; c &lt;= 'z' &amp;&amp; !(c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u')) {\n      numConsoantes++;\n    }\n  }\n  return numConsoantes;\n}\n</code></pre> <p>Progress</p> <p>Continuar...</p>"},{"location":"aulas/iot/ex0/solucao/#exercicio-8","title":"Exercicio 8","text":"<p>Exercise</p> <p>Ponteiros</p> <p>Crie um programa que recebe dois n\u00fameros inteiros do Monitor Serial e troque seus valores usando ponteiros. Imprima os valores antes e depois da troca no Monitor Serial.</p> <pre><code>void troca(int *a, int *b) {\n    int temp = *a;\n    *a = *b;\n    *b = temp;\n  }\n\n  int num1, num2;\n\n  void setup() {\n    Serial.begin(9600);\n    Serial.println(\"Digite o primeiro n\u00famero:\");\n  }\n\n  void loop() {\n    if (Serial.available() &gt; 0) {\n      num1 = Serial.parseInt();\n      Serial.println(\"Digite o segundo n\u00famero:\");\n\n      while (Serial.available() == 0) {} // Aguarda o segundo n\u00famero\n      num2 = Serial.parseInt();\n\n      Serial.print(\"Antes da troca: num1 = \");\n      Serial.print(num1);\n      Serial.print(\", num2 = \");\n      Serial.println(num2);\n\n      troca(&amp;num1, &amp;num2);\n\n      Serial.print(\"Depois da troca: num1 = \");\n      Serial.print(num1);\n      Serial.print(\", num2 = \");\n      Serial.println(num2);\n\n      while (true); // Pausa o programa ap\u00f3s a execu\u00e7\u00e3o\n    }\n  }\n</code></pre> <p>Progress</p> <p>Continuar...</p>"},{"location":"aulas/iot/ex0/solucao/#exercicio-9","title":"Exercicio 9","text":"<p>Exercise</p> <p>Estruturas (structs) e Tipos Definidos pelo Usu\u00e1rio</p> <p>Crie um programa que gerencia informa\u00e7\u00f5es de alunos, como nome, idade e notas. Utilize structs para armazenar as informa\u00e7\u00f5es e fun\u00e7\u00f5es para realizar opera\u00e7\u00f5es, como adicionar um aluno, remover um aluno, calcular a m\u00e9dia das notas e exibir as informa\u00e7\u00f5es dos alunos no Monitor Serial.</p> <p>Tip</p> <p>Esse eu fiz com ajuda do gpt, n\u00e3o consegui testar pra saber se est\u00e1 funcionando....</p> <pre><code>struct Aluno {\n    String nome;\n    int idade;\n    float notas[3];\n  };\n\n  Aluno alunos[10];\n  int alunoCount = 0;\n\n  void adicionarAluno() {\n    if (alunoCount &lt; 10) {\n      Serial.println(\"Digite o nome do aluno:\");\n      while (Serial.available() == 0) {}\n      alunos[alunoCount].nome = Serial.readString();\n\n      Serial.println(\"Digite a idade do aluno:\");\n      while (Serial.available() == 0) {}\n      alunos[alunoCount].idade = Serial.parseInt();\n\n      for (int i = 0; i &lt; 3; i++) {\n        Serial.print(\"Digite a nota \");\n        Serial.print(i + 1);\n        Serial.println(\":\");\n        while (Serial.available() == 0) {}\n        alunos[alunoCount].notas[i] = Serial.parseFloat();\n      }\n\n      alunoCount++;\n    } else {\n      Serial.println(\"N\u00famero m\u00e1ximo de alunos alcan\u00e7ado.\");\n    }\n  }\n\n  void exibirAlunos() {\n    for (int i = 0; i &lt; alunoCount; i++) {\n      Serial.print(\"Nome: \");\n      Serial.println(alunos[i].nome);\n      Serial.print(\"Idade: \");\n      Serial.println(alunos[i].idade);\n      float media = 0;\n      for (int j = 0; j &lt; 3; j++) {\n        Serial.print(\"Nota \");\n        Serial.print(j + 1);\n        Serial.print(\": \");\n        Serial.println(alunos[i].notas[j]);\n        media += alunos[i].notas[j];\n      }\n      Serial.print(\"M\u00e9dia: \");\n      Serial.println(media / 3);\n    }\n  }\n\n  void setup() {\n    Serial.begin(9600);\n    Serial.println(\"Gerenciamento de alunos:\");\n    adicionarAluno();\n    adicionarAluno();\n    exibirAlunos();\n  }\n\n  void loop() {}\n</code></pre>"},{"location":"aulas/iot/ex1/","title":"Comunica\u00e7\u00e3o Serial","text":""},{"location":"aulas/iot/ex1/#comunicacao-serial","title":"Comunica\u00e7\u00e3o Serial","text":"<p>A comunica\u00e7\u00e3o serial \u00e9 fundamental para a troca de dados entre o microcontrolador (o arduino no nosso caso) e outros dispositivos, como um computador, outro arduino, um celular.</p> <p>No arduino usamos a classe <code>Serial</code> que oferece v\u00e1rias op\u00e7\u00f5es e m\u00e9todos para gerenciar essa comunica\u00e7\u00e3o. </p> <p>Tip</p> <p>Documenta\u00e7\u00e3o oficial: https://www.arduino.cc/reference/pt/language/functions/communication/serial/</p> <p>Aqui est\u00e3o as principais:</p>"},{"location":"aulas/iot/ex1/#iniciando-a-comunicacao-serial","title":"Iniciando a comunica\u00e7\u00e3o serial:","text":"<ul> <li><code>Serial.begin(baudRate);</code> - Inicia a comunica\u00e7\u00e3o serial com a taxa de transmiss\u00e3o especificada.</li> </ul> <pre><code>void setup() {\n    Serial.begin(9600);\n}\n</code></pre> <p>Tip</p> <p>A taxa de transmiss\u00e3o padr\u00e3o \u00e9 de <code>9600</code> bps (bits por segundo), mas outros valores podem ser utilizados conforme a necessidade do projeto. Confira a tabela de taxas de transmiss\u00e3o mais comuns para saber mais.</p>"},{"location":"aulas/iot/ex1/#enviando-dados-pela-serial","title":"Enviando dados pela serial:","text":"<ul> <li><code>Serial.print(data);</code> - Envia dados para a porta serial, <code>sem pular para uma nova linha</code>.</li> <li><code>Serial.println(data);</code> - Envia dados para a porta serial, <code>seguido por um caractere de nova linha (\\n)</code>.</li> </ul> <pre><code>void loop() {\n\n    Serial.print(\"Hello, World!\");\n    Serial.println(\"Hello, World!\");\n\n}\n</code></pre>"},{"location":"aulas/iot/ex1/#recebendo-dados-pela-serial","title":"Recebendo dados pela serial:","text":"<p>A leitura de dados da porta serial no Arduino acontece quando voc\u00ea precisa receber informa\u00e7\u00f5es de outros dispositivos, como sensores, computadores ou outros microcontroladores. </p> <p>Antes de tentar ler os dados, \u00e9 importante verificar se h\u00e1 bytes dispon\u00edveis na porta serial. Isso pode ser feito utilizando a fun\u00e7\u00e3o <code>Serial.available()</code>, que retorna o n\u00famero de bytes prontos para serem lidos:</p> <pre><code>void loop() {\n\n    if (Serial.available() &gt; 0) {\n        // H\u00e1 dados dispon\u00edveis para leitura\n    }  \n}\n</code></pre> <p>Tip</p> <p>Certifique-se de sempre verificar a disponibilidade de dados antes de tentar l\u00ea-los, evitando assim erros ou falhas no processamento de informa\u00e7\u00f5es inexistentes.</p> <p>Depois de confirmar que h\u00e1 dados dispon\u00edveis, voc\u00ea pode utilizar diversas fun\u00e7\u00f5es da classe Serial para ler esses dados. As fun\u00e7\u00f5es mais comuns s\u00e3o:</p> <ul> <li><code>Serial.read();</code> - L\u00ea um \u00fanico byte da serial. O byte \u00e9 retornado como um n\u00famero inteiro entre 0 e 255. Se n\u00e3o houver dados dispon\u00edveis, a fun\u00e7\u00e3o retorna -1. Ideal para leituras byte a byte.</li> <li><code>Serial.readString();</code> - L\u00ea a entrada serial como uma <code>String</code> at\u00e9 que um caractere de nova linha (<code></code>) seja encontrado ou at\u00e9 que o tempo limite seja atingido. \u00datil para receber comandos ou mensagens de texto completas.</li> <li><code>Serial.readStringUntil(character);</code> - L\u00ea a entrada serial como uma <code>String</code> at\u00e9 que o caractere especificado seja encontrado.</li> <li><code>Serial.parseInt();</code> - L\u00ea o pr\u00f3ximo valor inteiro da porta serial, ignorando caracteres n\u00e3o num\u00e9ricos at\u00e9 encontrar um n\u00famero. A leitura continua at\u00e9 encontrar um caractere que n\u00e3o fa\u00e7a parte do n\u00famero.</li> <li><code>Serial.parseFloat();</code> - L\u00ea o pr\u00f3ximo valor em ponto flutuante at\u00e9 encontrar um caractere que n\u00e3o seja parte do n\u00famero, incluindo o ponto decimal.</li> </ul> <pre><code>void loop() {\n    // Lendo um \u00fanico byte\n    Serial.println(\"Digite unico byte\");\n    while (Serial.available() == 0) {} // Aguarda digitar\n    int valor = Serial.read();\n    Serial.print(\"Byte lido: \");\n    Serial.println(valor);\n    Serial.println();\n\n\n    // Lendo uma string completa\n    Serial.println(\"Digite uma string completa\");\n    while (Serial.available() == 0) {} // Aguarda digitar\n    String valorString = Serial.readString();\n    Serial.print(\"String recebida: \");\n    Serial.println(valorString);\n    Serial.println();\n\n    // Lendo uma string at\u00e9 encontrar um caractere espec\u00edfico\n    Serial.println(\"Digite uma string com ; no final\");\n    while (Serial.available() == 0) {} // Aguarda digitar\n    String partialString = Serial.readStringUntil(';');\n    Serial.print(\"String parcial at\u00e9 ';': \");\n    Serial.println(partialString);\n    Serial.println();\n\n    // Lendo um valor inteiro\n    Serial.println(\"Digite um valor do tipo inteiro\");\n    while (Serial.available() == 0) {} // Aguarda digitar\n    int intValue = Serial.parseInt();\n    Serial.print(\"Inteiro lido: \");\n    Serial.println(intValue);\n    Serial.println();\n\n    // Lendo um valor de ponto flutuante\n    Serial.println(\"Digite um valor do tipo float\");\n    while (Serial.available() == 0) {} // Aguarda digitar\n    float floatValue = Serial.parseFloat();\n    Serial.print(\"Float lido: \");\n    Serial.println(floatValue);\n    Serial.println();\n}\n</code></pre>"},{"location":"aulas/iot/ex1/#outros-metodos-uteis","title":"Outros m\u00e9todos \u00fateis:","text":"<ul> <li><code>Serial.setTimeout(time);</code> - O <code>time</code> determina quanto tempo a fun\u00e7\u00e3o aguardar\u00e1 antes de desistir da leitura, caso os dados n\u00e3o estejam dispon\u00edveis.</li> </ul>"},{"location":"aulas/iot/ex2/","title":"Blink LED","text":""},{"location":"aulas/iot/ex2/#o-que-esse-codigo-faz","title":"O que esse c\u00f3digo faz?","text":"<p>Este c\u00f3digo de exemplo demonstra como piscar um LED com arduino (blink led).</p>"},{"location":"aulas/iot/ex2/#circuito-protoboard","title":"Circuito protoboard","text":""},{"location":"aulas/iot/ex2/#codigo","title":"C\u00f3digo","text":"<pre><code>int led = 13; //defindo o valor 13 para a vari\u00e1vel led\n\nvoid setup(){\n    pinMode(led,OUTPUT); //declara led (pino 13 do arduino) como saida (OUTPUT)\n}\n\nvoid loop(){\n    digitalWrite(led, HIGH); //acende (HIGH) o led\n    delay(1000); //delay em milisegundos (1 seg)\n    digitalWrite(led, LOW); //apaga o led (LOW)\n    delay(1000); //delay em milisegundos\n}\n</code></pre> Circuito simulador"},{"location":"aulas/iot/ex2/#links-para-download","title":"Links para Download","text":"<ul> <li> <p>C\u00f3digo arduino </p> </li> <li> <p>Thinkercad online</p> </li> <li> <p>SimulIDE</p> </li> </ul>"},{"location":"aulas/iot/ex3/","title":"LED com Bot\u00e3o","text":""},{"location":"aulas/iot/ex3/#o-que-esse-codigo-faz","title":"O que esse c\u00f3digo faz?","text":"<p>Este c\u00f3digo de exemplo demonstra como acender e apagar um LED em um intervalo de 100 milissegundos ao pressionar um bot\u00e3o com Arduino.</p>"},{"location":"aulas/iot/ex3/#circuito-protoboard","title":"Circuito protoboard","text":""},{"location":"aulas/iot/ex3/#codigo","title":"C\u00f3digo","text":"<pre><code>const int led = 13; //define o apelido led para o valor 13\nconst int botao = 5; //define o apelido botao para o valor 5\n\nvoid setup(){\n  pinMode(led, OUTPUT); //declara o pino13 (led) como sa\u00edda\n  pinMode(botao, INPUT_PULLUP); //declara o pino5 (botao) como entrada\n}\n\nvoid loop(){\n  // Faz a leitura do botao\n  if (digitalRead(botao) == LOW) {\n    digitalWrite(led, HIGH); //acende o led\n    delay(100); //delay em milissegundos\n    digitalWrite(led, LOW); //apaga o led\n    delay(100); //delay em milissegundos\n  }\n}\n</code></pre> Circuito simulador"},{"location":"aulas/iot/ex3/#links-para-download","title":"Links para Download","text":"<ul> <li> <p>C\u00f3digo arduino</p> </li> <li> <p>Thinkercad online</p> </li> <li> <p>SimulIDE</p> </li> </ul>"},{"location":"aulas/iot/ex4/","title":"Potenci\u00f4metro e LED","text":""},{"location":"aulas/iot/ex4/#o-que-esse-codigo-faz","title":"O que esse c\u00f3digo faz?","text":"<p>Este c\u00f3digo de exemplo demonstra como controlar dois LEDs com Arduino usando um bot\u00e3o e um potenci\u00f4metro. Um LED acende e apaga em um intervalo de 100 milissegundos ao pressionar um bot\u00e3o, enquanto o outro LED acende e apaga no mesmo intervalo quando o valor do potenci\u00f4metro \u00e9 maior ou igual a 500.</p>"},{"location":"aulas/iot/ex4/#circuito-protoboard","title":"Circuito protoboard","text":""},{"location":"aulas/iot/ex4/#codigo","title":"C\u00f3digo","text":"<pre><code>const int led = 13; //define o apelido led para o valor 13\nconst int botao = 5; //define o apelido botao para o valor 5\nconst int ledPwm = 11; //define o apelido ledPwm para o valor 11\nconst int potAD = A0; //define o apelido potenciometro para o valor A0\n\nvoid setup(){\n  // Entradas e sa\u00eddas digitais\n  pinMode(led, OUTPUT); //declara o pino13 (led) como sa\u00edda\n  pinMode(botao, INPUT_PULLUP); //declara o pino5 (botao) como entrada\n\n  // Entradas e sa\u00eddas anal\u00f3gicas\n  pinMode(ledPwm, OUTPUT); //declara o pino11 (ledPwm) como sa\u00edda\n  pinMode(potAD, INPUT); //declara o pinoA0 (potenciometro) como entrada\n}\n\nvoid loop(){\n  // Faz a leitura do botao\n  if (digitalRead(botao) == LOW) {\n    digitalWrite(led, HIGH); //acende o led\n    delay(100); //delay em milissegundos\n    digitalWrite(led, LOW); //apaga o led\n    delay(100); //delay em milissegundos\n  }\n  // Faz a leitura anal\u00f3gica do potenciometro\n  int pot = analogRead(potAD);\n  if (pot &gt;= 500) {\n    digitalWrite(ledPwm, HIGH); //acende o led\n    delay(100); //delay em milissegundos\n    digitalWrite(ledPwm, LOW); //apaga o led\n    delay(100); //delay em milissegundos\n  }\n}\n</code></pre> Circuito simulador"},{"location":"aulas/iot/ex4/#links-para-download","title":"Links para Download","text":"<ul> <li> <p>C\u00f3digo arduino</p> </li> <li> <p>Thinkercad online</p> </li> <li> <p>SimulIDE</p> </li> </ul>"},{"location":"aulas/iot/ex5/","title":"Fun\u00e7\u00e3o millis()","text":""},{"location":"aulas/iot/ex5/#o-que-esse-codigo-faz","title":"O que esse c\u00f3digo faz?","text":"<p>Este c\u00f3digo de exemplo demonstra como controlar dois LEDs com Arduino usando um bot\u00e3o e um potenci\u00f4metro, substituindo o uso de <code>delay()</code> por <code>millis()</code>. Um LED alterna seu estado a cada 100 milissegundos ao pressionar um bot\u00e3o, enquanto o outro LED alterna seu estado no mesmo intervalo quando o valor do potenci\u00f4metro \u00e9 maior ou igual a 500.</p>"},{"location":"aulas/iot/ex5/#circuito-protoboard","title":"Circuito protoboard","text":""},{"location":"aulas/iot/ex5/#codigo","title":"C\u00f3digo","text":"<pre><code>const int led = 13; //define o apelido led para o valor 13\nconst int botao = 5; //define o apelido botao para o valor 5\nconst int ledPwm = 11; //define o apelido ledPwm para o valor 11\nconst int potAD = A0; //define o apelido potenciometro para o valor A0\n\nunsigned long tempo1 = 0, tempo2 = 0;\n\nvoid setup() {\n  // Entradas e sa\u00eddas digitais\n  pinMode(led, OUTPUT); //declara o pino13 (led) como sa\u00edda\n  pinMode(botao, INPUT_PULLUP); //declara o pino5 (botao) como entrada\n\n  // Entradas e sa\u00eddas anal\u00f3gicas\n  pinMode(ledPwm, OUTPUT); //declara o pino11 (ledPwm) como sa\u00edda\n  pinMode(potAD, INPUT); //declara o pinoA0 (potenciometro) como entrada\n}\n\nvoid loop() {\n  //usando millis no lugar do delay\n  if (millis() - tempo1 &gt;= 100){\n    tempo1 = millis(); \n    if (digitalRead(botao) == LOW){\n      digitalWrite(led, !digitalRead(led));    \n    }\n  }\n  // usando millis \n  int pot = analogRead(potAD);\n  if (millis() - tempo2 &gt;= 100 &amp;&amp; pot &gt;= 500){\n    tempo2 = millis(); \n    digitalWrite(ledPwm, !digitalRead(ledPwm));     \n  }\n}\n</code></pre> Circuito simulador"},{"location":"aulas/iot/ex5/#links-para-download","title":"Links para Download","text":"<ul> <li> <p>C\u00f3digo arduino</p> </li> <li> <p>Thinkercad online</p> </li> <li> <p>SimulIDE</p> </li> </ul>"},{"location":"aulas/iot/ex6/","title":"Interrup\u00e7\u00f5es","text":""},{"location":"aulas/iot/ex6/#o-que-esse-codigo-faz","title":"O que esse c\u00f3digo faz?","text":"<p>Este c\u00f3digo de exemplo demonstra como controlar dois LEDs com Arduino usando um bot\u00e3o e um potenci\u00f4metro. Um LED \u00e9 acionado por uma interrup\u00e7\u00e3o externa quando o bot\u00e3o \u00e9 pressionado, e o outro LED alterna seu estado quando o valor do potenci\u00f4metro \u00e9 maior ou igual a 500.</p>"},{"location":"aulas/iot/ex6/#circuito-protoboard","title":"Circuito protoboard","text":""},{"location":"aulas/iot/ex6/#codigo","title":"C\u00f3digo","text":"<pre><code>const int led = 13; //define o apelido led para o valor 13\nconst int botao = 2; //define o apelido botao para o valor 2\nconst int ledPwm = 11; //define o apelido ledPwm para o valor 11\nconst int potAD = A0; //define o apelido potenciometro para o valor A0\n\nvoid setup(){\n  // Entradas e sa\u00eddas digitais\n  pinMode(led, OUTPUT); //declara o pino13 (led) como sa\u00edda\n  pinMode(botao, INPUT_PULLUP); //declara o pino2 (botao) como entrada\n  // Entradas e sa\u00eddas anal\u00f3gicas\n  pinMode(ledPwm, OUTPUT); //declara o pino11 (ledPwm) como sa\u00edda\n  pinMode(potAD, INPUT); //declara o pinoA0 (potenciometro) como entrada\n\n  // Configura\u00e7\u00e3o da Interrup\u00e7\u00e3o\n  attachInterrupt(digitalPinToInterrupt(botao), interrupcaoPino2, RISING);  // Configura o pino2 como interrup\u00e7\u00e3o externa do tipo Rising (borda de LOW para HIGH)\n}\n\nvoid loop(){  \n  // Programa principal\n  int pot = analogRead(potAD);\n  if (pot &gt;= 500){\n    digitalWrite(ledPwm, !digitalRead(ledPwm)); \n    delay(100);    \n  } \n}\n\nvoid interrupcaoPino2() // Fun\u00e7\u00e3o de interrup\u00e7\u00e3o do pino2, \u00e9 executado quando o bot\u00e3o do pino2 \u00e9 pressionado\n{                    \n  digitalWrite(led, !digitalRead(led));\n}\n</code></pre> Circuito simulador"},{"location":"aulas/iot/ex6/#links-para-download","title":"Links para Download","text":"<ul> <li> <p>C\u00f3digo arduino</p> </li> <li> <p>Thinkercad online</p> </li> <li> <p>SimulIDE</p> </li> </ul>"},{"location":"aulas/iot/ex7/","title":"Comunica\u00e7\u00e3o com Python","text":""},{"location":"aulas/iot/ex7/#o-que-esse-codigo-faz","title":"O que esse c\u00f3digo faz?","text":"<p>Este c\u00f3digo de exemplo demonstra como realizar a comunica\u00e7\u00e3o serial entre um arduino e um computador, neste exeplo vamos usar um script python.</p> <p></p>"},{"location":"aulas/iot/ex7/#um-pouquinho-de-teoria","title":"Um pouquinho de teoria","text":"<ul> <li> <p><code>Defini\u00e7\u00e3o de comunica\u00e7\u00e3o serial</code>: A comunica\u00e7\u00e3o serial \u00e9 um m\u00e9todo de comunica\u00e7\u00e3o de dados em que os bits de informa\u00e7\u00f5es s\u00e3o transmitidos sequencialmente, um ap\u00f3s o outro, atrav\u00e9s de um \u00fanico canal de comunica\u00e7\u00e3o. \u00c9 uma abordagem simples e comum para transferir dados entre dispositivos, como microcontroladores e computadores.</p> </li> <li> <p><code>Taxa de transmiss\u00e3o (baud rate)</code>: A taxa de transmiss\u00e3o, ou baud rate, \u00e9 a velocidade na qual os bits s\u00e3o transmitidos atrav\u00e9s do canal de comunica\u00e7\u00e3o serial. \u00c9 medida em bits por segundo (bps). Taxas de transmiss\u00e3o comuns comumente utilizadas incluem 9600, 19200 e 115200 bps. A taxa de transmiss\u00e3o deve ser configurada corretamente em ambos os dispositivos de comunica\u00e7\u00e3o (transmissor e receptor) para garantir que os dados sejam transmitidos e recebidos com precis\u00e3o.</p> </li> <li> <p><code>Protocolos de comunica\u00e7\u00e3o serial</code>: Existem v\u00e1rios protocolos de comunica\u00e7\u00e3o serial dispon\u00edveis, cada um com suas pr\u00f3prias especifica\u00e7\u00f5es e caracter\u00edsticas. Alguns dos protocolos mais comuns incluem UART (Universal Asynchronous Receiver/Transmitter), SPI (Serial Peripheral Interface) e I2C (Inter-Integrated Circuit). Neste tutorial, estamos usando a comunica\u00e7\u00e3o UART atrav\u00e9s da porta serial dispon\u00edvel no Arduino.</p> </li> <li> <p><code>Aplica\u00e7\u00f5es da comunica\u00e7\u00e3o serial</code>: A comunica\u00e7\u00e3o serial \u00e9 amplamente utilizada em v\u00e1rias aplica\u00e7\u00f5es, como comunica\u00e7\u00e3o entre microcontroladores e perif\u00e9ricos (por exemplo, sensores, displays, etc.), comunica\u00e7\u00e3o entre computadores e dispositivos eletr\u00f4nicos (por exemplo, impressoras, modems, etc.), e at\u00e9 mesmo em redes de comunica\u00e7\u00e3o industrial (por exemplo, Modbus, Profibus, etc.).</p> </li> <li> <p><code>Vantagens da comunica\u00e7\u00e3o serial</code>: Algumas das principais vantagens da comunica\u00e7\u00e3o serial incluem sua simplicidade, baixo custo, capacidade de transmitir dados a longas dist\u00e2ncias e baixa contagem de pinos nos dispositivos envolvidos.</p> </li> </ul>"},{"location":"aulas/iot/ex7/#codigos","title":"C\u00f3digos","text":"<p>O c\u00f3digo funciona como um \"Eco\", o script Python enviar\u00e1 a mensagem para o Arduino, que a ler\u00e1 e a enviar\u00e1 de volta. A mensagem ser\u00e1 exibida no terminal ou prompt de comando.</p> <pre><code>void setup() {\n  Serial.begin(9600); // Inicia a comunica\u00e7\u00e3o serial com uma taxa de transmiss\u00e3o de 9600 bps\n}\n\nvoid loop() {\n  if (Serial.available() &gt; 0) { // Verifica se h\u00e1 dados dispon\u00edveis para leitura\n    String message = Serial.readString(); // L\u00ea a mensagem enviada pelo Python\n    Serial.println(message); // Envia a mensagem de volta para o Python\n  }\n}\n</code></pre> <p>A segunda parte \u00e9 o c\u00f3digo python (lembre-se de criar um arduino_serial.py)</p> <pre><code>import serial\nimport time\n\ndef main():\n    ser = serial.Serial('COM3', 9600) # Altere 'COM3' para a porta serial do seu Arduino\n    time.sleep(2) # D\u00e1 tempo para a conex\u00e3o ser estabelecida\n\n    while True:\n        msg = input(\"Digite uma mensagem para enviar ao Arduino: \")\n        ser.write(msg.encode()) # Envia a mensagem para o Arduino\n        time.sleep(1) # Aguarda a resposta do Arduino\n\n        while ser.inWaiting() &gt; 0:\n            response = ser.readline().decode().strip() # L\u00ea a resposta do Arduino\n            print(\"Resposta do Arduino:\", response)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"aulas/iot/ex7/#executando-o-script","title":"Executando o script","text":"<ul> <li>Abra o terminal ou prompt de comando e navegue at\u00e9 a pasta onde o arquivo \"arduino_serial.py\" est\u00e1 localizado.</li> <li>Execute o seguinte comando: python arduino_serial.py</li> <li>Digite a mensagem que deseja enviar para o Arduino e pressione Enter.</li> <li>O script Python enviar\u00e1 a mensagem para o Arduino, que a ler\u00e1 e a enviar\u00e1 de volta. A mensagem ser\u00e1 exibida no terminal ou prompt de comando.</li> </ul>"},{"location":"aulas/iot/ex7/#desafios","title":"Desafios","text":""},{"location":"aulas/iot/ex7/#desafio-1","title":"Desafio 1","text":"<p>Fa\u00e7a os ajustes necess\u00e1rios para solucionar o checkpoint.</p>"},{"location":"aulas/iot/ex8/","title":"PWM","text":""},{"location":"aulas/iot/ex8/#o-que-esse-codigo-faz","title":"O que esse c\u00f3digo faz?","text":"<p>Este c\u00f3digo de exemplo demonstra o uso de PWM</p>"},{"location":"aulas/iot/ex8/#circuito-protoboard","title":"Circuito protoboard","text":""},{"location":"aulas/iot/ex8/#codigo","title":"C\u00f3digo","text":"<pre><code>    const int ledPin = 11; // Pino do LED (suporta PWM)\n    const int potPin = A0; // Pino do potenci\u00f4metro\n\n    void setup() {\n      pinMode(ledPin, OUTPUT);\n    }\n\n    void loop() {\n      int sensorValue = analogRead(potPin); // L\u00ea o valor do potenci\u00f4metro\n      int pwmValue = map(sensorValue, 0, 1023, 0, 255); // Mapeia o valor lido para o intervalo do PWM (0-255)\n      analogWrite(ledPin, pwmValue); // Define o duty cycle do PWM\n      delay(10);\n    }\n</code></pre> Circuito simulador"},{"location":"aulas/iot/ex8/#links-para-download","title":"Links para Download","text":"<ul> <li> <p>C\u00f3digo arduino</p> </li> <li> <p>Thinkercad online</p> </li> </ul>"},{"location":"aulas/iot/intro/","title":"Introdu\u00e7\u00e3o","text":"<p>Fa\u00e7a o download do pdf de Introdu\u00e7\u00e3o.</p> <ul> <li>arquivo pdf: Introdu\u00e7\u00e3o</li> </ul>"},{"location":"aulas/iot/intro/dicas/","title":"Dicas Gerais","text":""},{"location":"aulas/iot/intro/dicas/#dicas-uteis-para-o-dia-a-dia","title":"Dicas \u00fateis para o dia a dia","text":""},{"location":"aulas/iot/intro/dicas/#dicas-gerais-para-qualquer-desafio","title":"Dicas gerais para qualquer desafio:","text":"<ul> <li><code>Entenda o problema</code>: Antes de come\u00e7ar a escrever o c\u00f3digo, certifique-se de compreender o problema que voc\u00ea precisa resolver. Leia o enunciado do exerc\u00edcio com aten\u00e7\u00e3o e fa\u00e7a anota\u00e7\u00f5es, se necess\u00e1rio.</li> <li><code>Planeje antes de programar</code>: Pense sobre a l\u00f3gica do programa e como abordar o problema antes de escrever o c\u00f3digo. Isso pode incluir a cria\u00e7\u00e3o de um fluxograma, escrever um pseudoc\u00f3digo ou listar os passos para resolver o problema.</li> <li><code>Divida e conquiste</code>: Separe problemas maiores ou mais complexos em partes menores e mais gerenci\u00e1veis. Isso facilita a resolu\u00e7\u00e3o do problema e a depura\u00e7\u00e3o do c\u00f3digo.</li> <li><code>Comente seu c\u00f3digo</code>: Escreva coment\u00e1rios claros e concisos ao longo do seu c\u00f3digo para explicar o que cada parte faz. Isso facilita a revis\u00e3o e a compreens\u00e3o do c\u00f3digo, tanto para voc\u00ea quanto para os outros.</li> <li><code>Mantenha o c\u00f3digo organizado</code>: Use indenta\u00e7\u00e3o e espa\u00e7amento consistentes para tornar seu c\u00f3digo mais leg\u00edvel e f\u00e1cil de entender.</li> <li><code>Nomeie vari\u00e1veis e fun\u00e7\u00f5es adequadamente</code>: Escolha nomes significativos e descritivos para vari\u00e1veis e fun\u00e7\u00f5es, para que seja f\u00e1cil identificar sua finalidade no c\u00f3digo.</li> <li><code>Pratique a reutiliza\u00e7\u00e3o de c\u00f3digo</code>: Sempre que poss\u00edvel, reutilize partes do c\u00f3digo que j\u00e1 foram escritas para resolver problemas semelhantes. Isso pode economizar tempo e esfor\u00e7o.</li> <li><code>Teste e depure</code>: Teste seu c\u00f3digo frequentemente para garantir que ele esteja funcionando corretamente e resolvendo o problema proposto. </li> <li><code>Aprenda com seus erros</code>: Se voc\u00ea encontrar um erro ou dificuldade, tente entender o motivo e aprenda com ele. Isso ajudar\u00e1 a evitar cometer o mesmo erro no futuro.</li> <li><code>Pe\u00e7a ajuda quando necess\u00e1rio</code>: N\u00e3o hesite em buscar ajuda de colegas, ou comunidades online, como f\u00f3runs e grupos de discuss\u00e3o, se voc\u00ea estiver enfrentando dificuldades ou tiver d\u00favidas.</li> </ul>"},{"location":"aulas/iot/intro/dicas/#hardware","title":"Hardware:","text":""},{"location":"aulas/iot/intro/dicas/#protoboard","title":"Protoboard","text":"<p>A protoboard \u00e9 uma ferramenta essencial para montagem de circuitos eletr\u00f4nicos de forma r\u00e1pida, flex\u00edvel e sem a necessidade de soldagem. Ela permite o desenvolvimento e teste de circuitos antes de sua implementa\u00e7\u00e3o final. Abaixo, est\u00e3o as principais caracter\u00edsticas e boas pr\u00e1ticas para uso eficiente da protoboard:</p> <ul> <li>Estrutura Interna: A protoboard possui trilhas de conex\u00f5es horizontais e verticais, que facilitam a distribui\u00e7\u00e3o de sinais e alimenta\u00e7\u00e3o el\u00e9trica. As trilhas centrais geralmente s\u00e3o divididas, permitindo a inser\u00e7\u00e3o de componentes de forma independente em cada lado.</li> <li>Uso Eficiente de Espa\u00e7o: Organize os componentes de maneira l\u00f3gica, utilizando as trilhas de alimenta\u00e7\u00e3o para VCC e GND nas extremidades da protoboard, e distribuindo os componentes ativos e passivos nas \u00e1reas centrais.</li> <li>Manuten\u00e7\u00e3o da Integridade das Conex\u00f5es: Certifique-se de que os fios de conex\u00e3o (jumpers) estejam firmemente encaixados nas trilhas, para evitar mau contato e poss\u00edveis falhas no circuito.</li> <li>Utiliza\u00e7\u00e3o de Capacitores de Desacoplamento: Em projetos mais complexos, \u00e9 recomend\u00e1vel a adi\u00e7\u00e3o de capacitores de desacoplamento pr\u00f3ximos aos CI's para estabilizar a alimenta\u00e7\u00e3o e reduzir ru\u00eddos.</li> </ul> <p></p> <ul> <li>Guia detalhado de utiliza\u00e7\u00e3o da protoboard: https://portal.vidadesilicio.com.br/protoboard/</li> </ul>"},{"location":"aulas/iot/intro/dicas/#chaves-e-botooes","title":"Chaves e boto\u00f5es","text":"<p>As chaves e bot\u00f5es s\u00e3o dispositivos de entrada essenciais em sistemas embarcados, proporcionando uma interface de controle direta e intuitiva. Eles desempenham um papel crucial em aplica\u00e7\u00f5es que exigem a intera\u00e7\u00e3o do usu\u00e1rio com o hardware.</p> <ul> <li>Refer\u00eancia para voc\u00ea conhecer mais sobre chaves e bot\u00f5es: https://www.robocore.net/tutoriais/introducao-a-chaves-e-botoes</li> </ul>"},{"location":"aulas/iot/intro/dicas/#leds","title":"Leds","text":"<p>Os LEDs (Light Emitting Diodes) s\u00e3o componentes amplamente utilizados como indicadores visuais em circuitos eletr\u00f4nicos. Eles s\u00e3o eficientes, confi\u00e1veis e oferecem uma resposta r\u00e1pida. Abaixo est\u00e3o algumas considera\u00e7\u00f5es importantes para o uso de LEDs em projetos de engenharia eletr\u00f4nica:</p> <ul> <li>C\u00e1lculo de Resistor de Limita\u00e7\u00e3o: Para garantir a longevidade do LED e evitar danos, \u00e9 essencial calcular corretamente o resistor de limita\u00e7\u00e3o de corrente. A f\u00f3rmula b\u00e1sica \u00e9: </li> </ul> \\[ R = \\frac{V_{fonte} - V_{LED}}{I_{LED}} \\] <p>onde \\( V_{fonte} \\) \u00e9 a tens\u00e3o de alimenta\u00e7\u00e3o, \\( V_{LED} \\) \u00e9 a tens\u00e3o de queda do LED, e \\( I_{LED} \\) \u00e9 a corrente desejada. - Polarity Awareness: LEDs s\u00e3o componentes polarizados, o que significa que a corrente s\u00f3 pode fluir em uma dire\u00e7\u00e3o (do anodo para o catodo). A invers\u00e3o de polaridade pode resultar na n\u00e3o ilumina\u00e7\u00e3o do LED ou at\u00e9 na sua danifica\u00e7\u00e3o. - Uso em Circuitos Digitais: LEDs podem ser utilizados para indicar estados de sinal em circuitos digitais. Integra\u00e7\u00f5es com microcontroladores devem considerar o uso de transistores quando a corrente exigida exceder a capacidade da porta de sa\u00edda. - Multiplexa\u00e7\u00e3o e Controle de Intensidade: Em projetos com m\u00faltiplos LEDs, t\u00e9cnicas de multiplexa\u00e7\u00e3o podem ser empregadas para economizar pinos de controle. Al\u00e9m disso, o PWM (Pulse Width Modulation) pode ser utilizado para controlar a intensidade luminosa.</p> <ul> <li>Refer\u00eancia para voc\u00ea conhecer mais sobre leds, resistor, circuitos.... aqui: https://www.makerhero.com/blog/aprenda-a-piscar-um-led-com-arduino/</li> </ul>"},{"location":"aulas/iot/intro/dicas/#outros-sensores-e-atuadores","title":"Outros Sensores e Atuadores","text":"<p>Os sensores e atuadores s\u00e3o utilizados na interface entre o mundo f\u00edsico e os sistemas eletr\u00f4nicos. Eles permitem que um sistema embarcado detecte vari\u00e1veis f\u00edsicas e tome a\u00e7\u00f5es baseadas em leituras de sensores. Abaixo est\u00e3o algumas considera\u00e7\u00f5es t\u00e9cnicas sobre o uso desses componentes:</p> <ul> <li>Tipos de Sensores: Sensores de temperatura, luminosidade, umidade, press\u00e3o, entre outros, devem ser escolhidos com base nas especifica\u00e7\u00f5es do projeto. Considere fatores como precis\u00e3o, intervalo de opera\u00e7\u00e3o e resposta temporal.</li> <li>Interfer\u00eancia e Isolamento: Sensores que operam em ambientes com ru\u00eddos eletromagn\u00e9ticos podem precisar de blindagem e filtragem adequada para garantir leituras precisas. Utilize capacitores e filtros passa-baixa quando necess\u00e1rio.</li> <li>Calibra\u00e7\u00e3o de Sensores: A calibra\u00e7\u00e3o regular dos sensores \u00e9 essencial para manter a precis\u00e3o das medi\u00e7\u00f5es. Isso pode envolver a compara\u00e7\u00e3o com padr\u00f5es conhecidos ou a aplica\u00e7\u00e3o de algoritmos de compensa\u00e7\u00e3o.</li> <li> <p>Integra\u00e7\u00e3o com Microcontroladores: Muitos sensores modernos utilizam interfaces digitais como I2C, SPI ou UART. \u00c9 importante compreender o protocolo de comunica\u00e7\u00e3o e garantir que o microcontrolador tenha recursos suficientes para gerenciar m\u00faltiplos dispositivos simultaneamente.</p> </li> <li> <p>Atuadores como servos, motores DC e rel\u00e9s podem ser controlados diretamente por microcontroladores ou atrav\u00e9s de circuitos de acionamento espec\u00edficos para manipular cargas maiores.</p> </li> </ul>"},{"location":"aulas/iot/intro/introarduino/","title":"Introdu\u00e7\u00e3o","text":""},{"location":"aulas/iot/intro/introarduino/#conceitos-basicos-e-introdutorios","title":"Conceitos b\u00e1sicos e introdut\u00f3rios","text":"<p>A programa\u00e7\u00e3o arduino n\u00e3o \u00e9 complicada, vou apresentar alguns conceitos importante relacionados a software, ferramentas e hardware para te ajudar a embarcar nesse universo.</p>"},{"location":"aulas/iot/intro/introarduino/#placa-arduino","title":"Placa Arduino","text":"<p>Existem diversas placas e vers\u00f5es de Arduinos, vamos trabalhar com o mais simples e famoso, o Arduino UNO. Essa placa possui pinos que podem ser configurados como entradas e saidas para sensores atuadores ou para comunica\u00e7\u00e3o com outros sistemas de hardwares, como mostra a figura:</p> <p></p> <ol> <li>Microcontrolador - este \u00e9 o c\u00e9rebro de um Arduino, \u00e9 nele que carregamos os programas. Pense nisso como um min\u00fasculo computador, projetado para executar apenas um n\u00famero espec\u00edfico de coisas.</li> <li>Porta USB - usada para conectar sua placa Arduino a um computador.</li> <li>Chip USB para Serial - o USB para Serial, respons\u00e1vel por fazer a convers\u00e3o de protocolos, \u00e9 um componente importante, pois \u00e9 o que torna poss\u00edvel programar e comunicar a placa Arduino a partir do seu computador. </li> <li>Pinos digitais - pinos que usam l\u00f3gica digital (0,1 ou LOW/HIGH). Comumente usado para chaves e para ligar/desligar um LED.</li> <li>Pinos anal\u00f3gicos - pinos que podem ler valores anal\u00f3gicos em uma resolu\u00e7\u00e3o de 10 bits (0-1023).</li> <li>Pinos de 5V / 3,3V - esses pinos s\u00e3o usados para alimentar (energia) componentes externos.</li> <li>GND - tamb\u00e9m conhecido como terra, negativo, Ground, \u00e9 utilizado para completar um circuito, onde o n\u00edvel el\u00e9trico est\u00e1 em 0 volt.</li> <li>VIN - significa Voltage In, onde voc\u00ea pode conectar fontes de alimenta\u00e7\u00e3o externas.</li> </ol>"},{"location":"aulas/iot/intro/introarduino/#pinagem","title":"Pinagem","text":"<p>A placa do Arduino UNO possui 14 pinos que podem ser configurados como Entrada/Saida (INPUT/OUTPUT) Digitai, 6 pinos de entrada Analogica com resolu\u00e7\u00e3o de 10 bits, Alguns pinos podem ser configurados para fun\u00e7\u00f5es especificas como Serial, PWM, SPI, TWI(I2C), ISR entre outros...   </p> <p></p>"},{"location":"aulas/iot/intro/introarduino/#software-embarcado","title":"Software Embarcado","text":"<p>O software embarcado \u00e9 o programa que define o funcionamento de um sistema embarcado, ou sej\u00e1, \u00e9 o c\u00f3digo que fica gravado no chip do Arduino. Ele \u00e9 projetado para executar tarefas espec\u00edficas, com um alto grau de efici\u00eancia e confiabilidade. De forma geral, a estrutura de um c\u00f3digo segue um padr\u00e3o, que pode ser dividido em tr\u00eas partes principais:</p> <ol> <li>Inicializa\u00e7\u00e3o: Nesta etapa, o c\u00f3digo realiza a configura\u00e7\u00e3o inicial dos perif\u00e9ricos, como sensores e atuadores, e estabelece a comunica\u00e7\u00e3o com outros dispositivos ou sistemas. Isso inclui a configura\u00e7\u00e3o de pinos de entrada e sa\u00edda, taxas de comunica\u00e7\u00e3o, entre outros, no Arduino inclue a \"void setup()\".</li> <li>La\u00e7o de Repeti\u00e7\u00e3o Infinito: O la\u00e7o infinito, tamb\u00e9m conhecido no Arduino como \"void loop()\", \u00e9 o cora\u00e7\u00e3o do software embarcado. Ele \u00e9 respons\u00e1vel por manter o programa em execu\u00e7\u00e3o cont\u00ednua, permitindo que o sistema embarcado execute suas tarefas de forma repetitiva e ininterrupta.</li> <li>Interrup\u00e7\u00f5es: As interrup\u00e7\u00f5es s\u00e3o eventos que ocorrem de forma ass\u00edncrona ao la\u00e7o infinito, permitindo que o software embarcado responda a eventos externos ou internos, como sinais de sensores ou temporizadores. Esses eventos s\u00e3o geralmente tratados por fun\u00e7\u00f5es espec\u00edficas chamadas rotinas de servi\u00e7o de interrup\u00e7\u00e3o (ISR). </li> </ol> <p>Neste exemplo, a fun\u00e7\u00e3o \"setup()\" \u00e9 respons\u00e1vel pela inicializa\u00e7\u00e3o, enquanto a fun\u00e7\u00e3o \"loop()\" cont\u00e9m as tarefas a serem executadas repetidamente no la\u00e7o infinito.</p> <p><pre><code>int led = 13;\n\nvoid setup(){\n    pinMode(led,OUTPUT);\n}\n\nvoid loop(){\n    digitalWrite(led, HIGH); \n    delay(1000); \n    digitalWrite(led, LOW); \n    delay(1000); \n}\n</code></pre> A representa\u00e7\u00e3o deste programa pode ser visualizada na figura abaixo: </p> <p></p> <p>Para saber mais desse exemplo acesse: <code>Laborat\u00f3rio -&gt; IoT e Sistemas Embarcados --&gt; Blink led</code></p>"},{"location":"aulas/iot/intro/introarduino/#a-linguagem-arduino","title":"A linguagem arduino","text":"<p>A linguagem Arduino \u00e9 propria MAS \u00e9 baseada em C/C++ e simplifica a programa\u00e7\u00e3o de microcontroladores atrav\u00e9s de um ambiente de desenvolvimento integrado (IDE) amig\u00e1vel e de f\u00e1cil acesso.</p> <p>Agumas semelhan\u00e7as com as linguagens C/C++ (e outras tambem...) s\u00e3o:</p> <ul> <li> <p>Sintaxes e Estrutura b\u00e1sica: A estrutura b\u00e1sica do c\u00f3digo Arduino, incluindo a defini\u00e7\u00e3o de fun\u00e7\u00f5es, vari\u00e1veis, constantes e operadores, \u00e9 semelhante \u00e0 encontrada em C e C++.</p> </li> <li> <p>Fun\u00e7\u00f5es e bibliotecas: A linguagem Arduino permite a utiliza\u00e7\u00e3o de fun\u00e7\u00f5es e bibliotecas padr\u00e3o de C/C++, al\u00e9m de bibliotecas espec\u00edficas para Arduino.</p> </li> <li> <p>Ponteiros e aloca\u00e7\u00e3o de mem\u00f3ria: Assim como em C e C++, a linguagem Arduino permite o uso de ponteiros e a manipula\u00e7\u00e3o de mem\u00f3ria, embora esses recursos sejam menos comuns em projetos de Arduino devido \u00e0 sua complexidade e aos recursos limitados dos microcontroladores.</p> </li> </ul>"},{"location":"aulas/iot/intro/introarduino/#entendo-elementos-basicos-de-codigo","title":"Entendo elementos b\u00e1sicos de c\u00f3digo","text":"<p>De forma geral a programa\u00e7\u00e3o de sistemas embarcados envolve o desenvolvimento de aplica\u00e7\u00f5es que interagem com o mundo f\u00edsico atrav\u00e9s de sensores e atuadores. Para compreender e dominar essas intera\u00e7\u00f5es, \u00e9 crucial aprender sobre os conceitos fundamentais, como Entrada e Sa\u00edda Digital, Debounce de bot\u00e3o Digital, Entrada e Sa\u00edda Anal\u00f3gica, Interrup\u00e7\u00e3o Externa e Interfaces de comunica\u00e7\u00e3o como UART/I2C/SPI (Comunica\u00e7\u00e3o Serial).</p>"},{"location":"aulas/iot/intro/introarduino/#saida-digital","title":"Sa\u00edda Digital","text":"<p>A sa\u00edda digital \u00e9 uma forma b\u00e1sica de comunica\u00e7\u00e3o com componentes externos, como LEDs e rel\u00e9s. Os pinos de sa\u00edda digital podem ser configurados para atuar como fonte ou dreno de corrente, dependendo da necessidade do circuito. Os sinais s\u00e3o transmitidos como valores discretos, geralmente <code>0 (LOW) e 1 (HIGH)</code>. No Arduino, \u00e9 poss\u00edvel configurar os pinos de entrada/sa\u00edda como sa\u00edda digital usando a fun\u00e7\u00e3o pinMode() e controlar o estado do pino usando a fun\u00e7\u00e3o <code>digitalWrite()</code>.</p> Dica <p>Veja o exemplo Blink led</p>"},{"location":"aulas/iot/intro/introarduino/#entrada-digital","title":"Entrada Digital","text":"<p>A entrada digital permite que um microcontrolador leia sinais digitais externos, geralmente <code>0 (LOW) e 1 (HIGH)</code>. No Arduino, os pinos de entrada/sa\u00edda podem ser configurados como entrada digital usando a fun\u00e7\u00e3o pinMode() e ler o estado do pino com a fun\u00e7\u00e3o <code>digitalRead()</code>.</p> Dica <p>Veja o exemplo Led bot\u00e3o</p>"},{"location":"aulas/iot/intro/introarduino/#entrada-analogica","title":"Entrada Anal\u00f3gica","text":"<p>A convers\u00e3o de sinais anal\u00f3gicos em valores digitais \u00e9 realizada por um conversor anal\u00f3gico-digital (ADC) presente no microcontrolador. O ADC possui uma resolu\u00e7\u00e3o espec\u00edfica, geralmente 10 bits no Arduino UNO, que determina a quantidade de valores poss\u00edveis para representar o sinal anal\u00f3gico.</p> Dica <p>Veja o exemplo Bot\u00e3o pod led</p>"},{"location":"aulas/iot/intro/introarduino/#pwm-saida-analogica","title":"PWM (Sa\u00edda \"Anal\u00f3gica\")","text":"<p>A t\u00e9cnica PWM permite controlar a energia entregue a dispositivos externos atrav\u00e9s da varia\u00e7\u00e3o do tempo de ativa\u00e7\u00e3o do sinal digital. A frequ\u00eancia do sinal PWM \u00e9 geralmente fixa, enquanto o duty cycle (raz\u00e3o entre o tempo de ativa\u00e7\u00e3o e o per\u00edodo do sinal) varia entre 0 e 100%. No Arduino UNO esse valor \u00e9 definido em 8bits, ou seja, de 0 at\u00e9 255.</p> Dica <p>Veja o exemplo PWM</p>"},{"location":"aulas/iot/intro/introarduino/#interrupcao-externa","title":"Interrup\u00e7\u00e3o Externa","text":"<p>As interrup\u00e7\u00f5es externas podem ser configuradas para serem disparadas em diferentes condi\u00e7\u00f5es, como mudan\u00e7a de estado, n\u00edvel alto ou baixo e bordas de subida ou descida. Ao ser disparada, a interrup\u00e7\u00e3o executa a rotina de tratamento de interrup\u00e7\u00e3o, interrompendo temporariamente o fluxo principal do programa.</p> Dica <p>Veja o exemplo Interrup\u00e7\u00e3o de pino</p>"},{"location":"aulas/iot/intro/introarduino/#o-uso-de-delay-em-sistemas-embarcados","title":"O uso de delay em sistemas embarcados","text":"<p>Evitar delays \u00e9 fundamental para garantir o bom funcionamento e a efici\u00eancia do sistema embarcado. O uso excessivo de delays pode resultar em um desempenho inadequado e na incapacidade de responder a eventos em tempo real. Ao inv\u00e9s de utilizar a fun\u00e7\u00e3o delay(), opte por utilizar millis() e t\u00e9cnicas de programa\u00e7\u00e3o n\u00e3o bloqueantes para criar temporiza\u00e7\u00f5es.</p> Dica <p>Veja o exemplo Fun\u00e7\u00e3o millis    </p>"},{"location":"aulas/iot/intro/introarduino/#uart-comunicacao-serial","title":"UART (Comunica\u00e7\u00e3o Serial)","text":"<p>UART (Universal Asynchronous Receiver-Transmitter) \u00e9 um protocolo de comunica\u00e7\u00e3o serial que permite a transmiss\u00e3o de dados entre dispositivos de forma ass\u00edncrona, sem a necessidade de um clock de refer\u00eancia compartilhado. No Arduino, a comunica\u00e7\u00e3o serial \u00e9 geralmente implementada usando as fun\u00e7\u00f5es <code>Serial.begin(), Serial.print(), Serial.println() e Serial.read()</code>.</p> Dica <p>Veja o exemplo Comunica\u00e7\u00e3o Serial </p>"},{"location":"aulas/iot/intro/introarduino/#referencias","title":"Referencias","text":"<p>A comunidade Arduino \u00e9 muito grande e gera muito material de qualidade, \u00e9 facil encontrar foruns, tutoriais e videos que te auxiliam no aprendizado. De toda a forma, abaixo tem alguns link da documenta\u00e7\u00e3o oficial que podem te ajudar.</p> <ul> <li>https://www.arduino.cc/reference/en/?_gl=1*19zvap6*_ga*MTA5MDMxODM2My4xNjgyNTEwNDg3*_ga_NEXN8H46L5*MTY4MjUyNzkzMS4yLjEuMTY4MjUyODg0Ni4wLjAuMA..</li> <li>https://docs.arduino.cc/learn/starting-guide/getting-started-arduino#general</li> </ul>"},{"location":"aulas/iot/lab0/","title":"Guia de Programa\u00e7\u00e3o C para Arduino","text":""},{"location":"aulas/iot/lab0/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Este guia fornece uma introdu\u00e7\u00e3o pr\u00e1tica \u00e0 programa\u00e7\u00e3o em C para o Arduino, abordando conceitos fundamentais e boas pr\u00e1ticas para desenvolvimento de projetos com microcontroladores. </p> <p>O objetivo \u00e9 ajudar voc\u00ea a entender a sintaxe b\u00e1sica do C e como aplic\u00e1-la em projetos de IoT e sistemas embarcados.</p>"},{"location":"aulas/iot/lab0/#estrutura-basica-de-um-programa-em-c-para-arduino","title":"Estrutura B\u00e1sica de um Programa em C para Arduino","text":"<p>Todo programa Arduino \u00e9 composto de duas fun\u00e7\u00f5es principais:</p>"},{"location":"aulas/iot/lab0/#funcao-setup","title":"Fun\u00e7\u00e3o <code>setup()</code>","text":"<p>A fun\u00e7\u00e3o <code>setup()</code> \u00e9 executada uma vez quando o programa come\u00e7a. \u00c9 usada para inicializar vari\u00e1veis, pinos de entrada e sa\u00edda, bibliotecas, etc.</p> <pre><code>void setup() {\n  // Inicializa o pino digital 13 como sa\u00edda\n  pinMode(13, OUTPUT);\n}\n</code></pre>"},{"location":"aulas/iot/lab0/#funcao-loop","title":"Fun\u00e7\u00e3o <code>loop()</code>","text":"<p>A fun\u00e7\u00e3o <code>loop()</code> cont\u00e9m o c\u00f3digo que \u00e9 executado repetidamente at\u00e9 que o Arduino seja desligado. \u00c9 aqui que a l\u00f3gica principal do programa reside.</p> <pre><code>void loop() {\n  digitalWrite(13, HIGH);   // Liga o LED\n  delay(1000);              // Espera por um segundo\n  digitalWrite(13, LOW);    // Desliga o LED\n  delay(1000);              // Espera por um segundo\n}\n</code></pre>"},{"location":"aulas/iot/lab0/#conceitos-fundamentais","title":"Conceitos Fundamentais","text":""},{"location":"aulas/iot/lab0/#variaveis","title":"Vari\u00e1veis","text":"<p>Vari\u00e1veis s\u00e3o usadas para armazenar dados que podem ser manipulados pelo programa. No Arduino, as vari\u00e1veis comuns incluem:</p> <ul> <li><code>int</code>: N\u00fameros inteiros</li> <li><code>float</code>: N\u00fameros de ponto flutuante</li> <li><code>char</code>: Caracteres individuais</li> <li><code>String</code>: Cadeias de caracteres</li> </ul> <pre><code>int ledPin = 13;\nfloat temperatura = 23.5;\nchar inicial = 'A';\nString texto = \"Hello, World!\";\n</code></pre>"},{"location":"aulas/iot/lab0/#estruturas-de-controle","title":"Estruturas de Controle","text":""},{"location":"aulas/iot/lab0/#condicionais","title":"Condicionais","text":"<ul> <li><code>if</code>, <code>else if</code>, <code>else</code>: Executa blocos de c\u00f3digo com base em condi\u00e7\u00f5es.</li> </ul> <pre><code>if (temperature &gt; 25) {\n  digitalWrite(ledPin, HIGH);\n} else {\n  digitalWrite(ledPin, LOW);\n}\n</code></pre>"},{"location":"aulas/iot/lab0/#loops","title":"Loops","text":"<ul> <li><code>for</code>: Executa um bloco de c\u00f3digo um n\u00famero espec\u00edfico de vezes.</li> <li><code>while</code>: Executa um bloco de c\u00f3digo enquanto uma condi\u00e7\u00e3o for verdadeira.</li> <li><code>do...while</code>: Executa um bloco de c\u00f3digo pelo menos uma vez, e ent\u00e3o repete enquanto a condi\u00e7\u00e3o for verdadeira.</li> </ul> <pre><code>for (int i = 0; i &lt; 10; i++) {\n  Serial.println(i);\n}\n\nwhile (digitalRead(buttonPin) == LOW) {\n  // Espera o bot\u00e3o ser pressionado\n}\n\nint j = 0;\ndo {\n  Serial.println(j);\n  j++;\n} while (j &lt; 5);\n</code></pre>"},{"location":"aulas/iot/lab0/#funcoes","title":"Fun\u00e7\u00f5es","text":"<p>Fun\u00e7\u00f5es s\u00e3o blocos de c\u00f3digo que executam tarefas espec\u00edficas. Elas ajudam a organizar e reutilizar o c\u00f3digo.</p> <pre><code>void acenderLed(int pino) {\n  digitalWrite(pino, HIGH);\n}\n\nvoid apagarLed(int pino) {\n  digitalWrite(pino, LOW);\n}\n\nvoid loop() {\n  acenderLed(ledPin);\n  delay(1000);\n  apagarLed(ledPin);\n  delay(1000);\n}\n</code></pre>"},{"location":"aulas/iot/lab0/#arrays","title":"Arrays","text":"<p>Arrays s\u00e3o cole\u00e7\u00f5es de vari\u00e1veis do mesmo tipo, armazenadas em locais de mem\u00f3ria cont\u00edguos. Eles s\u00e3o \u00fateis para armazenar listas de valores.</p> <pre><code>int numeros[5] = {10, 20, 30, 40, 50};\n\nvoid setup() {\n  Serial.begin(9600);\n  for (int i = 0; i &lt; 5; i++) {\n    Serial.println(numeros[i]);\n  }\n}\n</code></pre>"},{"location":"aulas/iot/lab0/#manipulacao-de-arrays","title":"Manipula\u00e7\u00e3o de Arrays","text":"<pre><code>void inverterArray(int arr[], int tamanho) {\n  for (int i = 0; i &lt; tamanho / 2; i++) {\n    int temp = arr[i];\n    arr[i] = arr[tamanho - 1 - i];\n    arr[tamanho - 1 - i] = temp;\n  }\n}\n\nvoid setup() {\n  int dados[5] = {1, 2, 3, 4, 5};\n  inverterArray(dados, 5);\n  for (int i = 0; i &lt; 5; i++) {\n    Serial.println(dados[i]);\n  }\n}\n</code></pre>"},{"location":"aulas/iot/lab0/#ponteiros","title":"Ponteiros","text":"<p>Ponteiros s\u00e3o vari\u00e1veis que armazenam endere\u00e7os de mem\u00f3ria. Eles s\u00e3o poderosos para manipula\u00e7\u00e3o de dados e mem\u00f3ria.</p> <pre><code>int valor = 10;\nint *ponteiro = &amp;valor;\n\nvoid setup() {\n  Serial.begin(9600);\n  Serial.println(*ponteiro); // Acessa o valor usando o ponteiro\n  *ponteiro = 20;\n  Serial.println(valor); // Valor alterado atrav\u00e9s do ponteiro\n}\n</code></pre>"},{"location":"aulas/iot/lab0/#ponteiros-e-arrays","title":"Ponteiros e Arrays","text":"<pre><code>void imprimirArray(int *arr, int tamanho) {\n  for (int i = 0; i &lt; tamanho; i++) {\n    Serial.println(*(arr + i));\n  }\n}\n\nvoid setup() {\n  int numeros[3] = {100, 200, 300};\n  imprimirArray(numeros, 3);\n}\n</code></pre>"},{"location":"aulas/iot/lab0/#structs","title":"Structs","text":"<p>Structs s\u00e3o usadas para agrupar vari\u00e1veis relacionadas sob um \u00fanico nome. Elas s\u00e3o \u00fateis para representar objetos complexos.</p> <pre><code>typedef struct {\n  int idade;\n  float altura;\n  char nome[50];\n} Pessoa;\n\nvoid setup() {\n  Serial.begin(9600);\n  Pessoa aluno = {20, 1.75, \"Jo\u00e3o\"};\n  Serial.print(\"Nome: \");\n  Serial.println(aluno.nome);\n  Serial.print(\"Idade: \");\n  Serial.println(aluno.idade);\n  Serial.print(\"Altura: \");\n  Serial.println(aluno.altura);\n}\n</code></pre>"},{"location":"aulas/iot/lab0/#manipulacao-de-structs","title":"Manipula\u00e7\u00e3o de Structs","text":"<pre><code>void imprimirPessoa(Pessoa p) {\n  Serial.print(\"Nome: \");\n  Serial.println(p.nome);\n  Serial.print(\"Idade: \");\n  Serial.println(p.idade);\n  Serial.print(\"Altura: \");\n  Serial.println(p.altura);\n}\n\nvoid setup() {\n  Pessoa professor = {35, 1.80, \"Carlos\"};\n  imprimirPessoa(professor);\n}\n</code></pre>"},{"location":"aulas/iot/lab0/#enum","title":"enum","text":"<p>Um enum (abrevia\u00e7\u00e3o de enumerador ou tipo enumerado) em C \u00e9 um tipo de dado que permite atribuir nomes simb\u00f3licos a constantes inteiras. Esses nomes tornam o c\u00f3digo mais leg\u00edvel e f\u00e1cil de entender, especialmente quando se trabalha com conjuntos espec\u00edficos de valores constantes.</p> <pre><code>//Declara\u00e7\u00e3o\nenum estadosMotor { PARADO, FRENTE, TRAS };\n\n// Definine estado inicial\nestadosMotor estadoAtual = PARADO;\n\nvoid setup() {\n    pinMode(13, OUTPUT);\n    Serial.begin(9600);\n}\n\nvoid loop() {\n    switch (estadoAtual) {\n        case PARADO:\n            digitalWrite(13, LOW);\n            Serial.println(\"Motor parado.\");\n            break;\n        case FRENTE:\n            digitalWrite(13, HIGH);\n            Serial.println(\"Motor para frente.\");\n            break;\n        case TRAS:\n            digitalWrite(13, HIGH);\n            Serial.println(\"Motor para tr\u00e1s.\");\n            break;\n    }\n    delay(1000);\n}\n</code></pre>"},{"location":"aulas/iot/lab0/#boas-praticas","title":"Boas Pr\u00e1ticas","text":"<ul> <li>Comente seu c\u00f3digo: Use coment\u00e1rios para explicar o que o c\u00f3digo faz, especialmente para l\u00f3gica complexa.</li> <li>Use nomes descritivos: Escolha nomes de vari\u00e1veis e fun\u00e7\u00f5es que descrevam claramente seu prop\u00f3sito.</li> <li>Modularize o c\u00f3digo: Divida o c\u00f3digo em fun\u00e7\u00f5es para melhorar a legibilidade e a reutiliza\u00e7\u00e3o.</li> <li>Teste frequentemente: Teste pequenas partes do c\u00f3digo \u00e0 medida que desenvolve para identificar problemas rapidamente.</li> </ul>"},{"location":"aulas/iot/lab0/#recursos-adicionais","title":"Recursos Adicionais","text":"<ul> <li>Documenta\u00e7\u00e3o Oficial do Arduino</li> <li>Curso de C para Arduino (YouTube)</li> <li>Livro: \"Programming Arduino: Getting Started with Sketches\"</li> </ul> <p>Este guia serve como um ponto de partida para a programa\u00e7\u00e3o em C no Arduino.</p>"},{"location":"aulas/iot/lab1/","title":"Lab01 - LED","text":""},{"location":"aulas/iot/lab1/#lab1-desafios","title":"Lab1 - Desafios","text":""},{"location":"aulas/iot/lab1/#desafio-1","title":"Desafio 1","text":"<p>Com base no exemplo Blink led, fa\u00e7a:</p> <p></p> <p>Monte um circuito com 2 LEDs, usando como refer\u00eancia a imagem acima. Escreva um c\u00f3digo que fa\u00e7a os LEDs piscarem de forma s\u00edncrona a cada 0,5 segundos.</p>"},{"location":"aulas/iot/lab1/#desafio-2","title":"Desafio 2","text":"<p>Agora, fa\u00e7a os LEDs acenderem conforme a carta de tempo abaixo. Onde:</p> <p>1 = n\u00edvel l\u00f3gico alto (HIGH)</p> <p>0 = n\u00edvel l\u00f3gico baixo (LOW)</p> <p>delay = 500ms</p> <p></p>"},{"location":"aulas/iot/lab1/#desafio-3","title":"Desafio 3","text":"<p>Utilizando o Buzzer, fa\u00e7a seu nome em C\u00f3digo Morse. Use como refer\u00eancia a tabela abaixo.</p> <p></p>"},{"location":"aulas/iot/lab10/","title":"Index","text":""},{"location":"aulas/iot/lab10/#o-que-vamos-ver-neste-lab","title":"O que vamos ver neste lab?","text":"<ul> <li>Raspberry Pi: <ul> <li>Conhecendo os pinos</li> <li>Usando a biblioteca RPI.GPIO </li> <li>Montando um Webserver em Flask </li> </ul> </li> </ul> <p>Progress</p> <p>Continuar...</p>"},{"location":"aulas/iot/lab10/#conhecendo-os-pinos-da-raspberry-pi","title":"Conhecendo os pinos da Raspberry Pi","text":"<p>Podemos utilizar a Raspberry Pi para conectar sensores e atuadores, de forma semelhante como foi feito utilizando o Arduino, para isso utilizamos os barramento de pinos da Raspberry Pi chamado de GPIO (General Purpose Input Output). Ao todo s\u00e3o 40 pinos (para RPI 2 ou superior) e de forma geral cada pino possui uma fun\u00e7\u00e3o ou caracteristica especifica.</p> <p>Warning</p> <p>Cuidado: Devemos ter aten\u00e7\u00e3o para n\u00e3o conectar os perifericos na placa de forma incorreta. Existe risco de queimar a Raspberry Pi.  </p> <p>A imagem abaixo \u00e9 um guia simples para cada pino. Parece complicado na primeira vez, mas \u00e9 tranquilo.</p> <p></p> <p>Vamos conhecer o que \u00e9 cada pino:</p> <pre><code>- Pinos de Alimenta\u00e7\u00e3o: \n    - 3.3V (ao todo 2 pinos)\n    - 5V (ao todo 2 pinos)\n    - GND/Ground/0V (ao todo 8 pinos)\n\n- Pinos de interface:\n    - GPIO (General purpose input and output): S\u00e3o os pinos de entrada/saida. A tens\u00e3o de saida \u00e9 de 3.3V.\n    - I2C/SPI/UART: Protocolos de comunica\u00e7\u00e3o especificos utilizados para realizar a interface m\u00f3dulos epecificos com a Raspberry Pi.\n</code></pre> <p>Warning</p> <p>Aten\u00e7\u00e3o: Observe a correla\u00e7\u00e3o dos pinos para n\u00e3o ligar invertido. </p> <p>Exercise</p> <p>Quantos pinos GPIO est\u00e3o disponiveis?</p> <p>Progress</p> <p>Continuar...</p>"},{"location":"aulas/iot/lab10/#configurando-os-gpios","title":"Configurando os GPIOs","text":"<p>No final do lab07 montamos um simples pisca led e programamos configurando os valores dos registradores. Existem formas mais simples de programar os GPIOs da rasbperry pi, vamos programar em Python :) </p> <p>Vamos utilizara biblioteca <code>RPI.GPIO</code>, que permite de forma simples configurar e usar os GPIOs com script em Python, vamos preparar o nosso ambiente de desenvolvimento:</p> <p>Exercise</p> <ul> <li> <p>Inicialize a Raspberry Pi. (modo Desktop ou SSH).</p> <ul> <li>Se tiver d\u00favida de como fazer, volte para o lab07.</li> </ul> </li> <li> <p>Abra o terminal da raspberry pi.</p> </li> <li> <p>Certifique-se de estar com acesso a internet.</p> </li> </ul> <p>No terminal da raspberry pi, atualize os reposit\u00f3rios:</p> <pre><code>sudo apt update\n</code></pre> <p>Em seguida, tente instalar o pacote RPi.GPIO: A documenta\u00e7\u00e3o da biblioteca est\u00e1 disponivel no aqui.</p> <pre><code>sudo apt install rpi.gpio\n</code></pre> <p>Se ainda n\u00e3o estiver instalado, ser\u00e1 instalado. Se j\u00e1 estiver instalado, ser\u00e1 atualizado se uma vers\u00e3o mais recente estiver dispon\u00edvel.</p> <p>Progress</p> <p>Continuar...</p>"},{"location":"aulas/iot/lab10/#conhecendo-a-biblioteca-rpigpio","title":"Conhecendo a biblioteca RPi.GPIO","text":"<p>\u00c9 uma biblioteca simples de usar e vamos ver as principais fun\u00e7\u00f5es da RPi.GPIO atrav\u00e9s do c\u00f3digo de exemplo abaixo:</p> <ul> <li> <p><code>GPIO.setmode()</code> = Define o modo de acesso aos pino da raspberry pi, existem 2 modos de definir a mesma coisa:</p> <ul> <li>GPIO.BOARD  = Posi\u00e7\u00e3o f\u00edsica do pino na raspberry pi</li> <li>GPIO.BCM    = Numero ap\u00f3s GPIOxx</li> </ul> </li> </ul> <p>exemplo: BOARD 11 = GPIO17</p> <ul> <li> <p><code>GPIO.setup()</code> = Define a fun\u00e7\u00e3o do pino, entrada (GPIO.IN) ou saida (GPIO.OUT)</p> </li> <li> <p><code>GPIO.output()</code> = Define o estado do pino definido como saida em nivel logico baixo (GPIO.LOW) ou alto (GPIO.HIGH)</p> </li> <li> <p><code>GPIO.input()</code> = Faz a leitura do estado do pino definido como entrada. Geralmente quando usamos um pino como entrada configuramos no setup o parametro pull_up_down (como exemplo: GPIO.setup(18, GPIO.IN, pull_up_down=GPIO.PUD_UP))</p> </li> </ul> <p>Exercise</p> <p>Monte o circuito abaixo:</p> <p></p> <ul> <li>No terminal da RPI, digite:</li> </ul> <pre><code>cd ~\nmkdir src\ncd src\ntouch blinkled.py  \n</code></pre> <ul> <li>Criamos um diretorio chamado src e um arquivo python chamado blinkled.py</li> <li>Abra o arquivo blinkled.py e escreva o c\u00f3digo abaixo.</li> <li>Para abrir o arquivo digite: nano blinkled.py</li> <li>Ap\u00f3s digitar o c\u00f3digo python, salve e feche o arquivo: Ctlr+X &gt;&gt;&gt; Y </li> <li> <p>Vamos rodar nosso c\u00f3digo python, no terminal digite:</p> <ul> <li>python blinkled.py</li> </ul> </li> <li> <p>Se tudo deu certo, o led est\u00e1 piscando. :)</p> <ul> <li>para interromper o c\u00f3digo aperte Ctrl+C.</li> </ul> </li> </ul> <p>Warning</p> <p>Os 2 c\u00f3digos realizam a mesma fun\u00e7\u00e3o, a diferen\u00e7a est\u00e1 apenas no setmode. Escolha um dos c\u00f3digos para testar. </p> <pre><code>import RPi.GPIO as GPIO  ### import da biblioteca gpio\nimport time\n\n# usando o a posi\u00e7\u00e3o fis\u00edca do pino na raspberry pi\nGPIO.setmode(GPIO.BOARD)\n\n# configura o pino fisico 11 como saida\nGPIO.setup(11, GPIO.OUT)\n\nwhille True:  \n    # escreve no pino 11 nivel logico alto\n    GPIO.output(11, GPIO.HIGH)\n    time.sleep(1) # delay de 1s\n\n    # escreve no pino 11 nivel logico baixo\n    GPIO.output(11, GPIO.LOW)\n    time.sleep(1) # delay de 1s\n\nGPIO.cleanup()  # Limpa configura\u00e7\u00e3o finaliza o programa\n</code></pre> <pre><code>import RPi.GPIO as GPIO  ### import da biblioteca gpio\n\n# usando o numero ap\u00f3s GPIOxx da raspberry pi\nGPIO.setmode(GPIO.BCM)\n\n# configura o GPIO17 como saida\nGPIO.setup(17, GPIO.OUT)\n\nwhille True:  \n    # escreve no GPIO17 nivel logico alto\n    GPIO.output(17, GPIO.HIGH)\n    time.sleep(1) # delay de 1s\n\n    # escreve no GPIO17 nivel logico baixo\n    GPIO.output(17, GPIO.LOW)\n    time.sleep(1) # delay de 1s\n\nGPIO.cleanup()  # Limpa configura\u00e7\u00e3o finaliza o programa\n</code></pre> <p>Agora que j\u00e1 entendemos a estrutura b\u00e1sica do script python, fa\u00e7a os exercicios abaixo para praticar</p> <p>Exercise</p> <p>Sem\u00e1faro de transito: </p> <pre><code>- Monte um circuito com 3 leds (1 verde, 1 amarelo, 1 vermelho);\n- crie um novo script chamado semaforo.py;\n- Escreva um c\u00f3digo que ir\u00e1 acender os leds na sequ\u00eancia e intervalo:\n    - Verde (5segundos)\n    - Amarelo (3segundos)\n    - Vermelho (6segundos)\n    - loop (volta para o verde)\n</code></pre> <p>Exercise</p> <p>leitura de bot\u00e3o e Led: </p> <ul> <li>Monte o circuito: </li> </ul> <p></p> <ul> <li>Escreva um c\u00f3digo que:<ul> <li>Enquanto nenhum bot\u00e3o for pressionado, os leds ficam apagados;</li> <li>Se o bot\u00e3o1 for pressionado:<ul> <li>os leds acendem na sequ\u00eancia: Verde - Amarelo - Vermelho</li> </ul> </li> <li>Se o bot\u00e3o2 for pressionado:<ul> <li>os leds acendem na sequencia: Vermelho - Amarelo - Verde </li> </ul> </li> </ul> </li> </ul> <p>Dica: Geralmente quando usamos algum pino como entrada configuramos no setup o parametro pull_up_down (como exemplo: GPIO.setup(18, GPIO.IN, pull_up_down=GPIO.PUD_UP) ou GPIO.setup(18, GPIO.IN, pull_up_down=GPIO.PUD_DOWN).</p> <p>Exercise</p> <p>Sensor de temperatura: Para quem tiver curiosidade pode dar uma olhada como utilizar o sensor de temperatura DTH11 neste link.</p> <p>Progress</p> <p>Continuar...</p>"},{"location":"aulas/iot/lab10/#montando-um-webserver-em-flask","title":"Montando um Webserver em Flask","text":"<p>Vamos montar um webserver na raspberry pi com flask. A ideia deste exemplo \u00e9 controlar por um navegador web o status de um led entre ligado e desligado:</p> <p></p>"},{"location":"aulas/iot/lab10/#instalando-o-flask-e-configurando-o-ambiente","title":"Instalando o Flask e configurando o ambiente","text":"<p>No terminal da raspberry pi, atualize os reposit\u00f3rios:</p> <pre><code>sudo apt update\n</code></pre> <p>Instale os pacotes do flask</p> <pre><code>sudo apt-get install python3-flask\n</code></pre> <p>Agora vamos criar nossa arvore de projeto:</p> <pre><code>- webserver\n    - static\n        - index.css\n    - templates\n        - index.html\n    - app.py\n</code></pre> <p>No terminal da raspberry pi, digite:</p> <pre><code>cd ~\nmkdir webserver\ncd webserver\nmkdir static templates\nls\n</code></pre> <p>Vamos criar o <code>app.py</code>. No terminal da raspberry pi, digite:</p> <pre><code>nano app.py\n</code></pre> <p>Com o editor nano aberto digite:</p> <pre><code>'''\n    Arnaldo Viana\n'''\nimport RPi.GPIO as GPIO\nfrom flask import Flask, render_template, request\n\napp = Flask(__name__)\n\nGPIO.setmode(GPIO.BCM)\nGPIO.setwarnings(False)\n\n#define actuators GPIOs\nledRed = 2\n\n#initialize GPIO status variables\nledRedSts = 0\n\n# Define led pins as output\nGPIO.setup(ledRed, GPIO.OUT)   \n\n# turn leds OFF \nGPIO.output(ledRed, GPIO.LOW)\n\n@app.route(\"/\")\ndef index():\n    # Read GPIO Status\n    ledRedSts = GPIO.input(ledRed)\n\n    templateData = {\n            'ledRed'  : ledRedSts,\n        }\n    return render_template('index.html', **templateData)\n\n@app.route(\"/&lt;deviceName&gt;/&lt;action&gt;\")\ndef action(deviceName, action):\n    if deviceName == 'ledRed':\n        actuator = ledRed\n\n    if action == \"on\":\n        GPIO.output(actuator, GPIO.HIGH)\n    if action == \"off\":\n        GPIO.output(actuator, GPIO.LOW)\n\n    ledRedSts = GPIO.input(ledRed)\n\n    templateData = {\n            'ledRed'  : ledRedSts,\n    }\n    return render_template('index.html', **templateData)\n\nif __name__ == \"__main__\":\n   app.run(host='0.0.0.0', port=80, debug=True)\n</code></pre> <p>show! Salve e feche o editor nano. Ctrl+X &gt;&gt; Y</p> <p>Vamos criar a pagina html <code>index.html</code>. No terminal da raspberry pi, digite:</p> <pre><code>cd templates\nnano index.html\n</code></pre> <p>Com o editor nano aberto digite:</p> <pre><code>&lt;!DOCTYPE html&gt;\n   &lt;head&gt;\n      &lt;title&gt;Webserver&lt;/title&gt;\n      &lt;link rel=\"stylesheet\" href='../static/index.css'/&gt;\n   &lt;/head&gt;\n\n   &lt;body&gt;\n\n        &lt;h2&gt; Controle LED &lt;/h2&gt;\n\n        &lt;h3&gt; RED LED ==&gt;  {{ ledRed  }}  ==&gt;  \n            {% if  ledRed   == 1 %}\n                &lt;a href=\"/ledRed/off\"class=\"button\"&gt;TURN OFF&lt;/a&gt;\n            {% else %}\n                &lt;a href=\"/ledRed/on\" class=\"button\"&gt;TURN ON&lt;/a&gt; \n            {% endif %} \n        &lt;/h3&gt;\n\n   &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>show! Salve e feche o editor nano. Ctrl+X &gt;&gt; Y</p> <p>Vamos criar o arquivo de estilo css <code>index.css</code>. No terminal da raspberry pi, digite:</p> <pre><code>cd ..\ncd static\nnano index.html\n</code></pre> <p>Com o editor nano aberto digite:</p> <pre><code>body {\n   background: blue;\n   color: yellow;\n}\n\n.button {\n  font: bold 15px Arial;\n  text-decoration: none;\n  background-color: #EEEEEE;\n  color: #333333;\n  padding: 2px 6px 2px 6px;\n  border-collapse: separete;\n  border-spacing: 0;\n  border-top: 1px solid #CCCCCC;\n  border-right: 1px solid #333333;\n  border-bottom: 1px solid #333333;\n  border-left: 1px solid #CCCCCC;\n}\n</code></pre> <p>show! Salve e feche o editor nano. Ctrl+X &gt;&gt; Y</p>"},{"location":"aulas/iot/lab10/#hora-de-testar","title":"Hora de testar","text":"<p>Vamos testar nosso webserver simples.</p> <p>No terminal da raspberry pi, digite:</p> <pre><code>cd ..\nsudo python app.py\n</code></pre> <p>Deixe o flask rodando na raspberry e no computador ou no smartphone (Deve estar na mesma rede da raspberry), abra o navegador web e digite o ip da raspberry pi. O resultado esperado \u00e9 abrir uma pagina web e controlar o led. </p> <p>Exercise</p> <p>Compreenda o c\u00f3digo app.py e monte o circuito adequado para conseguir visualizar o led acender e apagar.</p> <p>Exercise</p> <p>Altere o c\u00f3digo app.py e adicione mais 2 led e 2 bot\u00f5es (totalizando 3 leds, 2 bot\u00f5es), lembre-se de adaptar os arquivos HTML para exibir no frontend os status. </p> <p>Exercise</p> <p>Aproveite os seus conhecimentos web e proponha melhorias de UI/UX para o exercicio anterior.</p>"},{"location":"aulas/iot/lab2/","title":"Lab02 - Bot\u00e3o","text":""},{"location":"aulas/iot/lab2/#lab2-desafios","title":"Lab2 - Desafios","text":""},{"location":"aulas/iot/lab2/#desafio-1","title":"Desafio 1","text":"<p>Neste desafio vamos explorar como realizar a leitura de um pino digital do arduino, para isso monte o circuito do abaixo e vamos explorar o seu funcionamento:</p> <p></p> <p>Use esse c\u00f3digo de base:</p> <pre><code>// const \u00e9 uma constante. logo o valor n\u00e3o muda\nconst int buttonPin = 2;\nconst int ledPin = 13;\n// cria uma vari\u00e1vel\nint buttonState = 0;\n\nvoid setup() {\n    // configura bot\u00e3o no pino do arduino como entrada:\n    pinMode(ledPin, OUTPUT);\n    // configura bot\u00e3o no pino do arduino como entrada:\n    pinMode(buttonPin, INPUT_PULLUP);\n}\n\nvoid loop() {\n    // L\u00ea o estado do bot\u00e3o:\n    buttonState = digitalRead(buttonPin);\n    // se o bot\u00e3o estiver em n\u00edvel l\u00f3gico alto\n    if (buttonState == LOW) {\n        // liga o led\n        digitalWrite(ledPin, HIGH);\n    } else {\n        // apaga o led\n        digitalWrite(ledPin, LOW);\n        delay(1000);\n    }\n}\n</code></pre> <ol> <li> <p>Rode o c\u00f3digo fornecido de base. Observe, entenda e anote o seu funcionamento;</p> <ul> <li>O que acontece quando pressiona e solta o bot\u00e3o?</li> <li>\u25cb O que acontece quando pressiona e segura o bot\u00e3o?</li> </ul> </li> </ol> <p>Warning</p> <p>Repare que o seu funcionamento \u00e9 um pouco lento e as vezes, parece que ele n\u00e3o funciona corretamente. </p> <p>Showwww agora vamos avan\u00e7ar um pouquinho\u2026.</p> <ol> <li> <p>Altere o c\u00f3digo para funcionar da seguinte forma:</p> <ul> <li> <p>Quando pressionar e soltar o bot\u00e3o:</p> <ul> <li>O led muda o seu estado, de apagado para ligado e vice-versa\u2026</li> </ul> </li> <li> <p>Quando pressionar e segurar o bot\u00e3o:</p> <ul> <li>O led muda o seu estado uma \u00fanica vez. (Se estava ligado, apaga e fica apagado)</li> </ul> </li> </ul> </li> </ol> <p>Tip</p> <p>Para funcionar corretamente precisamos dominar o conceito de <code>debounce de bot\u00f5es</code>, ele \u00e9 um conceito importante ao trabalhar com o Arduino ou qualquer microcontrolador. Ele se refere ao processo de evitar leituras falsas ou inst\u00e1veis quando um bot\u00e3o f\u00edsico \u00e9 pressionado ou liberado. Isso \u00e9 especialmente importante porque os bot\u00f5es mec\u00e2nicos podem gerar ru\u00eddos el\u00e9tricos durante essas a\u00e7\u00f5es, levando a m\u00faltiplas leituras em vez de uma \u00fanica leitura limpa. Existem diversas formas de criar um debounce, vamos seguir boas as boas pr\u00e1ticas e implementar o debounce com a fun\u00e7\u00e3o <code>millis()</code>, como no exemplo abaixo. Note que esse c\u00f3digo \u00e9 parte da solu\u00e7\u00e3o e n\u00e3o a solu\u00e7\u00e3o completa, voc\u00ea precisa entender e ajustar ao desafio proposto.</p> <pre><code>const int buttonPin = 2;   // Pino do bot\u00e3o\nint lastButtonState = HIGH; // \u00daltimo estado do bot\u00e3o\nint buttonState;            // Estado atual do bot\u00e3o\nunsigned long lastDebounceTime = 0;  // \u00daltimo tempo de debounce\nunsigned long debounceDelay = 50;    // Tempo de debounce de 50ms\n\nvoid setup() {\n  pinMode(buttonPin, INPUT_PULLUP);\n}\n\nvoid loop() {\n  int leBotao = digitalRead(buttonPin);\n\n  if (leBotao != lastButtonState) {\n    lastDebounceTime = millis();\n  }\n\n  if ((millis() - lastDebounceTime) &gt; debounceDelay) {\n    if (leBotao != buttonState) {\n      buttonState = leBotao;\n\n      if (buttonState == LOW) {\n        // Bot\u00e3o pressionado\n      }\n    }\n  }\n\n  lastButtonState = leBotao;\n}\n</code></pre>"},{"location":"aulas/iot/lab2/#desafio-2","title":"Desafio 2","text":"<p>Altere o c\u00f3digo do desafio 1 e implemente um log que exibe o status do bot\u00e3o e do led.</p> <p>Tip</p> <p>Para conseguir resolver esse desafio, Voc\u00ea deve inicialiar o periferico de comunica\u00e7\u00e3o serial. Fazemos isso com a instru\u00e7\u00e3o <code>Serial.begin(9600);</code> dentro da fun\u00e7\u00e3o <code>void setup()</code>.</p> <p><pre><code>void setup() {\n  // Inicia a comunica\u00e7\u00e3o serial com uma taxa de 9600 bps\n  Serial.begin(9600);\n}\n\nvoid loop() {\n  // Seu c\u00f3digo aqui\n}\n</code></pre> Al\u00e9m disso, depois de usar <code>Serial.begin()</code>, voc\u00ea pode usar a fun\u00e7\u00e3o <code>Serial.print()</code> ou <code>Serial.println()</code> para enviar dados pela serial e criar seu log. Para visualizar, use o <code>Serial Monitor</code> no arduinoIDE clique em <code>Ferramentas -&gt; Monitor Serial</code></p>"},{"location":"aulas/iot/lab2/#desafio-3","title":"Desafio 3","text":"<p>Neste desafio vamos explorar novos recursos do arduino. Para isso implemente um c\u00f3digo que faz a leitura do pino analogico A0 que altera o tempo de delay do led. Monte o circuito abaixo:</p> <p></p> <p>Tip</p> <p>Vamos conhecer e utilizar as fun\u00e7\u00f5es do arduino <code>analogRead()</code> e <code>analogWrite()</code>.</p> <ul> <li> <p>analogRead: A fun\u00e7\u00e3o analogRead \u00e9 usada para ler valores de sinais anal\u00f3gicos atrav\u00e9s de pinos anal\u00f3gicos no Arduino. Ela converte a tens\u00e3o anal\u00f3gica presente no pino em um valor digital que pode variar de 0 a 1023, correspondendo a uma faixa de 0 a 5 volts (no Arduino Uno e outros modelos semelhantes).</p> <pre><code>int analogValue = analogRead(A0); // L\u00ea o valor anal\u00f3gico do pino A0\n</code></pre> </li> <li> <p>analogWrite: A fun\u00e7\u00e3o analogWrite \u00e9 usada para gerar uma sa\u00edda PWM (modula\u00e7\u00e3o por largura de pulso) em um pino digital. Embora seja chamada de \"analogWrite\", na verdade ela gera um sinal digital pulsante com diferentes larguras de pulso, simulando uma sa\u00edda anal\u00f3gica. Ela \u00e9 frequentemente usada para controlar a intensidade luminosa de LEDs ou a velocidade de motores. </p> </li> <li> <p>Importante destacar que a fun\u00e7\u00e3o <code>analogWrite</code> funciona apenas em pinos espec\u00edficos do Arduino que suportam PWM, geralmente marcados com o s\u00edmbolo <code>\"~\"</code>.</p> <pre><code>int pwmValue = 128;\nanalogWrite(9, pwmValue); // Gera um sinal PWM no pino 9 com ciclo de trabalho de 50%\n</code></pre> </li> <li> <p>Utilize a fun\u00e7\u00e3o <code>map()</code> do arduino para fazer a convers\u00e3o de valores. Pesquise no google essa fun\u00e7\u00e3o.</p> </li> </ul>"},{"location":"aulas/iot/lab3/","title":"Lab03 - Serial","text":""},{"location":"aulas/iot/lab3/#comunicacao-serial-no-wokwi","title":"Comunica\u00e7\u00e3o Serial no Wokwi","text":"<p>Ao final deste laborat\u00f3rio, voc\u00ea ser\u00e1 capaz de:</p> <ul> <li>Compreender os princ\u00edpios da comunica\u00e7\u00e3o serial</li> <li>Implementar comunica\u00e7\u00e3o entre Arduino e computador</li> <li>Enviar e receber dados via porta serial</li> <li>Processar comandos recebidos pela porta serial</li> </ul>"},{"location":"aulas/iot/lab3/#fundamentos-da-comunicacao-serial","title":"Fundamentos da Comunica\u00e7\u00e3o Serial","text":""},{"location":"aulas/iot/lab3/#o-que-e-comunicacao-serial","title":"O que \u00e9 Comunica\u00e7\u00e3o Serial?","text":"<p>A comunica\u00e7\u00e3o serial \u00e9 um protocolo de comunica\u00e7\u00e3o onde os dados s\u00e3o transmitidos um bit de cada vez, sequencialmente, atrav\u00e9s de um canal de comunica\u00e7\u00e3o. \u00c9 amplamente utilizada para comunica\u00e7\u00e3o entre microcontroladores e outros dispositivos devido \u00e0 sua simplicidade e confiabilidade.</p>"},{"location":"aulas/iot/lab3/#parametros-da-comunicacao-serial","title":"Par\u00e2metros da Comunica\u00e7\u00e3o Serial","text":"<ul> <li>Baud Rate: Velocidade de transmiss\u00e3o em bits por segundo (bps)</li> <li>Bits de Dados: N\u00famero de bits em cada pacote (geralmente 8)</li> <li>Bits de Parada: Bits que indicam o fim de um pacote (geralmente 1)</li> <li>Paridade: M\u00e9todo de verifica\u00e7\u00e3o de erros (geralmente nenhum)</li> </ul>"},{"location":"aulas/iot/lab3/#uart-no-arduino","title":"UART no Arduino","text":"<p>O Arduino possui um conversor USB-Serial integrado que permite a comunica\u00e7\u00e3o com o computador atrav\u00e9s da porta USB. </p> <p>Internamente, o microcontrolador utiliza o perif\u00e9rico UART (Universal Asynchronous Receiver/Transmitter) para implementar a comunica\u00e7\u00e3o serial.</p> <p></p>"},{"location":"aulas/iot/lab3/#comunicacao-arduino-computador","title":"Comunica\u00e7\u00e3o Arduino-Computador","text":""},{"location":"aulas/iot/lab3/#configuracao-basica","title":"Configura\u00e7\u00e3o B\u00e1sica","text":"<p>Para iniciar a comunica\u00e7\u00e3o serial no Arduino, utilizamos a fun\u00e7\u00e3o <code>Serial.begin()</code> no <code>setup()</code>, especificando o baud rate:</p> <pre><code>void setup() {\n  Serial.begin(9600); // Inicia comunica\u00e7\u00e3o serial a 9600 bps\n}\n</code></pre>"},{"location":"aulas/iot/lab3/#enviando-dados-para-o-computador","title":"Enviando Dados para o Computador","text":"<p>Para enviar dados do Arduino para o computador, utilizamos as fun\u00e7\u00f5es:</p> <ul> <li><code>Serial.print()</code>: Envia dados sem quebra de linha</li> <li><code>Serial.println()</code>: Envia dados com quebra de linha no final</li> </ul> <pre><code>void loop() {\n  Serial.println(\"Ol\u00e1, Mundo!\"); // Envia a mensagem com quebra de linha\n  delay(1000); // Espera 1 segundo\n}\n</code></pre>"},{"location":"aulas/iot/lab3/#recebendo-dados-do-computador","title":"Recebendo Dados do Computador","text":"<p>Para receber dados do computador no Arduino, utilizamos as fun\u00e7\u00f5es:</p> <ul> <li><code>Serial.available()</code>: Verifica se h\u00e1 dados dispon\u00edveis para leitura</li> <li><code>Serial.read()</code>: L\u00ea um byte da porta serial</li> <li><code>Serial.readString()</code>: L\u00ea uma string completa da porta serial</li> </ul> <pre><code>void loop() {\n  if (Serial.available() &gt; 0) { // Verifica se h\u00e1 dados dispon\u00edveis\n    String comando = Serial.readString(); // L\u00ea a string enviada\n    // Processa o comando recebido\n  }\n}\n</code></pre> <p>Vamos pra pr\u00e1tica!!!</p>"},{"location":"aulas/iot/lab3/#configuracao-do-ambiente-no-wokwi","title":"Configura\u00e7\u00e3o do Ambiente no Wokwi","text":"<ul> <li> <p>Acesse o site Wokwi.</p> </li> <li> <p>Clique em Start New Project e selecione Arduino Uno.</p> </li> </ul>"},{"location":"aulas/iot/lab3/#desafio-1-comunicacao-entre-arduino-e-computador","title":"Desafio 1: Comunica\u00e7\u00e3o entre Arduino e Computador","text":"<p>Vamos estabelecer uma comunica\u00e7\u00e3o b\u00e1sica entre o Arduino e o computador utilizando o Monitor Serial do Wokwi.</p> <ul> <li>Carregue o seguinte c\u00f3digo no simulador:</li> </ul> <pre><code>void setup() {\n  Serial.begin(9600); // Inicia a comunica\u00e7\u00e3o serial a 9600 bps\n}\n\nvoid loop() {\n  Serial.println(\"Ol\u00e1, Mundo!\"); // Envia a mensagem \"Ol\u00e1, Mundo!\" para o computador\n  delay(1000); // Espera 1 segundo\n}\n</code></pre> <ul> <li> <p>Observe o painel \"Serial Monitor\" que aparecer\u00e1 automaticamente</p> </li> <li> <p>Voc\u00ea deve ver a mensagem \"Ol\u00e1, Mundo!\" sendo exibida a cada segundo como a imagem a seguir.</p> </li> </ul> <p></p> <ul> <li>Modifique o c\u00f3digo para enviar seu nome e n\u00famero de matr\u00edcula</li> </ul>"},{"location":"aulas/iot/lab3/#desafio-2-cronometro-virtual","title":"Desafio 2: Cron\u00f4metro virtual","text":"<p>Com base no que foi aboradado no lab passado, voc\u00ea dever\u00e1 criar um cron\u00f4metro virtual que exibe o tempo decorrido desde o in\u00edcio da simula\u00e7\u00e3o usando a fun\u00e7\u00e3o <code>millis()</code>.</p> <p>Instru\u00e7\u00f5es:</p> <ul> <li>Utilize a fun\u00e7\u00e3o <code>millis()</code> para exibir no Serial Monitor quanto tempo se passou desde que o Arduino come\u00e7ou a funcionar, em segundos.</li> <li> <p>Fa\u00e7a com que a mensagem seja atualizada inicialmente a cada 1 segundo.</p> </li> <li> <p>Agora com a base do c\u00f3digo funcionando ok, modifique seu c\u00f3digo para que o intervalo entre as mensagens seja alterado <code>dinamicamente</code>, da seguinte forma:</p> </li> <li> <p>1 segundo durante os primeiros 10 segundos.</p> </li> <li>2 segundos entre 10 e 20 segundos.</li> <li>5 segundos ap\u00f3s 20 segundos.</li> </ul> <p>\u00c9 esperado como resultado que seja exibido no serial monitor:</p> <pre><code>Tempo decorrido: 1 segundos\nTempo decorrido: 2 segundos\n//assim at\u00e9 10...\nTempo decorrido: 10 segundos\nTempo decorrido: 12 segundos\n//assim at\u00e9 20...\nTempo decorrido: 20 segundos\nTempo decorrido: 25 segundos\nTempo decorrido: 30 segundos\n//assim acima de 20...\n</code></pre>"},{"location":"aulas/iot/lab3/#desafio-3-recebendo-dados-do-computador","title":"Desafio 3: Recebendo Dados do Computador","text":"<p>Vamos criar uma interface para controlar um LED atrav\u00e9s de comandos enviados pelo Serial Monitor.</p> <p>Carregue o seguinte c\u00f3digo no seu Arduino:</p> <pre><code>String comando = \"\"; // Vari\u00e1vel para armazenar o comando recebido\nconst int ledPin = 10;\n\nvoid setup() {\n  Serial.begin(9600);\n  pinMode(ledPin, OUTPUT); // Define o pino como sa\u00edda\n  Serial.println(\"Digite LIGAR ou DESLIGAR para controlar o LED\");\n}\n\nvoid loop() {\n  if (Serial.available()) { // Verifica se h\u00e1 dados dispon\u00edveis para leitura\n    comando = Serial.readStringUntil('\\n'); // L\u00ea a string at\u00e9 encontrar uma quebra de linha\n    comando.trim(); // Remove espa\u00e7os e quebras de linha extras\n\n    if (comando.equalsIgnoreCase(\"LIGAR\")) {\n      digitalWrite(ledPin, HIGH); // Acende o LED no pino\n      Serial.println(\"LED Ligado!\");\n    } else if (comando.equalsIgnoreCase(\"DESLIGAR\")) {\n      digitalWrite(ledPin, LOW); // Apaga o LED no pino\n      Serial.println(\"LED Desligado!\");\n    } else {\n      Serial.println(\"Comando n\u00e3o reconhecido. Use LIGAR ou DESLIGAR\");\n    }\n  }\n}\n</code></pre>"},{"location":"aulas/iot/lab3/#montagem-no-wokwi","title":"Montagem no Wokwi:","text":"<ul> <li> <p>Adicione um LED ao pino que est\u00e1 configurado no c\u00f3digo (da mesma forma que fizemos no labs anteriores).</p> </li> <li> <p>Inicie a simula\u00e7\u00e3o, digite <code>LIGAR</code> e pressione Enter.</p> </li> <li> <p>O LED do Arduino deve acender.</p> </li> <li> <p>Digite <code>DESLIGAR</code> e pressione Enter para apagar o LED.</p> </li> </ul> <p></p> <p>Warning</p> <p>A comunica\u00e7\u00e3o serial \u00e9 sens\u00edvel a mai\u00fasculas e min\u00fasculas. Certifique-se de digitar os comandos exatamente como est\u00e3o no c\u00f3digo.</p> <p>voc\u00ea pode usar o m\u00e9todo <code>equalsIgnoreCase()</code> para torna o c\u00f3digo mais robusto, aceitando varia\u00e7\u00f5es como \"ligar\", \"Ligar\" ou \"LIgaR\".</p>"},{"location":"aulas/iot/lab3/#desafio-4-cronometro-dinamico","title":"Desafio 4: Cron\u00f4metro Din\u00e2mico","text":"<p>Com base nos desafio 2 e desafio 3, crie um <code>cron\u00f4metro inteligente</code>. </p> <p>O intervalo das mensagens n\u00e3o ser\u00e1 fixo: ele dever\u00e1 mudar conforme comandos enviados pelo usu\u00e1rio atrav\u00e9s do Monitor Serial.</p>"},{"location":"aulas/iot/lab3/#requisitos-do-desafio","title":"Requisitos do Desafio:","text":"<ul> <li> <p>Crie um cron\u00f4metro que exiba, a cada intervalo, o tempo decorrido desde o in\u00edcio da execu\u00e7\u00e3o (em segundos).</p> </li> <li> <p>Inicialmente, o intervalo entre mensagens deve ser de 1 segundo.</p> </li> <li> <p>Atrav\u00e9s do Monitor Serial, permita que o usu\u00e1rio altere dinamicamente o intervalo entre as mensagens digitando comandos como:</p> </li> <li> <p>intervalo 500 (define o intervalo para 500 ms)</p> </li> <li> <p>intervalo 2000 (define o intervalo para 2000 ms)</p> </li> <li> <p>Implemente tamb\u00e9m comandos especiais:</p> </li> <li> <p><code>pausar</code> para interromper temporariamente a exibi\u00e7\u00e3o das mensagens.</p> </li> <li><code>continuar</code> para retomar a contagem.</li> </ul> <p>\u00c9 esperado como resultado que seja exibido no serial monitor:</p> <pre><code>Tempo decorrido: 1 segundos\nTempo decorrido: 2 segundos\nTempo decorrido: 3 segundos\n// Usu\u00e1rio digita: intervalo 3000\nIntervalo alterado para 3000 ms.\nTempo decorrido: 6 segundos\nTempo decorrido: 9 segundos\n// Usu\u00e1rio digita: pausar\nCron\u00f4metro pausado.\n// Usu\u00e1rio digita: continuar\nCron\u00f4metro retomado.\nTempo decorrido: 12 segundos\nTempo decorrido: 15 segundos\n// Usu\u00e1rio digita: intervalo 2000\nIntervalo alterado para 2000 ms.\nTempo decorrido: 17 segundos\nTempo decorrido: 19 segundos\n</code></pre>"},{"location":"aulas/iot/lab4/","title":"Lab04 - M\u00e1quinas de Estados (FSM)","text":""},{"location":"aulas/iot/lab4/#maquinas-de-estados-finitos","title":"M\u00e1quinas de Estados Finitos","text":""},{"location":"aulas/iot/lab4/#entendendo-maquinas-de-estados-finitos","title":"Entendendo M\u00e1quinas de Estados Finitos","text":"<p>Imagine um personagem de videogame que pode estar:</p> <ul> <li>Parado</li> <li>Andando</li> <li>Correndo</li> <li>Pulando</li> </ul> <p>O personagem s\u00f3 pode estar em um desses estados por vez, e muda de estado baseado em comandos do jogador. Isso \u00e9 uma m\u00e1quina de estados!</p>"},{"location":"aulas/iot/lab4/#o-que-e-uma-maquina-de-estados-finitos","title":"O que \u00e9 uma M\u00e1quina de Estados Finitos?","text":"<p>Uma M\u00e1quina de Estados Finitos (FSM - Finite State Machine) \u00e9 um modelo matem\u00e1tico de computa\u00e7\u00e3o que pode estar em exatamente um de um n\u00famero finito de estados em qualquer momento. \u00c9 uma ferramenta poderosa para modelar comportamentos de sistemas que possuem estados distintos e transi\u00e7\u00f5es entre eles.</p> <p>Uma FSM \u00e9 definida formalmente pelos seguintes componentes:</p> <ul> <li>Estados (S): Um conjunto finito e n\u00e3o-vazio de condi\u00e7\u00f5es ou situa\u00e7\u00f5es nas quais o sistema pode existir</li> <li>Alfabeto de entrada (\u03a3): Um conjunto finito de eventos ou entradas que podem desencadear transi\u00e7\u00f5es</li> <li>Estado inicial (s\u2080): O estado em que a m\u00e1quina come\u00e7a sua opera\u00e7\u00e3o</li> <li>Fun\u00e7\u00e3o de transi\u00e7\u00e3o (\u03b4): Define como a m\u00e1quina muda de um estado para outro baseado no estado atual e no evento que ocorre</li> <li>Estados finais (F): Um conjunto (possivelmente vazio) de estados que indicam a conclus\u00e3o da opera\u00e7\u00e3o da m\u00e1quina</li> </ul>"},{"location":"aulas/iot/lab4/#representacao-visual-de-uma-fsm","title":"Representa\u00e7\u00e3o Visual de uma FSM","text":"<p>Uma FSM pode ser representada visualmente atrav\u00e9s de um diagrama de estados, onde:</p> <ul> <li>Os c\u00edrculos (Dormir,Acordar) representam estados</li> <li>As setas (---&gt;)representam transi\u00e7\u00f5es entre estados</li> <li>Os r\u00f3tulos nas setas (Alarme, Dormir) indicam os eventos que desencadeiam as transi\u00e7\u00f5es</li> </ul> <pre><code>+--------+   Alarme   +--------+\n| Dormir |-----------&gt;| Acordar|\n+--------+            +--------+\n    ^                     |\n    |      Dormir         |\n    +---------------------+\n</code></pre>"},{"location":"aulas/iot/lab4/#por-que-usar-maquinas-de-estados-finitos","title":"Por que usar M\u00e1quinas de Estados Finitos?","text":"<p>As FSMs s\u00e3o particularmente \u00fateis em programa\u00e7\u00e3o de sistemas embarcados pelos seguintes motivos:</p> <ol> <li>Organiza\u00e7\u00e3o do c\u00f3digo: Permite dividir o comportamento do sistema em estados distintos e gerenci\u00e1veis</li> <li>Clareza: Torna o fluxo de controle mais claro e f\u00e1cil de entender</li> <li>Manuten\u00e7\u00e3o: Facilita a adi\u00e7\u00e3o de novos comportamentos sem modificar o c\u00f3digo existente</li> <li>Previsibilidade: Garante que o sistema esteja sempre em um estado conhecido</li> </ol>"},{"location":"aulas/iot/lab4/#aplicando-em-projetos-de-sistemas-embarcados","title":"Aplicando em projetos de sistemas embarcados","text":"<p>No Arduino, uma FSM nos ajuda a organizar o c\u00f3digo em partes mais gerenci\u00e1veis:</p> <ul> <li>Em vez de c\u00f3digo confuso com muitos ifs aninhados</li> </ul> <p><pre><code>// Abordagem com ifs aninhados\nvoid loop() {\n  if (botaoPressionado) {\n    if (ledAceso) {\n      // Fazer algo\n    } else {\n      // Fazer outra coisa\n    }\n  } else {\n    // Fazer algo completamente diferente\n  }\n}\n</code></pre> - Usamos uma abordagem clara baseada em estados</p> <pre><code>// Usando maquina de estados\n\nenum Estado {DESLIGADO, LIGADO, PISCANDO};\n\nEstado estadoAtual = DESLIGADO;\n\nvoid loop() {\n  // Verifica entradas e muda estado se necess\u00e1rio\n  verificarTransicoes();\n\n  // Executar a\u00e7\u00f5es do estado atual\n  switch (estadoAtual) {\n    case DESLIGADO:\n      // A\u00e7\u00f5es quando desligado\n      break;\n    case LIGADO:\n      // A\u00e7\u00f5es quando ligado\n      break;\n    case PISCANDO:\n      // A\u00e7\u00f5es quando piscando\n      break;\n  }\n}\n</code></pre>"},{"location":"aulas/iot/lab4/#tipos-de-maquinas-de-estados-finitos","title":"Tipos de M\u00e1quinas de Estados Finitos","text":"<p>Existem dois tipos principais de FSMs:</p> <ol> <li>Determin\u00edsticas: Para cada estado e entrada, h\u00e1 exatamente uma transi\u00e7\u00e3o para um pr\u00f3ximo estado</li> <li>N\u00e3o-determin\u00edsticas: Para cada estado e entrada, pode haver m\u00faltiplas transi\u00e7\u00f5es poss\u00edveis</li> </ol> <p>Para nossos prop\u00f3sitos com Arduino, utilizaremos principalmente FSMs determin\u00edsticas.</p>"},{"location":"aulas/iot/lab4/#implementacao-de-fsms-em-arduino","title":"Implementa\u00e7\u00e3o de FSMs em Arduino","text":"<p>Existem v\u00e1rias maneiras de implementar uma FSM em Arduino:</p>"},{"location":"aulas/iot/lab4/#1-usando-uma-variavel-de-estado-e-estruturas-switch-case","title":"1. Usando uma vari\u00e1vel de estado e estruturas switch-case","text":"<pre><code>enum State {IDLE, RUNNING, JUMPING};\nState currentState = IDLE;\n\nvoid loop() {\n  switch (currentState) {\n    case IDLE:\n      // C\u00f3digo para o estado IDLE\n      if (conditionToRun) {\n        currentState = RUNNING;\n      }\n      break;\n\n    case RUNNING:\n      // C\u00f3digo para o estado RUNNING\n      if (conditionToJump) {\n        currentState = JUMPING;\n      }\n      break;\n\n    case JUMPING:\n      // C\u00f3digo para o estado JUMPING\n      if (conditionToIdle) {\n        currentState = IDLE;\n      }\n      break;\n  }\n}\n</code></pre>"},{"location":"aulas/iot/lab4/#2-usando-funcoes-para-cada-estado","title":"2. Usando fun\u00e7\u00f5es para cada estado","text":"<pre><code>enum State {IDLE, RUNNING, JUMPING};\nState currentState = IDLE;\n\nvoid loop() {\n  // Executa a fun\u00e7\u00e3o correspondente ao estado atual\n  switch (currentState) {\n    case IDLE:    handleIdleState();    break;\n    case RUNNING: handleRunningState(); break;\n    case JUMPING: handleJumpingState(); break;\n  }\n}\n\nvoid handleIdleState() {\n  // C\u00f3digo para o estado IDLE\n  if (conditionToRun) {\n    currentState = RUNNING;\n  }\n}\n\nvoid handleRunningState() {\n  // C\u00f3digo para o estado RUNNING\n  if (conditionToJump) {\n    currentState = JUMPING;\n  }\n}\n\nvoid handleJumpingState() {\n  // C\u00f3digo para o estado JUMPING\n  if (conditionToIdle) {\n    currentState = IDLE;\n  }\n}\n</code></pre>"},{"location":"aulas/iot/lab4/#exemplo-pratico-semaforo","title":"Exemplo Pr\u00e1tico: Sem\u00e1foro","text":"<p>Um sem\u00e1foro \u00e9 um <code>exemplo cl\u00e1ssico</code>!! Lembra da nossa primeira aula? Implementamos um sem\u00e1foro que faz a transi\u00e7\u00e3o entre VERMELHO, VERDE e AMARELO. </p> <p>Agora vamos implementar esse cl\u00e1ssico usando FSM, alem disso vamos adicionar um bot\u00e3o para modo de emerg\u00eancia.</p> <p>Neste novo exemplo temos as seguinte condi\u00e7\u00f5es de opera\u00e7\u00e3o:</p> <ul> <li>Os <code>estados</code> s\u00e3o <code>VERMELHO</code>, <code>VERDE</code>, <code>AMARELO</code> e <code>EMERG\u00caNCIA</code></li> <li>As <code>transi\u00e7\u00f5es</code> ocorrem ap\u00f3s um determinado <code>tempo</code></li> <li>O sistema sempre segue a sequ\u00eancia VERMELHO \u2192 VERDE \u2192 AMARELO \u2192 VERMELHO</li> <li>De qualquer um desses estados, <code>pressionar o bot\u00e3o</code> leva ao estado de <code>EMERG\u00caNCIA</code></li> <li>Do estado de EMERG\u00caNCIA, <code>pressionar o bot\u00e3o novamente</code> retorna ao estado VERMELHO (reiniciando o ciclo normal)</li> </ul> <p>Podemos visualizar o diagrama de estados da seguite forma:</p> <p></p> <p>A implementa\u00e7\u00e3o do c\u00f3digo fica da seguinte forma:</p> <pre><code>// Enumera\u00e7\u00e3o para representar os estados do sem\u00e1foro\nenum TrafficLightState {\n  RED,      // Estado vermelho\n  GREEN,    // Estado verde\n  YELLOW,   // Estado amarelo\n  EMERGENCY // Estado de emerg\u00eancia (piscando amarelo)\n};\n\n// Configura\u00e7\u00e3o dos pinos dos LEDs e do bot\u00e3o\nconst int redPin = 11;      // Pino do LED vermelho\nconst int yellowPin = 10;   // Pino do LED amarelo\nconst int greenPin = 9;     // Pino do LED verde\nconst int buttonPin = 2;    // Pino do bot\u00e3o de emerg\u00eancia\n\n// Configura\u00e7\u00e3o de tempos para cada estado\nconst unsigned long redDuration = 5000;     // Dura\u00e7\u00e3o do estado vermelho\nconst unsigned long greenDuration = 4000;   // Dura\u00e7\u00e3o do estado verde\nconst unsigned long yellowDuration = 2000;  // Dura\u00e7\u00e3o do estado amarelo\nconst unsigned long blinkInterval = 500;    // Intervalo de piscar no modo emerg\u00eancia\nconst unsigned long debounceDelay = 50;     // Tempo de debounce para o bot\u00e3o\n\n// Vari\u00e1veis de controle\nTrafficLightState currentState = RED;       // Estado atual do sem\u00e1foro\nunsigned long stateStartTime;               // Tempo de in\u00edcio do estado atual\nunsigned long lastBlinkTime = 0;            // \u00daltimo tempo de piscar no modo emerg\u00eancia\nbool ledState = false;                      // Estado atual do LED no modo emerg\u00eancia\nbool lastButtonState = LOW;                 // \u00daltimo estado est\u00e1vel do bot\u00e3o\nbool emergencyMode = false;                 // Indica se o modo de emerg\u00eancia est\u00e1 ativo\nunsigned long lastDebounceTime = 0;         // \u00daltimo tempo de debounce\nbool buttonPressed = false;                 // Indica se o bot\u00e3o foi pressionado\n\n// Fun\u00e7\u00e3o para inicializar os pinos e a comunica\u00e7\u00e3o serial\nvoid setup() {\n  pinMode(redPin, OUTPUT);\n  pinMode(yellowPin, OUTPUT);\n  pinMode(greenPin, OUTPUT);\n  pinMode(buttonPin, INPUT_PULLUP);\n\n  Serial.begin(9600);\n  Serial.println(\"Sem\u00e1foro - M\u00e1quina de Estados\");\n\n  stateStartTime = millis();  // Inicia o tempo do estado atual\n}\n\n// Fun\u00e7\u00e3o para controlar os LEDs de acordo com o estado atual\nvoid setLights(bool red, bool yellow, bool green) {\n  digitalWrite(redPin, red);\n  digitalWrite(yellowPin, yellow);\n  digitalWrite(greenPin, green);\n}\n\n// Fun\u00e7\u00e3o principal que executa a m\u00e1quina de estados\nvoid loop() {\n  unsigned long currentTime = millis();\n  bool buttonState = !digitalRead(buttonPin);  // L\u00ea o estado do bot\u00e3o (invertido devido ao PULLUP)\n\n  // Implementa\u00e7\u00e3o do debounce\n  if (buttonState != lastButtonState) {\n    lastDebounceTime = currentTime;  // Reinicia o tempo de debounce\n  }\n\n  // Verifica se o bot\u00e3o est\u00e1 est\u00e1vel h\u00e1 mais tempo que o debounceDelay\n  if ((currentTime - lastDebounceTime) &gt; debounceDelay) {\n    // Verifica se o estado do bot\u00e3o mudou\n    if (buttonState != buttonPressed) {\n      buttonPressed = buttonState;  // Atualiza o estado do bot\u00e3o\n\n      // Se o bot\u00e3o foi pressionado (mudou para HIGH)\n      if (buttonPressed) {\n        emergencyMode = !emergencyMode;  // Alterna o modo de emerg\u00eancia\n        if (emergencyMode) {\n          currentState = EMERGENCY;\n          Serial.println(\"Modo de emerg\u00eancia ATIVADO\");\n        } else {\n          currentState = RED;\n          Serial.println(\"Modo de emerg\u00eancia DESATIVADO\");\n        }\n        stateStartTime = currentTime;  // Reinicia o tempo do estado atual\n      }\n    }\n  }\n\n  lastButtonState = buttonState;  // Atualiza o \u00faltimo estado do bot\u00e3o\n\n  // M\u00e1quina de estados do sem\u00e1foro\n  switch (currentState) {\n    case RED:\n      setLights(HIGH, LOW, LOW);  // Acende o vermelho e apaga os outros\n\n      // Transi\u00e7\u00e3o para o estado verde ap\u00f3s o tempo de dura\u00e7\u00e3o\n      if (currentTime - stateStartTime &gt;= redDuration &amp;&amp; !emergencyMode) {\n        currentState = GREEN;\n        stateStartTime = currentTime;\n        Serial.println(\"Transi\u00e7\u00e3o: VERMELHO -&gt; VERDE\");\n      }\n      break;\n\n    case GREEN:\n      setLights(LOW, LOW, HIGH);  // Acende o verde e apaga os outros\n\n      // Transi\u00e7\u00e3o para o estado amarelo ap\u00f3s o tempo de dura\u00e7\u00e3o\n      if (currentTime - stateStartTime &gt;= greenDuration &amp;&amp; !emergencyMode) {\n        currentState = YELLOW;\n        stateStartTime = currentTime;\n        Serial.println(\"Transi\u00e7\u00e3o: VERDE -&gt; AMARELO\");\n      }\n      break;\n\n    case YELLOW:\n      setLights(LOW, HIGH, LOW);  // Acende o amarelo e apaga os outros\n\n      // Transi\u00e7\u00e3o para o estado vermelho ap\u00f3s o tempo de dura\u00e7\u00e3o\n      if (currentTime - stateStartTime &gt;= yellowDuration &amp;&amp; !emergencyMode) {\n        currentState = RED;\n        stateStartTime = currentTime;\n        Serial.println(\"Transi\u00e7\u00e3o: AMARELO -&gt; VERMELHO\");\n      }\n      break;\n\n    case EMERGENCY:\n      // Pisca o LED amarelo no modo de emerg\u00eancia\n      if (currentTime - lastBlinkTime &gt;= blinkInterval) {\n        ledState = !ledState;\n        setLights(LOW, ledState, LOW);  // Alterna o estado do LED amarelo\n        lastBlinkTime = currentTime;\n      }\n      break;\n  }\n}\n</code></pre>"},{"location":"aulas/iot/lab4/#desafio1","title":"DESAFIO1","text":"<p>Teste o c\u00f3digo acima do sem\u00e1foro, para isso:</p> <ul> <li>Analise o c\u00f3digo fornecido acima.</li> <li>Identifique claramente todos os pinos utilizados.</li> <li>Monte o circuito corretamente no simulador Wokwi.</li> <li>Carregue o c\u00f3digo e observe as mudan\u00e7as dos estados.</li> <li>Teste o modo emerg\u00eancia ativando e desativando pelo bot\u00e3o.</li> </ul>"},{"location":"aulas/iot/lab4/#desafio2","title":"DESAFIO2","text":"<p>Agora, chegou a hora de voc\u00ea dar show! E colocar a m\u00e1quina pra funcionar. </p> <p>Ap\u00f3s testar e entender o c\u00f3digo base acima:</p> <ol> <li> <p>Adicione mais um bot\u00e3o:</p> <ul> <li>Um bot\u00e3o ser\u00e1 dedicado exclusivamente \u00e0 ativa\u00e7\u00e3o do modo emerg\u00eancia.(j\u00e1 implementado)</li> <li>Outro bot\u00e3o servir\u00e1 para acelerar as transi\u00e7\u00f5es entre estados no modo normal.</li> </ul> </li> <li> <p>Crie um novo estado adicional:</p> <ul> <li>Adicione o estado \"PEDESTRE\", onde todos os LEDs ficam apagados por um breve momento (3 seg.) ao pressionar um <code>terceiro bot\u00e3o</code>, simulando travessia.</li> </ul> </li> <li> <p>Adicione um sensor anal\u00f3gico (o potenci\u00f4metro que vimos no lab2):</p> <ul> <li>O potenci\u00f4metro deve determinar o tempo que o sem\u00e1foro fica no estado VERDE, variando entre 2 a 10 segundos.</li> </ul> </li> </ol>"},{"location":"aulas/iot/lab5/","title":"Lab05 - EEPROM","text":""},{"location":"aulas/iot/lab5/#memoria-eeprom","title":"Memoria EEPROM","text":"<p>A mem\u00f3ria EEPROM (Electrically Erasable Programmable Read-Only Memory) \u00e9 uma mem\u00f3ria n\u00e3o vol\u00e1til, o que significa que os dados armazenados nela persistem mesmo depois de desligar o Arduino. \u00c9 \u00fatil para armazenar pequenas quantidades de dados que precisam ser preservados entre reinicializa\u00e7\u00f5es, como configura\u00e7\u00f5es ou contadores.</p> <p>Warning</p> <p>Essa memoria n\u00e3o \u00e9 infinita, pelo contrario! a memoria EEPROM \u00e9 bem pequena, no caso do Arduino UNO \u00e9 de apenas 1KB (1024 Bytes) tenha isso em mente para n\u00e3o ultrapassar esse valor.</p>"},{"location":"aulas/iot/lab5/#principais-funcoes-da-eeprom-no-arduino","title":"Principais Fun\u00e7\u00f5es da EEPROM no Arduino","text":"<ul> <li><code>EEPROM.write(endere\u00e7o, valor)</code>: Grava um byte em um endere\u00e7o espec\u00edfico.</li> <li><code>EEPROM.read(endere\u00e7o)</code>: L\u00ea um byte do endere\u00e7o especificado.</li> <li><code>EEPROM.update(endere\u00e7o, valor)</code>: Grava um valor apenas se for diferente do valor j\u00e1 armazenado (economiza ciclos de grava\u00e7\u00e3o).</li> </ul>"},{"location":"aulas/iot/lab5/#desafio-1-escrevendo-e-lendo-dados-na-eeprom","title":"Desafio 1: Escrevendo e Lendo Dados na EEPROM","text":"<p>Vamos aprender a escrever e ler dados na mem\u00f3ria EEPROM. Vamos escrever apenas 1 unico valor inteiro.</p> <p>Monte um circuito com um bot\u00e3o no pino 2 e carregue o seguinte c\u00f3digo no seu Arduino:</p> <pre><code>#include &lt;EEPROM.h&gt;\n\nconst int buttonPin = 2; // Pino do bot\u00e3o\nint lastButtonState = HIGH;\nint buttonState; \n\nunsigned long lastDebounceTime = 0;\nunsigned long debounceDelay = 50;    \n\nint endereco = 0;\n\nvoid setup() {\n  Serial.begin(9600);\n  pinMode(buttonPin, INPUT_PULLUP); \n\n  EEPROM.write(endereco, 123); // Escreve o valor 123 na posi\u00e7\u00e3o 0 da EEPROM\n  delay(10); // Pequeno delay para garantir a escrita na memoria\n}\n\nvoid loop() {\n  int reading = digitalRead(buttonPin); // L\u00ea o estado do bot\u00e3o\n  if (reading != lastButtonState) {\n    lastDebounceTime = millis();\n  }\n  if ((millis() - lastDebounceTime) &gt; debounceDelay) {\n    if (reading != buttonState) {\n      buttonState = reading;\n\n      // Se o bot\u00e3o estiver pressionado (estado LOW devido ao pull-up)\n      if (buttonState == LOW) {\n\n        int valor = EEPROM.read(0); // L\u00ea o valor na posi\u00e7\u00e3o 0 da EEPROM\n        Serial.println(valor); // Imprime o valor\n      }\n    }\n  }\n\n  lastButtonState = reading; // Atualiza o estado anterior do bot\u00e3o\n}\n</code></pre>"},{"location":"aulas/iot/lab5/#desafio-2-fsm-com-eeprom","title":"Desafio 2: FSM com EEPROM","text":"<p>Desenvolva uma M\u00e1quina de Estados Finitos (FSM) com as seguintes caracter\u00edsticas:</p> <p>Implemente tr\u00eas estados (MODO1, MODO2, MODO3).</p> <p>Use dois bot\u00f5es:</p> <p>Bot\u00e3o 1: avan\u00e7ar para o pr\u00f3ximo estado.</p> <p>Bot\u00e3o 2: voltar ao estado anterior.</p> <p>Salve o estado atual na EEPROM sempre que houver mudan\u00e7a de estado.</p> <p>Quando o Arduino reiniciar, recupere o \u00faltimo estado salvo e retorne diretamente para ele.</p>"},{"location":"aulas/iot/lab5/#desafio-2-armazenando-e-recuperando-strings","title":"Desafio 2: Armazenando e Recuperando Strings","text":"<p>Agora altere o c\u00f3digo para escrever e ler <code>Strings</code>. </p> <p>Altre o c\u00f3digo do desafio 1 para:</p> <pre><code>- Salve na memoria EEPROM a frase: `Let's Rock the Future` \n- Recuperar o valor salvo na memoria quando apertar o bot\u00e3o.\n</code></pre> <p>Tip</p> <ul> <li>Defina a frase como do tipo String. <code>String frase = \"sua frase aqui\"</code></li> <li>Conhe\u00e7a um pouco mais do objeto String lendo a documenta\u00e7\u00e3o aqui</li> <li>Para salvar na memoria EEPROM temos que rodar um <code>la\u00e7o for</code> para salvar caractere por caractere. <code>for (int i = 0; i &lt; frase.length(); i++){}</code></li> <li>Dentro do la\u00e7o for, defina a posi\u00e7\u00e3o inicial da memoria e salve cada indice do frase <code>EEPROM.write(startPos + i, frase[i]);</code></li> <li>Fa\u00e7a a mesma coisa para recuperar os dados.     </li> </ul>"},{"location":"aulas/iot/lab5/#desafio-3-exiba-os-resultados-em-um-display-lcd","title":"Desafio 3: Exiba os resultados em um display LCD","text":"<p>Agora vamos exibir o valor da memoria EEPROM em um display LCD. Para isso, busque por refer\u00eancias na internet de como realizar a liga\u00e7\u00e3o e elaborar o circuito.  </p> <p>Tip</p> <p>execute os codigo exemplo que encontrar na internet para verificar o funcionamento do circuito antes de escrever seu proprio c\u00f3digo</p>"},{"location":"aulas/iot/lab6/","title":"Lab06 - Introdu\u00e7\u00e3o ao Node-RED","text":"<p>Neste Laborat\u00f3rio vamos trabalhar com Node-red e conhecer o protocolo MQTT.</p> <ul> <li>arquivo pdf do projeto: Slide</li> </ul>"},{"location":"aulas/iot/lab6/#introducao-a-iot","title":"Introdu\u00e7\u00e3o \u00e0 IoT","text":""},{"location":"aulas/iot/lab6/#agenda","title":"Agenda","text":"<ul> <li>Instala\u00e7\u00e3o do Node-RED e primeiros testes</li> <li>Montagem de um dashboard no Node-RED</li> <li>Cria\u00e7\u00e3o de um end-point</li> <li>Apre MQTT</li> </ul>"},{"location":"aulas/iot/lab6/#conectando-dispositivos-a-aplicacoes","title":"Conectando dispositivos a aplica\u00e7\u00f5es","text":"<p>Agora que j\u00e1 exploramos as funcionalidades do Arduino e sua capacidade de conectar sensores e atuadores, vamos prosseguir conectando o Arduino a aplica\u00e7\u00f5es que fazem uso desse dispositivo.</p> <p>Em primeiro lugar, vamos relembrar a arquitetura que usaremos para os dispositivos de IoT se conectarem \u00e0s suas aplica\u00e7\u00f5es.</p>"},{"location":"aulas/iot/lab6/#arquitetura-basica-de-iot","title":"Arquitetura b\u00e1sica de IoT","text":"<p>A arquitetura de implanta\u00e7\u00e3o apresentada aqui \u00e9 um modelo padr\u00e3o para inspirar projetos reais. Ela inclui os elementos fundamentais para a conectividade, sem detalhar solu\u00e7\u00f5es para problemas acess\u00f3rios.</p> <p></p> <ul> <li>Interoperabilidade: facilita a compatibilidade entre diferentes projetos de IoT.</li> <li>Modularidade: define m\u00f3dulos que podem ser criados separadamente ou usados como \"off-the-shelf\".</li> </ul>"},{"location":"aulas/iot/lab6/#dispositivos-de-iot","title":"Dispositivos de IoT","text":"<p>Os dispositivos de IoT interagem com o ambiente ao seu redor, capturando dados de sensores ou executando comandos por meio de atuadores.</p> <ul> <li>Cada funcionalidade no dispositivo pode ser considerada uma aplica\u00e7\u00e3o (Endpoint Application).</li> <li>Cada aplica\u00e7\u00e3o deve ser univocamente endere\u00e7\u00e1vel.</li> </ul>"},{"location":"aulas/iot/lab6/#conector-de-iot","title":"Conector de IoT","text":"<p>Os conectores de IoT gerenciam mensagens que chegam dos dispositivos ou s\u00e3o destinadas a eles, adaptando-as ao protocolo de cada dispositivo.</p> <ul> <li>Pode haver conectores diferentes para protocolos variados.</li> <li>Protocolos comuns em IoT: MQTT, WebSocket, CoAP, LoRaWAN.</li> </ul>"},{"location":"aulas/iot/lab6/#gerenciamento-de-dispositivos-e-dados","title":"Gerenciamento de dispositivos e dados","text":"<p>Este componente faz o gerenciamento remoto dos dispositivos e de seus dados, autorizando o acesso de outras aplica\u00e7\u00f5es.</p> <ul> <li>Cadastra novos dispositivos e aplica\u00e7\u00f5es.</li> <li>Monitora a disponibilidade dos dispositivos.</li> <li>Envia comandos de gerenciamento, como inicializa\u00e7\u00e3o, reinicializa\u00e7\u00e3o, desligamento e atualiza\u00e7\u00e3o de firmware.</li> </ul>"},{"location":"aulas/iot/lab6/#bancos-de-dados-e-analise-de-dados","title":"Bancos de dados e an\u00e1lise de dados","text":"<p>Armazena dados provenientes das aplica\u00e7\u00f5es e comandos destinados aos dispositivos.</p> <ul> <li>Bancos de dados NoSQL s\u00e3o mais indicados para a IoT devido \u00e0 natureza diversificada e em constante mudan\u00e7a dos dados.</li> <li>Analisadores de dados monitoram os dados para melhor aproveitamento.</li> </ul>"},{"location":"aulas/iot/lab6/#gateway","title":"Gateway","text":"<p>O gateway conecta dispositivos sem acesso direto \u00e0 internet e realiza a convers\u00e3o de protocolos entre os dispositivos de IoT e o conector de IoT.</p> <p></p> <ul> <li>Gerencia m\u00faltiplos protocolos, especialmente em LAN\u2019s, PAN\u2019s e HAN\u2019s (ex: Zigbee, Bluetooth, LoRa, Thread/6LoWPAN).</li> </ul>"},{"location":"aulas/iot/lab6/#node-red","title":"Node-RED","text":"<p>O Node-RED \u00e9 uma plataforma de programa\u00e7\u00e3o visual para sistemas baseados em eventos. Ele executa como um servidor web e \u00e9 amplamente utilizado para conectar dispositivos de IoT.</p> <ul> <li>Programado em Node.js, \u00e9 uma ferramenta visual para editar fluxos de mensagens.</li> <li>Dispon\u00edvel em servi\u00e7os de Cloud como o IBM Bluemix.</li> </ul>"},{"location":"aulas/iot/lab6/#instalacao-do-node-red","title":"Instala\u00e7\u00e3o do Node-RED","text":"<ol> <li>Instale o Node.js (vers\u00e3o LTS) no site Node.js.</li> <li>No terminal, digite:      <pre><code>npm install -g --unsafe-perm node-red\n</code></pre></li> <li>Para rodar o Node-RED:      <pre><code>node-red\n</code></pre></li> <li>Acesse no navegador: http://localhost:1880</li> </ol>"},{"location":"aulas/iot/lab6/#primeiro-fluxo-no-node-red","title":"Primeiro fluxo no Node-RED","text":"<ul> <li>Conecte um n\u00f3 de entrada do tipo \"inject\" a um n\u00f3 \"debug\", fa\u00e7a o deploy e observe o resultado no painel de debug.</li> <li>Modifique o n\u00f3 \"inject\" e veja as altera\u00e7\u00f5es no resultado.</li> </ul>"},{"location":"aulas/iot/lab6/#desafios-no-node-red","title":"Desafios no Node-RED","text":""},{"location":"aulas/iot/lab6/#desafio-1-monitor-de-clima","title":"Desafio 1: Monitor de clima","text":"<ol> <li>Cadastre-se no site OpenWeather, crie um token e leia a documenta\u00e7\u00e3o da API Current.</li> <li>Crie uma URL para obter o tempo de uma cidade de sua prefer\u00eancia e compare o resultado com a sa\u00edda no Node-RED.</li> </ol>"},{"location":"aulas/iot/lab6/#desafio-2-dashboard","title":"Desafio 2: Dashboard","text":"<p>Crie um dashboard que exiba informa\u00e7\u00f5es de duas ou mais cidades, incluindo: - Temperatura atual - Temperatura m\u00ednima - Temperatura m\u00e1xima - Velocidade do vento - Umidade relativa - Sensa\u00e7\u00e3o t\u00e9rmica</p> <p>Atualize os dados a cada 5 segundos.</p>"},{"location":"aulas/iot/lab6/#mqtt","title":"MQTT","text":"<p>O MQTT \u00e9 um protocolo leve para publica\u00e7\u00e3o e recebimento de mensagens, adequado para dispositivos com alta lat\u00eancia e baixa largura de banda. </p>"},{"location":"aulas/iot/lab6/#servidores-mqtt","title":"Servidores MQTT","text":"<ul> <li>Brokers p\u00fablicos: iot.eclipse.org, test.mosquitto.org, dev.rabbitmq.com, broker.mqttdashboard.com.</li> <li>Uso local: O servidor Mosquitto \u00e9 indicado para redes locais com poucos dispositivos.</li> </ul>"},{"location":"aulas/iot/lab6/#desafio-3-cliente-mqtt-no-node-red","title":"Desafio 3: Cliente MQTT no Node-RED","text":"<p>Crie um chat usando o MQTT, configurando um t\u00f3pico com camelCase para enviar e receber mensagens.</p>"},{"location":"aulas/iot/lab7/","title":"Lab07 - Fluxos e Dashboards","text":""},{"location":"aulas/iot/lab7/#lab6-desafios","title":"Lab6 - Desafios","text":"<p>Os <code>desafios 1, 2 e 3</code> devem ser entregues e comp\u00f5em parte da nota do CP6.</p>"},{"location":"aulas/iot/lab7/#conteudo-deste-laboratorio","title":"Conte\u00fado deste laborat\u00f3rio","text":"<ul> <li> <p>Instala\u00e7\u00e3o e uso de bibliotecas externas para arduino</p> <ul> <li>Arduino JSON</li> <li>Sensor de temperatura e umidade DHT11</li> </ul> </li> <li> <p>Comunica\u00e7\u00e3o serial entre Arduino e o Node-RED</p> <ul> <li>Como mandar dados do arduino para o node-RED no formato JSON</li> </ul> </li> <li> <p>Como desenvolver um sistema supervis\u00f3rio para monitoramento de temperatura e umidade</p> </li> </ul>"},{"location":"aulas/iot/lab7/#instalacao-e-uso-de-bibliotecas-externas-para-arduino","title":"Instala\u00e7\u00e3o e uso de bibliotecas externas para arduino","text":"<p>Normalmente os criadores das bibliotecas descrevem o passo-a-passo para utilizar as bibliotecas criadas, mas de forma geral podemos instalar uma biblioteca externa de duas formas: </p> <ul> <li> <p>Por Download: </p> <ul> <li>Fazer o download do arquivo .zip da biblioteca </li> <li>Descompactar o arquivo dentro da pasta ~/Arduino/libraries/</li> <li>Pronto! Podemos usar em nosso projeto.</li> <li>De forma geral \u00e9 isso, eventualmente o criador da biblioteca ir\u00e1 orientar eventuais etapas adicionais. </li> </ul> </li> <li> <p>Pelo gerenciador de bibliotecas:</p> <ul> <li>abra o Arduino IDE</li> <li>acesse: Sketch ==&gt; Include Library ==&gt; Manage Libraries\u2026 </li> <li>Digite na busca o nome da biblioteca</li> <li>Encontre a op\u00e7\u00e3o desejada e clique em instalar</li> <li>Pronto! Podemos usar em nosso projeto.</li> <li>Algumas libs dependem de outras de outras libs, nesse caso \u00e9 necess\u00e1rio instalar todas as libs.</li> </ul> </li> </ul> Imagem passo-a-passo <p></p> <p>DICA: Explore a documenta\u00e7\u00e3o e os exemplos da biblioteca instalada.   </p>"},{"location":"aulas/iot/lab7/#biblioteca-arduinojson","title":"Biblioteca ArduinoJson","text":"<p>A biblioteca ArduinoJSON \u00e9 escrita em C++ para realizar a comunica\u00e7\u00e3o de dados no formato JSON (JavaScript Object Notation) com aplica\u00e7\u00f5es para IoT.  Pra quem conhece Python a estrutura \u00e9 muito parecida com a de dicion\u00e1rios:</p> <p>{\"Key1\":\"Value1\", \"Key2\":\"Value2\", \"Key3\":\"Value3\",\"....\":.\"....\"}  </p> <p>Documenta\u00e7\u00e3o oficial em: arduinoJSON - https://arduinojson.org/</p> <p>Exercise</p> <p>Fa\u00e7a a instala\u00e7\u00e3o da biblioteca arduinoJSON direto pelo ArduinoIDE, no campo de busca digite <code>ArduinoJson</code> e instale a biblioteca. Para mais detalhes de como realizar a instala\u00e7\u00e3o acesse aqui a documenta\u00e7\u00e3o oficial - https://arduinojson.org/v6/doc/installation/</p>"},{"location":"aulas/iot/lab7/#sensor-dht11","title":"Sensor DHT11","text":"<p>O DHT11 \u00e9 um sensor digital de temperatura e umidade muito utilizado em diversas aplica\u00e7\u00f5es. Para facilitar o trabalho utilizamos uma biblioteca para realizar as leituras de temperatura e umidade. </p> <p></p> Pino Descri\u00e7\u00e3o 1 Alimenta\u00e7\u00e3o, VCC, 3,5V ~ 5,5V 2 DATA, transmiss\u00e3o de dados 3 NC, N\u00e3o Conectado 4 Alimenta\u00e7\u00e3o, GND, 0v <p>Cuidado para n\u00e3o inverter os pinos de alimenta\u00e7\u00e3o. </p> <p>Exercise</p> <p>Fa\u00e7a a instala\u00e7\u00e3o das bibliotecas para usar o DHT11: Adafruit Unified Sensor Libs: </p> <ol> <li> <p>Adafruit Sensor</p> </li> <li> <p>DHT Sensor. </p> </li> </ol> <p>Ap\u00f3s o download descompacte o arquivo .zip e mova-o para a pasta <code>~/Arduino/Libraries/</code></p>"},{"location":"aulas/iot/lab7/#testando-o-sensor-dht11","title":"Testando o sensor DHT11","text":"<p>Para testar o funcionamento do sensor vamos executar 2 etapas: Montagem do hardware e Desenvolvimento do Software.</p>"},{"location":"aulas/iot/lab7/#o-hardware-de-teste","title":"O hardware de teste","text":"<p>Monte o circuito da imagem abaixo e n\u00e3o esque\u00e7a de conectar o resistor </p> <p></p> <p>Exercise</p> <p>De acordo com o circuito qual o pino do arduino \u00e9 utilizado para realizar comunica\u00e7\u00e3o digital com o sensor DHT11?</p>"},{"location":"aulas/iot/lab7/#o-codigo-de-teste","title":"O c\u00f3digo de teste","text":"<p>Crie um novo projeto no ArduinoIDE e tilize o c\u00f3digo de teste abaixo: Este c\u00f3digo foi adaptado do site filipeflop</p> <pre><code>/*\nC\u00f3digo para teste do sensor DHT11 \n\n*/\n#include \"DHT.h\"\n#define DHTPIN  7  //define o pino usado no arduino\n#define DHTTYPE DHT11\nDHT dht(DHTPIN, DHTTYPE); //declara a objeto da classe\n\nvoid setup() \n{\n  Serial.begin(9600);\n  Serial.println(\"DHTxx test!\");\n  dht.begin();\n}\n\nvoid loop() \n{\n  float h = dht.readHumidity();  // faz leitura da umidade\n  float t = dht.readTemperature();  // faz leitura da temperatura\n\n  // testa se retorno \u00e9 valido, caso contr\u00e1rio algo est\u00e1 errado.\n  if (isnan(t) || isnan(h)) \n  {\n    Serial.println(\"Falha na leitura do sensor DHT\");\n  } \n  else\n  {\n    Serial.print(\"Umidade: \");\n    Serial.print(h);\n    Serial.print(\" %t\");\n    Serial.print(\"Temperatura: \");\n    Serial.print(t);\n    Serial.println(\" *C\");\n  }\n  delay(500); //delay de 0,5s\n}\n</code></pre>"},{"location":"aulas/iot/lab7/#o-teste","title":"O teste","text":"<p>Ap\u00f3s montar o circuito e escrever o c\u00f3digo, carregue o c\u00f3digo no arduino e abra o Monitor Serial para visualizar o funcionamento com mas medidas da temperatura e umidade, o resultado esperado deve ser igual da imagem abaixo.</p> <p></p> <p>Parab\u00e9ns!! Primeira parte concluida, vamos em frente... </p>"},{"location":"aulas/iot/lab7/#usando-a-biblioteca-arduinojson","title":"Usando a biblioteca ArduinoJson","text":"<p>Vamos alterar nosso c\u00f3digo para enviar as informa\u00e7\u00f5es do sensor DHT11 em formato JSON, observe o c\u00f3digo abaixo com as altera\u00e7\u00f5es:</p> <pre><code>/*\nC\u00f3digo exemplo demonstrando o funcionamento do Sensor DHT11 enviando \ninforma\u00e7\u00f5es via serial no formato JSON para o servidor node-Red que recebe e transmite via protocolo MQTT \n\n*/\n\n/////Json\n#include &lt;ArduinoJson.h&gt;\nconst int TAMANHO = 50;  //define o tamanho do buffer para o json\n\n///// Sensor DHT\n#include \"DHT.h\"\n#define DHTPIN  7  //define o pino usado no arduino\n#define DHTTYPE DHT11\nDHT dht(DHTPIN, DHTTYPE); //declara a objeto da classe\n\n////// Outras declara\u00e7\u00f5es\n#define led 13 //define led conectado no pino 13\n\nvoid setup() \n{\n  //inicialia c sensor\n  dht.begin();\n\n  //inicializa comunica\u00e7\u00e3o serial\n  Serial.begin(9600);\n\n  //configura pinos de saida do arduinos\n  pinMode(led, OUTPUT);\n}\n\nvoid loop() \n{\n  StaticJsonDocument&lt;TAMANHO&gt; json; //Aloca buffer para objeto json\n\n  // Faz a leitura da temperatura  \n  float temp = dht.readTemperature();\n  // faz a leitura da humidade\n  float umi = dht.readHumidity();\n\n  //formato de escrita do json\n  json[\"temperatura\"] = temp;\n  json[\"umidade\"] = umi;\n\n  serializeJson(json, Serial);\n  Serial.println();\n\n  //delay\n  delay(500);\n}\n</code></pre> <p>Um ponto importante: Definir a variavel <code>TAMANHO</code> que serve como buffer em bytes para alocar o JSON que vamos trabalhar. Para isso podemos utilizar o <code>ArduinoJson Assistant</code> neste link: https://arduinojson.org/v6/assistant/#/step1, siga o passo-a-passo da ferramenta para descobrir o valor minimo que devemos utilizar. </p> <p>Exercise</p> <p>Utilizando o <code>ArduinoJson Assistant</code> qual o valor recomendado para o json do exemplo abaixo?</p> <pre><code>{\n\"valorSensor1\":10.10258,\n\"valorSensor2\":50.28546\n}    \n</code></pre> <p>Etapa 2 concluida! Agora o nosso programa envia dados no formato Json, facilitando a integra\u00e7\u00e3o com outros sistemas incluindo o Node-RED.</p>"},{"location":"aulas/iot/lab7/#comunicacao-serial-com-node-red","title":"Comunica\u00e7\u00e3o serial com node-RED","text":"<p>No flow do node-red, vamos usar o node <code>serialport</code> para realizar a comunica\u00e7\u00e3o serial entre o node-red e o arduino conectado na porta que conectado na porta USB, por padr\u00e3o esse n\u00e3o vem instalado. Fa\u00e7a a instala\u00e7\u00e3o do node <code>node-red-node-serialport</code>.</p> <p></p> <p>No node-RED monte o flow:</p> <p></p> <p>Agora configure o node da serial da seguinte forma: </p> <pre><code>- Serial Port: com o nome da porta COM que est\u00e1 alocada para o arduino\n- baud rate: para 9600.\n</code></pre> <p></p> <p>Fa\u00e7a o deplay e se tudo estiver correto, no debug vai aparecer as mensagens recebidas pelo arduino.</p> <p></p>"},{"location":"aulas/iot/lab7/#desenvolvimento-de-um-sistema-supervisorio-para-monitoramento-de-temperatura-e-umidade","title":"Desenvolvimento de um sistema supervis\u00f3rio para monitoramento de temperatura e umidade","text":"<p>Para o desenvolvimento do sistema de supervis\u00f3rio ficar completo basta adaptar o fluxo que temos no node-RED para receber os t\u00f3picos de temperatura e umidade separados e enviar para o dashboard.</p> <p>Exercise</p> <p>Fa\u00e7a as adapta\u00e7\u00f5es necess\u00e1rias para exibir os valores de temperatura e umidade em 2 gauge e 2 chart como na imagem abaixo:</p> <p></p> <p>Exercise</p> <p>Baseado na solu\u00e7\u00e3o do desafio anterior, altere o fluxo para enviar os dados do node-RED via protocolo MQTT. Agora em um segundo computador crie um fluxo no node-RED que recebe os topicos enviados pelo primeiro flow em MQTT.</p>"},{"location":"aulas/iot/lab7/#controlando-o-arduino-pelo-node-red","title":"Controlando o arduino pelo node-RED","text":"<p>Chegou a hora de fazer o caminho de volta, ja mandamos dados para o node-RED, agora \u00e9 vez de receber dados do node-RED. </p> <p>Exercise</p> <p>Adicione um <code>dashboard switch</code> e configure para enviar a string \u201cliga\u201d e \u201cdesliga\u201d pela serial, para controlar um LED do arduino. DICA: Veja o exemplo abaixo como refer\u00eancia.</p> <pre><code>#include &lt;ArduinoJson.h&gt;\nconst int LED = 3;\nconst int TAMANHO = 200;\nvoid setup() {\n  Serial.begin(9600);\n  //O valor padr\u00e3o de 1000ms \u00e9 muito tempo\n  Serial.setTimeout(10);\n  pinMode(LED,OUTPUT);\n}\nvoid loop() {\n  if (Serial.available() &gt; 0) {\n    //L\u00ea o JSON dispon\u00edvel na porta serial:\n    StaticJsonDocument&lt;TAMANHO&gt; json;\n    deserializeJson(json, Serial);\n    if(json.containsKey(\"led\")) {\n      int valor = json[\"led\"];\n      analogWrite(LED, valor);\n    }\n  } \n  delay(300);\n}\n</code></pre>"},{"location":"aulas/iot/lab7/#desafios","title":"Desafios","text":"<p>J\u00e1 construimos toda a infraestrutura com a base necess\u00e1ria para desenvolver os desafios deste lab.</p>"},{"location":"aulas/iot/lab7/#desafio-1-alerta-de-condicoes-climaticas","title":"Desafio 1: Alerta de Condi\u00e7\u00f5es Clim\u00e1ticas","text":"<p>Objetivo: Criar um sistema de alerta que notifica o usu\u00e1rio quando a temperatura e/ou umidade ultrapassam um limite definido.</p>"},{"location":"aulas/iot/lab7/#instrucoes","title":"Instru\u00e7\u00f5es:","text":"<ul> <li>Utilize o Node-RED para definir limites de temperatura e umidade (por exemplo, temperatura acima de 30\u00b0C e umidade abaixo de 40%).</li> <li>Quando os valores lidos pelo sensor DHT11 ultrapassarem esses limites, um alerta deve ser exibido no dashboard.</li> <li>Adicione um LED no Arduino para acender quando os limites forem ultrapassados.</li> </ul>"},{"location":"aulas/iot/lab7/#desafio-2-registro-de-dados","title":"Desafio 2: Registro de Dados","text":"<p>Objetivo: Armazenar os dados de temperatura e umidade em um banco de dados ou arquivo para an\u00e1lise posterior.</p>"},{"location":"aulas/iot/lab7/#instrucoes_1","title":"Instru\u00e7\u00f5es:","text":"<ul> <li>Utilize o Node-RED para encaminhar os dados recebidos do Arduino para um banco de dados de sua escolha (pode ser um banco de dados SQL, NoSQL ou at\u00e9 mesmo um arquivo CSV).</li> </ul>"},{"location":"aulas/iot/lab7/#desafio-3-integracao-com-outros-sensores","title":"Desafio 3: Integra\u00e7\u00e3o com Outros Sensores","text":"<p>Objetivo: Integrar outros sensores ao sistema e exibir seus dados no Node-RED.</p>"},{"location":"aulas/iot/lab7/#instrucoes_2","title":"Instru\u00e7\u00f5es:","text":"<ul> <li>Escolha um ou mais sensores adicionais compat\u00edveis com Arduino (por exemplo, sensor de luminosidade, sensor de movimento, sensor de g\u00e1s, etc.).</li> <li>Integre o(s) sensor(es) escolhido(s) ao seu circuito Arduino.</li> <li>Modifique o c\u00f3digo do Arduino para ler os dados do(s) novo(s) sensor(es) e enviar esses dados para o Node-RED em formato JSON, juntamente com os dados de temperatura e umidade.</li> <li>No Node-RED, configure o dashboard para exibir os dados do(s) novo(s) sensor(es) em tempo real, seja atrav\u00e9s de gr\u00e1ficos, medidores ou outros widgets relevantes.</li> </ul> <p>Como um desafio adicional configurar alertas ou a\u00e7\u00f5es espec\u00edficas com base nos dados do(s) novo(s) sensor(es). Por exemplo, se um sensor de luminosidade detectar que est\u00e1 escuro, um LED pode ser acionado automaticamente.</p>"},{"location":"aulas/iot/lab8/","title":"Lab08 - Integra\u00e7\u00e3o com MQTT","text":""},{"location":"aulas/iot/lab8/#lab7-api-rest-json-node-red","title":"Lab7 - API REST JSON NODE-RED","text":""},{"location":"aulas/iot/lab8/#criando-servidor-no-node-red","title":"Criando servidor no Node-Red","text":""},{"location":"aulas/iot/lab8/#estrutura-da-api","title":"Estrutura da API:","text":"<ol> <li> <p>Acender ou apagar o LED:</p> </li> <li> <p>Endpoint: <code>/led</code></p> </li> <li> <p>M\u00e9todos:</p> <ul> <li>GET: Retorna o estado atual do LED (0 ou 1).</li> <li>POST: Muda o estado do LED. O corpo da solicita\u00e7\u00e3o deve conter um JSON com o novo estado.</li> <li>PUT: Mesma funcionalidade do POST.</li> <li>DELETE: Desliga o LED.</li> </ul> </li> <li> <p>Capturar o status do bot\u00e3o:</p> </li> <li> <p>Endpoint: <code>/button</code></p> </li> <li>M\u00e9todo:<ul> <li>GET: Retorna o estado atual do bot\u00e3o (pressionado ou n\u00e3o pressionado).</li> </ul> </li> </ol> <p>O objetivo \u00e9 criar os endponts <code>/led</code> e <code>/button</code> que v\u00e3o representar o estado do sensor e atuador conectado ao dispositivo inteligente IoT. </p> <p>O resultado das rotas ser\u00e1:</p> <ul> <li>Endpoint <code>/led</code>:</li> </ul> <p></p> <ul> <li>Endpoint <code>/button</code>:</li> </ul> <p></p> <ul> <li>Monte o fluxo e teste:</li> </ul> <p></p> <p>onde:</p> <ul> <li>function10:</li> </ul> <pre><code>// Supondo que o estado do LED \u00e9 armazenado em uma vari\u00e1vel global.\nvar ledState = global.get(\"ledState\") || 0; // Se n\u00e3o estiver definido, assume 0.\nmsg.payload = {\n    \"state\": ledState\n};\nreturn msg;\n</code></pre> <ul> <li>function11:</li> </ul> <pre><code>var newState = msg.payload.state;\nif (newState === 0 || newState === 1) {\n    global.set(\"ledState\", newState);\n    msg.payload = {\n        \"message\": \"LED atualizado com sucesso.\"\n    };\n} else {\n    msg.payload = {\n        \"message\": \"Estado inv\u00e1lido.\"\n    };\n    msg.statusCode = 400; // C\u00f3digo de erro para \"Bad Request\"\n}\nreturn msg;\n</code></pre> <ul> <li>function12:</li> </ul> <pre><code>global.set(\"ledState\", 0);\nmsg.payload = {\n    \"message\": \"LED desligado com sucesso.\"\n};\nreturn msg;\n</code></pre> <ul> <li>function13:</li> </ul> <pre><code>// Supondo que o estado do bot\u00e3o \u00e9 armazenado em uma vari\u00e1vel global.\nvar buttonState = global.get(\"buttonState\") || 0; \nmsg.payload = {\n    \"state\": buttonState\n};\nreturn msg;\n</code></pre> <ul> <li>function14:</li> </ul> <pre><code>global.set(\"buttonState\", 1);\nreturn msg;\n</code></pre> <ul> <li>function15:</li> </ul> <pre><code>global.set(\"buttonState\", 0);\nreturn msg;\n</code></pre>"},{"location":"aulas/iot/lab8/#cors-cross-origin-resource-sharing","title":"CORS - Cross-Origin Resource Sharing","text":"<p>Quando voc\u00ea cria uma API REST, especialmente para aplica\u00e7\u00f5es de IoT, \u00e9 comum que diferentes clientes (como navegadores web, aplicativos m\u00f3veis ou outros dispositivos) tentem acess\u00e1-la de diferentes origens. Portanto, lidar com o Controle de Acesso de Origem Cruzada (CORS - Cross-Origin Resource Sharing) \u00e9 uma considera\u00e7\u00e3o importante.</p> <p>Por padr\u00e3o, por motivos de seguran\u00e7a, os navegadores restringem solicita\u00e7\u00f5es HTTP de serem feitas entre sites. Isso significa que, se voc\u00ea tiver uma interface web rodando em um dom\u00ednio ou porta e tentar fazer uma solicita\u00e7\u00e3o AJAX para sua API Node-RED em um dom\u00ednio ou porta diferente, o navegador bloquear\u00e1 a solicita\u00e7\u00e3o, a menos que a API indique que essas solicita\u00e7\u00f5es cruzadas s\u00e3o aceit\u00e1veis.</p> <p>O cabe\u00e7alho <code>\"Content-Type\":\"application/json\"</code> informa aos clientes que a API retornar\u00e1 dados no formato JSON. O cabe\u00e7alho <code>\"Access-Control-Allow-Origin\":\"*\"</code> permite que qualquer site fa\u00e7a solicita\u00e7\u00f5es \u00e0 sua API. Isso \u00e9 adequado para desenvolvimento ou em ambientes controlados, mas tenha cuidado ao usar essa configura\u00e7\u00e3o em produ\u00e7\u00e3o devido a considera\u00e7\u00f5es de seguran\u00e7a.</p>"},{"location":"aulas/iot/lab8/#dicas-para-realizar-requisicoes","title":"Dicas para realizar requisi\u00e7\u00f5es","text":"<p>Utilizar o <code>curl</code> \u00e9 uma forma simples de testar APIs diretamente do terminal ou linha de comando.</p> <p>No <code>Windows</code> o <code>CMD</code> interpreta alguns caracteres de maneira especial, a gente precisa ajustar a sintaxe ou usar o <code>PowerShell</code> em vez do CMD.</p> <p>Se for no CMD, o ajuste \u00e9 <code>\\\"</code> para usar <code>\"</code> interna da chave do JSON e fica:</p> <pre><code>curl -X POST -H \"Content-Type: application/json\" -d \"{\\\"state\\\": 0}\" http://localhost:1880/led\n</code></pre> <p>Se for no PowerShell, n\u00e3o muda: permanece:</p> <pre><code>curl -X POST -H 'Content-Type: application/json' -d '{\"state\": 0}' http://localhost:1880/led\n ```\n\nOs outros comandos permanecem iguais:\n\n\n```bash\ncurl -X GET http://localhost:1880/led\n\ncurl -X DELETE http://localhost:1880/led\n\ncurl -X GET http://localhost:1880/button\n</code></pre> <p>Se utilizar o Postman n\u00e3o ter\u00e1 esse problema. </p> <p></p>"},{"location":"aulas/iot/lab8/#mais-informacoes","title":"Mais informa\u00e7\u00f5es","text":"<p>Fa\u00e7a o download do pdf da aula.</p> <ul> <li>arquivo pdf: lab7</li> </ul>"},{"location":"aulas/iot/lab9/","title":"Index","text":""},{"location":"aulas/iot/lab9/#raspberry-pi","title":"Raspberry PI","text":"<p>At\u00e9 este momento do nosso curso, desenvolvemos pequenos projetos envolvendo sensores/atuadores e o nosso hardware(placa de desenvolvimento) foi o Arduino UNO, al\u00e9m disso aprendemos como integrar com Python e Node-Red.</p> <p>Neste laborat\u00f3rio vamos come\u00e7ar nossa jornada de computa\u00e7\u00e3o embarcada com aplica\u00e7\u00f5es voltadas paara a Internet das Coisas com o hardware <code>Raspberry PI</code>.  Nesta etapa vamos ver dentre outras coisas: o que \u00e9 a Respberry Pi, Sistema Operacional Linux, como dar boot na placa Raspberry PI, como configurar e utilizar os GPIO - Pinos de Entrada/Saida, como realizar integra\u00e7\u00e3o com Arduino, Node-Red e muito mais...</p>"},{"location":"aulas/iot/lab9/#o-que-vamos-ver-neste-lab","title":"O que vamos ver neste lab?","text":"<ul> <li>Raspberry PI: o que \u00e9? Qual a diferen\u00e7a para o Arduino? </li> <li>Raspberry Pi: Getting Started <ul> <li>Overview - Conhecendo o hardware</li> <li>Flash SD Card - Como dar boot do Sistema Operacional na Raspberry PI</li> <li>Modos de uso - GUI x Headless<ul> <li>Headless - Configurando acesso SSH e rede Wifi.</li> <li>Headless - VNC Viewer</li> <li>GUI - Modo Desktop  </li> </ul> </li> <li>Controlando os GPIO - Blink LED.<ul> <li>Controle por CLI</li> <li>Shell Script</li> <li>...</li> </ul> </li> </ul> </li> </ul>"},{"location":"aulas/iot/lab9/#raspberry-pi-x-arduino","title":"Raspberry PI x Arduino","text":"<p>Antes de falar da Raspberry PI, vamos lembrar que o Arduino UNO, que usamos, possui um <code>microcontrolador</code> de 8-bit link do datasheet. Sua arquitetura RISC \u00e9 simples, e cobre bem os requisitos m\u00ednimos de um sistema embarcado. Contudo, n\u00e3o \u00e9 possivel rodar um sistema operacional completo, o que pode limitar algumas possibiildades de sistemas mais complexos.</p> <p>Para rodar um Sistema Operacional completo precissamos de um <code>processador</code> por exemplo o processador Intel 386, I5, I7, Celeron e muitos outros (link do datasheet de um Intel I7) que usamos em nossos notebooks e desktops por exemplo. Em apica\u00e7\u00f5es de computa\u00e7\u00e3o embarcada geralmente usamos um substituto para o notebook ou desktop, para atender requisitos tecnicos de custo, consumo de energia, peso, tamanho dentre outros... nesses casos podemos utilizar <code>SBC</code> (Single Board Computer).</p> <p>Os computadores de placa \u00fanica (SBC) s\u00e3o computadores completos (combina\u00e7\u00e3o de um processador, mem\u00f3ria, suporte de rede, video, audio, entrada e sa\u00edda e outros...) em uma placa s\u00f3, com a vantagem de ser de baixo custo e possuir pequenas dimens\u00f5es comparado ao computador convensional.</p> <p>\u00c9 neste ponto que vamos come\u00e7ar a falar da <code>Raspberry PI</code> que \u00e9 a mais famosa e mais conhecida SBC e que suporta um Sistema Operacional Embarcado (Linux) ou seja, com ela \u00e9 possivel desenvolver e implementar uma infinidade de projetos. </p> <p>A placa Raspberry Pi foi lan\u00e7ada em 2012 pela Raspberry Pi Fundation, sendo uma classe de pequenos computadores port\u00e1teis de baix\u00edssimo custo, baseado nos processadores multim\u00eddia de arquitetura ARM da Broadcom, o mesmo que utilizados para celulares. O projeto foi um sucesso, vem crescendo e se atualizando, hoje temos diversos modelos para diversas aplica\u00e7\u00f5es diferentes como a Raspberry PI 3, 4, Zero e outros.</p> <p>link da documenta\u00e7\u00e3o oficial</p> <p>Link para conhecer outros modelos de SBC</p> <p>Agora que j\u00e1 entendemos um pouco o que \u00e9 Raspberry PI, vamos aprender a usar....</p> <p>Progress</p> <p>Continuar...</p>"},{"location":"aulas/iot/lab9/#raspbeery-pi-getting-started","title":"Raspbeery PI - Getting Started","text":""},{"location":"aulas/iot/lab9/#overview","title":"Overview","text":"<p>Existem varios modelos de Raspberry PI, em nosso curso vamos utilizar a <code>Raspberry PI 3 Model B+</code>. </p> <p></p> <p></p> <p>Para complementar:</p> <ul> <li> <p>Fonte de Alimenta\u00e7\u00e3o: 5V @ &gt;2A</p> </li> <li> <p>Cart\u00e3o SD Card: micro SD Card &gt;8GB Classe 10 ou superior</p> </li> </ul>"},{"location":"aulas/iot/lab9/#sistema-operacional","title":"Sistema Operacional","text":"<p>Podemos utilizar diversas distribu\u00e7\u00f5es na RBI, dentre elas as mais comuns s\u00e3o:</p> <ul> <li>Raspbian - SO de uso geral </li> <li>Ubuntu - SO de uso geral</li> <li>RetroPie - Emulador de video game</li> <li>OSMC - Media Center </li> <li>Home Assistent - Automa\u00e7\u00e3o Resid\u00eancial</li> <li>E muitos outross...</li> </ul> <p>Fim da teoria, vamos pra parte pr\u00e1tica!! Leia com aten\u00e7\u00e3o este guia e siga todos os passos.  </p> <p>Progress</p> <p>Continuar...</p>"},{"location":"aulas/iot/lab9/#flash-sd-card","title":"Flash SD Card","text":"<p>O SO (Sistema Operacional) da RPI fica armazenado no <code>micro SD Card</code> que deve ser de pelo menos 8GB Classe 10 ou superior, existem diversas formas de realizar a grava\u00e7\u00e3o do SO, para isso se prepare pois chegou a hora de por a m\u00e3o na massa. </p> <p>As outras vers\u00f5es do SO podem ser encontras no link https://www.raspberrypi.com/software/operating-systems/. Em nosso curso vamos utlizar o <code>Raspberry Pi OS (legacy)</code> baseado na Distribui\u00e7\u00e3o Debian 10 (Buster). </p> <p></p> <p>Info</p> <p>Pra facilitar, o link para downlod j\u00e1 est\u00e1 aqui</p> <p>Para gravar o SD Card podemos utilizar algumas op\u00e7\u00f5es o mais simples \u00e9 o <code>Balena Etcher</code> que roda em diversas plataformas.</p> <p>Para facilitar, o link para download do balena Etcher https://www.balena.io/etcher/ </p> <p>Exercise</p> <p>Agora voc\u00ea deve:</p> <pre><code>- Remova o SD Card da RPI, conecte o cart\u00e3o ao adaptador USB e plugue no seu notebook\n- Fa\u00e7a o Download do RPI OS \n- Fa\u00e7a o Download do Balena Etcher\n- No seu notebook, Abrir o Balena Etcher e siguir os passos para gravar o SD Card\n- Ap\u00f3s a grava\u00e7\u00e3o remova o adaptador da USB e conecte no computador novamente.\n- Se tudo deu certo:\n    -  Ir\u00e3o aparecer duas partici\u00e7\u00f5es referentes, sendo uma delas chamada \"boot\"\n    -  Caso contr\u00e1rio, alguma coisa deu errada, formate o SD Card em FAT32 e grave novamente.\n</code></pre> <p>Progress</p> <p>Continuar...        </p>"},{"location":"aulas/iot/lab9/#modo-de-uso-interface-grafica","title":"Modo de uso - Interface Gr\u00e1fica","text":"<p>Apenas para conhecimento extra, pois n\u00e3o \u00e9 desta forma que vamos usar a Raspberry PI em nosso curso </p> <p>Para utilizar a Raspberry como um computador normal \u00e9 muito simples basta conectar na Raspberry PI: O SD Card gravado, um monitor HDMI, um teclado e um mouse. Com tudo conectado corretamente conecte a fonte de alimenta\u00e7\u00e3o 5V, o sistema operacional ir\u00e1 inicializar e voc\u00ea pode usar :) .  </p> <p></p> <p></p>"},{"location":"aulas/iot/lab9/#modo-de-uso-headless","title":"Modo de uso - Headless","text":"<p>Agora sim! Aten\u00e7\u00e3o nos pr\u00f3ximos passos...</p> <p>Vamos utilizar o Rasbperry PI no modo <code>Headless</code>, ou seja, sem conectar monitor, teclado e mouse. Para utilizar este modo \u00e9 necess\u00e1rio realizar algumas configura\u00e7\u00f5es no micro SD Card antes de dar boot na Raspberry PI.</p>"},{"location":"aulas/iot/lab9/#habilitar-ssh","title":"Habilitar SSH","text":"<p>Para habilitar o SSH \u00e9 necess\u00e1rio criar um arquivo vazio (sem extens\u00e3o) chamado ssh dentro da pasta boot. </p> <p>Exercise</p> <p>Agora voc\u00ea deve:</p> <pre><code>- Conecte o micro SD Card no adaptador USB, e plugue no notebook\n- Acesse a parti\u00e7\u00e3o chamada boot \n- crie um arquivo chamado ssh na raiz da parti\u00e7\u00e3o boot\n- este arquivo n\u00e3o possui extens\u00e3o\n</code></pre> <p></p> <p>O resultado esperado deve ser semelhante ao da imagem abaixo:</p> <p></p> <p>Progress</p> <p>Continuar...</p>"},{"location":"aulas/iot/lab9/#configuracao-de-rede-wi-fi","title":"Configura\u00e7\u00e3o de Rede Wi-fi","text":"<p>A configura\u00e7\u00e3o de rede do Wi-fi \u00e9 feita atrav\u00e9s da configura\u00e7\u00e3o de um arquivo chamado wpa_supplicant.conf que deve ser criado dentro da pasta boot.</p> <p>Exercise</p> <p>Agora voc\u00ea deve:</p> <pre><code>- crie um arquivo chamado wpa_supplicant.conf na raiz da parti\u00e7\u00e3o boot\n- abra o arquivo criado com algum editor de texto (bloco de notas ou vscode)\n- configure o arquivo da mesma forma que o texto abaixo\n</code></pre> <p>Neste ponto \u00e9 importe ter uma rede wifi para se conecetar. Temos 2 op\u00e7\u00f5es de redes: Personal e Enterprise (Recomendado) - Para uma rede personal use a configura\u00e7\u00e3o abaixo. Esta configura\u00e7\u00e3o \u00e9 a mais indicada e segura para ser usada em aula, para isso rotei a internet de seu celular.</p> <ul> <li> <p>Personal: (RECOMENDADO) - Use o roteador da sua casa ou habilite seu Celular como Roteador </p> <p><pre><code>country=BR\nctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\nupdate_config=1\n\nnetwork={\n        scan_ssid=1\n        ssid=\"COLOQUEO_O_NOME_DA_REDE\"\n        psk=\"COLOQUE_A_SENHA_DA_REDE\"\n}\n</code></pre> * Enterprise: Redes WPA2 </p> </li> </ul> <p>A rede da FIAP requer autentica\u00e7\u00e3o enterprise, <code>n\u00e3o recomendo</code> pois seu usu\u00e1rio e senha ficar\u00e1 salvo na raspberry pi e qualquer pessoa mal intencionada pode se utilizar desta vulnerabilidade.</p> <pre><code>```shell\n\n# Connect to a WPA2 Enterprise network with wpa_supplicant with this .conf file.\n# I used this to connect to my university's wireless network on Arch linux.\n# Here's the command I used:\n#\n#   wpa_supplicant -i wlan0 -c ./wpa_supplicant.conf\n# \n\nnetwork={\n  ssid=\"YOUR_SSID\"\n  scan_ssid=1\n  key_mgmt=WPA-EAP\n  identity=\"YOUR_USERNAME\"\n  password=\"YOUR_PASSWORD\"\n  eap=PEAP\n  phase1=\"peaplabel=0\"\n  phase2=\"auth=MSCHAPV2\"\n}\n\n```\n</code></pre> <p>Configura\u00e7\u00e3o finalizada! Agora vamos ligar! </p> <p>Progress</p> <p>Continuar...</p>"},{"location":"aulas/iot/lab9/#boot-raspberry-pi","title":"Boot Raspberry PI","text":"<p>Para ter acesso SSH ao raspberry PI vamos utilizar o o software <code>PuTTy</code>.</p> <p>Para facilitar, o link para download do PuTY https://www.putty.org/</p> <p>Agora com tudo configurado e instalado chegou a hora de ligar e testar.</p> <p></p> <p>O seu notebook e a raspberry pi devem estar na mesma rede Wifi do seu Smartphone/Roteador como indica a imagem abaixo.</p> <p></p> <p>Exercise</p> <p>Agora voc\u00ea deve:</p> <pre><code>- Conecte o micro SD Card na Raspberry PI\n- Mantenha sua rede wifi ligada (Smartphone como roteador)\n- Conecte seu computador(notebook) na mesma rede Wifi configurada na Raspbeery Pi\n- Ligue a fonte de alimenta\u00e7\u00e3o na raspberry pi\n- Aguarde alguns segundos e vefifique o ip que foi atribuido ao Raspberry PI\n- No seu computador, abra o puTTY e digite o ip da Raspberry PI\n- Se tudo estiver correto, um terminal ir\u00e1 abrir e vai solicitar login e senha\n</code></pre> <p></p> <p>Por padr\u00e3o, o login e senha da raspberry pi ser\u00e1:</p> <p>login: pi</p> <p>senha: raspberry</p> <p>Finalizado! Agora estamos com nosso raspberry conectado e funcionando. </p> <p>Progress</p> <p>Continuar...</p>"},{"location":"aulas/iot/lab9/#primeiro-teste-da-raspberry","title":"Primeiro teste da raspberry","text":"<p>Vamos fazer o nosso helloWord com a Raspberry Pi, apenas para testar, Monte o circuito da imagem abaixo:</p> <p></p> <p>Agora no terminal da Raspberry Pi execute os comando de forma sequencial:</p> <pre><code># Seta o pino GPIO 17 e configura como saida (output)\necho \"17\" &gt; /sys/class/gpio/export\necho \"out\" &gt; /sys/class/gpio/gpio17/direction\n\n# Escreve na saida do led (nivel logico alto)\necho \"1\" &gt; /sys/class/gpio/gpio17/value\n\n\n# Escreve na saida do led (nivel logico baixo)\necho \"0\" &gt; /sys/class/gpio/gpio17/value\n\n# libera o pino\necho \"17\" &gt; /sys/class/gpio/unexport\n</code></pre> <p>Se tudo deu certo at\u00e9 este ponto, conseguimos ver o led Apagar e acender. </p> <p>Exercise</p> <p>Agora \u00e9 com voc\u00ea, o Raspberry PI permite o acesso aos seus pinos com o uso de diversas linguagens de programa\u00e7\u00e3o diferentes, escolha uma de sua prefer\u00eancia e monte um c\u00f3digo que fa\u00e7a o led Piscar no intervalo de 1 seg. Dica: Pesquise na internet por exemplos, exemplos </p>"}]}