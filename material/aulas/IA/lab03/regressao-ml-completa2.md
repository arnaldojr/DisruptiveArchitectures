# üìä Regress√£o em Machine Learning: Guia Completo

## üéØ Objetivos de Aprendizagem

Ao final desta aula, voc√™ ser√° capaz de:

- **Compreender** os fundamentos matem√°ticos e conceituais da regress√£o
- **Distinguir** entre diferentes tipos de problemas de regress√£o
- **Implementar** modelos de regress√£o linear simples e m√∫ltipla
- **Aplicar** t√©cnicas de regress√£o polinomial e regulariza√ß√£o
- **Avaliar** modelos usando m√©tricas apropriadas
- **Interpretar** resultados e diagnosticar problemas comuns
- **Escolher** o modelo mais adequado para diferentes cen√°rios

---

## üìã √çndice

1. [Conceitos Fundamentais](#conceitos-fundamentais)
2. [Tipos de Regress√£o](#tipos-de-regressao)
3. [Regress√£o Linear](#regressao-linear)
4. [An√°lise Explorat√≥ria de Dados](#analise-exploratoria)
5. [M√©tricas de Avalia√ß√£o](#metricas-avaliacao)
6. [Regress√£o Polinomial](#regressao-polinomial)
7. [Regulariza√ß√£o](#regularizacao)
8. [Diagn√≥stico de Modelos](#diagnostico-modelos)
9. [Casos Pr√°ticos](#casos-praticos)
10. [Exerc√≠cios e Projetos](#exercicios-projetos)

---

## üèóÔ∏è Conceitos Fundamentais

### O que √© Regress√£o?

A **regress√£o** √© uma t√©cnica de aprendizado supervisionado que visa **predizer valores cont√≠nuos** (num√©ricos) com base em vari√°veis de entrada (features).

### üé≠ Classifica√ß√£o vs Regress√£o

| Aspecto | Classifica√ß√£o | Regress√£o |
|---------|---------------|-----------|
| **Sa√≠da** | Categ√≥rica/Discreta | Num√©rica/Cont√≠nua |
| **Exemplos** | Spam/N√£o-spam, Gato/Cachorro | Pre√ßo, Temperatura, Altura |
| **M√©tricas** | Acur√°cia, Precis√£o, Recall | MSE, RMSE, R¬≤ |
| **Algoritmos** | KNN, SVM, Random Forest | Linear, Polinomial, Ridge |

### üßÆ Matem√°tica por tr√°s da Regress√£o

A regress√£o busca encontrar uma fun√ß√£o que melhor relacione as vari√°veis independentes (X) com a vari√°vel dependente (y):

```
y = f(X) + Œµ
```

Onde:
- **y**: vari√°vel dependente (target)
- **X**: vari√°veis independentes (features)
- **f(X)**: fun√ß√£o que queremos aprender
- **Œµ**: erro aleat√≥rio

---

## üî¢ Tipos de Regress√£o

### 1. Regress√£o Linear Simples
- **Uma vari√°vel independente**
- Rela√ß√£o linear entre X e y
- Equa√ß√£o: `y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œµ`

### 2. Regress√£o Linear M√∫ltipla
- **M√∫ltiplas vari√°veis independentes**
- Equa√ß√£o: `y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô + Œµ`

### 3. Regress√£o Polinomial
- **Rela√ß√µes n√£o-lineares**
- Equa√ß√£o: `y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx¬≤ + ... + Œ≤‚Çôx‚Åø + Œµ`

### 4. Regress√£o Regularizada
- **Ridge**: penaliza coeficientes grandes
- **Lasso**: pode zerar coeficientes (sele√ß√£o de features)
- **Elastic Net**: combina Ridge e Lasso

---

## üìà Regress√£o Linear

### Fundamentos Matem√°ticos

A regress√£o linear busca encontrar a melhor reta que passa pelos dados, minimizando o erro entre os valores preditos e reais.

#### M√©todo dos M√≠nimos Quadrados

O objetivo √© minimizar a **Soma dos Quadrados dos Res√≠duos (SSR)**:

```
SSR = Œ£(y·µ¢ - ≈∑·µ¢)¬≤
```

#### F√≥rmulas dos Coeficientes

Para regress√£o linear simples:

```
Œ≤‚ÇÅ = Œ£((x·µ¢ - xÃÑ)(y·µ¢ - »≥)) / Œ£((x·µ¢ - xÃÑ)¬≤)
Œ≤‚ÇÄ = »≥ - Œ≤‚ÇÅxÃÑ
```

### Pressupostos da Regress√£o Linear

1. **Linearidade**: rela√ß√£o linear entre X e y
2. **Independ√™ncia**: observa√ß√µes independentes
3. **Homocedasticidade**: vari√¢ncia constante dos res√≠duos
4. **Normalidade**: res√≠duos seguem distribui√ß√£o normal
5. **Aus√™ncia de multicolinearidade**: features n√£o correlacionadas

### Implementa√ß√£o em Python

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Exemplo b√°sico
# Criando dados sint√©ticos
np.random.seed(42)
X = np.random.randn(100, 1)
y = 2 + 3 * X.ravel() + np.random.randn(100)

# Dividindo os dados
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Criando e treinando o modelo
model = LinearRegression()
model.fit(X_train, y_train)

# Fazendo predi√ß√µes
y_pred = model.predict(X_test)

# Avaliando o modelo
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Coeficiente: {model.coef_[0]:.2f}")
print(f"Intercepto: {model.intercept_:.2f}")
print(f"MSE: {mse:.2f}")
print(f"R¬≤: {r2:.2f}")
```

---

## üîç An√°lise Explorat√≥ria de Dados

### Etapas Essenciais

#### 1. Carregamento e Inspe√ß√£o Inicial

```python
# Exemplo com dataset de habita√ß√£o da Calif√≥rnia
from sklearn.datasets import fetch_california_housing
import pandas as pd
import seaborn as sns

# Carregando os dados
housing = fetch_california_housing()
df = pd.DataFrame(housing.data, columns=housing.feature_names)
df['target'] = housing.target

# Informa√ß√µes b√°sicas
print(df.info())
print(df.describe())
print(df.isnull().sum())
```

#### 2. Visualiza√ß√£o de Distribui√ß√µes

```python
# Histogramas das vari√°veis
fig, axes = plt.subplots(2, 4, figsize=(15, 8))
for i, column in enumerate(df.columns):
    ax = axes[i//4, i%4]
    df[column].hist(ax=ax, bins=30)
    ax.set_title(column)
plt.tight_layout()
plt.show()
```

#### 3. Matriz de Correla√ß√£o

```python
# Calculando e visualizando correla√ß√µes
correlation_matrix = df.corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Matriz de Correla√ß√£o')
plt.show()

# Correla√ß√µes com o target
target_corr = correlation_matrix['target'].sort_values(ascending=False)
print("Correla√ß√µes com o target:")
print(target_corr)
```

#### 4. Gr√°ficos de Dispers√£o

```python
# Scatter plots das vari√°veis mais correlacionadas
top_features = target_corr.abs().sort_values(ascending=False)[1:4].index

fig, axes = plt.subplots(1, 3, figsize=(15, 5))
for i, feature in enumerate(top_features):
    df.plot.scatter(x=feature, y='target', ax=axes[i])
    axes[i].set_title(f'{feature} vs Target')
plt.tight_layout()
plt.show()
```

---

## üìä M√©tricas de Avalia√ß√£o

### M√©tricas Principais

#### 1. Erro Quadr√°tico M√©dio (MSE)
```
MSE = (1/n) √ó Œ£(y·µ¢ - ≈∑·µ¢)¬≤
```
- **Unidade**: quadrado da unidade do target
- **Penaliza**: erros grandes mais fortemente

#### 2. Raiz do Erro Quadr√°tico M√©dio (RMSE)
```
RMSE = ‚àöMSE
```
- **Unidade**: mesma do target
- **Interpreta√ß√£o**: erro m√©dio em termos absolutos

#### 3. Erro Absoluto M√©dio (MAE)
```
MAE = (1/n) √ó Œ£|y·µ¢ - ≈∑·µ¢|
```
- **Robustez**: menos sens√≠vel a outliers

#### 4. Coeficiente de Determina√ß√£o (R¬≤)
```
R¬≤ = 1 - (SSres/SStot)
```
- **Interpreta√ß√£o**: propor√ß√£o da vari√¢ncia explicada
- **Faixa**: 0 a 1 (quanto maior, melhor)

#### 5. R¬≤ Ajustado
```
R¬≤adj = 1 - [(1-R¬≤)(n-1)/(n-p-1)]
```
- **Penaliza**: modelos com muitas vari√°veis

### Implementa√ß√£o Pr√°tica

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

def avaliar_modelo(y_true, y_pred, nome_modelo="Modelo"):
    """
    Fun√ß√£o para avaliar um modelo de regress√£o
    """
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    # R¬≤ ajustado (precisa do n√∫mero de features)
    n = len(y_true)
    p = 1  # n√∫mero de features (ajustar conforme necess√°rio)
    r2_adj = 1 - ((1 - r2) * (n - 1) / (n - p - 1))
    
    print(f"=== Avalia√ß√£o do {nome_modelo} ===")
    print(f"MSE: {mse:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"R¬≤: {r2:.4f}")
    print(f"R¬≤ Ajustado: {r2_adj:.4f}")
    
    return {
        'MSE': mse,
        'RMSE': rmse,
        'MAE': mae,
        'R2': r2,
        'R2_adj': r2_adj
    }
```

---

## üåÄ Regress√£o Polinomial

### Conceito

A regress√£o polinomial estende a regress√£o linear para capturar **rela√ß√µes n√£o-lineares** entre vari√°veis.

### Matematicamente

Para um polin√¥mio de grau n:
```
y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx¬≤ + Œ≤‚ÇÉx¬≥ + ... + Œ≤‚Çôx‚Åø + Œµ
```

### Implementa√ß√£o

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
import numpy as np
import matplotlib.pyplot as plt

# Gerando dados n√£o-lineares
np.random.seed(42)
X = np.linspace(-3, 3, 100).reshape(-1, 1)
y = 0.5 * X.ravel()**3 - 2 * X.ravel()**2 + X.ravel() + np.random.normal(0, 1, 100)

# Fun√ß√£o para treinar modelo polinomial
def treinar_modelo_polinomial(X, y, grau):
    # Criando pipeline
    modelo = Pipeline([
        ('poly', PolynomialFeatures(degree=grau)),
        ('linear', LinearRegression())
    ])
    
    # Treinando
    modelo.fit(X, y)
    
    return modelo

# Testando diferentes graus
graus = [1, 2, 3, 4, 8]
fig, axes = plt.subplots(1, len(graus), figsize=(20, 4))

for i, grau in enumerate(graus):
    # Treinando modelo
    modelo = treinar_modelo_polinomial(X, y, grau)
    
    # Predi√ß√µes
    X_plot = np.linspace(-3, 3, 300).reshape(-1, 1)
    y_pred = modelo.predict(X_plot)
    
    # Plotando
    axes[i].scatter(X, y, alpha=0.6, label='Dados')
    axes[i].plot(X_plot, y_pred, color='red', label=f'Grau {grau}')
    axes[i].set_title(f'Polin√¥mio Grau {grau}')
    axes[i].legend()
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### ‚ö†Ô∏è Overfitting vs Underfitting

- **Underfitting** (grau muito baixo): modelo muito simples
- **Overfitting** (grau muito alto): modelo muito complexo
- **Solu√ß√£o**: valida√ß√£o cruzada para escolher grau √≥timo

```python
from sklearn.model_selection import cross_val_score

def encontrar_grau_otimo(X, y, max_grau=10):
    graus = range(1, max_grau + 1)
    scores = []
    
    for grau in graus:
        modelo = Pipeline([
            ('poly', PolynomialFeatures(degree=grau)),
            ('linear', LinearRegression())
        ])
        
        # Valida√ß√£o cruzada
        cv_scores = cross_val_score(modelo, X, y, cv=5, scoring='neg_mean_squared_error')
        scores.append(-cv_scores.mean())
    
    # Plotando
    plt.figure(figsize=(10, 6))
    plt.plot(graus, scores, marker='o')
    plt.xlabel('Grau do Polin√¥mio')
    plt.ylabel('MSE (Valida√ß√£o Cruzada)')
    plt.title('Escolha do Grau √ìtimo')
    plt.grid(True, alpha=0.3)
    
    grau_otimo = graus[np.argmin(scores)]
    plt.axvline(x=grau_otimo, color='red', linestyle='--', 
                label=f'Grau √ìtimo: {grau_otimo}')
    plt.legend()
    plt.show()
    
    return grau_otimo
```

---

## üéØ Regulariza√ß√£o

### Por que Regularizar?

- **Evitar overfitting**
- **Reduzir complexidade** do modelo
- **Melhorar generaliza√ß√£o**
- **Lidar com multicolinearidade**

### Ridge Regression (L2)

Adiciona penalidade proporcional ao **quadrado dos coeficientes**:

```
Custo = MSE + Œ± √ó Œ£Œ≤·µ¢¬≤
```

```python
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

# Ridge Regression
ridge = Ridge()

# Buscando melhor alpha
alphas = np.logspace(-4, 4, 50)
ridge_cv = GridSearchCV(ridge, {'alpha': alphas}, cv=5, scoring='neg_mean_squared_error')
ridge_cv.fit(X_train, y_train)

print(f"Melhor alpha (Ridge): {ridge_cv.best_params_['alpha']:.4f}")

# Modelo final
ridge_final = Ridge(alpha=ridge_cv.best_params_['alpha'])
ridge_final.fit(X_train, y_train)
```

### Lasso Regression (L1)

Adiciona penalidade proporcional ao **valor absoluto dos coeficientes**:

```
Custo = MSE + Œ± √ó Œ£|Œ≤·µ¢|
```

```python
from sklearn.linear_model import Lasso

# Lasso Regression
lasso = Lasso()
lasso_cv = GridSearchCV(lasso, {'alpha': alphas}, cv=5, scoring='neg_mean_squared_error')
lasso_cv.fit(X_train, y_train)

print(f"Melhor alpha (Lasso): {lasso_cv.best_params_['alpha']:.4f}")

# Modelo final
lasso_final = Lasso(alpha=lasso_cv.best_params_['alpha'])
lasso_final.fit(X_train, y_train)

# Verificando sele√ß√£o de features
features_selecionadas = np.where(lasso_final.coef_ != 0)[0]
print(f"Features selecionadas pelo Lasso: {len(features_selecionadas)}")
```

### Elastic Net

Combina **Ridge e Lasso**:

```
Custo = MSE + Œ±‚ÇÅ √ó Œ£Œ≤·µ¢¬≤ + Œ±‚ÇÇ √ó Œ£|Œ≤·µ¢|
```

```python
from sklearn.linear_model import ElasticNet

# Elastic Net
elastic = ElasticNet()
param_grid = {
    'alpha': alphas,
    'l1_ratio': np.linspace(0, 1, 11)  # 0 = Ridge, 1 = Lasso
}

elastic_cv = GridSearchCV(elastic, param_grid, cv=5, scoring='neg_mean_squared_error')
elastic_cv.fit(X_train, y_train)

print(f"Melhores par√¢metros (Elastic Net): {elastic_cv.best_params_}")
```

---

## üî¨ Diagn√≥stico de Modelos

### An√°lise de Res√≠duos

```python
def analisar_residuos(modelo, X_test, y_test):
    # Predi√ß√µes
    y_pred = modelo.predict(X_test)
    residuos = y_test - y_pred
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # 1. Res√≠duos vs Predi√ß√µes
    axes[0, 0].scatter(y_pred, residuos, alpha=0.6)
    axes[0, 0].axhline(y=0, color='red', linestyle='--')
    axes[0, 0].set_xlabel('Valores Preditos')
    axes[0, 0].set_ylabel('Res√≠duos')
    axes[0, 0].set_title('Res√≠duos vs Predi√ß√µes')
    
    # 2. Q-Q Plot (normalidade dos res√≠duos)
    from scipy import stats
    stats.probplot(residuos, dist="norm", plot=axes[0, 1])
    axes[0, 1].set_title('Q-Q Plot')
    
    # 3. Histograma dos res√≠duos
    axes[1, 0].hist(residuos, bins=30, alpha=0.7)
    axes[1, 0].set_xlabel('Res√≠duos')
    axes[1, 0].set_ylabel('Frequ√™ncia')
    axes[1, 0].set_title('Distribui√ß√£o dos Res√≠duos')
    
    # 4. Valores Reais vs Preditos
    axes[1, 1].scatter(y_test, y_pred, alpha=0.6)
    min_val = min(y_test.min(), y_pred.min())
    max_val = max(y_test.max(), y_pred.max())
    axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'red', linestyle='--')
    axes[1, 1].set_xlabel('Valores Reais')
    axes[1, 1].set_ylabel('Valores Preditos')
    axes[1, 1].set_title('Reais vs Preditos')
    
    plt.tight_layout()
    plt.show()
    
    # Testes estat√≠sticos
    from scipy.stats import shapiro, jarque_bera
    
    print("=== Testes de Diagn√≥stico ===")
    
    # Teste de normalidade
    shapiro_stat, shapiro_p = shapiro(residuos[:5000])  # Limitando para performance
    print(f"Teste Shapiro-Wilk (normalidade): p-value = {shapiro_p:.6f}")
    
    # Teste Jarque-Bera
    jb_stat, jb_p = jarque_bera(residuos)
    print(f"Teste Jarque-Bera (normalidade): p-value = {jb_p:.6f}")
    
    # Homocedasticidade (vari√¢ncia constante)
    from scipy.stats import spearmanr
    corr_stat, corr_p = spearmanr(np.abs(residuos), y_pred)
    print(f"Correla√ß√£o |res√≠duos| vs predi√ß√µes: {corr_stat:.4f} (p = {corr_p:.6f})")
```

### Detec√ß√£o de Outliers

```python
def detectar_outliers(X, y, modelo):
    # Res√≠duos padronizados
    y_pred = modelo.predict(X)
    residuos = y - y_pred
    residuos_padronizados = residuos / np.std(residuos)
    
    # Dist√¢ncia de Cook
    n, p = X.shape
    mse = np.mean(residuos**2)
    
    # Leverage (alavancagem)
    if hasattr(X, 'values'):
        X_array = X.values
    else:
        X_array = X
    
    # Adicionando coluna de intercepto
    X_with_intercept = np.column_stack([np.ones(n), X_array])
    H = X_with_intercept @ np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T
    leverage = np.diag(H)
    
    # Dist√¢ncia de Cook
    cook_distance = (residuos_padronizados**2 / p) * (leverage / (1 - leverage)**2)
    
    # Identificando outliers
    outliers_residuos = np.abs(residuos_padronizados) > 3
    outliers_cook = cook_distance > 4/n  # Regra pr√°tica
    outliers_leverage = leverage > 2*p/n  # Regra pr√°tica
    
    print(f"Outliers por res√≠duos padronizados (|z| > 3): {np.sum(outliers_residuos)}")
    print(f"Outliers por dist√¢ncia de Cook (D > 4/n): {np.sum(outliers_cook)}")
    print(f"Outliers por leverage (h > 2p/n): {np.sum(outliers_leverage)}")
    
    # Plotando
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # Res√≠duos padronizados
    axes[0].scatter(range(len(residuos_padronizados)), residuos_padronizados, alpha=0.6)
    axes[0].axhline(y=3, color='red', linestyle='--', label='¬±3œÉ')
    axes[0].axhline(y=-3, color='red', linestyle='--')
    axes[0].set_xlabel('√çndice')
    axes[0].set_ylabel('Res√≠duos Padronizados')
    axes[0].set_title('Res√≠duos Padronizados')
    axes[0].legend()
    
    # Dist√¢ncia de Cook
    axes[1].scatter(range(len(cook_distance)), cook_distance, alpha=0.6)
    axes[1].axhline(y=4/n, color='red', linestyle='--', label=f'4/n = {4/n:.4f}')
    axes[1].set_xlabel('√çndice')
    axes[1].set_ylabel('Dist√¢ncia de Cook')
    axes[1].set_title('Dist√¢ncia de Cook')
    axes[1].legend()
    
    # Leverage
    axes[2].scatter(range(len(leverage)), leverage, alpha=0.6)
    axes[2].axhline(y=2*p/n, color='red', linestyle='--', label=f'2p/n = {2*p/n:.4f}')
    axes[2].set_xlabel('√çndice')
    axes[2].set_ylabel('Leverage')
    axes[2].set_title('Leverage')
    axes[2].legend()
    
    plt.tight_layout()
    plt.show()
    
    return {
        'outliers_residuos': np.where(outliers_residuos)[0],
        'outliers_cook': np.where(outliers_cook)[0],
        'outliers_leverage': np.where(outliers_leverage)[0],
        'cook_distance': cook_distance,
        'leverage': leverage,
        'residuos_padronizados': residuos_padronizados
    }
```

---

## üè† Casos Pr√°ticos

### Projeto 1: Predi√ß√£o de Pre√ßos de Im√≥veis

```python
# Dataset: California Housing
from sklearn.datasets import fetch_california_housing
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import pandas as pd

# Carregando dados
housing = fetch_california_housing()
X = pd.DataFrame(housing.data, columns=housing.feature_names)
y = housing.target

print("Features dispon√≠veis:")
for i, feature in enumerate(housing.feature_names):
    print(f"{i+1}. {feature}: {housing.feature_names[i]}")

# An√°lise explorat√≥ria
print(f"\nShape dos dados: {X.shape}")
print(f"Target (pre√ßo m√©dio): min={y.min():.2f}, max={y.max():.2f}, m√©dia={y.mean():.2f}")

# Dividindo dados
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Comparando diferentes modelos
modelos = {
    'Linear': LinearRegression(),
    'Ridge': Ridge(alpha=1.0),
    'Lasso': Lasso(alpha=0.1),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)
}

resultados = {}

for nome, modelo in modelos.items():
    # Treinamento
    modelo.fit(X_train, y_train)
    
    # Predi√ß√µes
    y_pred = modelo.predict(X_test)
    
    # Avalia√ß√£o

    resultados[nome] = avaliar_modelo(y_test, y_pred, nome)
    print()

# Compara√ß√£o visual
import matplotlib.pyplot as plt

metricas = ['MSE', 'RMSE', 'MAE', 'R2']
fig, axes = plt.subplots(2, 2, figsize=(12, 8))

for i, metrica in enumerate(metricas):
    ax = axes[i//2, i%2]
    valores = [resultados[modelo][metrica] for modelo in modelos.keys()]
    ax.bar(modelos.keys(), valores)
    ax.set_title(f'{metrica}')
    ax.tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()
```

### Projeto 2: An√°lise de Tend√™ncias Temporais

```python
# Exemplo com dados sint√©ticos de vendas
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

# Gerando dados temporais
np.random.seed(42)
dates = pd.date_range('2020-01-01', periods=365*3, freq='D')
trend = np.linspace(100, 200, len(dates))
seasonality = 20 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25)
noise = np.random.normal(0, 10, len(dates))
sales = trend + seasonality + noise

# Criando DataFrame
df_temporal = pd.DataFrame({
    'date': dates,
    'sales': sales
})

# Features temporais
df_temporal['year'] = df_temporal['date'].dt.year
df_temporal['month'] = df_temporal['date'].dt.month
df_temporal['day_of_year'] = df_temporal['date'].dt.dayofyear
df_temporal['day_of_week'] = df_temporal['date'].dt.dayofweek

# Regress√£o com features temporais
X_temp = df_temporal[['year', 'month', 'day_of_year', 'day_of_week']]
y_temp = df_temporal['sales']

# Divis√£o temporal (importante para s√©ries temporais)
split_date = '2022-06-01'
train_mask = df_temporal['date'] < split_date
test_mask = df_temporal['date'] >= split_date

X_train_temp = X_temp[train_mask]
X_test_temp = X_temp[test_mask]
y_train_temp = y_temp[train_mask]
y_test_temp = y_temp[test_mask]

# Modelo
modelo_temporal = LinearRegression()
modelo_temporal.fit(X_train_temp, y_train_temp)
y_pred_temp = modelo_temporal.predict(X_test_temp)

# Visualiza√ß√£o
plt.figure(figsize=(15, 6))
plt.plot(df_temporal['date'][train_mask], y_train_temp, label='Treino', alpha=0.7)
plt.plot(df_temporal['date'][test_mask], y_test_temp, label='Teste (Real)', alpha=0.7)
plt.plot(df_temporal['date'][test_mask], y_pred_temp, label='Predi√ß√£o', alpha=0.7)
plt.axvline(x=pd.to_datetime(split_date), color='red', linestyle='--', label='Divis√£o')
plt.xlabel('Data')
plt.ylabel('Vendas')
plt.title('Predi√ß√£o de Vendas Temporais')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Avalia√ß√£o
avaliar_modelo(y_test_temp, y_pred_temp, "Modelo Temporal")
```

---

## üéØ Exerc√≠cios e Projetos

### üìù Exerc√≠cio 1: Implementa√ß√£o Manual

**Objetivo**: Implementar regress√£o linear simples do zero.

```python
def regressao_linear_manual(X, y):
    """
    Implementa regress√£o linear simples manualmente
    """
    # Adicionando coluna de intercepto
    n = len(X)
    X_with_intercept = np.column_stack([np.ones(n), X])
    
    # F√≥rmula matricial: Œ≤ = (X'X)^(-1)X'y
    beta = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y
    
    return beta

# Teste com dados sint√©ticos
np.random.seed(42)
X_simples = np.random.randn(100, 1)
y_simples = 2 + 3 * X_simples.ravel() + np.random.randn(100)

# Implementa√ß√£o manual
beta_manual = regressao_linear_manual(X_simples, y_simples)
print(f"Coeficientes manuais: intercepto={beta_manual[0]:.4f}, slope={beta_manual[1]:.4f}")

# Compara√ß√£o com sklearn
modelo_sklearn = LinearRegression()
modelo_sklearn.fit(X_simples, y_simples)
print(f"Coeficientes sklearn: intercepto={modelo_sklearn.intercept_:.4f}, slope={modelo_sklearn.coef_[0]:.4f}")
```

### üìä Exerc√≠cio 2: An√°lise Completa

**Objetivo**: Realizar an√°lise completa de um dataset real.

**Etapas**:
1. Carregar dataset (sugest√£o: Boston Housing, Wine Quality, ou Auto MPG)
2. An√°lise explorat√≥ria completa
3. Tratamento de dados faltantes e outliers
4. Sele√ß√£o de features
5. Compara√ß√£o de m√∫ltiplos modelos
6. Valida√ß√£o cruzada
7. Interpreta√ß√£o dos resultados

### üèÜ Projeto Final: Sistema de Predi√ß√£o

**Objetivo**: Criar um sistema completo de predi√ß√£o.

**Requisitos**:
- Interface amig√°vel (Streamlit ou Gradio)
- M√∫ltiplos modelos
- Visualiza√ß√µes interativas
- Explicabilidade (SHAP ou LIME)
- Deploy (opcional)

---

## üìö Recursos Adicionais

### üìñ Bibliografia Recomendada

1. **"Introduction to Statistical Learning"** - James, Witten, Hastie, Tibshirani
2. **"Pattern Recognition and Machine Learning"** - Christopher Bishop
3. **"The Elements of Statistical Learning"** - Hastie, Tibshirani, Friedman

### üåê Links √öteis

- [Scikit-learn Documentation](https://scikit-learn.org/stable/)
- [Statsmodels Documentation](https://www.statsmodels.org/)
- [Kaggle Learn - Machine Learning](https://www.kaggle.com/learn/machine-learning)

### üîß Ferramentas Complementares

- **Statsmodels**: An√°lises estat√≠sticas detalhadas
- **XGBoost/LightGBM**: Gradient boosting
- **SHAP**: Explicabilidade de modelos
- **Plotly**: Visualiza√ß√µes interativas

---

## üéØ Checklist de Aprendizagem

Marque conforme avan√ßa nos estudos:

**Conceitos Fundamentais**
- [ ] Diferen√ßa entre classifica√ß√£o e regress√£o
- [ ] Pressupostos da regress√£o linear
- [ ] M√©todo dos m√≠nimos quadrados

**Implementa√ß√£o Pr√°tica**
- [ ] Implementar regress√£o linear do zero
- [ ] Usar scikit-learn para modelos
- [ ] Criar pipeline completo

**Avalia√ß√£o de Modelos**
- [ ] Calcular e interpretar m√©tricas
- [ ] Realizar valida√ß√£o cruzada
- [ ] Diagnosticar problemas

**T√©cnicas Avan√ßadas**
- [ ] Regress√£o polinomial
- [ ] Regulariza√ß√£o (Ridge, Lasso, Elastic Net)
- [ ] Sele√ß√£o de features

**Projetos**
- [ ] Completar exerc√≠cios pr√°ticos
- [ ] Desenvolver projeto final
- [ ] Apresentar resultados

---

## üí° Dicas de Estudo

1. **Pratique com dados reais**: Use datasets do Kaggle ou UCI
2. **Visualize sempre**: Gr√°ficos s√£o fundamentais para entendimento
3. **Valide seus modelos**: Nunca confie apenas nas m√©tricas de treino
4. **Interprete os resultados**: N√∫meros sem contexto n√£o t√™m valor
5. **Documente seu c√≥digo**: Coment√°rios facilitam revis√£o
6. **Experimente diferentes abordagens**: Compare m√∫ltiplos modelos
7. **Mantenha-se atualizado**: Acompanhe novas t√©cnicas e ferramentas

---

*üéì **Conclus√£o**: A regress√£o √© uma ferramenta poderosa e vers√°til em Machine Learning. Dominar seus conceitos e aplica√ß√µes √© fundamental para qualquer cientista de dados. Continue praticando e explorando novos datasets!*

---

**Autor**: Material Did√°tico de Ci√™ncia de Dados  
**Data**: Agosto 2025  
**Vers√£o**: 2.0 - Atualizada e Expandida
