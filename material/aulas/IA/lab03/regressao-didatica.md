# ğŸ“ˆ RegressÃ£o em Machine Learning: Do BÃ¡sico ao PrÃ¡tico

## Notebook DidÃ¡tico para Segunda Aula de ML

Este material foi criado especialmente para alunos que estÃ£o na **segunda aula de Machine Learning**. 

### ğŸ¯ Objetivos
- Entender o que Ã© regressÃ£o e quando usar
- Aprender a diferenÃ§a entre classificaÃ§Ã£o e regressÃ£o  
- Implementar regressÃ£o linear com scikit-learn
- Avaliar modelos de regressÃ£o
- Explorar regressÃ£o polinomial
- Entender overfitting e underfitting

### ğŸ“š PrÃ©-requisitos
- âœ… ProgramaÃ§Ã£o bÃ¡sica em Python
- âœ… Conceitos bÃ¡sicos de pandas e matplotlib
- âœ… Conhecimento da aula anterior sobre ML
- âŒ **NÃƒO precisa** ser expert em estatÃ­stica
- âŒ **NÃƒO precisa** conhecer scikit-learn profundamente

---

## ğŸ” 1. Conceitos Fundamentais

### RegressÃ£o vs ClassificaÃ§Ã£o

| Aspecto | **ClassificaÃ§Ã£o** | **RegressÃ£o** |
|---------|------------------|---------------|
| **O que prediz?** | Categorias/Classes | NÃºmeros contÃ­nuos |
| **Exemplos** | "Spam" ou "NÃ£o-spam" | PreÃ§o: R$ 250.000 |
| **Tipo** | Discreto | ContÃ­nuo |

### Exemplo: PreÃ§o de ImÃ³veis
**Problema**: Dado caracterÃ­sticas de uma casa â†’ Predizer o **preÃ§o**
- âœ… **Ã‰ regressÃ£o** porque queremos um **valor numÃ©rico**
- âŒ **NÃ£o Ã© classificaÃ§Ã£o** porque nÃ£o Ã© categoria

---

## ğŸ“ 2. RegressÃ£o Linear Simples

### MatemÃ¡tica (Simples!)
Lembra da equaÃ§Ã£o da reta?
```
y = a + b Ã— x
```

Em ML vira:
```
PreÃ§o = Î²â‚€ + Î²â‚ Ã— Tamanho
```

Onde:
- **Î²â‚€** = intercepto (onde corta o eixo Y)
- **Î²â‚** = coeficiente angular (inclinaÃ§Ã£o)

### Objetivo
Encontrar os melhores valores de Î²â‚€ e Î²â‚ que fazem a reta passar o mais prÃ³ximo possÃ­vel dos dados.

---

## ğŸ’» 3. ImplementaÃ§Ã£o PrÃ¡tica

### CÃ³digo Completo com ExplicaÃ§Ãµes

```python
# ğŸ“š Importando bibliotecas
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

print("âœ… Bibliotecas importadas!")

# ğŸ  Carregando dados de casas da CalifÃ³rnia
print("ğŸ“¥ Carregando dados...")
housing = fetch_california_housing()
df = pd.DataFrame(housing.data, columns=housing.feature_names)
df['target'] = housing.target

print(f"ğŸ“Š Dataset: {df.shape[0]} linhas, {df.shape[1]} colunas")
print("ğŸ” Primeiras linhas:")
print(df.head())

# ğŸ“– Significado das colunas:
# MedInc: Renda mÃ©dia (em dezenas de milhares)
# HouseAge: Idade mÃ©dia das casas
# AveRooms: NÃºmero mÃ©dio de quartos
# AveBedrms: NÃºmero mÃ©dio de quartos de dormir
# Population: PopulaÃ§Ã£o da Ã¡rea
# AveOccup: OcupaÃ§Ã£o mÃ©dia
# Latitude/Longitude: Coordenadas geogrÃ¡ficas
# target: PREÃ‡O (em centenas de milhares de dÃ³lares)

# ğŸ” InformaÃ§Ãµes bÃ¡sicas
print("\\nğŸ“‹ InformaÃ§Ãµes do dataset:")
print(df.info())
print("\\nğŸ“Š EstatÃ­sticas:")
print(df.describe().round(2))

# ğŸš¨ Verificando dados faltantes
print("\\nğŸ” Dados faltantes:")
print(df.isnull().sum())

# ğŸ“Š VisualizaÃ§Ãµes
plt.figure(figsize=(15, 10))

# Histogramas
for i, col in enumerate(df.columns[:9], 1):
    plt.subplot(3, 3, i)
    plt.hist(df[col], bins=30, alpha=0.7)
    plt.title(f'{col}')
    plt.xlabel(col)
    plt.ylabel('FrequÃªncia')

plt.tight_layout()
plt.suptitle('ğŸ“Š DistribuiÃ§Ã£o das VariÃ¡veis', y=1.02, fontsize=16)
plt.show()

# ğŸ”— Matriz de CorrelaÃ§Ã£o
plt.figure(figsize=(10, 8))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
            square=True, linewidths=0.5)
plt.title('ğŸ”— Matriz de CorrelaÃ§Ã£o', fontsize=16, fontweight='bold')
plt.show()

# ğŸ¯ CorrelaÃ§Ãµes com o target
print("\\nğŸ¯ CorrelaÃ§Ãµes com o preÃ§o (target):")
correlacoes = correlation_matrix['target'].sort_values(ascending=False)
for var, corr in correlacoes.items():
    if var != 'target':
        emoji = "ğŸ“ˆ" if corr > 0 else "ğŸ“‰"
        print(f"{emoji} {var}: {corr:.3f}")

# ğŸ“ˆ Scatter plots das variÃ¡veis mais correlacionadas
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
fig.suptitle('ğŸ“ˆ RelaÃ§Ã£o com o PreÃ§o das Casas', fontsize=16)

# Top correlaÃ§Ãµes positivas e negativas
top_vars = ['MedInc', 'AveRooms', 'HouseAge', 'AveBedrms']

for i, var in enumerate(top_vars):
    row = i // 2
    col = i % 2
    axes[row, col].scatter(df[var], df['target'], alpha=0.5)
    axes[row, col].set_xlabel(var)
    axes[row, col].set_ylabel('PreÃ§o')
    axes[row, col].set_title(f'{var} vs PreÃ§o')

plt.tight_layout()
plt.show()

# ğŸ¯ Preparando dados para o modelo
# Vamos comeÃ§ar simples: usar apenas a renda (MedInc) para predizer preÃ§o
X_simples = df[['MedInc']]  # Feature: Renda
y = df['target']            # Target: PreÃ§o

print(f"\\nğŸ¯ Dados preparados:")
print(f"Features (X): {X_simples.shape}")
print(f"Target (y): {y.shape}")

# âœ‚ï¸ Dividindo dados em treino e teste
X_train, X_test, y_train, y_test = train_test_split(
    X_simples, y, test_size=0.2, random_state=42
)

print(f"\\nâœ‚ï¸ DivisÃ£o dos dados:")
print(f"Treino: {X_train.shape[0]} amostras")
print(f"Teste: {X_test.shape[0]} amostras")

# ğŸ¤– Criando e treinando o modelo
print("\\nğŸ¤– Treinando modelo de regressÃ£o linear...")
modelo = LinearRegression()
modelo.fit(X_train, y_train)

print("âœ… Modelo treinado!")

# ğŸ“Š Coeficientes do modelo
print(f"\\nğŸ“Š Coeficientes do modelo:")
print(f"Intercepto (Î²â‚€): {modelo.intercept_:.4f}")
print(f"Coeficiente (Î²â‚): {modelo.coef_[0]:.4f}")
print(f"\\nğŸ“ EquaÃ§Ã£o: PreÃ§o = {modelo.intercept_:.4f} + {modelo.coef_[0]:.4f} Ã— Renda")

# ğŸ”® Fazendo prediÃ§Ãµes
y_pred = modelo.predict(X_test)

print(f"\\nğŸ”® Exemplos de prediÃ§Ãµes:")
for i in range(5):
    renda = X_test.iloc[i, 0]
    real = y_test.iloc[i]
    predito = y_pred[i]
    print(f"Renda: {renda:.2f} â†’ Real: ${real:.1f}k, Predito: ${predito:.1f}k")

# ğŸ“ Avaliando o modelo
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"\\nğŸ“ MÃ©tricas de AvaliaÃ§Ã£o:")
print(f"MSE (Erro QuadrÃ¡tico MÃ©dio): {mse:.4f}")
print(f"RMSE (Raiz do MSE): {rmse:.4f}")
print(f"MAE (Erro Absoluto MÃ©dio): {mae:.4f}")
print(f"RÂ² (Coeficiente de DeterminaÃ§Ã£o): {r2:.4f}")

print(f"\\nğŸ’¡ InterpretaÃ§Ã£o do RÂ²:")
print(f"Nosso modelo explica {r2*100:.1f}% da variaÃ§Ã£o nos preÃ§os")

# ğŸ“ˆ Visualizando resultados
plt.figure(figsize=(12, 5))

# GrÃ¡fico 1: Reta de regressÃ£o
plt.subplot(1, 2, 1)
plt.scatter(X_test, y_test, alpha=0.5, label='Dados reais')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='RegressÃ£o Linear')
plt.xlabel('Renda MÃ©dia')
plt.ylabel('PreÃ§o das Casas')
plt.title('ğŸ“ˆ RegressÃ£o Linear')
plt.legend()

# GrÃ¡fico 2: Predito vs Real
plt.subplot(1, 2, 2)
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)
plt.xlabel('PreÃ§o Real')
plt.ylabel('PreÃ§o Predito')
plt.title('ğŸ“Š Real vs Predito')

plt.tight_layout()
plt.show()

# ğŸŒŸ Melhorando o modelo: usando mÃºltiplas features
print("\\nğŸŒŸ Vamos melhorar! Usando mÃºltiplas variÃ¡veis...")

# Selecionando as features mais correlacionadas
features_importantes = ['MedInc', 'AveRooms', 'HouseAge', 'AveBedrms']
X_multiplo = df[features_importantes]

# Dividindo novamente
X_train_mult, X_test_mult, y_train_mult, y_test_mult = train_test_split(
    X_multiplo, y, test_size=0.2, random_state=42
)

# Treinando modelo mÃºltiplo
modelo_mult = LinearRegression()
modelo_mult.fit(X_train_mult, y_train_mult)

# PrediÃ§Ãµes
y_pred_mult = modelo_mult.predict(X_test_mult)

# AvaliaÃ§Ã£o
r2_mult = r2_score(y_test_mult, y_pred_mult)
rmse_mult = np.sqrt(mean_squared_error(y_test_mult, y_pred_mult))

print(f"\\nğŸ“Š ComparaÃ§Ã£o dos modelos:")
print(f"Modelo Simples (1 variÃ¡vel):")
print(f"  RÂ²: {r2:.4f} | RMSE: {rmse:.4f}")
print(f"Modelo MÃºltiplo (4 variÃ¡veis):")
print(f"  RÂ²: {r2_mult:.4f} | RMSE: {rmse_mult:.4f}")

melhoria_r2 = ((r2_mult - r2) / r2) * 100
print(f"\\nğŸš€ Melhoria no RÂ²: {melhoria_r2:.1f}%")

# ğŸ“Š ImportÃ¢ncia das features
print(f"\\nğŸ“Š ImportÃ¢ncia das variÃ¡veis (coeficientes):")
for feature, coef in zip(features_importantes, modelo_mult.coef_):
    print(f"{feature}: {coef:.4f}")

# ğŸ”„ RegressÃ£o Polinomial
print("\\nğŸ”„ Explorando RegressÃ£o Polinomial...")
print("(Para capturar relaÃ§Ãµes nÃ£o-lineares)")

# Usando apenas renda para simplicidade
X_poly_base = df[['MedInc']]
y_poly = df['target']

# Testando diferentes graus
graus = [1, 2, 3, 4, 5]
resultados_poly = {}

plt.figure(figsize=(15, 10))

for i, grau in enumerate(graus, 1):
    # TransformaÃ§Ã£o polinomial
    poly_features = PolynomialFeatures(degree=grau)
    X_poly = poly_features.fit_transform(X_poly_base)
    
    # DivisÃ£o treino/teste
    X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(
        X_poly, y_poly, test_size=0.2, random_state=42
    )
    
    # Modelo
    modelo_poly = LinearRegression()
    modelo_poly.fit(X_train_poly, y_train_poly)
    
    # PrediÃ§Ãµes
    y_pred_poly = modelo_poly.predict(X_test_poly)
    
    # MÃ©tricas
    r2_poly = r2_score(y_test_poly, y_pred_poly)
    rmse_poly = np.sqrt(mean_squared_error(y_test_poly, y_pred_poly))
    
    resultados_poly[grau] = {'r2': r2_poly, 'rmse': rmse_poly}
    
    # Plotando
    plt.subplot(2, 3, i)
    
    # Dados para plotar a curva suave
    X_plot = np.linspace(X_poly_base.min().values[0], X_poly_base.max().values[0], 100).reshape(-1, 1)
    X_plot_poly = poly_features.transform(X_plot)
    y_plot = modelo_poly.predict(X_plot_poly)
    
    # Amostra dos dados para nÃ£o sobrecarregar o grÃ¡fico
    sample_idx = np.random.choice(len(X_test_poly), 1000, replace=False)
    X_sample = X_poly_base.iloc[sample_idx]
    y_sample = y_poly.iloc[sample_idx]
    
    plt.scatter(X_sample, y_sample, alpha=0.3, s=10)
    plt.plot(X_plot, y_plot, color='red', linewidth=2)
    plt.title(f'Grau {grau} (RÂ²={r2_poly:.3f})')
    plt.xlabel('Renda')
    plt.ylabel('PreÃ§o')

# GrÃ¡fico de comparaÃ§Ã£o
plt.subplot(2, 3, 6)
graus_list = list(resultados_poly.keys())
r2_list = [resultados_poly[g]['r2'] for g in graus_list]
rmse_list = [resultados_poly[g]['rmse'] for g in graus_list]

plt.plot(graus_list, r2_list, 'o-', label='RÂ²', linewidth=2)
plt.xlabel('Grau do PolinÃ´mio')
plt.ylabel('RÂ²')
plt.title('ğŸ“ˆ Performance vs Complexidade')
plt.grid(True, alpha=0.3)
plt.legend()

plt.tight_layout()
plt.show()

# ğŸ“Š Resultados dos polinÃ´mios
print(f"\\nğŸ“Š Resultados da RegressÃ£o Polinomial:")
for grau, metricas in resultados_poly.items():
    print(f"Grau {grau}: RÂ² = {metricas['r2']:.4f}, RMSE = {metricas['rmse']:.4f}")

# ğŸ¯ ConclusÃµes
print(f"\\nğŸ¯ CONCLUSÃ•ES E APRENDIZADOS:")
print(f"=" * 50)
print(f"1. ğŸ“ˆ RegressÃ£o Linear Ã© Ãºtil para relaÃ§Ãµes lineares")
print(f"2. ğŸ“Š MÃºltiplas variÃ¡veis geralmente melhoram a prediÃ§Ã£o")
print(f"3. ğŸ”„ PolinÃ´mios capturam relaÃ§Ãµes nÃ£o-lineares")
print(f"4. âš ï¸  Graus muito altos podem causar overfitting")
print(f"5. ğŸ“ RÂ² mede o quanto o modelo explica a variaÃ§Ã£o")
print(f"6. ğŸ¯ RMSE dÃ¡ o erro mÃ©dio em unidades originais")

print(f"\\nğŸš€ PrÃ³ximos passos:")
print(f"â€¢ Experimentar outros algoritmos (Random Forest, SVM)")
print(f"â€¢ Usar validaÃ§Ã£o cruzada")
print(f"â€¢ Feature engineering (criar novas variÃ¡veis)")
print(f"â€¢ RegularizaÃ§Ã£o (Ridge, Lasso)")

print(f"\\nâœ… ParabÃ©ns! VocÃª completou sua introduÃ§Ã£o Ã  regressÃ£o!")
```

---

## ğŸ¯ 4. ExercÃ­cios PrÃ¡ticos

### ğŸ“ ExercÃ­cio 1: PrediÃ§Ã£o Simples
1. Use apenas a variÃ¡vel `HouseAge` para predizer preÃ§o
2. Calcule as mÃ©tricas (RÂ², RMSE, MAE)
3. Compare com o modelo que usa `MedInc`

### ğŸ“ ExercÃ­cio 2: MÃºltiplas VariÃ¡veis
1. Crie um modelo usando todas as 8 variÃ¡veis
2. Compare a performance com o modelo de 4 variÃ¡veis
3. Qual teve melhor resultado?

### ğŸ“ ExercÃ­cio 3: RegressÃ£o Polinomial
1. Teste graus 1 a 10 para a variÃ¡vel `AveRooms`
2. Identifique qual grau dÃ¡ melhor RÂ² no teste
3. HÃ¡ sinais de overfitting?

---

## ğŸ“ 5. Conceitos Importantes

### Overfitting vs Underfitting
- **Underfitting**: Modelo muito simples (baixo RÂ² tanto no treino quanto no teste)
- **Overfitting**: Modelo muito complexo (alto RÂ² no treino, baixo no teste)
- **Just Right**: Modelo equilibrado (RÂ² similar no treino e teste)

### MÃ©tricas de AvaliaÃ§Ã£o
- **RÂ²**: % da variaÃ§Ã£o explicada (0 a 1, quanto maior melhor)
- **MSE**: Erro quadrÃ¡tico mÃ©dio (quanto menor melhor)
- **RMSE**: MSE em unidades originais (quanto menor melhor)
- **MAE**: Erro absoluto mÃ©dio (robusto a outliers)

### Quando Usar RegressÃ£o
- âœ… Target Ã© **numÃ©rico contÃ­nuo**
- âœ… RelaÃ§Ã£o linear ou polinomial
- âœ… Dados tÃªm poucos outliers
- âŒ Target Ã© categÃ³rico â†’ use classificaÃ§Ã£o

---

## ğŸ“š 6. PrÃ³ximos Passos

1. **Algoritmos mais avanÃ§ados**: Random Forest, XGBoost
2. **RegularizaÃ§Ã£o**: Ridge, Lasso, Elastic Net
3. **ValidaÃ§Ã£o cruzada**: AvaliaÃ§Ã£o mais robusta
4. **Feature engineering**: Criar variÃ¡veis melhores
5. **Ensemble methods**: Combinar mÃºltiplos modelos

---

**ğŸ‰ ParabÃ©ns!** VocÃª completou sua introduÃ§Ã£o Ã  regressÃ£o em Machine Learning!
