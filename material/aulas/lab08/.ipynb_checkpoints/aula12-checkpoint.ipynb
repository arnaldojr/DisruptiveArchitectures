{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f98ac337",
   "metadata": {},
   "source": [
    "## 2. Redes Neurais\n",
    "\n",
    "### Objetivos\n",
    "\n",
    "  - Conhecer e praticar Redes Neurais Convolucionais\n",
    "  - Conhecer uma intuição sobre Convolução, Pooling \n",
    "  - Praticar a classificação de objeto usando framework TensorFlow\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44177b",
   "metadata": {},
   "source": [
    "## Redes Neurais Convolucionais\n",
    "\n",
    "A Redes Neurais Convolucionais ou CNN (Convolutional Neural Network) ou até mesmo ConvNet, são redes neurais de aprendizado profundo, `Deep Learning` muito utilizadas na área de Visão Computacional `classificação`,`detecção de objetos` ou `segmentação semântica`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af020ebb",
   "metadata": {},
   "source": [
    "### Diferença de MLP para CNN\n",
    "\n",
    "- Em uma rede MLP, cada pixel é tratado de forma isolada, sem considerar os demais pixels, dificultando a caracterização de features mais complexas. Não é levado em consideração se o pixel está na borda ou centro da imagem. Em um CNN o processo de convolução leva em consideração esta condição. \n",
    "\n",
    "- Outro ponto importante está relacionado a quantidade de parâmetros para treinamento para uma imagem. Exemplo: uma imagem de 400x600 na escala de cinza e 100 neurônios na primeira camada. Parâmetros = (400x600*100 +100) = 24.000.100 de parâmetros para treinamento, apenas na primeira camada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e3134",
   "metadata": {},
   "source": [
    "<img src=\"flatten.png\" width=\"400p\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ce171d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 240000)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               24000100  \n",
      "=================================================================\n",
      "Total params: 24,000,100\n",
      "Trainable params: 24,000,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=(600,400)),\n",
    "    layers.Dense(units=100)\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a999696",
   "metadata": {},
   "source": [
    "## Convolução\n",
    "\n",
    "A **convolução**  permite uma **filtragem no domínio espacial**. Esse processo ocorre com a aplicação de **filtros** (pequenas matrizes), posicionadas sob cada pixel da imagem. Estes filtros, normalmente, são chamados de **kernels (ou núcleos)**. O resultado final do valor do pixel é calculado através de um **produto de convolução**.\n",
    "\n",
    "Normalmente os **kernels** são matrizes 3x3. E os pesos são ajustados a cada iteração pelo **backpropagation**\n",
    "\n",
    "\n",
    "<img src=\"same_padding_no_strides.gif\">\n",
    "\n",
    "\n",
    "<img src=\"convolution.png\">\n",
    "\n",
    "\n",
    "<img src=\"conv3d.gif\">\n",
    "\n",
    "\n",
    "<img src=\"convexp.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576fddab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 398, 598, 100)     2800      \n",
      "=================================================================\n",
      "Total params: 2,800\n",
      "Trainable params: 2,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(100, (3, 3), activation='relu', input_shape=(400,600, 3)),\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f65ba",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "A camada de `pooling` é semelhante a camada de convolução, mas a matriz de saída é uma redução da camada de entrada, por consequência a potência computacional diminui, além disso, é nesta etapa que são extraídas as característica `features` mais importantes da imagem. \n",
    "\n",
    "O pooling mais comum é utilizando um kernel 2x2, e um passo `stride` de 2, por consequência a imagem de saída terá a metade da imagem de entrada. A operação de pooling irá selecionar dentro da janela do kernel o valor que será aplicado na próxima camada, pode ser o maior valor `Maxpooling()` ou a média `AveragePooling()`\n",
    "\n",
    "<img src=\"pooling.png\">\n",
    "\n",
    "\n",
    "<img src=\"poolingexp.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66504966",
   "metadata": {},
   "source": [
    "<img src=\"convnet.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50f08d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "326acbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o dataset Fashion Mnist\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "#normaliza os dados para o pixel ficar com valores entre 0 e \n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e7a090d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = train_images.reshape(-1,28,28,1)\n",
    "train_images.shape\n",
    "test_images = test_images.reshape(-1,28,28,1)\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "392e3cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 30)        300       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 30)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 5070)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               507100    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 508,410\n",
      "Trainable params: 508,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    \n",
    "    layers.Conv2D(30, (3,3), activation='relu', input_shape=(28, 28,1)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    " \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f98f1795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2023 - accuracy: 0.9258\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.1744 - accuracy: 0.9350\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.1497 - accuracy: 0.9452\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epochs_hist = model.fit(train_images, train_labels, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8eecc38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "history_df = pd.DataFrame(epochs_hist.history)\n",
    "\n",
    "history_df['loss'].plot();\n",
    "history_df['accuracy'].plot();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbd9f3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 - 7s - loss: 0.1484 - accuracy: 0.9429\n",
      "313/313 - 1s - loss: 0.2869 - accuracy: 0.9024\n"
     ]
    }
   ],
   "source": [
    "#Validadção\n",
    "train_loss, train_acc = model.evaluate(train_images,  train_labels, verbose=2)\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a555abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões com o modelo treinado\n",
    "\n",
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5a63d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classe predita foi 0 com 99%. Classe correta é 6, Shirt.\n"
     ]
    }
   ],
   "source": [
    "#Vericação dos itens preditos\n",
    "\n",
    "item = 4\n",
    "\n",
    "print(\"\\nClasse predita foi {} com {:2.0f}%. Classe correta é {}, {}.\".format(np.argmax(predictions[item]), \n",
    "                                                                 100*np.max(predictions[item]),\n",
    "                                                                 test_labels[item], \n",
    "                                                                 class_names[test_labels[item]]))\n",
    "\n",
    "a=100*np.max(predictions[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f03e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(i, predictions_array, true_label, img):\n",
    "  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "  plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "  if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "  else:\n",
    "    color = 'red'\n",
    "\n",
    "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3029fbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(item, predictions, test_labels, test_images)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f8d85e",
   "metadata": {},
   "source": [
    "## Desafio1\n",
    "\n",
    "Implemente uma rede LeNet-5.\n",
    "\n",
    "A leNet-5 foi publicada por leCun em 1998. E é composta basicamente por:\n",
    "\n",
    "\n",
    "<img src=\"lenet.png\">\n",
    "\n",
    "\n",
    "- Convolutional Layers (CONV);\n",
    "- Pooling Layers (POOL);\n",
    "- Fully-Connected Layers (FC).\n",
    "\n",
    "\n",
    "Um exemplo de aplicação: https://github.com/gary30404/convolutional-neural-network-from-scratch-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc848f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e7c34f9",
   "metadata": {},
   "source": [
    "## Desafio 2 \n",
    "\n",
    "Comente quais alterações você faria na rede LeNet-5?\n",
    "\n",
    "Resposta: \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c63d49",
   "metadata": {},
   "source": [
    "## Desafio 3 \n",
    "\n",
    "Implemente a rede AlexNet:\n",
    "\n",
    "\n",
    "<img src=\"AlexNet-1.png\">\n",
    "\n",
    "\n",
    "paper: https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba753ba0",
   "metadata": {},
   "source": [
    "![](img/AlexNet-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ffb1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implemente sua resposta\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8d68938bd6f1c8d824a292cb48fdc812f23ce0d2e12c844cec6ac89d2f668725"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
