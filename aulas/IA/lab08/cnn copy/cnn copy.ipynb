{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnaldojr/DisruptiveArchitectures/blob/master/material/aulas/IA/lab08/cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "345d3fd2",
      "metadata": {},
      "source": [
        "# Laboratório: Redes Neurais Convolucionais (CNNs) - Do Básico ao Avançado\n",
        "\n",
        "## 🎯 Objetivos de Aprendizagem\n",
        "\n",
        "Ao final desta aula, você será capaz de:\n",
        "\n",
        "1. **Compreender** os fundamentos matemáticos das CNNs\n",
        "2. **Implementar** redes convolucionais do zero usando TensorFlow/Keras\n",
        "3. **Aplicar** técnicas avançadas como data augmentation e transfer learning\n",
        "4. **Comparar** CNNs com MLPs tradicionais\n",
        "5. **Resolver** problemas reais de visão computacional\n",
        "6. **Otimizar** modelos para diferentes cenários\n",
        "\n",
        "## 📚 Material de Apoio Obrigatório\n",
        "\n",
        "📖 **Leitura complementar**: [cnn_guia_completo.md](./cnn_guia_completo.md) - Guia teórico completo sobre CNNs\n",
        "\n",
        "## 🧠 Por que CNNs são Revolucionárias?\n",
        "\n",
        "### 💡 Problema com MLPs Tradicionais\n",
        "\n",
        "Imagine processar uma imagem **400×600 pixels** com um MLP:\n",
        "- **Parâmetros**: 400 × 600 × 100 + 100 = **24.000.100** parâmetros só na primeira camada!\n",
        "- **Problemas**: \n",
        "  - 🚫 Ignora estrutura espacial\n",
        "  - 🚫 Sensível à posição\n",
        "  - 🚫 Computacionalmente caro\n",
        "  - 🚫 Overfitting garantido\n",
        "\n",
        "### ✨ Solução das CNNs\n",
        "\n",
        "- ✅ **Compartilhamento de pesos**: Mesmos filtros em toda imagem\n",
        "- ✅ **Invariância espacial**: Reconhece padrões independente da posição  \n",
        "- ✅ **Hierarquia de features**: Bordas → Formas → Objetos\n",
        "- ✅ **Eficiência**: Drasticamente menos parâmetros\n",
        "\n",
        "### 🔬 Inspiração Biológica\n",
        "\n",
        "As CNNs são inspiradas no **córtex visual** dos mamíferos:\n",
        "\n",
        "```\n",
        "Células simples → Células complexas → Área V1 → V2 → V4 → IT\n",
        "      ↓              ↓              ↓    ↓    ↓    ↓\n",
        "   Bordas         Formas       Texturas → Partes → Objetos\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f98ac337",
      "metadata": {
        "id": "f98ac337"
      },
      "source": [
        "## 🔧 Configuração do Ambiente\n",
        "\n",
        "Vamos começar importando todas as bibliotecas necessárias e configurando o ambiente de desenvolvimento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe5bb14",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importações essenciais\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurações para reprodutibilidade\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configurações de visualização\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# Verificar GPU disponível\n",
        "print(\"🔧 CONFIGURAÇÃO DO AMBIENTE\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"TensorFlow versão: {tf.__version__}\")\n",
        "print(f\"Keras versão: {keras.__version__}\")\n",
        "print(f\"GPUs disponíveis: {len(tf.config.list_physical_devices('GPU'))}\")\n",
        "\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"✅ GPU detectada e configurada!\")\n",
        "    for gpu in tf.config.list_physical_devices('GPU'):\n",
        "        print(f\"   📱 {gpu}\")\n",
        "else:\n",
        "    print(\"⚠️ Executando em CPU - considere usar GPU para melhor performance\")\n",
        "\n",
        "print(\"\\n🚀 Ambiente configurado com sucesso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad44177b",
      "metadata": {
        "id": "ad44177b"
      },
      "source": [
        "## 📊 Parte 1: CNN vs MLP - Comparação Prática\n",
        "\n",
        "Vamos demonstrar **por que CNNs são superiores** para dados de imagem através de uma comparação direta.\n",
        "\n",
        "### 🔍 Análise de Parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6f015b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstração: MLP vs CNN - Número de Parâmetros\n",
        "print(\"🔢 COMPARAÇÃO: MLP vs CNN - NÚMERO DE PARÂMETROS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Definindo dimensões de uma imagem típica\n",
        "altura, largura, canais = 400, 600, 3\n",
        "total_pixels = altura * largura * canais\n",
        "\n",
        "print(f\"📐 Imagem exemplo: {altura}×{largura}×{canais} = {total_pixels:,} pixels\")\n",
        "\n",
        "# MLP Tradicional\n",
        "print(f\"\\n🧠 MLP TRADICIONAL:\")\n",
        "print(f\"   Entrada: {total_pixels:,} pixels (flattened)\")\n",
        "print(f\"   Primeira camada: 100 neurônios\")\n",
        "parametros_mlp = total_pixels * 100 + 100  # pesos + bias\n",
        "print(f\"   Parâmetros primeira camada: {parametros_mlp:,}\")\n",
        "print(f\"   💾 Memória aproximada: {parametros_mlp * 4 / 1024**2:.2f} MB\")\n",
        "\n",
        "# CNN Equivalente\n",
        "print(f\"\\n🔍 CNN EQUIVALENTE:\")\n",
        "print(f\"   Entrada: {altura}×{largura}×{canais}\")\n",
        "print(f\"   Conv2D: 32 filtros 3×3\")\n",
        "parametros_cnn = (3 * 3 * canais * 32) + 32  # kernel_size × input_channels × filters + bias\n",
        "print(f\"   Parâmetros primeira camada: {parametros_cnn:,}\")\n",
        "print(f\"   💾 Memória aproximada: {parametros_cnn * 4 / 1024**2:.4f} MB\")\n",
        "\n",
        "print(f\"\\n📊 COMPARAÇÃO:\")\n",
        "reducao = parametros_mlp / parametros_cnn\n",
        "print(f\"   🔻 Redução de parâmetros: {reducao:.0f}x\")\n",
        "print(f\"   💰 Economia de memória: {(1 - parametros_cnn/parametros_mlp)*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1797784",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando os modelos para comparação visual\n",
        "print(\"🏗️ CONSTRUINDO MODELOS PARA COMPARAÇÃO\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Modelo MLP (simplificado para demonstração)\n",
        "mlp_model = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28, 1)),  # MNIST para exemplo prático\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "], name=\"MLP_Model\")\n",
        "\n",
        "# Modelo CNN equivalente\n",
        "cnn_model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "], name=\"CNN_Model\")\n",
        "\n",
        "print(\"📋 RESUMO DOS MODELOS:\")\n",
        "print(\"\\n🧠 MLP Model:\")\n",
        "mlp_model.summary()\n",
        "\n",
        "print(\"\\n🔍 CNN Model:\")\n",
        "cnn_model.summary()\n",
        "\n",
        "# Comparação de parâmetros\n",
        "mlp_params = mlp_model.count_params()\n",
        "cnn_params = cnn_model.count_params()\n",
        "\n",
        "print(f\"\\n📊 COMPARAÇÃO FINAL:\")\n",
        "print(f\"   MLP parâmetros: {mlp_params:,}\")\n",
        "print(f\"   CNN parâmetros: {cnn_params:,}\")\n",
        "print(f\"   CNN é {mlp_params/cnn_params:.1f}x mais eficiente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe453c62",
      "metadata": {},
      "source": [
        "### 🎨 Visualizando a Diferença Estrutural\n",
        "\n",
        "<img src=\"https://github.com/arnaldojr/DisruptiveArchitectures/blob/master/material/aulas/lab08/flatten.png?raw=1\" width=\"600px\">\n",
        "\n",
        "**Problemas do MLP para Imagens:**\n",
        "- 🚫 **Perda de estrutura espacial**: Pixels são tratados independentemente\n",
        "- 🚫 **Invariância limitada**: Sensível à posição dos objetos\n",
        "- 🚫 **Explosão de parâmetros**: Cresce exponencialmente com o tamanho da imagem\n",
        "- 🚫 **Overfitting**: Muitos parâmetros para poucos dados\n",
        "\n",
        "**Vantagens da CNN:**\n",
        "- ✅ **Preserva estrutura espacial**: Convoluções mantêm relações espaciais\n",
        "- ✅ **Compartilhamento de pesos**: Mesmo filtro detecta padrão em qualquer posição\n",
        "- ✅ **Hierarquia de features**: Aprende características progressivamente\n",
        "- ✅ **Eficiência computacional**: Muito menos parâmetros"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99b7156a",
      "metadata": {},
      "source": [
        "## 🔬 Parte 2: Operação de Convolução - O Coração das CNNs\n",
        "\n",
        "### 🧮 Fundamentos Matemáticos\n",
        "\n",
        "A **convolução** é uma operação matemática fundamental que permite **filtragem no domínio espacial**. É aplicada através de **filtros/kernels** que \"varrem\" a imagem para detectar padrões específicos.\n",
        "\n",
        "#### 📐 Fórmula da Convolução 2D:\n",
        "```\n",
        "S(i,j) = (I * K)(i,j) = ΣΣ I(i+m, j+n) × K(m,n)\n",
        "                        m n\n",
        "```\n",
        "\n",
        "Onde:\n",
        "- `I`: Imagem de entrada\n",
        "- `K`: Kernel/filtro  \n",
        "- `S`: Feature map (resultado)\n",
        "\n",
        "### 🎯 Como Funciona a Convolução\n",
        "\n",
        "#### 1️⃣ **Kernel percorre a imagem**\n",
        "<img src=\"https://github.com/arnaldojr/DisruptiveArchitectures/blob/master/material/aulas/lab08/same_padding_no_strides.gif?raw=1\" width=\"400px\">\n",
        "\n",
        "*O kernel (cinza) varre a imagem (azul) produzindo o feature map (verde)*\n",
        "\n",
        "#### 2️⃣ **Operação em cada posição**\n",
        "<img src=\"https://github.com/arnaldojr/DisruptiveArchitectures/blob/master/material/aulas/lab08/convolution.png?raw=1\" width=\"500px\">\n",
        "\n",
        "*Produto elemento a elemento + soma = valor do pixel no feature map*\n",
        "\n",
        "#### 3️⃣ **Resultado para cada pixel**\n",
        "<img src=\"https://github.com/arnaldojr/DisruptiveArchitectures/blob/master/material/aulas/lab08/conv3d.gif?raw=1\" width=\"400px\">\n",
        "\n",
        "*Visualização 3D da operação de convolução*\n",
        "\n",
        "#### 4️⃣ **Resultado final na imagem**\n",
        "<img src=\"https://github.com/arnaldojr/DisruptiveArchitectures/blob/master/material/aulas/lab08/convexp.png?raw=1\" width=\"600px\">\n",
        "\n",
        "*Diferentes kernels detectam diferentes características*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cde19410",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstração prática da operação de convolução\n",
        "print(\"🔬 DEMONSTRAÇÃO PRÁTICA: OPERAÇÃO DE CONVOLUÇÃO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Criando uma imagem simples para demonstração\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Imagem 5x5 simples\n",
        "imagem = np.array([\n",
        "    [1, 2, 3, 0, 1],\n",
        "    [0, 1, 2, 3, 1], \n",
        "    [1, 0, 1, 2, 0],\n",
        "    [2, 1, 0, 1, 2],\n",
        "    [1, 0, 2, 1, 0]\n",
        "])\n",
        "\n",
        "# Kernel detector de borda vertical\n",
        "kernel_vertical = np.array([\n",
        "    [-1, 0, 1],\n",
        "    [-1, 0, 1],\n",
        "    [-1, 0, 1]\n",
        "])\n",
        "\n",
        "# Kernel detector de borda horizontal  \n",
        "kernel_horizontal = np.array([\n",
        "    [-1, -1, -1],\n",
        "    [ 0,  0,  0],\n",
        "    [ 1,  1,  1]\n",
        "])\n",
        "\n",
        "# Kernel detector de borda (Laplaciano)\n",
        "kernel_borda = np.array([\n",
        "    [-1, -1, -1],\n",
        "    [-1,  8, -1],\n",
        "    [-1, -1, -1]\n",
        "])\n",
        "\n",
        "def aplicar_convolucao_manual(imagem, kernel):\n",
        "    \"\"\"Aplica convolução manualmente para demonstração\"\"\"\n",
        "    img_h, img_w = imagem.shape\n",
        "    kernel_h, kernel_w = kernel.shape\n",
        "    \n",
        "    # Tamanho da saída\n",
        "    out_h = img_h - kernel_h + 1\n",
        "    out_w = img_w - kernel_w + 1\n",
        "    \n",
        "    resultado = np.zeros((out_h, out_w))\n",
        "    \n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            # Região da imagem\n",
        "            regiao = imagem[i:i+kernel_h, j:j+kernel_w]\n",
        "            # Produto elemento a elemento + soma\n",
        "            resultado[i, j] = np.sum(regiao * kernel)\n",
        "    \n",
        "    return resultado\n",
        "\n",
        "# Aplicando diferentes kernels\n",
        "resultado_vertical = aplicar_convolucao_manual(imagem, kernel_vertical)\n",
        "resultado_horizontal = aplicar_convolucao_manual(imagem, kernel_horizontal)\n",
        "resultado_borda = aplicar_convolucao_manual(imagem, kernel_borda)\n",
        "\n",
        "# Visualização\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "\n",
        "# Imagem original\n",
        "im1 = axes[0,0].imshow(imagem, cmap='gray')\n",
        "axes[0,0].set_title('🖼️ Imagem Original')\n",
        "axes[0,0].set_xticks([])\n",
        "axes[0,0].set_yticks([])\n",
        "\n",
        "# Kernels\n",
        "im2 = axes[0,1].imshow(kernel_vertical, cmap='RdBu')\n",
        "axes[0,1].set_title('🔍 Kernel Vertical')\n",
        "axes[0,1].set_xticks([])\n",
        "axes[0,1].set_yticks([])\n",
        "\n",
        "im3 = axes[0,2].imshow(kernel_horizontal, cmap='RdBu')\n",
        "axes[0,2].set_title('🔍 Kernel Horizontal')\n",
        "axes[0,2].set_xticks([])\n",
        "axes[0,2].set_yticks([])\n",
        "\n",
        "im4 = axes[0,3].imshow(kernel_borda, cmap='RdBu')\n",
        "axes[0,3].set_title('🔍 Kernel Borda')\n",
        "axes[0,3].set_xticks([])\n",
        "axes[0,3].set_yticks([])\n",
        "\n",
        "# Resultados\n",
        "axes[1,0].text(0.5, 0.5, 'Resultados →', ha='center', va='center', \n",
        "               transform=axes[1,0].transAxes, fontsize=12, fontweight='bold')\n",
        "axes[1,0].set_xticks([])\n",
        "axes[1,0].set_yticks([])\n",
        "\n",
        "im5 = axes[1,1].imshow(resultado_vertical, cmap='RdBu')\n",
        "axes[1,1].set_title('📊 Bordas Verticais')\n",
        "axes[1,1].set_xticks([])\n",
        "axes[1,1].set_yticks([])\n",
        "\n",
        "im6 = axes[1,2].imshow(resultado_horizontal, cmap='RdBu')\n",
        "axes[1,2].set_title('📊 Bordas Horizontais') \n",
        "axes[1,2].set_xticks([])\n",
        "axes[1,2].set_yticks([])\n",
        "\n",
        "im7 = axes[1,3].imshow(resultado_borda, cmap='RdBu')\n",
        "axes[1,3].set_title('📊 Todas as Bordas')\n",
        "axes[1,3].set_xticks([])\n",
        "axes[1,3].set_yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('🔬 Demonstração: Diferentes Kernels = Diferentes Características', \n",
        "             fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Observe como cada kernel detecta características diferentes!\")\n",
        "print(\"📝 Os valores nos feature maps indicam a 'força' da característica detectada\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6276477",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo detalhado: calculando um pixel manualmente\n",
        "print(\"🧮 EXEMPLO DETALHADO: CALCULANDO UM PIXEL\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"🖼️ Imagem (região 3×3):\")\n",
        "regiao = imagem[0:3, 0:3]\n",
        "print(regiao)\n",
        "\n",
        "print(\"\\n🔍 Kernel (detector de borda):\")\n",
        "print(kernel_borda)\n",
        "\n",
        "print(\"\\n🧮 Cálculo passo a passo:\")\n",
        "print(\"Produto elemento a elemento:\")\n",
        "produto = regiao * kernel_borda\n",
        "print(produto)\n",
        "\n",
        "print(f\"\\n➕ Soma de todos os elementos: {np.sum(produto)}\")\n",
        "print(f\"📊 Resultado para o pixel (0,0) do feature map: {np.sum(produto)}\")\n",
        "\n",
        "print(\"\\n💡 Interpretação:\")\n",
        "if np.sum(produto) > 0:\n",
        "    print(\"   ✅ Valor positivo → Borda detectada!\")\n",
        "else:\n",
        "    print(\"   ❌ Valor baixo → Sem borda significativa\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "409d0366",
      "metadata": {},
      "source": [
        "## 💻 Implementação em Código - TensorFlow/Keras\n",
        "\n",
        "### 🔧 Camada Convolucional\n",
        "\n",
        "A implementação de uma camada convolucional é surpreendentemente simples:\n",
        "\n",
        "```python\n",
        "layers.Conv2D(filters=100, kernel_size=(3, 3), activation='relu', input_shape=(height, width, channels))\n",
        "```\n",
        "\n",
        "**Parâmetros principais:**\n",
        "- 🔢 **filters**: Número de filtros (kernels) - define quantos feature maps são gerados\n",
        "- 📐 **kernel_size**: Tamanho do filtro - (3,3) é mais comum\n",
        "- ⚡ **activation**: Função de ativação aplicada após convolução\n",
        "- 📊 **input_shape**: Formato da entrada (apenas na primeira camada)\n",
        "\n",
        "**Parâmetros avançados:**\n",
        "- 👣 **strides**: Passo do filtro (default: (1,1))\n",
        "- 🎯 **padding**: 'valid' (sem padding) ou 'same' (mantém dimensão)\n",
        "- 🔄 **dilation_rate**: Convoluções dilatadas para campo receptivo maior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b98d0bcc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo prático: implementando camada convolucional\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(\"💻 IMPLEMENTAÇÃO: CAMADA CONVOLUCIONAL\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Criando uma camada convolucional simples\n",
        "model_conv = keras.Sequential([\n",
        "    layers.Conv2D(\n",
        "        filters=100, \n",
        "        kernel_size=(3, 3), \n",
        "        activation='relu', \n",
        "        input_shape=(800, 600, 3),\n",
        "        name=\"conv_layer_1\"\n",
        "    ),\n",
        "])\n",
        "\n",
        "print(\"📋 RESUMO DO MODELO:\")\n",
        "model_conv.summary()\n",
        "\n",
        "print(f\"\\n🔍 ANÁLISE DA CAMADA:\")\n",
        "print(f\"   📐 Entrada: (800, 600, 3)\")\n",
        "print(f\"   🔢 Filtros: 100\")\n",
        "print(f\"   📏 Kernel: 3×3\")\n",
        "print(f\"   📊 Saída: (798, 798, 100)\")  # 800-3+1 = 798\n",
        "print(f\"   🎯 Parâmetros: {(3*3*3 + 1) * 100:,}\")  # (kernel×channels + bias) × filters\n",
        "\n",
        "# Calculando parâmetros manualmente para verificação\n",
        "kernel_params = 3 * 3 * 3  # kernel_size × kernel_size × input_channels\n",
        "bias_params = 1\n",
        "total_per_filter = kernel_params + bias_params\n",
        "total_params = total_per_filter * 100  # × number of filters\n",
        "\n",
        "print(f\"\\n🧮 CÁLCULO DE PARÂMETROS:\")\n",
        "print(f\"   Por filtro: {kernel_params} (pesos) + {bias_params} (bias) = {total_per_filter}\")\n",
        "print(f\"   Total: {total_per_filter} × {100} filtros = {total_params:,} parâmetros\")\n",
        "\n",
        "print(\"\\n💡 OBSERVAÇÕES:\")\n",
        "print(\"   ✅ Cada filtro aprende a detectar uma característica específica\")\n",
        "print(\"   ✅ Mais filtros = mais características detectadas\")\n",
        "print(\"   ✅ Kernel 3×3 é o mais comum (bom balance eficiência/expressividade)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a693e5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo com diferentes configurações\n",
        "print(\"🎛️ EXPLORANDO DIFERENTES CONFIGURAÇÕES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Testando diferentes configurações\n",
        "configs = [\n",
        "    {\"filters\": 32, \"kernel_size\": (3,3), \"nome\": \"Config Básica\"},\n",
        "    {\"filters\": 64, \"kernel_size\": (5,5), \"nome\": \"Kernel Maior\"},\n",
        "    {\"filters\": 128, \"kernel_size\": (1,1), \"nome\": \"Pointwise Conv\"},\n",
        "    {\"filters\": 32, \"kernel_size\": (3,3), \"strides\": (2,2), \"nome\": \"Stride 2\"},\n",
        "]\n",
        "\n",
        "for i, config in enumerate(configs):\n",
        "    print(f\"\\n{i+1}️⃣ {config['nome']}:\")\n",
        "    \n",
        "    # Removendo 'nome' para criar a camada\n",
        "    layer_config = {k: v for k, v in config.items() if k != 'nome'}\n",
        "    \n",
        "    model_temp = keras.Sequential([\n",
        "        layers.Conv2D(**layer_config, input_shape=(64, 64, 3))\n",
        "    ])\n",
        "    \n",
        "    # Análise da configuração\n",
        "    output_shape = model_temp.layers[0].output_shape[1:]  # Remove batch dimension\n",
        "    params = model_temp.count_params()\n",
        "    \n",
        "    print(f\"   📊 Saída: {output_shape}\")\n",
        "    print(f\"   🔢 Parâmetros: {params:,}\")\n",
        "    \n",
        "    # Interpretação\n",
        "    if 'strides' in config and config['strides'] == (2,2):\n",
        "        print(\"   💡 Stride 2 → reduz dimensão pela metade\")\n",
        "    if config['kernel_size'] == (1,1):\n",
        "        print(\"   💡 Kernel 1×1 → combina canais sem considerar vizinhança espacial\")\n",
        "    if config['kernel_size'] == (5,5):\n",
        "        print(\"   💡 Kernel 5×5 → campo receptivo maior, mais contexto\")\n",
        "\n",
        "print(f\"\\n📈 RESUMO:\")\n",
        "print(\"   • Mais filtros → mais características detectadas\")\n",
        "print(\"   • Kernel maior → mais contexto, mais parâmetros\")\n",
        "print(\"   • Stride > 1 → redução de dimensionalidade\")\n",
        "print(\"   • Kernel 1×1 → redução/expansão de canais eficiente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f39d7e9",
      "metadata": {},
      "source": [
        "## 🎯 Desafio 1: Análise de Parâmetros\n",
        "\n",
        "**Pergunta:** Compare a quantidade de `Total params` - em uma rede CNN esse valor é menor ou maior comparado com uma rede MLP?\n",
        "\n",
        "### 💡 Para Responder:\n",
        "1. **Observe** os modelos criados acima\n",
        "2. **Compare** CNN (100 filtros 3×3) vs MLP (entrada flattened)\n",
        "3. **Analise** como o compartilhamento de pesos afeta o total\n",
        "4. **Considere** o que acontece com imagens maiores\n",
        "\n",
        "### 🔍 Experimento Guiado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c628bca",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎯 DESAFIO 1: EXPERIMENTO PRÁTICO\n",
        "print(\"🎯 DESAFIO 1: ANÁLISE COMPARATIVA DE PARÂMETROS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Vamos comparar sistematicamente\n",
        "tamanhos_imagem = [(28, 28), (64, 64), (128, 128), (224, 224)]\n",
        "\n",
        "print(\"📊 COMPARAÇÃO SISTEMÁTICA: MLP vs CNN\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for altura, largura in tamanhos_imagem:\n",
        "    print(f\"\\n🖼️ IMAGEM {altura}×{largura}×3:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # MLP Model\n",
        "    mlp_temp = keras.Sequential([\n",
        "        layers.Flatten(input_shape=(altura, largura, 3)),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    # CNN Model  \n",
        "    cnn_temp = keras.Sequential([\n",
        "        layers.Conv2D(32, (3,3), activation='relu', input_shape=(altura, largura, 3)),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    mlp_params = mlp_temp.count_params()\n",
        "    cnn_params = cnn_temp.count_params()\n",
        "    razao = mlp_params / cnn_params\n",
        "    \n",
        "    print(f\"   🧠 MLP parâmetros: {mlp_params:,}\")\n",
        "    print(f\"   🔍 CNN parâmetros: {cnn_params:,}\")\n",
        "    print(f\"   📈 Razão MLP/CNN: {razao:.1f}x\")\n",
        "    \n",
        "    if razao > 1:\n",
        "        print(f\"   ✅ CNN é {razao:.1f}x mais eficiente!\")\n",
        "    else:\n",
        "        print(f\"   ⚠️ MLP é mais eficiente para esta configuração\")\n",
        "\n",
        "print(f\"\\n🎯 SUA ANÁLISE:\")\n",
        "print(\"=\" * 30)\n",
        "print(\"Com base nos resultados acima, complete:\")\n",
        "print(\"\\n1️⃣ Em geral, CNNs têm _______ parâmetros que MLPs\")\n",
        "print(\"2️⃣ Isso acontece porque CNNs usam _______\")\n",
        "print(\"3️⃣ Quando a imagem fica maior, a diferença _______\")\n",
        "print(\"4️⃣ Para imagens pequenas, a vantagem da CNN _______\")\n",
        "\n",
        "print(f\"\\n💭 REFLEXÃO:\")\n",
        "print(\"Por que a CNN mantém vantagem mesmo com imagens grandes?\")\n",
        "print(\"Sua resposta: ________________________________\")\n",
        "\n",
        "# COMPLETE SUAS RESPOSTAS AQUI:\n",
        "print(f\"\\n📝 SUAS RESPOSTAS:\")\n",
        "resposta_1 = \"______\"  # menor/maior\n",
        "resposta_2 = \"______\"  # compartilhamento de pesos/mais camadas/etc\n",
        "resposta_3 = \"______\"  # aumenta/diminui/mantém\n",
        "resposta_4 = \"______\"  # é maior/é menor/desaparece\n",
        "\n",
        "print(f\"1️⃣ Em geral, CNNs têm {resposta_1} parâmetros que MLPs\")\n",
        "print(f\"2️⃣ Isso acontece porque CNNs usam {resposta_2}\")\n",
        "print(f\"3️⃣ Quando a imagem fica maior, a diferença {resposta_3}\")\n",
        "print(f\"4️⃣ Para imagens pequenas, a vantagem da CNN {resposta_4}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b35d6f5",
      "metadata": {},
      "source": [
        "## 🏊 Parte 3: Pooling - Reduzindo Dimensionalidade com Inteligência\n",
        "\n",
        "### 🎯 O que é Pooling?\n",
        "\n",
        "O **pooling** é uma operação de **subsampling** que:\n",
        "- 📉 **Reduz dimensionalidade** dos feature maps\n",
        "- 🎯 **Mantém características importantes** \n",
        "- ⚡ **Diminui custo computacional**\n",
        "- 🛡️ **Adiciona invariância** a pequenas translações\n",
        "- 🚫 **Reduz overfitting** \n",
        "\n",
        "### 🔍 Tipos de Pooling\n",
        "\n",
        "#### 1️⃣ **Max Pooling** (Mais Comum)\n",
        "- **Operação**: Seleciona o **valor máximo** na janela\n",
        "- **Intuição**: Preserva as características **mais ativas**\n",
        "- **Uso**: Detecção de bordas, texturas\n",
        "\n",
        "#### 2️⃣ **Average Pooling**\n",
        "- **Operação**: Calcula a **média** dos valores na janela\n",
        "- **Intuição**: Suaviza e reduz ruído\n",
        "- **Uso**: Menos comum, algumas arquiteturas específicas\n",
        "\n",
        "#### 3️⃣ **Global Average Pooling**\n",
        "- **Operação**: Média de todo o feature map → 1 valor\n",
        "- **Vantagem**: Substitui camadas Dense finais\n",
        "- **Benefício**: Reduz drasticamente overfitting\n",
        "\n",
        "### 📐 Matemática do Pooling\n",
        "\n",
        "**Max Pooling 2×2:**\n",
        "```\n",
        "Entrada (4×4):           Saída (2×2):\n",
        "┌─────────────┐         ┌─────────┐\n",
        "│1  3  2  4│           │max(1,3,0,1) max(2,4,1,2)│\n",
        "│0  1  1  2│    →      │    = 3         = 4     │\n",
        "│2  2  0  1│           │max(2,2,3,1) max(0,1,3,5)│\n",
        "│3  1  3  5│           │    = 3         = 5     │\n",
        "└─────────────┘         └─────────────────────────┘\n",
        "\n",
        "Resultado: [3, 4]\n",
        "           [3, 5]\n",
        "```\n",
        "\n",
        "### 🖼️ Visualização do Pooling\n",
        "\n",
        "<img src=\"https://github.com/arnaldojr/DisruptiveArchitectures/blob/master/material/aulas/lab08/pooling.png?raw=1\" width=\"500px\">\n",
        "\n",
        "*O pooling 2×2 reduz a dimensionalidade pela metade*\n",
        "\n",
        "### 📊 Resultado Visual\n",
        "\n",
        "<img src=\"https://github.com/arnaldojr/DisruptiveArchitectures/blob/master/material/aulas/lab08/poolingexp1.png?raw=1\" width=\"600px\">\n",
        "\n",
        "*Comparação: imagem original vs após max pooling*\n",
        "\n",
        "### ⚡ Parâmetros do Pooling\n",
        "\n",
        "**Características importantes:**\n",
        "- 🔢 **Zero parâmetros**: Não há pesos para aprender\n",
        "- 🎛️ **Pool size**: Tamanho da janela (geralmente 2×2)\n",
        "- 👣 **Stride**: Passo do deslocamento (geralmente = pool_size)\n",
        "- 📐 **Padding**: Raramente usado em pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "010c5de9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstração prática do pooling\n",
        "print(\"🏊 DEMONSTRAÇÃO PRÁTICA: OPERAÇÕES DE POOLING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Criando uma imagem exemplo para demonstração\n",
        "np.random.seed(42)\n",
        "imagem_exemplo = np.random.randint(0, 10, (8, 8))\n",
        "\n",
        "print(\"🖼️ IMAGEM EXEMPLO (8×8):\")\n",
        "print(imagem_exemplo)\n",
        "\n",
        "def max_pooling_manual(imagem, pool_size=2, stride=2):\n",
        "    \"\"\"Implementação manual do max pooling para demonstração\"\"\"\n",
        "    h, w = imagem.shape\n",
        "    out_h = (h - pool_size) // stride + 1\n",
        "    out_w = (w - pool_size) // stride + 1\n",
        "    \n",
        "    resultado = np.zeros((out_h, out_w))\n",
        "    \n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            h_start = i * stride\n",
        "            h_end = h_start + pool_size\n",
        "            w_start = j * stride  \n",
        "            w_end = w_start + pool_size\n",
        "            \n",
        "            # Max pooling\n",
        "            resultado[i, j] = np.max(imagem[h_start:h_end, w_start:w_end])\n",
        "    \n",
        "    return resultado\n",
        "\n",
        "def avg_pooling_manual(imagem, pool_size=2, stride=2):\n",
        "    \"\"\"Implementação manual do average pooling\"\"\"\n",
        "    h, w = imagem.shape\n",
        "    out_h = (h - pool_size) // stride + 1\n",
        "    out_w = (w - pool_size) // stride + 1\n",
        "    \n",
        "    resultado = np.zeros((out_h, out_w))\n",
        "    \n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            h_start = i * stride\n",
        "            h_end = h_start + pool_size\n",
        "            w_start = j * stride\n",
        "            w_end = w_start + pool_size\n",
        "            \n",
        "            # Average pooling\n",
        "            resultado[i, j] = np.mean(imagem[h_start:h_end, w_start:w_end])\n",
        "    \n",
        "    return resultado\n",
        "\n",
        "# Aplicando diferentes tipos de pooling\n",
        "max_pool_result = max_pooling_manual(imagem_exemplo)\n",
        "avg_pool_result = avg_pooling_manual(imagem_exemplo)\n",
        "\n",
        "# Visualização\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Imagem original\n",
        "im1 = axes[0].imshow(imagem_exemplo, cmap='viridis', interpolation='nearest')\n",
        "axes[0].set_title('🖼️ Imagem Original (8×8)')\n",
        "axes[0].set_xticks(range(8))\n",
        "axes[0].set_yticks(range(8))\n",
        "plt.colorbar(im1, ax=axes[0])\n",
        "\n",
        "# Max pooling\n",
        "im2 = axes[1].imshow(max_pool_result, cmap='viridis', interpolation='nearest')\n",
        "axes[1].set_title('🔥 Max Pooling (4×4)')\n",
        "axes[1].set_xticks(range(4))\n",
        "axes[1].set_yticks(range(4))\n",
        "plt.colorbar(im2, ax=axes[1])\n",
        "\n",
        "# Average pooling\n",
        "im3 = axes[2].imshow(avg_pool_result, cmap='viridis', interpolation='nearest')\n",
        "axes[2].set_title('📊 Average Pooling (4×4)')\n",
        "axes[2].set_xticks(range(4))\n",
        "axes[2].set_yticks(range(4))\n",
        "plt.colorbar(im3, ax=axes[2])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n📐 RESULTADOS:\")\n",
        "print(f\"   Original: {imagem_exemplo.shape}\")\n",
        "print(f\"   Max Pooling: {max_pool_result.shape}\")\n",
        "print(f\"   Average Pooling: {avg_pool_result.shape}\")\n",
        "print(f\"   Redução: {imagem_exemplo.size // max_pool_result.size}x menos pixels\")\n",
        "\n",
        "print(f\"\\n🔍 COMPARAÇÃO DE VALORES:\")\n",
        "print(\"Max Pooling resultado:\")\n",
        "print(max_pool_result.astype(int))\n",
        "print(\"\\nAverage Pooling resultado:\")\n",
        "print(np.round(avg_pool_result, 1))\n",
        "\n",
        "print(\"\\n💡 OBSERVAÇÕES:\")\n",
        "print(\"   ✅ Max pooling preserva características mais 'fortes'\")\n",
        "print(\"   ✅ Average pooling suaviza e reduz ruído\")\n",
        "print(\"   ✅ Ambos reduzem dimensionalidade sem parâmetros treináveis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9c852a8",
      "metadata": {},
      "source": [
        "## Implementação em código\n",
        "\n",
        "Para implementar pooling em Keras é muito simples:\n",
        "\n",
        "```python\n",
        "layers.MaxPool2D(pool_size=2, strides=2)\n",
        "```\n",
        "\n",
        "**Parâmetros principais:**\n",
        "- 🎛️ **pool_size**: Tamanho da janela de pooling (2×2 é padrão)\n",
        "- 👣 **strides**: Passo do deslocamento (geralmente = pool_size)  \n",
        "- 🎯 **padding**: 'valid' (padrão) ou 'same'\n",
        "\n",
        "**Outros tipos:**\n",
        "- `layers.AveragePooling2D()`: Average pooling\n",
        "- `layers.GlobalMaxPooling2D()`: Max pooling global\n",
        "- `layers.GlobalAveragePooling2D()`: Average pooling global"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1edf97e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo completo: Conv2D + MaxPooling2D\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(\"🏗️ EXEMPLO: CONVOLUÇÃO + POOLING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(filters=100, kernel_size=(3, 3), activation='relu', \n",
        "                  input_shape=(800, 600, 3), name=\"conv2d_layer\"),\n",
        "    layers.MaxPool2D(pool_size=2, strides=2, name=\"maxpool_layer\")\n",
        "])\n",
        "\n",
        "print(\"📋 RESUMO DO MODELO:\")\n",
        "model.summary()\n",
        "\n",
        "print(\"\\n🔍 ANÁLISE DAS TRANSFORMAÇÕES:\")\n",
        "print(\"   📐 Entrada: (800, 600, 3)\")\n",
        "print(\"   🔄 Após Conv2D: (798, 798, 100)\")  # 800-3+1 = 798\n",
        "print(\"   🏊 Após MaxPool: (399, 399, 100)\")  # 798/2 = 399\n",
        "print(\"   📉 Redução total: ~4x menos pixels\")\n",
        "\n",
        "print(\"\\n💡 OBSERVAÇÕES IMPORTANTES:\")\n",
        "print(\"   ✅ Pooling NÃO adiciona parâmetros\")\n",
        "print(\"   ✅ Reduz dimensionalidade espacial\") \n",
        "print(\"   ✅ Mantém número de canais\")\n",
        "print(\"   ✅ Diminui custo computacional\")\n",
        "\n",
        "# Comparando com e sem pooling\n",
        "print(\"\\n⚖️ COMPARAÇÃO: COM vs SEM POOLING\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Sem pooling\n",
        "model_sem_pool = keras.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Com pooling  \n",
        "model_com_pool = keras.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "print(f\"Sem pooling: {model_sem_pool.count_params():,} parâmetros\")\n",
        "print(f\"Com pooling: {model_com_pool.count_params():,} parâmetros\")\n",
        "print(f\"Diferença: {abs(model_sem_pool.count_params() - model_com_pool.count_params()):,} parâmetros\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83635f02",
      "metadata": {},
      "source": [
        "## 🎯 Desafio 2: Análise do Pooling\n",
        "\n",
        "**Perguntas:**\n",
        "1. Qual a dimensão da imagem antes e depois do pooling?\n",
        "2. A camada de pooling alterou o `total params`?\n",
        "3. Por que o pooling não tem parâmetros treináveis?\n",
        "\n",
        "### 🔍 Análise Guiada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a3efd66",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎯 DESAFIO 2: ANÁLISE DO POOLING\n",
        "print(\"🎯 DESAFIO 2: ANÁLISE DETALHADA DO POOLING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Vamos analisar passo a passo\n",
        "model_analise = keras.Sequential([\n",
        "    layers.Conv2D(100, (3,3), activation='relu', input_shape=(800, 600, 3)),\n",
        "    layers.MaxPool2D(pool_size=2, strides=2)\n",
        "])\n",
        "\n",
        "print(\"📊 ANÁLISE DAS DIMENSÕES:\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Simulando o forward pass para ver as dimensões\n",
        "import numpy as np\n",
        "input_shape = (1, 800, 600, 3)  # Batch size = 1\n",
        "dummy_input = np.random.random(input_shape)\n",
        "\n",
        "# Passando pela primeira camada (Conv2D)\n",
        "conv_output = model_analise.layers[0](dummy_input)\n",
        "print(f\"🔄 Após Convolução: {conv_output.shape}\")\n",
        "print(f\"   Entrada: (800, 600, 3)\")\n",
        "print(f\"   Saída:   {conv_output.shape[1:]}\") \n",
        "\n",
        "# Passando pela segunda camada (MaxPool2D)  \n",
        "pool_output = model_analise.layers[1](conv_output)\n",
        "print(f\"\\n🏊 Após Max Pooling: {pool_output.shape}\")\n",
        "print(f\"   Entrada: {conv_output.shape[1:]}\")\n",
        "print(f\"   Saída:   {pool_output.shape[1:]}\")\n",
        "\n",
        "# Calculando redução\n",
        "altura_reducao = conv_output.shape[1] / pool_output.shape[1]\n",
        "largura_reducao = conv_output.shape[2] / pool_output.shape[2]\n",
        "total_pixels_antes = conv_output.shape[1] * conv_output.shape[2]\n",
        "total_pixels_depois = pool_output.shape[1] * pool_output.shape[2]\n",
        "reducao_total = total_pixels_antes / total_pixels_depois\n",
        "\n",
        "print(f\"\\n📐 REDUÇÃO DE DIMENSÕES:\")\n",
        "print(f\"   Altura: {altura_reducao:.1f}x menor\")  \n",
        "print(f\"   Largura: {largura_reducao:.1f}x menor\")\n",
        "print(f\"   Total de pixels: {reducao_total:.1f}x redução\")\n",
        "\n",
        "print(f\"\\n🔢 ANÁLISE DE PARÂMETROS:\")\n",
        "print(\"=\" * 30)\n",
        "conv_params = model_analise.layers[0].count_params()\n",
        "pool_params = model_analise.layers[1].count_params()\n",
        "total_params = model_analise.count_params()\n",
        "\n",
        "print(f\"   Conv2D parâmetros: {conv_params:,}\")\n",
        "print(f\"   MaxPool2D parâmetros: {pool_params:,}\")\n",
        "print(f\"   Total parâmetros: {total_params:,}\")\n",
        "\n",
        "print(f\"\\n🎯 SUAS RESPOSTAS:\")\n",
        "print(\"=\" * 20)\n",
        "print(\"1️⃣ Dimensão antes do pooling: ________________\")\n",
        "print(\"2️⃣ Dimensão depois do pooling: _______________\")\n",
        "print(\"3️⃣ Pooling alterou total params? _____________\")\n",
        "print(\"4️⃣ Por que pooling não tem parâmetros? _______\")\n",
        "print(\"   _________________________________________\")\n",
        "\n",
        "print(f\"\\n💡 DICAS PARA RESPONDER:\")\n",
        "print(\"   • Observe os valores impressos acima\")\n",
        "print(\"   • Lembre-se: pooling é uma operação fixa (max ou média)\")\n",
        "print(\"   • Compare com a convolução que TEM parâmetros\")\n",
        "print(\"   • Pense na diferença entre 'operação' e 'aprendizado'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eaf3f14",
      "metadata": {},
      "source": [
        "## 🔍 Parte 4: Arquitetura Completa da CNN - Extrator + Classificador\n",
        "\n",
        "### 🏗️ Estrutura Geral\n",
        "\n",
        "Uma CNN completa é composta por **duas partes principais**:\n",
        "\n",
        "#### 1️⃣ **Extrator de Características** (Feature Extractor)\n",
        "- **Função**: Detectar padrões visuais hierárquicos\n",
        "- **Componentes**: Conv2D + Pooling + Ativação\n",
        "- **Processo**: Bordas → Texturas → Formas → Objetos\n",
        "- **Saída**: Feature maps com características extraídas\n",
        "\n",
        "#### 2️⃣ **Classificador** (Classifier)  \n",
        "- **Função**: Tomar decisão baseada nas características\n",
        "- **Componentes**: Flatten + Dense layers (MLP)\n",
        "- **Processo**: Features → Combinações → Probabilidades\n",
        "- **Saída**: Classificação final\n",
        "\n",
        "### 🎯 Arquitetura Visual\n",
        "\n",
        "<img src=\"https://github.com/arnaldojr/DisruptiveArchitectures/blob/master/material/aulas/lab08/convnet.png?raw=1\" width=\"800px\">\n",
        "\n",
        "### 🔄 Fluxo de Processamento\n",
        "\n",
        "```\n",
        "Imagem → [Conv→ReLU→Pool]×N → Flatten → [Dense→ReLU]×M → Softmax → Classes\n",
        "  ↓             ↓                ↓            ↓              ↓\n",
        " Raw         Features         Vector      Hidden        Probabilities\n",
        "Pixels      Extraction      Formato 1D   Layers        por Classe\n",
        "```\n",
        "\n",
        "### 🧮 Hierarquia de Características\n",
        "\n",
        "| Camada | Detecta | Exemplo |\n",
        "|--------|---------|---------|\n",
        "| **Conv1** | 🔍 Bordas, linhas | `/`, `\\`, `—`, `|` |\n",
        "| **Conv2** | 🔺 Formas simples | Cantos, curvas |\n",
        "| **Conv3** | 🎯 Partes de objetos | Olhos, rodas, janelas |\n",
        "| **Conv4+** | 🖼️ Objetos completos | Faces, carros, casas |\n",
        "\n",
        "### ⚡ Por que essa Divisão?\n",
        "\n",
        "**Extrator (Convolucional):**\n",
        "- ✅ Preserva informação espacial\n",
        "- ✅ Detecta padrões locais\n",
        "- ✅ Invariante à posição\n",
        "- ✅ Compartilha pesos\n",
        "\n",
        "**Classificador (Dense):**\n",
        "- ✅ Combina informação global\n",
        "- ✅ Aprende relações complexas\n",
        "- ✅ Produz probabilidades\n",
        "- ✅ Decisão final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e6cf8c6",
      "metadata": {},
      "source": [
        "## 👗 Parte 5: Exemplo Prático - Fashion MNIST com CNN\n",
        "\n",
        "### 🎯 Por que Fashion MNIST?\n",
        "\n",
        "O **Fashion MNIST** é um dataset ideal para aprender CNNs:\n",
        "\n",
        "- 📊 **Estrutura**: 70.000 imagens 28×28 em escala de cinza\n",
        "- 👕 **Classes**: 10 tipos de roupas e acessórios\n",
        "- 🎓 **Complexidade**: Mais desafiador que dígitos do MNIST tradicional\n",
        "- 📈 **Benchmark**: Padrão da indústria para testes iniciais\n",
        "\n",
        "### 🏷️ Classes do Dataset:\n",
        "\n",
        "| Código | Classe | Emoji | Exemplo |\n",
        "|--------|--------|-------|---------|\n",
        "| 0 | T-shirt/top | 👕 | Camisetas |\n",
        "| 1 | Trouser | 👖 | Calças |\n",
        "| 2 | Pullover | 🧥 | Suéter |\n",
        "| 3 | Dress | 👗 | Vestidos |\n",
        "| 4 | Coat | 🧥 | Casacos |\n",
        "| 5 | Sandal | 👡 | Sandálias |\n",
        "| 6 | Shirt | 👔 | Camisas |\n",
        "| 7 | Sneaker | 👟 | Tênis |\n",
        "| 8 | Bag | 👜 | Bolsas |\n",
        "| 9 | Ankle boot | 👢 | Botas |\n",
        "\n",
        "### 🔄 Nossa Estratégia:\n",
        "\n",
        "1. **Carregar e explorar** os dados\n",
        "2. **Pré-processar** as imagens  \n",
        "3. **Construir CNN** progressivamente\n",
        "4. **Treinar e avaliar** o modelo\n",
        "5. **Visualizar resultados** e interpretar erros\n",
        "6. **Comparar** com MLP tradicional\n",
        "\n",
        "### 💡 O que Esperamos Aprender:\n",
        "\n",
        "- Como CNNs **extraem características** hierárquicas\n",
        "- Diferença de **performance** CNN vs MLP\n",
        "- **Interpretação** dos filtros aprendidos\n",
        "- **Análise de erros** e limitações"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "723cd883",
      "metadata": {},
      "source": [
        "## 👗 Parte 5: Exemplo Prático - Fashion MNIST com CNN\n",
        "\n",
        "### 🎯 Por que Fashion MNIST?\n",
        "\n",
        "O **Fashion MNIST** é um dataset ideal para aprender CNNs:\n",
        "\n",
        "- 📊 **Estrutura**: 70.000 imagens 28×28 em escala de cinza\n",
        "- 👕 **Classes**: 10 tipos de roupas e acessórios\n",
        "- 🎓 **Complexidade**: Mais desafiador que dígitos do MNIST tradicional\n",
        "- 📈 **Benchmark**: Padrão da indústria para testes iniciais\n",
        "\n",
        "### 🏷️ Classes do Dataset:\n",
        "\n",
        "| Código | Classe | Emoji | Exemplo |\n",
        "|--------|--------|-------|---------|\n",
        "| 0 | T-shirt/top | 👕 | Camisetas |\n",
        "| 1 | Trouser | 👖 | Calças |\n",
        "| 2 | Pullover | 🧥 | Suéter |\n",
        "| 3 | Dress | 👗 | Vestidos |\n",
        "| 4 | Coat | 🧥 | Casacos |\n",
        "| 5 | Sandal | 👡 | Sandálias |\n",
        "| 6 | Shirt | 👔 | Camisas |\n",
        "| 7 | Sneaker | 👟 | Tênis |\n",
        "| 8 | Bag | 👜 | Bolsas |\n",
        "| 9 | Ankle boot | 👢 | Botas |\n",
        "\n",
        "### 🔄 Nossa Estratégia:\n",
        "\n",
        "1. **Carregar e explorar** os dados\n",
        "2. **Pré-processar** as imagens  \n",
        "3. **Construir CNN** progressivamente\n",
        "4. **Treinar e avaliar** o modelo\n",
        "5. **Visualizar resultados** e interpretar erros\n",
        "6. **Comparar** com MLP tradicional\n",
        "\n",
        "### 💡 O que Esperamos Aprender:\n",
        "\n",
        "- Como CNNs **extraem características** hierárquicas\n",
        "- Diferença de **performance** CNN vs MLP\n",
        "- **Interpretação** dos filtros aprendidos\n",
        "- **Análise de erros** e limitações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "50f08d5c",
      "metadata": {
        "id": "50f08d5c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd13dd93",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📊 CARREGANDO E EXPLORANDO O FASHION MNIST\n",
        "print(\"📊 CARREGANDO FASHION MNIST\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Importa o dataset Fashion MNIST\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Informações básicas do dataset\n",
        "print(f\"📐 Formato dos dados de treino: {train_images.shape}\")\n",
        "print(f\"📐 Formato dos dados de teste: {test_images.shape}\")\n",
        "print(f\"🏷️ Formato dos rótulos de treino: {train_labels.shape}\")\n",
        "print(f\"🏷️ Formato dos rótulos de teste: {test_labels.shape}\")\n",
        "\n",
        "print(f\"\\n📊 ANÁLISE DOS DADOS:\")\n",
        "print(f\"   Total de imagens de treino: {len(train_images):,}\")\n",
        "print(f\"   Total de imagens de teste: {len(test_images):,}\")\n",
        "print(f\"   Dimensão de cada imagem: {train_images[0].shape}\")\n",
        "print(f\"   Valores dos pixels: {train_images[0].min()} a {train_images[0].max()}\")\n",
        "print(f\"   Número de classes: {len(np.unique(train_labels))}\")\n",
        "\n",
        "# Nomes das classes\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "print(f\"\\n👗 CLASSES DO DATASET:\")\n",
        "for i, nome in enumerate(class_names):\n",
        "    count = np.sum(train_labels == i)\n",
        "    print(f\"   {i}: {nome} ({count:,} imagens)\")\n",
        "\n",
        "# Normalização: fundamental para CNNs!\n",
        "print(f\"\\n🔄 NORMALIZANDO OS DADOS...\")\n",
        "print(\"Por que normalizar?\")\n",
        "print(\"   ✅ Acelera convergência do treinamento\")\n",
        "print(\"   ✅ Evita dominância de pixels com valores altos\")\n",
        "print(\"   ✅ Melhora estabilidade numérica\")\n",
        "print(\"   ✅ Padrão para redes neurais\")\n",
        "\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "print(f\"   Novos valores dos pixels: {train_images[0].min():.1f} a {train_images[0].max():.1f}\")\n",
        "print(\"✅ Normalização concluída!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96298330",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔄 PREPARANDO DADOS PARA CNN\n",
        "print(\"🔄 PREPARANDO FORMATO PARA CNN\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"📐 RESHAPE NECESSÁRIO:\")\n",
        "print(f\"   Formato original: {train_images.shape}\")\n",
        "print(\"   Formato necessário para CNN: (samples, height, width, channels)\")\n",
        "print(\"   \")\n",
        "print(\"   🔍 Por que precisamos de 4 dimensões?\")\n",
        "print(\"      • samples: número de imagens\")\n",
        "print(\"      • height: altura da imagem\")  \n",
        "print(\"      • width: largura da imagem\")\n",
        "print(\"      • channels: canais de cor (1=grayscale, 3=RGB)\")\n",
        "\n",
        "# Reshape adicionando dimensão de canal\n",
        "train_images = train_images.reshape(-1, 28, 28, 1)\n",
        "test_images = test_images.reshape(-1, 28, 28, 1)\n",
        "\n",
        "print(f\"\\n✅ APÓS RESHAPE:\")\n",
        "print(f\"   Dados de treino: {train_images.shape}\")\n",
        "print(f\"   Dados de teste: {test_images.shape}\")\n",
        "print(f\"   \")\n",
        "print(f\"   📊 Interpretação:\")\n",
        "print(f\"      • {train_images.shape[0]:,} imagens de treino\")\n",
        "print(f\"      • {train_images.shape[1]}×{train_images.shape[2]} pixels cada\")\n",
        "print(f\"      • {train_images.shape[3]} canal (escala de cinza)\")\n",
        "\n",
        "# Visualizando algumas amostras\n",
        "print(f\"\\n🖼️ VISUALIZANDO AMOSTRAS:\")\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
        "fig.suptitle('👗 Amostras do Fashion MNIST', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i in range(10):\n",
        "    ax = axes[i//5, i%5]\n",
        "    ax.imshow(train_images[i].reshape(28, 28), cmap='gray')\n",
        "    ax.set_title(f'{class_names[train_labels[i]]}')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Dados preparados para treinamento da CNN!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "084f6bc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🏗️ CONSTRUINDO A CNN\n",
        "print(\"🏗️ CONSTRUINDO ARQUITETURA CNN\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"🎯 ESTRATÉGIA DE DESIGN:\")\n",
        "print(\"   1️⃣ Começar com CNN simples\")\n",
        "print(\"   2️⃣ Camada Conv2D para extração de características\")\n",
        "print(\"   3️⃣ MaxPooling para redução dimensional\")\n",
        "print(\"   4️⃣ Flatten para converter para 1D\")\n",
        "print(\"   5️⃣ Dense layers para classificação\")\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Arquitetura inicial simples\n",
        "model = keras.Sequential([\n",
        "    # 🔍 EXTRATOR DE CARACTERÍSTICAS\n",
        "    layers.Conv2D(32, (3,3), activation='relu', padding='same', \n",
        "                  input_shape=(28, 28, 1), name='conv2d_1'),\n",
        "    layers.MaxPooling2D((2,2), name='maxpool_1'),\n",
        "    \n",
        "    # 🧠 CLASSIFICADOR  \n",
        "    layers.Flatten(name='flatten'),\n",
        "    layers.Dense(128, activation='relu', name='dense_1'),\n",
        "    layers.Dense(10, activation='softmax', name='output')  # 10 classes\n",
        "])\n",
        "\n",
        "print(\"📋 RESUMO DA ARQUITETURA:\")\n",
        "model.summary()\n",
        "\n",
        "print(f\"\\n🔍 ANÁLISE DETALHADA:\")\n",
        "print(f\"   📐 Input: (28, 28, 1) - Imagem grayscale\")\n",
        "print(f\"   🔄 Conv2D: 32 filtros 3×3 → (28, 28, 32)\")\n",
        "print(f\"   🏊 MaxPool: 2×2 → (14, 14, 32)\")\n",
        "print(f\"   📏 Flatten: → ({14*14*32},)\")\n",
        "print(f\"   🧠 Dense: 128 neurônios → (128,)\")  \n",
        "print(f\"   🎯 Output: 10 classes → (10,)\")\n",
        "\n",
        "# Calculando parâmetros manualmente para compreensão\n",
        "conv_params = (3*3*1*32) + 32  # kernel × input_channels × filters + bias\n",
        "dense1_params = (14*14*32*128) + 128  # input × neurons + bias\n",
        "dense2_params = (128*10) + 10  # input × neurons + bias\n",
        "\n",
        "print(f\"\\n🧮 CÁLCULO MANUAL DE PARÂMETROS:\")\n",
        "print(f\"   Conv2D: {conv_params:,} parâmetros\")\n",
        "print(f\"   Dense 1: {dense1_params:,} parâmetros\")\n",
        "print(f\"   Dense 2: {dense2_params:,} parâmetros\")\n",
        "print(f\"   Total calculado: {conv_params + dense1_params + dense2_params:,}\")\n",
        "print(f\"   Total do modelo: {model.count_params():,}\")\n",
        "\n",
        "print(f\"\\n💡 OBSERVAÇÕES:\")\n",
        "print(\"   ✅ A maioria dos parâmetros está nas camadas Dense\")\n",
        "print(\"   ✅ Conv2D tem poucos parâmetros mas extrai características importantes\")\n",
        "print(\"   ✅ MaxPooling não tem parâmetros treináveis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "329f1faa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎯 COMPILAÇÃO E TREINAMENTO DO MODELO\n",
        "print(\"🎯 COMPILAÇÃO DO MODELO\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"⚙️ CONFIGURAÇÕES DE TREINAMENTO:\")\n",
        "print(\"   🔧 Optimizer: Adam (adaptativo, eficiente)\")\n",
        "print(\"   📉 Loss: sparse_categorical_crossentropy (para múltiplas classes)\")\n",
        "print(\"   📊 Metrics: accuracy (para acompanhar performance)\")\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"✅ Modelo compilado com sucesso!\")\n",
        "\n",
        "print(f\"\\n🚀 INICIANDO TREINAMENTO:\")\n",
        "print(f\"   📚 Épocas: 5 (poucas para demonstração)\")\n",
        "print(f\"   📊 Validation split: 20% dos dados de treino\")\n",
        "print(f\"   🎯 Objetivo: Entender o processo de aprendizado\")\n",
        "\n",
        "print(f\"\\n⏱️ Treinamento em andamento...\")\n",
        "\n",
        "# Treinamento com mais épocas e monitoramento\n",
        "epochs_hist = model.fit(\n",
        "    train_images, train_labels, \n",
        "    epochs=5,\n",
        "    validation_split=0.2,\n",
        "    verbose=1,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "print(f\"\\n🎉 TREINAMENTO CONCLUÍDO!\")\n",
        "print(f\"✅ Modelo treinado por {len(epochs_hist.history['loss'])} épocas\")\n",
        "print(f\"📈 Última acurácia de treino: {epochs_hist.history['accuracy'][-1]:.3f}\")\n",
        "print(f\"📊 Última acurácia de validação: {epochs_hist.history['val_accuracy'][-1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc0ef950",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📈 VISUALIZAÇÃO DO TREINAMENTO\n",
        "print(\"📈 ANÁLISE DO TREINAMENTO\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Convertendo histórico para DataFrame para análise\n",
        "import pandas as pd\n",
        "history_df = pd.DataFrame(epochs_hist.history)\n",
        "\n",
        "print(\"🔍 MÉTRICAS POR ÉPOCA:\")\n",
        "print(history_df.round(4))\n",
        "\n",
        "# Visualização aprimorada dos gráficos\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Gráfico de Loss\n",
        "ax1.plot(history_df['loss'], 'b-', label='Loss de Treino', linewidth=2)\n",
        "ax1.plot(history_df['val_loss'], 'r-', label='Loss de Validação', linewidth=2)\n",
        "ax1.set_title('📉 Evolução da Função de Loss', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Época')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Gráfico de Acurácia\n",
        "ax2.plot(history_df['accuracy'], 'b-', label='Acurácia de Treino', linewidth=2)\n",
        "ax2.plot(history_df['val_accuracy'], 'r-', label='Acurácia de Validação', linewidth=2)\n",
        "ax2.set_title('📊 Evolução da Acurácia', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Época')\n",
        "ax2.set_ylabel('Acurácia')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Análise dos resultados\n",
        "print(f\"\\n🔍 ANÁLISE DOS RESULTADOS:\")\n",
        "print(f\"   📈 Melhoria no treino: {history_df['accuracy'].iloc[-1] - history_df['accuracy'].iloc[0]:.3f}\")\n",
        "print(f\"   📊 Melhoria na validação: {history_df['val_accuracy'].iloc[-1] - history_df['val_accuracy'].iloc[0]:.3f}\")\n",
        "\n",
        "# Verificação de overfitting\n",
        "gap_inicial = history_df['accuracy'].iloc[0] - history_df['val_accuracy'].iloc[0]\n",
        "gap_final = history_df['accuracy'].iloc[-1] - history_df['val_accuracy'].iloc[-1]\n",
        "\n",
        "print(f\"\\n🎯 DIAGNÓSTICO DE OVERFITTING:\")\n",
        "print(f\"   Gap inicial (treino-val): {gap_inicial:.3f}\")\n",
        "print(f\"   Gap final (treino-val): {gap_final:.3f}\")\n",
        "\n",
        "if abs(gap_final) < 0.05:\n",
        "    print(\"   ✅ Modelo bem balanceado!\")\n",
        "elif gap_final > 0.1:\n",
        "    print(\"   ⚠️ Possível overfitting - considere regularização\")\n",
        "elif gap_final < -0.05:\n",
        "    print(\"   🔄 Modelo pode ter mais capacidade - considere treinar mais\")\n",
        "else:\n",
        "    print(\"   👍 Desempenho razoável\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "056dc121",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎯 AVALIAÇÃO COMPLETA DO MODELO\n",
        "print(\"🎯 AVALIAÇÃO FINAL DO MODELO\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"📊 AVALIANDO PERFORMANCE...\")\n",
        "\n",
        "# Avaliação nos dados de treino e teste\n",
        "train_loss, train_acc = model.evaluate(train_images, train_labels, verbose=0)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
        "\n",
        "print(f\"\\n📈 RESULTADOS FINAIS:\")\n",
        "print(f\"   🎓 Treino - Loss: {train_loss:.4f} | Acurácia: {train_acc:.4f} ({train_acc*100:.1f}%)\")\n",
        "print(f\"   🧪 Teste  - Loss: {test_loss:.4f} | Acurácia: {test_acc:.4f} ({test_acc*100:.1f}%)\")\n",
        "\n",
        "# Análise de overfitting\n",
        "overfitting_gap = train_acc - test_acc\n",
        "print(f\"\\n🔍 ANÁLISE DE GENERALIZAÇÃO:\")\n",
        "print(f\"   Gap treino-teste: {overfitting_gap:.4f}\")\n",
        "\n",
        "if overfitting_gap < 0.02:\n",
        "    print(\"   ✅ Excelente generalização!\")\n",
        "elif overfitting_gap < 0.05:\n",
        "    print(\"   👍 Boa generalização\")\n",
        "elif overfitting_gap < 0.1:\n",
        "    print(\"   ⚠️ Leve overfitting\")\n",
        "else:\n",
        "    print(\"   🚫 Overfitting significativo - modelo memorizou o treino\")\n",
        "\n",
        "# Comparação com baseline\n",
        "print(f\"\\n🎯 COMPARAÇÃO COM BASELINES:\")\n",
        "random_acc = 1/10  # 10 classes, chute aleatório\n",
        "print(f\"   🎲 Baseline aleatório: {random_acc:.3f} ({random_acc*100:.1f}%)\")\n",
        "print(f\"   🚀 Melhoria sobre baseline: {(test_acc/random_acc):.1f}x\")\n",
        "\n",
        "# Interpretação do resultado\n",
        "if test_acc > 0.85:\n",
        "    print(\"   🏆 Performance excelente!\")\n",
        "elif test_acc > 0.75:\n",
        "    print(\"   ✅ Performance boa\")\n",
        "elif test_acc > 0.65:\n",
        "    print(\"   👍 Performance razoável\")\n",
        "else:\n",
        "    print(\"   ⚠️ Performance baixa - modelo precisa melhorar\")\n",
        "\n",
        "print(f\"\\n💡 OBSERVAÇÕES:\")\n",
        "print(\"   • Fashion MNIST é mais desafiador que MNIST dígitos\")\n",
        "print(\"   • CNNs simples já superam MLPs tradicionais\")\n",
        "print(\"   • Margem para melhoria com arquiteturas mais complexas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea5856b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔮 FAZENDO PREDIÇÕES\n",
        "print(\"🔮 GERANDO PREDIÇÕES DO MODELO\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"🎯 Fazendo predições no conjunto de teste...\")\n",
        "predictions = model.predict(test_images, verbose=0)\n",
        "\n",
        "print(f\"✅ Predições concluídas!\")\n",
        "print(f\"📊 Formato das predições: {predictions.shape}\")\n",
        "print(f\"💡 Cada linha contém probabilidades para as 10 classes\")\n",
        "\n",
        "# Analisando as predições\n",
        "print(f\"\\n🔍 ANÁLISE DAS PREDIÇÕES:\")\n",
        "max_probs = np.max(predictions, axis=1)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "print(f\"   📈 Confiança média: {np.mean(max_probs):.3f}\")\n",
        "print(f\"   📊 Confiança mínima: {np.min(max_probs):.3f}\")\n",
        "print(f\"   📊 Confiança máxima: {np.max(max_probs):.3f}\")\n",
        "\n",
        "# Histograma de confiança\n",
        "print(f\"\\n📊 DISTRIBUIÇÃO DE CONFIANÇA:\")\n",
        "confidence_ranges = [(0.0, 0.5), (0.5, 0.7), (0.7, 0.9), (0.9, 1.0)]\n",
        "for low, high in confidence_ranges:\n",
        "    count = np.sum((max_probs >= low) & (max_probs < high))\n",
        "    percentage = count / len(max_probs) * 100\n",
        "    print(f\"   {low:.1f}-{high:.1f}: {count:,} predições ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\n💡 INTERPRETAÇÃO:\")\n",
        "high_confidence = np.sum(max_probs > 0.9)\n",
        "low_confidence = np.sum(max_probs < 0.5)\n",
        "print(f\"   ✅ Alta confiança (>90%): {high_confidence} predições\")\n",
        "print(f\"   ⚠️ Baixa confiança (<50%): {low_confidence} predições\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84662bf4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔍 ANÁLISE DETALHADA DE PREDIÇÕES INDIVIDUAIS\n",
        "print(\"🔍 ANÁLISE DE PREDIÇÕES INDIVIDUAIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Escolhendo alguns exemplos para análise\n",
        "exemplos = [42, 1000, 2500, 4000, 7500]\n",
        "\n",
        "for i, item in enumerate(exemplos):\n",
        "    print(f\"\\n📊 EXEMPLO {i+1} (Índice {item}):\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Informações da predição\n",
        "    classe_predita = np.argmax(predictions[item])\n",
        "    confianca = 100 * np.max(predictions[item])\n",
        "    classe_real = test_labels[item]\n",
        "    \n",
        "    print(f\"   🤖 Predição: {class_names[classe_predita]} (confiança: {confianca:.1f}%)\")\n",
        "    print(f\"   ✅ Real: {class_names[classe_real]}\")\n",
        "    \n",
        "    # Verificando se acertou\n",
        "    if classe_predita == classe_real:\n",
        "        print(f\"   🎯 ACERTOU! ✓\")\n",
        "    else:\n",
        "        print(f\"   ❌ ERROU! ✗\")\n",
        "    \n",
        "    # Top 3 predições\n",
        "    top3_indices = np.argsort(predictions[item])[-3:][::-1]\n",
        "    print(f\"   🏆 Top 3 predições:\")\n",
        "    for j, idx in enumerate(top3_indices):\n",
        "        prob = predictions[item][idx] * 100\n",
        "        print(f\"      {j+1}º: {class_names[idx]} ({prob:.1f}%)\")\n",
        "\n",
        "# Estatísticas gerais\n",
        "acertos = np.sum(predicted_classes == test_labels)\n",
        "total = len(test_labels)\n",
        "acuracia_final = acertos / total\n",
        "\n",
        "print(f\"\\n📈 ESTATÍSTICAS GERAIS:\")\n",
        "print(f\"   ✅ Acertos: {acertos:,} de {total:,}\")\n",
        "print(f\"   📊 Acurácia: {acuracia_final:.3f} ({acuracia_final*100:.1f}%)\")\n",
        "print(f\"   ❌ Erros: {total - acertos:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "9f03e7ee",
      "metadata": {
        "id": "9f03e7ee"
      },
      "outputs": [],
      "source": [
        "def plot_image(i, predictions_array, true_label, img):\n",
        "  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red'\n",
        "\n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "3029fbb5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "3029fbb5",
        "outputId": "781828d5-03b4-48da-f557-aa919c93bbff"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(item, predictions, test_labels, test_images)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99cdcc91",
      "metadata": {},
      "source": [
        "## 🎯 Desafio 3: Implementando a LeNet-5 - A CNN Pioneira\n",
        "\n",
        "### 📚 Contexto Histórico\n",
        "\n",
        "A **LeNet-5**, desenvolvida por **Yann LeCun** em 1998, foi uma das primeiras CNNs bem-sucedidas e estabeleceu muitos dos princípios fundamentais ainda usados hoje.\n",
        "\n",
        "### 🏗️ Arquitetura da LeNet-5\n",
        "\n",
        "<img src=\"https://github.com/arnaldojr/DisruptiveArchitectures/blob/master/material/aulas/lab08/lenet.png?raw=1\" width=\"800px\">\n",
        "\n",
        "**Especificações originais:**\n",
        "```\n",
        "INPUT(32×32×1) → CONV1(28×28×6) → POOL1(14×14×6) → \n",
        "CONV2(10×10×16) → POOL2(5×5×16) → FC1(120) → FC2(84) → OUTPUT(10)\n",
        "```\n",
        "\n",
        "### 🎯 Características da LeNet-5:\n",
        "\n",
        "1. **Entrada**: 32×32 pixels (grayscale)\n",
        "2. **C1**: 6 filtros 5×5, sem padding\n",
        "3. **S2**: Subsampling (average pooling) 2×2\n",
        "4. **C3**: 16 filtros 5×5\n",
        "5. **S4**: Subsampling 2×2\n",
        "6. **FC5**: 120 neurônios\n",
        "7. **FC6**: 84 neurônios\n",
        "8. **OUTPUT**: 10 classes\n",
        "\n",
        "### 💡 Desafio para Você:\n",
        "\n",
        "**Implemente a LeNet-5 adaptada para Fashion MNIST (28×28) seguindo estas especificações:**\n",
        "\n",
        "```python\n",
        "# Sua implementação deve seguir esta estrutura:\n",
        "def create_lenet5_fashion():\n",
        "    model = keras.Sequential([\n",
        "        # C1: Convolutional Layer\n",
        "        # S2: Subsampling Layer (Pooling)\n",
        "        # C3: Convolutional Layer\n",
        "        # S4: Subsampling Layer\n",
        "        # Flatten\n",
        "        # FC5: Dense Layer (120 neurons)\n",
        "        # FC6: Dense Layer (84 neurons)\n",
        "        # Output Layer (10 classes)\n",
        "    ])\n",
        "    return model\n",
        "```\n",
        "\n",
        "### 📋 Requisitos do Desafio:\n",
        "\n",
        "1. ✅ **Implementar LeNet-5** seguindo a arquitetura original\n",
        "2. ✅ **Comparar com sua CNN simples** anterior\n",
        "3. ✅ **Analisar diferenças** de performance\n",
        "4. ✅ **Documentar observações** sobre cada camada\n",
        "5. ✅ **Visualizar resultados** e métricas\n",
        "\n",
        "### 🎯 Métricas de Sucesso:\n",
        "\n",
        "- **Acurácia > 85%** no Fashion MNIST\n",
        "- **Menos overfitting** que a CNN simples\n",
        "- **Análise comparativa** detalhada\n",
        "- **Código bem documentado**\n",
        "\n",
        "### 💭 Perguntas para Reflexão:\n",
        "\n",
        "1. **Por que LeNet-5 tem 2 camadas convolucionais?**\n",
        "2. **Qual a vantagem de ter camadas FC decrescentes (120→84→10)?**\n",
        "3. **Como a LeNet-5 se compara com CNNs modernas?**\n",
        "4. **O que você mudaria na arquitetura original?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4959834f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎯 DESAFIO 3: SUA IMPLEMENTAÇÃO DA LENET-5\n",
        "print(\"🎯 DESAFIO 3: IMPLEMENTANDO LENET-5\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def create_lenet5_fashion():\n",
        "    \"\"\"\n",
        "    Implementa a arquitetura LeNet-5 adaptada para Fashion MNIST\n",
        "    \n",
        "    Arquitetura:\n",
        "    - Conv2D: 6 filtros 5x5\n",
        "    - AvgPool2D: 2x2\n",
        "    - Conv2D: 16 filtros 5x5  \n",
        "    - AvgPool2D: 2x2\n",
        "    - Flatten\n",
        "    - Dense: 120 neurônios\n",
        "    - Dense: 84 neurônios\n",
        "    - Dense: 10 classes (softmax)\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"🏗️ Construindo LeNet-5...\")\n",
        "    \n",
        "    # COMPLETE A IMPLEMENTAÇÃO AQUI:\n",
        "    # ====================================\n",
        "    \n",
        "    model = keras.Sequential([\n",
        "        # 🔍 PRIMEIRA CAMADA CONVOLUCIONAL\n",
        "        # layers.Conv2D(?, (?,?), activation='?', input_shape=(28,28,1)),\n",
        "        \n",
        "        # 🏊 PRIMEIRA CAMADA DE POOLING  \n",
        "        # layers.AveragePooling2D((?,?)),\n",
        "        \n",
        "        # 🔍 SEGUNDA CAMADA CONVOLUCIONAL\n",
        "        # layers.Conv2D(?, (?,?), activation='?'),\n",
        "        \n",
        "        # 🏊 SEGUNDA CAMADA DE POOLING\n",
        "        # layers.AveragePooling2D((?,?)),\n",
        "        \n",
        "        # 📏 FLATTEN\n",
        "        # layers.Flatten(),\n",
        "        \n",
        "        # 🧠 CAMADAS DENSAS\n",
        "        # layers.Dense(?, activation='?'),\n",
        "        # layers.Dense(?, activation='?'),\n",
        "        # layers.Dense(?, activation='?')  # Saída\n",
        "    ])\n",
        "    \n",
        "    # ====================================\n",
        "    \n",
        "    return model\n",
        "\n",
        "# TESTE SUA IMPLEMENTAÇÃO:\n",
        "print(\"🧪 TESTANDO SUA IMPLEMENTAÇÃO:\")\n",
        "\n",
        "# Descomente as linhas abaixo após implementar:\n",
        "# lenet5_model = create_lenet5_fashion()\n",
        "# lenet5_model.summary()\n",
        "\n",
        "# print(f\"\\n📊 COMPARAÇÃO COM CNN SIMPLES:\")\n",
        "# print(f\"   CNN Simples: {model.count_params():,} parâmetros\")\n",
        "# print(f\"   LeNet-5: {lenet5_model.count_params():,} parâmetros\")\n",
        "\n",
        "print(\"\\n💡 DICAS:\")\n",
        "print(\"   • Use activation='tanh' para ser mais fiel ao original\")\n",
        "print(\"   • Average pooling era usado na LeNet-5 original\")\n",
        "print(\"   • Analise o número de parâmetros em cada camada\")\n",
        "print(\"   • Compare a performance com sua CNN simples\")\n",
        "\n",
        "print(\"\\n🎯 APÓS IMPLEMENTAR, TREINE E COMPARE:\")\n",
        "print(\"   1. Compile o modelo\")\n",
        "print(\"   2. Treine por algumas épocas\")\n",
        "print(\"   3. Compare métricas com CNN simples\")\n",
        "print(\"   4. Analise diferenças arquiteturais\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62169f5d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔍 ANÁLISE COMPARATIVA (Complete após implementar)\n",
        "print(\"🔍 ANÁLISE COMPARATIVA: CNN SIMPLES vs LENET-5\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Esta seção deve ser preenchida após você implementar a LeNet-5\n",
        "\n",
        "print(\"📊 RESULTADOS COMPARATIVOS:\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Exemplo de template para suas análises:\n",
        "comparacao = {\n",
        "    'Métrica': ['Parâmetros', 'Acurácia Treino', 'Acurácia Teste', 'Tempo Treino', 'Overfitting'],\n",
        "    'CNN Simples': ['?', '?', '?', '?', '?'],\n",
        "    'LeNet-5': ['?', '?', '?', '?', '?']\n",
        "}\n",
        "\n",
        "print(\"| Métrica | CNN Simples | LeNet-5 |\")\n",
        "print(\"|---------|-------------|---------|\")\n",
        "for i, metrica in enumerate(comparacao['Métrica']):\n",
        "    print(f\"| {metrica} | {comparacao['CNN Simples'][i]} | {comparacao['LeNet-5'][i]} |\")\n",
        "\n",
        "print(f\"\\n🎯 SUAS CONCLUSÕES:\")\n",
        "print(\"=\" * 20)\n",
        "print(\"1. Qual modelo teve melhor performance? Por quê?\")\n",
        "print(\"   Resposta: ________________________________\")\n",
        "print(\"\")\n",
        "print(\"2. Qual a principal diferença arquitetural?\")\n",
        "print(\"   Resposta: ________________________________\")\n",
        "print(\"\")\n",
        "print(\"3. LeNet-5 ainda é relevante hoje? Por quê?\")\n",
        "print(\"   Resposta: ________________________________\")\n",
        "print(\"\")\n",
        "print(\"4. O que você mudaria na LeNet-5 original?\")\n",
        "print(\"   Resposta: ________________________________\")\n",
        "\n",
        "print(f\"\\n🏆 PRÓXIMOS PASSOS:\")\n",
        "print(\"   • Experimente diferentes funções de ativação\")\n",
        "print(\"   • Teste com dropout para reduzir overfitting\")\n",
        "print(\"   • Implemente data augmentation\")\n",
        "print(\"   • Compare com arquiteturas modernas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a3c9d0",
      "metadata": {},
      "source": [
        "## 🚀 Desafio Extra: CNN para CIFAR-10\n",
        "\n",
        "### 🎯 Por que CIFAR-10 é Mais Desafiador?\n",
        "\n",
        "O **CIFAR-10** é significativamente mais complexo que Fashion MNIST:\n",
        "\n",
        "| Aspecto | Fashion MNIST | CIFAR-10 |\n",
        "|---------|---------------|----------|\n",
        "| **Resolução** | 28×28 | 32×32 |\n",
        "| **Canais** | 1 (grayscale) | 3 (RGB) |\n",
        "| **Classes** | 10 roupas | 10 objetos |\n",
        "| **Complexidade** | Texturas simples | Objetos naturais |\n",
        "| **Variabilidade** | Baixa | Alta |\n",
        "\n",
        "### 🏷️ Classes do CIFAR-10:\n",
        "- ✈️ **airplane** | 🚗 **automobile** | 🐦 **bird** | 🐱 **cat** | 🦌 **deer**\n",
        "- 🐶 **dog** | 🐸 **frog** | 🐴 **horse** | 🚢 **ship** | 🚛 **truck**\n",
        "\n",
        "### 💡 Estratégias Recomendadas:\n",
        "\n",
        "#### 1️⃣ **Arquitetura Mais Profunda**\n",
        "```python\n",
        "# CNN mais robusta para CIFAR-10\n",
        "model = Sequential([\n",
        "    # Bloco 1\n",
        "    Conv2D(32, (3,3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(32, (3,3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Dropout(0.25),\n",
        "    \n",
        "    # Bloco 2  \n",
        "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Dropout(0.25),\n",
        "    \n",
        "    # Classificador\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n",
        "#### 2️⃣ **Técnicas Essenciais**\n",
        "- **Data Augmentation**: Rotação, flip, zoom\n",
        "- **Batch Normalization**: Estabiliza treinamento\n",
        "- **Dropout**: Reduz overfitting\n",
        "- **Learning Rate Scheduling**: Melhora convergência\n",
        "\n",
        "#### 3️⃣ **Métricas de Sucesso**\n",
        "- 🎯 **Baseline**: >70% acurácia\n",
        "- 🚀 **Bom**: >80% acurácia  \n",
        "- 🏆 **Excelente**: >85% acurácia\n",
        "\n",
        "### 🎯 Seu Desafio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "610e3e26",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🚀 DESAFIO EXTRA: CNN PARA CIFAR-10\n",
        "print(\"🚀 DESAFIO EXTRA: CNN PARA CIFAR-10\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Passo 1: Carregamento e Preparação dos Dados\n",
        "print(\"📊 CARREGANDO CIFAR-10...\")\n",
        "\n",
        "# COMPLETE AQUI:\n",
        "# ===============================================\n",
        "\n",
        "# (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# print(f\"Formato dos dados: {x_train.shape}\")\n",
        "# print(f\"Classes: {np.unique(y_train)}\")\n",
        "\n",
        "# # Normalização\n",
        "# x_train = x_train.astype('float32') / 255.0\n",
        "# x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# # Nomes das classes\n",
        "# cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "#                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# ===============================================\n",
        "\n",
        "print(\"✅ Dados carregados e normalizados!\")\n",
        "\n",
        "# Passo 2: Visualização dos Dados\n",
        "print(\"\\n🖼️ VISUALIZANDO AMOSTRAS DO CIFAR-10:\")\n",
        "\n",
        "# COMPLETE A VISUALIZAÇÃO:\n",
        "# ===============================================\n",
        "\n",
        "# fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
        "# for i in range(10):\n",
        "#     ax = axes[i//5, i%5]\n",
        "#     ax.imshow(x_train[i])\n",
        "#     ax.set_title(f'{cifar10_classes[y_train[i][0]]}')\n",
        "#     ax.axis('off')\n",
        "# plt.show()\n",
        "\n",
        "# ===============================================\n",
        "\n",
        "# Passo 3: Construção da CNN\n",
        "print(\"\\n🏗️ CONSTRUINDO CNN PARA CIFAR-10:\")\n",
        "\n",
        "def create_cifar10_cnn():\n",
        "    \"\"\"\n",
        "    Crie uma CNN robusta para CIFAR-10\n",
        "    \n",
        "    Requisitos:\n",
        "    - Pelo menos 3 blocos convolucionais\n",
        "    - Batch Normalization\n",
        "    - Dropout para regularização\n",
        "    - Data Augmentation\n",
        "    \"\"\"\n",
        "    \n",
        "    # COMPLETE SUA IMPLEMENTAÇÃO:\n",
        "    # ===================================\n",
        "    \n",
        "    # model = keras.Sequential([\n",
        "    #     # Bloco 1: Conv + BatchNorm + Pool + Dropout\n",
        "    #     \n",
        "    #     # Bloco 2: Conv + BatchNorm + Pool + Dropout\n",
        "    #     \n",
        "    #     # Bloco 3: Conv + BatchNorm + Pool + Dropout\n",
        "    #     \n",
        "    #     # Classificador: Flatten + Dense + Dropout + Output\n",
        "    # ])\n",
        "    \n",
        "    # ===================================\n",
        "    \n",
        "    return None  # Substitua por seu modelo\n",
        "\n",
        "# Passo 4: Data Augmentation\n",
        "print(\"\\n🔄 CONFIGURANDO DATA AUGMENTATION:\")\n",
        "\n",
        "# COMPLETE A CONFIGURAÇÃO:\n",
        "# ===============================================\n",
        "\n",
        "# datagen = ImageDataGenerator(\n",
        "#     rotation_range=15,\n",
        "#     width_shift_range=0.1,\n",
        "#     height_shift_range=0.1,\n",
        "#     horizontal_flip=True,\n",
        "#     zoom_range=0.1\n",
        "# )\n",
        "\n",
        "# ===============================================\n",
        "\n",
        "# Passo 5: Treinamento\n",
        "print(\"\\n🚀 TREINAMENTO DO MODELO:\")\n",
        "\n",
        "# COMPLETE O TREINAMENTO:\n",
        "# ===============================================\n",
        "\n",
        "# model = create_cifar10_cnn()\n",
        "# model.compile(\n",
        "#     optimizer='adam',\n",
        "#     loss='sparse_categorical_crossentropy', \n",
        "#     metrics=['accuracy']\n",
        "# )\n",
        "\n",
        "# # Callbacks\n",
        "# callbacks = [\n",
        "#     keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
        "#     keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=5)\n",
        "# ]\n",
        "\n",
        "# # Treinamento\n",
        "# history = model.fit(\n",
        "#     datagen.flow(x_train, y_train, batch_size=32),\n",
        "#     steps_per_epoch=len(x_train) // 32,\n",
        "#     epochs=50,\n",
        "#     validation_data=(x_test, y_test),\n",
        "#     callbacks=callbacks\n",
        "# )\n",
        "\n",
        "# ===============================================\n",
        "\n",
        "print(\"✅ Configure seu modelo e inicie o treinamento!\")\n",
        "\n",
        "# Passo 6: Avaliação e Análise\n",
        "print(\"\\n📊 TEMPLATE PARA ANÁLISE DE RESULTADOS:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "resultado_template = \"\"\"\n",
        "🎯 RESULTADOS FINAIS:\n",
        "   Acurácia no teste: ____%\n",
        "   Tempo de treinamento: ____\n",
        "   Épocas necessárias: ____\n",
        "\n",
        "🔍 ANÁLISE:\n",
        "   1. Overfitting observado? ____\n",
        "   2. Data augmentation ajudou? ____\n",
        "   3. Qual classe teve pior performance? ____\n",
        "   4. Principais desafios encontrados: ____\n",
        "\n",
        "🚀 MELHORIAS PROPOSTAS:\n",
        "   1. ____________________\n",
        "   2. ____________________\n",
        "   3. ____________________\n",
        "\n",
        "📈 COMPARAÇÃO COM FASHION MNIST:\n",
        "   Dificuldade: ____x maior\n",
        "   Tempo: ____x maior\n",
        "   Performance: ____% vs ____%\n",
        "\"\"\"\n",
        "\n",
        "print(resultado_template)\n",
        "\n",
        "print(\"\\n💡 DICAS IMPORTANTES:\")\n",
        "print(\"   • CIFAR-10 requer mais épocas que Fashion MNIST\")\n",
        "print(\"   • Use GPU se disponível para acelerar treinamento\")\n",
        "print(\"   • Monitore overfitting cuidadosamente\")\n",
        "print(\"   • Experimente diferentes learning rates\")\n",
        "print(\"   • Transfer learning pode ser uma alternativa\")\n",
        "\n",
        "print(\"\\n🏆 META FINAL:\")\n",
        "print(\"   Alcançar >80% acurácia no CIFAR-10 com sua CNN!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de5cdb9a",
      "metadata": {},
      "source": [
        "## 🎓 Conclusões e Próximos Passos\n",
        "\n",
        "### ✅ O que aprendemos hoje:\n",
        "\n",
        "1. **Fundamentos**: Operação de convolução, pooling e arquitetura CNN\n",
        "2. **Implementação**: TensorFlow/Keras para construir CNNs\n",
        "3. **Comparação**: CNNs vs MLPs tradicionais em termos de eficiência\n",
        "4. **Aplicação**: Classificação de imagens com Fashion MNIST\n",
        "5. **Arquiteturas históricas**: LeNet-5 e sua importância\n",
        "6. **Desafios práticos**: CIFAR-10 como próximo passo\n",
        "\n",
        "### 🚀 Próximos passos recomendados:\n",
        "\n",
        "1. **Arquiteturas modernas**: ResNet, VGG, EfficientNet\n",
        "2. **Transfer Learning**: Usar modelos pré-treinados\n",
        "3. **Técnicas avançadas**: Data augmentation, batch normalization\n",
        "4. **Aplicações**: Detecção de objetos, segmentação semântica\n",
        "5. **Vision Transformers**: Estado da arte atual\n",
        "\n",
        "### 📚 Recursos para continuar aprendendo:\n",
        "\n",
        "- 📖 **Livros**: \"Deep Learning\" (Goodfellow), \"Hands-On ML\" (Géron)\n",
        "- 🎓 **Cursos**: CS231n (Stanford), Fast.AI\n",
        "- 💻 **Prática**: Kaggle competitions, Papers With Code\n",
        "- 🏆 **Projetos**: Crie seu próprio classificador de imagens\n",
        "\n",
        "### 💡 Dicas finais:\n",
        "\n",
        "1. **Comece simples**: LeNet-5 → VGG → ResNet → Modernas\n",
        "2. **Entenda os dados**: EDA é fundamental\n",
        "3. **Experimente**: Diferentes arquiteturas e hiperparâmetros\n",
        "4. **Monitore**: Overfitting vs underfitting\n",
        "5. **Pratique**: Projetos reais consolidam o aprendizado\n",
        "\n",
        "---\n",
        "\n",
        "🎉 **Parabéns por completar este laboratório!** \n",
        "\n",
        "Você agora tem uma base sólida em CNNs. Continue explorando e construindo - o mundo da Visão Computacional está cheio de oportunidades fascinantes! 🌟\n",
        "\n",
        "**Bons estudos e que a força convolucional esteja com você!** 🤖✨"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "8d68938bd6f1c8d824a292cb48fdc812f23ce0d2e12c844cec6ac89d2f668725"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
